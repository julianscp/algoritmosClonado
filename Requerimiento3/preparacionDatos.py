# Proyecto/Requerimiento3/PrepararDatos.py

import re
import pandas as pd
from pathlib import Path
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk

# Descargar recursos necesarios de NLTK (solo la primera vez)
nltk.download('punkt', quiet=True)
nltk.download('punkt_tab', quiet=True)  # â† Agrega esta
nltk.download('stopwords', quiet=True)

# --- CONFIGURACIONES ---
BASE_DIR = Path(__file__).resolve().parent
IN_FILE = BASE_DIR / "../Requerimiento1/ArchivosFiltrados/articulosOptimos.bib"
OUT_DIR = BASE_DIR / "DatosProcesados"
OUT_DIR.mkdir(parents=True, exist_ok=True)

OUT_FILE = OUT_DIR / "abstracts_limpios.csv"
print(">>> Script iniciado correctamente")
print("Ruta esperada del archivo:", IN_FILE)
print("Existe archivo:", IN_FILE.exists())

# --- FUNCIONES AUXILIARES ---

def limpiar_texto(texto: str) -> str:
    """Normaliza el texto: elimina etiquetas, minÃºsculas, quita signos, nÃºmeros y stopwords."""
    if not texto:
        return ""
    
    # --- ELIMINAR ETIQUETAS HTML / MathML ---
    texto = re.sub(r'<[^>]+>', ' ', texto)  # elimina cualquier cosa entre < y >
    
    # MinÃºsculas
    texto = texto.lower()

    # Quitar caracteres especiales, nÃºmeros y saltos de lÃ­nea
    texto = re.sub(r'[^a-zÃ¡Ã©Ã­Ã³ÃºÃ¼Ã±\s]', ' ', texto)
    texto = re.sub(r'\s+', ' ', texto).strip()

    # TokenizaciÃ³n
    tokens = word_tokenize(texto, language='english')

    # Stopwords en inglÃ©s y espaÃ±ol
    stop_words = set(stopwords.words('english')).union(set(stopwords.words('spanish')))

    # Filtrar stopwords
    tokens_filtrados = [t for t in tokens if t not in stop_words and len(t) > 2]

    # Unir de nuevo en texto limpio
    texto_limpio = ' '.join(tokens_filtrados)

    return texto_limpio


def extraer_abstracts(bib_path: Path):
    """Extrae los abstracts de un archivo .bib."""
    abstracts = []
    current_entry = {}
    inside_entry = False

    with open(bib_path, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            line = line.strip()

            # Detectar inicio y fin de un registro BibTeX
            if line.startswith('@'):
                inside_entry = True
                current_entry = {}
                continue
            if inside_entry and line == '}':
                if 'abstract' in current_entry:
                    abstracts.append(current_entry['abstract'])
                inside_entry = False
                continue

            # Extraer campo abstract (maneja varias lÃ­neas)
            if inside_entry:
                match = re.match(r'abstract\s*=\s*[{"](.+)[}"],?', line, re.IGNORECASE)
                if match:
                    current_entry['abstract'] = match.group(1)
                else:
                    # Manejar abstracts multilÃ­nea
                    if 'abstract' in current_entry:
                        current_entry['abstract'] += ' ' + line

    return abstracts


def main():
    print("ðŸ“˜ Cargando y procesando abstracts...")

    abstracts = extraer_abstracts(IN_FILE)
    print(f"âœ… {len(abstracts)} abstracts extraÃ­dos del archivo.")

    datos_limpios = [limpiar_texto(abs_) for abs_ in abstracts]

    df = pd.DataFrame({
        'abstract_original': abstracts,
        'abstract_limpio': datos_limpios
    })

    df.to_csv(OUT_FILE, index=False, encoding='utf-8-sig')
    print(f"ðŸ’¾ Archivo procesado guardado en: {OUT_FILE}")
    print("âœ… Limpieza completa y datos listos para anÃ¡lisis.")


if __name__ == "__main__":
    main()
