abstract_original,abstract_limpio
"In recent years, the intersection of music and coding has emerged as a cross-disciplinary approach to engage learners in both musical and computational expression. This article examines the motivations and framing of recent music and coding research through an analysis of 65 articles published between 2004 and 2024. We survey the key findings of this body of work, exploring the extent to which they generalize across diverse populations and educational contexts. We situate these findings within the broader aims of the field of computer science education, critiquing the use of music as a motivational tool in computing education rather than as a topic worthy of exploration in its own right. Methodological limitations are identified, with recommendations for future research to address these gaps. Drawing on theories of onto-epistemic heterogeneity, we call for a more inclusive approach to computing education that integrates sociocultural, historical, and critical perspectives to more effectively support learners from diverse linguistic, cultural, and economic backgrounds.",recent years intersection music coding emerged cross disciplinary approach engage learners musical computational expression article examines motivations framing recent music coding research analysis articles published survey key findings body work exploring extent generalize across diverse populations educational contexts situate findings within broader aims field computer science education critiquing use music motivational tool computing education rather topic worthy exploration right methodological limitations identified recommendations future research address gaps drawing theories onto epistemic heterogeneity call inclusive approach computing education integrates sociocultural historical critical perspectives effectively support learners diverse linguistic cultural economic backgrounds
"Computational Thinking (CT) is a critical set of 21st-century skills that young learners must develop to be successful in contemporary work and life. As more K-12 schools begin to integrate CT into the curriculum, Elementary Preservice Teachers (EPSTs) must be prepared to teach CT to their future students. Therefore, elementary teacher preparation programs should explore ways to incorporate CT into their coursework, ensuring that EPSTs graduate with the necessary knowledge and skills to teach CT. This study investigates the impacts of a 3-week CT module implemented in a technology integration course in an elementary teacher preparation program on EPSTs’ CT value beliefs, self-efficacy, and teaching efficacy. The purpose of the module was to introduce EPSTs to basic concepts and big ideas in CT, support their application of CT in block-based coding environments, and promote connection building between CT and authentic real-world scenarios that balance different types of CT activities. Guided by the Interconnected Model of Professional Growth (IMPG), this study examined EPSTs’ learning as a dynamic process of change across the personal and external domains. The module was located in the external domain as a stimulus for learning, while the Teacher Beliefs about Coding and Computational Thinking (TBaCCT) instrument was used to assess changes in value beliefs, self-efficacy, and teaching efficacy within the personal domain. A convergent parallel mixed-methods design was used to analyze pre- and post-surveys administered to 50 EPSTs and semi-structured interviews with four of the EPSTs. The results showed that the beliefs of the CT value of EPSTs evolved and their self-efficacy and teaching efficacy of CT significantly improved after participating in the module, suggesting that the interaction between external learning experiences and internal belief systems—facilitated by reflection and action—contributed to early professional growth. Notably, EPSTs also developed a more accurate understanding of CT, recognized the benefits of both plugged and unplugged CT activities, and expressed a desire for continued CT learning and professional development. Although EPSTs expressed concerns with the scarcity of physical computing toys in schools, they demonstrated a commitment to incorporating CT activities into their teaching and exploring alternative funding avenues to acquire such resources. Practical implications include potential approaches for elementary teacher preparation programs to embed CT into existing curricula.",computational thinking critical set century skills young learners must develop successful contemporary work life schools begin integrate curriculum elementary preservice teachers epsts must prepared teach future students therefore elementary teacher preparation programs explore ways incorporate coursework ensuring epsts graduate necessary knowledge skills teach study investigates impacts week module implemented technology integration course elementary teacher preparation program epsts value beliefs self efficacy teaching efficacy purpose module introduce epsts basic concepts big ideas support application block based coding environments promote connection building authentic real world scenarios balance different types activities guided interconnected model professional growth impg study examined epsts learning dynamic process change across personal external domains module located external domain stimulus learning teacher beliefs coding computational thinking tbacct instrument used assess changes value beliefs self efficacy teaching efficacy within personal domain convergent parallel mixed methods design used analyze pre post surveys administered epsts semi structured interviews four epsts results showed beliefs value epsts evolved self efficacy teaching efficacy significantly improved participating module suggesting interaction external learning experiences internal belief systems facilitated reflection action contributed early professional growth notably epsts also developed accurate understanding recognized benefits plugged unplugged activities expressed desire continued learning professional development although epsts expressed concerns scarcity physical computing toys schools demonstrated commitment incorporating activities teaching exploring alternative funding avenues acquire resources practical implications include potential approaches elementary teacher preparation programs embed existing curricula
"This paper focuses on the runtime verification of hyperproperties expressed in Hyper-recHML, an expressive yet simple logic for describing properties of sets of traces. To this end, we consider a simple language of monitors that observe sets of system executions and report verdicts w.r.t. a given Hyper-recHML formula. We first employ a unique omniscient monitor that centrally observes all system traces. Since centralised monitors are not ideal for distributed settings, we also provide a language for decentralized monitors, where each trace has a dedicated monitor; these monitors yield a unique verdict by communicating their observations to one another. For both the centralized and the decentralized settings, we provide a synthesis procedure that, given a formula, yields a monitor that is correct (i.e., sound and violation complete). A key step in proving the correctness of the synthesis for decentralized monitors is a result showing that, for each formula, the synthesized centralized monitor and its corresponding decentralized one are weakly bisimilar for a suitable notion of weak bisimulation.",paper focuses runtime verification hyperproperties expressed hyper rechml expressive yet simple logic describing properties sets traces end consider simple language monitors observe sets system executions report verdicts given hyper rechml formula first employ unique omniscient monitor centrally observes system traces since centralised monitors ideal distributed settings also provide language decentralized monitors trace dedicated monitor monitors yield unique verdict communicating observations one another centralized decentralized settings provide synthesis procedure given formula yields monitor correct sound violation complete key step proving correctness synthesis decentralized monitors result showing formula synthesized centralized monitor corresponding decentralized one weakly bisimilar suitable notion weak bisimulation
"The remarkable progress of Vision Transformer (ViT) models has significantly advanced performance in computer vision tasks. However, the deployment of ViTs in resource-constrained environments remains a challenge, as the attention computation mechanisms within these models form a significant bottleneck, requiring substantial memory and computational resources. To address this challenge, we introduce TAFP-ViT, a tailored hardware-software co-design framework for Vision Transformers. On the software level, TAFP-ViT leverages a learnable compressor to perform multi-head shared compression on feature maps, and fuses decompression reconstruction, QKV generation and QKV processing together for calculation, thereby greatly reducing memory and computation requirements. Furthermore, TAFP-ViT combines dynamic inter-layer token pruning to eliminate unimportant tokens and hardware-friendly intra-block row pruning to diminish redundant computations. The proposed software design converts the calculations before and after SoftMax into dense and sparse triple matrix multiplication (TMM) forms respectively. On the hardware level, TAFP-ViT proposes a configurable systolic array (SA) to efficiently adapt to the QKV fusion computation pattern. The SA has flexible PE units that can effectively support general matrix multiplication (GEMM), dense and sparse TMM. The TMM and flexible dataflows allow TAFP-ViT to avoid handling transpositions and storing intermediate computation results, greatly enhancing computational efficiency. Besides, TAFP-ViT innovatively designs a Top-k engine to support dynamic pruning on the fly with high throughput and low resource consumption. Experiments show that the proposed TAFP-ViT achieves remarkable speedups of 123.91×, 29.5×, and 3.01∼ 20.65× compared to conventional CPUs, GPUs, and previous state-of-the-art works, respectively. Additionally, TAFP-ViT reaches a throughput of up to 731.5 GOP/s and an impressive energy efficiency of 77.9 GOPS/W.",remarkable progress vision transformer vit models significantly advanced performance computer vision tasks however deployment vits resource constrained environments remains challenge attention computation mechanisms within models form significant bottleneck requiring substantial memory computational resources address challenge introduce tafp vit tailored hardware software design framework vision transformers software level tafp vit leverages learnable compressor perform multi head shared compression feature maps fuses decompression reconstruction qkv generation qkv processing together calculation thereby greatly reducing memory computation requirements furthermore tafp vit combines dynamic inter layer token pruning eliminate unimportant tokens hardware friendly intra block row pruning diminish redundant computations proposed software design converts calculations softmax dense sparse triple matrix multiplication tmm forms respectively hardware level tafp vit proposes configurable systolic array efficiently adapt qkv fusion computation pattern flexible units effectively support general matrix multiplication gemm dense sparse tmm tmm flexible dataflows allow tafp vit avoid handling transpositions storing intermediate computation results greatly enhancing computational efficiency besides tafp vit innovatively designs top engine support dynamic pruning fly high throughput low resource consumption experiments show proposed tafp vit achieves remarkable speedups compared conventional cpus gpus previous state art works respectively additionally tafp vit reaches throughput gop impressive energy efficiency gops
"In the past, Arabic Dialects (AD) have been poorly documented linguistically due to the lack of written forms and orthographies. However, in recent years, AD have become more widely used as a means of communication since social media and the everywhere availability of the internet have created a massive overflow of information and textual data, leading to a growing interest in Natural Language Processing (NLP) for these dialects. The highly inflectional morphology and the lack of standard orthography for AD pose an important challenge for NLP work. In this article, we handle the problem of lacking standard orthography during our work to build a morphological analyzer for Egyptian Arabic (EGY). To identify the guidelines for detecting conventional orthography, we depend on a corpus of 597,000 words that were gathered from various sources and genres. While analyzing the corpus morphologically, we handle the conventional orthography problem by assigning each word the conventional EGY Lemma and stem as close as possible to the EGY pronunciation no matter how it is typically written. Nevertheless, there are some common phenomena and complex cases involved in detecting conventional orthography during the morphological annotation process. Therefore, we take a closer look at and discuss these common phenomena and complex cases as we detect conventional orthography. These conventional orthographies are represented in a manner that facilitates the parsing of them correctly by the morphological analyzer. We tested the coverage of our morphological analyzer and compared it to one of the state-of-the-art morphological analyzers.",past arabic dialects poorly documented linguistically due lack written forms orthographies however recent years become widely used means communication since social media everywhere availability internet created massive overflow information textual data leading growing interest natural language processing nlp dialects highly inflectional morphology lack standard orthography pose important challenge nlp work article handle problem lacking standard orthography work build morphological analyzer egyptian arabic egy identify guidelines detecting conventional orthography depend corpus words gathered various sources genres analyzing corpus morphologically handle conventional orthography problem assigning word conventional egy lemma stem close possible egy pronunciation matter typically written nevertheless common phenomena complex cases involved detecting conventional orthography morphological annotation process therefore take closer look discuss common phenomena complex cases detect conventional orthography conventional orthographies represented manner facilitates parsing correctly morphological analyzer tested coverage morphological analyzer compared one state art morphological analyzers
"Rhetorical figures play a major role in everyday communication, making text and speech more interesting, memorable, or persuasive through their association between form and meaning. Computational detection of rhetorical figures plays an important part in thorough understanding of complex communication patterns. In this survey, we provide a comprehensive overview of computational approaches to lesser-known rhetorical figures. We explore the linguistic and computational perspectives on rhetorical figures and highlight their significance in the field of Natural Language Processing. We present different figures in detail and investigate datasets, definitions, rhetorical functions, and detection approaches. We identify challenges such as dataset scarcity, language limitations, and reliance on rule-based methods.",rhetorical figures play major role everyday communication making text speech interesting memorable persuasive association form meaning computational detection rhetorical figures plays important part thorough understanding complex communication patterns survey provide comprehensive overview computational approaches lesser known rhetorical figures explore linguistic computational perspectives rhetorical figures highlight significance field natural language processing present different figures detail investigate datasets definitions rhetorical functions detection approaches identify challenges dataset scarcity language limitations reliance rule based methods
"Sleep is a vital physiological state that significantly impacts overall health. Continuous monitoring of sleep posture, heart rate, respiratory rate, and body movement is crucial for diagnosing and managing sleep disorders. Current monitoring solutions often disrupt natural sleep due to discomfort or raise privacy and instrumentation concerns. We introduce PillowSense, a fabric-based sleep monitoring system seamlessly integrated into a pillowcase. PillowSense utilizes a dual-layer fabric design. The top layer comprises conductive fabrics for sensing electrocardiogram (ECG) and surface electromyogram (sEMG), while the bottom layer features pressure-sensitive fabrics to monitor sleep location and movement. The system processes ECG and sEMG signals sequentially to infer multiple sleep variables and incorporates an adversarial neural network to enhance posture classification accuracy. We fabricate prototypes using off-the-shelf hardware and conduct both lab-based and in-the-wild longitudinal user studies to evaluate the system's effectiveness. Across 151 nights and 912.2 hours of real-world sleep data, the system achieves an F1 score of 88% for classifying seven sleep postures, and clinically-acceptable accuracy in vital sign monitoring. PillowSense's comfort, washability, and robustness in multi-user scenarios underscore its potential for unobtrusive, large-scale sleep monitoring.",sleep vital physiological state significantly impacts overall health continuous monitoring sleep posture heart rate respiratory rate body movement crucial diagnosing managing sleep disorders current monitoring solutions often disrupt natural sleep due discomfort raise privacy instrumentation concerns introduce pillowsense fabric based sleep monitoring system seamlessly integrated pillowcase pillowsense utilizes dual layer fabric design top layer comprises conductive fabrics sensing electrocardiogram ecg surface electromyogram semg bottom layer features pressure sensitive fabrics monitor sleep location movement system processes ecg semg signals sequentially infer multiple sleep variables incorporates adversarial neural network enhance posture classification accuracy fabricate prototypes using shelf hardware conduct lab based wild longitudinal user studies evaluate system effectiveness across nights hours real world sleep data system achieves score classifying seven sleep postures clinically acceptable accuracy vital sign monitoring pillowsense comfort washability robustness multi user scenarios underscore potential unobtrusive large scale sleep monitoring
"""The future gets away with a lot, making itself at home in our lives before we've had a chance to say no thank you Shoshana Zuboff [41]   ""… and central amongst our goals should be a socio-technical future that is demonstrably better than the past.""--- Hutchinson et al [23]},",future gets away lot making home lives chance say thank shoshana zuboff central amongst goals socio technical future demonstrably better past hutchinson
"Motivated by rough set theory, we introduce a novel semantics for the basic modal language based on the possibility lower approximation operator of subset approximation structures. The study investigates axiomatization, expressiveness, and invariance results related to this semantics. From a rough set perspective, it provides a formal language for reasoning about the possibility lower approximation operator. Additionally, the axiomatization results obtained here provide characterizing properties of the operator.",motivated rough set theory introduce novel semantics basic modal language based possibility lower approximation operator subset approximation structures study investigates axiomatization expressiveness invariance results related semantics rough set perspective provides formal language reasoning possibility lower approximation operator additionally axiomatization results obtained provide characterizing properties operator
"Ehrenfeucht-Fraïssé games provide means to characterize elementary equivalence for first-order logic, and by standard translation also for modal logics. We propose a novel generalization of Ehrenfeucht-Fraïssé games to hybrid-dynamic logics which is direct and fully modular: parameterized by the features of the hybrid language we wish to include, for instance, the modal and hybrid language operators as well as first-order existential quantification. We use these games to establish a new modular Fraïssé-Hintikka theorem for hybrid-dynamic propositional logic and its various fragments. We study the relationship between countable game equivalence (determined by countable Ehrenfeucht-Fraïssé games) and bisimulation (determined by countable back-and-forth systems). In general, the former turns out to be weaker than the latter, but under certain conditions on the language, the two coincide. As a corollary we obtain an analogue of the Hennessy-Milner theorem. We also prove that for reachable image-finite Kripke structures elementary equivalence implies isomorphism.",ehrenfeucht fra ssé games provide means characterize elementary equivalence first order logic standard translation also modal logics propose novel generalization ehrenfeucht fra ssé games hybrid dynamic logics direct fully modular parameterized features hybrid language wish include instance modal hybrid language operators well first order existential quantification use games establish new modular fra ssé hintikka theorem hybrid dynamic propositional logic various fragments study relationship countable game equivalence determined countable ehrenfeucht fra ssé games bisimulation determined countable back forth systems general former turns weaker latter certain conditions language two coincide corollary obtain analogue hennessy milner theorem also prove reachable image finite kripke structures elementary equivalence implies isomorphism
"When working to build machines that have a form of intelligence, it is natural to be inspired by human intelligence. Of course, humans are very different from machines, in their embodiment and myriad other ways. Humans exploit their bodies to experience the world, create an internal model of it, and use this model to reason, learn, and make contextual and informed decisions. Machines lack the same embodiment, but often have access to both more memory and more computing power. Despite these crucial disanalogies, it is still useful to leverage our knowledge of how the human mind reasons and makes decisions to design and build machines that demonstrate behaviors similar to that of a human. In this article, we present a novel AI architecture, Slow and Fast AI (SOFAI), that is inspired by the “thinking fast and slow” cognitive theory of human decision making. SOFAI is a multi-agent architecture that employs both “fast” and “slow” solvers underneath a metacognitive agent that is able to both choose among a set of solvers as well as reflect on and learn from past experience. Experimental results on the behavior of two instances of the SOFAI architecture show that, compared to using just one of the two decision modalities, SOFAI is markedly better in terms of decision quality, resource consumption, and efficiency.",working build machines form intelligence natural inspired human intelligence course humans different machines embodiment myriad ways humans exploit bodies experience world create internal model use model reason learn make contextual informed decisions machines lack embodiment often access memory computing power despite crucial disanalogies still useful leverage knowledge human mind reasons makes decisions design build machines demonstrate behaviors similar human article present novel architecture slow fast sofai inspired thinking fast slow cognitive theory human decision making sofai multi agent architecture employs fast slow solvers underneath metacognitive agent able choose among set solvers well reflect learn past experience experimental results behavior two instances sofai architecture show compared using one two decision modalities sofai markedly better terms decision quality resource consumption efficiency
"A key hurdle to the success of quantum computers is the ability to initialize qubits into a pure state, which can be achieved by cooling qubits down to very low temperatures. Computational cooling of qubits, whereby a subset of the qubits is cooled at the expense of heating the other qubits via the application of special sets of logic gates, offers a route to effectively cool qubits. Here, we present QuL, a programming library which can be used to generate, analyze, and test quantum circuits for various computational cooling protocols. In its most basic usage, QuL enables a novice user to easily produce cooling circuits with minimal input or knowledge required. The programming library, however, offers flexibility to more advanced users to finely tune the cooling protocol used to generate the quantum circuit. Finally, QuL offers methods to assess and compare various cooling protocols for users interested in studying optimal implementation of computational cooling in general, or on specific quantum backends. It is our hope that QuL will not only facilitate the execution of computational cooling on current quantum computers, but also serve as a tool to investigate open questions in the optimal implementation of computational cooling.",key hurdle success quantum computers ability initialize qubits pure state achieved cooling qubits low temperatures computational cooling qubits whereby subset qubits cooled expense heating qubits via application special sets logic gates offers route effectively cool qubits present qul programming library used generate analyze test quantum circuits various computational cooling protocols basic usage qul enables novice user easily produce cooling circuits minimal input knowledge required programming library however offers flexibility advanced users finely tune cooling protocol used generate quantum circuit finally qul offers methods assess compare various cooling protocols users interested studying optimal implementation computational cooling general specific quantum backends hope qul facilitate execution computational cooling current quantum computers also serve tool investigate open questions optimal implementation computational cooling
"With the development of mobile technologies, users can easily access Online social networks (OSNs), consequently, massive contents including personal experiences, observations, or opinions are generated online. These contents are being shared and exchanged in OSNs, which have a significant influence on the minds of people toward politics, societies, economics, and so on. In this case, sentiment diffusion which focuses on how the process of information diffusion in OSNs is affected by sentiments has become an important issue. In this survey, we conduct a comprehensive review of the problem. Specifically, we first present the definition and classification of sentiment, introduce the most advanced computational sentiment analysis techniques, list their applications, and disclose available sentiment analysis resources. Then the main OSN components, functionalities, and structural features applied in modeling social networks are analyzed and summarized. Next, a thorough overview of the information diffusion technologies in OSNs, including graph and non-graph models is made. At last, a systematic and in-depth overview of the current state and issues of research on sentiment diffusion in OSNs is provided. This survey provides the necessary knowledge and new insights for relevant researchers to better understand the research state, remaining challenges, and future directions in this field.",development mobile technologies users easily access online social networks osns consequently massive contents including personal experiences observations opinions generated online contents shared exchanged osns significant influence minds people toward politics societies economics case sentiment diffusion focuses process information diffusion osns affected sentiments become important issue survey conduct comprehensive review problem specifically first present definition classification sentiment introduce advanced computational sentiment analysis techniques list applications disclose available sentiment analysis resources main osn components functionalities structural features applied modeling social networks analyzed summarized next thorough overview information diffusion technologies osns including graph non graph models made last systematic depth overview current state issues research sentiment diffusion osns provided survey provides necessary knowledge new insights relevant researchers better understand research state remaining challenges future directions field
"Artificial Intelligence has expanded its influence far beyond traditional boundaries in our society. One prominent application of artificial intelligence is the use of large language models, which have transcended their initial roles in high-tech industries and academic research and are now actively utilized by individual users. These models have continually improved over the years in their generative capabilities and performance across numerous tasks. However, they still pose a persistent risk of reproducing biases and stereotypes. Previous research has predominantly focused on quantitatively measuring biases in these large language models. In this study, we seek to assess not just the presence of bias itself, but the perception of stereotypes by these models via in-depth exploration of their responses. We demonstrate how the computational grounded theory framework, which integrates qualitative and quantitative approaches, can be applied in this context to assess the conceptualization of stereotypes. Furthermore, we contrast language model results with a survey of 400 human participants who also completed similar prompts as the model in order to understand people’s perception of gender stereotypes. The results indicate substantial similarities between language model and human perceptions of stereotypes, highlighting that a model’s perception stems from societal perception of stereotypes.",artificial intelligence expanded influence far beyond traditional boundaries society one prominent application artificial intelligence use large language models transcended initial roles high tech industries academic research actively utilized individual users models continually improved years generative capabilities performance across numerous tasks however still pose persistent risk reproducing biases stereotypes previous research predominantly focused quantitatively measuring biases large language models study seek assess presence bias perception stereotypes models via depth exploration responses demonstrate computational grounded theory framework integrates qualitative quantitative approaches applied context assess conceptualization stereotypes furthermore contrast language model results survey human participants also completed similar prompts model order understand people perception gender stereotypes results indicate substantial similarities language model human perceptions stereotypes highlighting model perception stems societal perception stereotypes
"Cybersecurity experts must broaden their perspectives beyond traditional disciplinary boundaries to provide the best protection possible: they must practice transdisciplinary cybersecurity. Taking influence from the Stakeholder Theory in business ethics, this paper debuts a new framework for evaluating existing cybersecurity practices or postures, engaging with other disciplines to foster learning and create new methods, and encouraging new ways of thinking about, perceiving, and executing cybersecurity practices. Three use cases showcase applications of the framework in real-world scenarios. The ultimate goal of this paper is to encourage transdisciplinary thinking in cybersecurity and any other discipline. By using the tool presented here and developing their transdisciplinary thinking, experts can be better prepared to face the often unique and complex modern-day challenges.",cybersecurity experts must broaden perspectives beyond traditional disciplinary boundaries provide best protection possible must practice transdisciplinary cybersecurity taking influence stakeholder theory business ethics paper debuts new framework evaluating existing cybersecurity practices postures engaging disciplines foster learning create new methods encouraging new ways thinking perceiving executing cybersecurity practices three use cases showcase applications framework real world scenarios ultimate goal paper encourage transdisciplinary thinking cybersecurity discipline using tool presented developing transdisciplinary thinking experts better prepared face often unique complex modern day challenges
"Computational Linguistics is an interdisciplinary field of computer science and linguistics that focuses on designing computational models and algorithms for processing, analyzing, and generating human language. Over recent years, this field has made substantial progress. While its primary emphasis tends to center around widely spoken languages, there is equal importance in investigating languages that are not commonly spoken but have contributed immensely to the literature, culture, and philosophy of the society. Thus, this survey article comprehensively delves into the exploration of computational tasks undertaken for Sanskrit, an ancient language of the Indian sub-continent steeped in a wealth of literary heritage. The purpose of this study is to provide an overview of the progress made thus far in the computational analysis of Sanskrit, while also reviewing the current digital infrastructure that supports these efforts. Additionally, our study also identifies potential avenues for future research, serving as a reference for anyone interested in advancing their exploration in this field.",computational linguistics interdisciplinary field computer science linguistics focuses designing computational models algorithms processing analyzing generating human language recent years field made substantial progress primary emphasis tends center around widely spoken languages equal importance investigating languages commonly spoken contributed immensely literature culture philosophy society thus survey article comprehensively delves exploration computational tasks undertaken sanskrit ancient language indian sub continent steeped wealth literary heritage purpose study provide overview progress made thus far computational analysis sanskrit also reviewing current digital infrastructure supports efforts additionally study also identifies potential avenues future research serving reference anyone interested advancing exploration field
"In this article, the QCSP (Quantified Constraint Satisfaction Problems) framework is shifted to the QCHR (Quantified Constraint Handling Rules) framework by enabling dynamic generation of quantifiers and access to user-defined constraints. QCSP offers a natural framework to express PSPACE problems as finite two-players games. But to define a QCSP model, the alternation of quantifiers must be formerly known and cannot be built dynamically even if the worst case will not occur. To overcome this issue, the new QCHR formalism, that allows to generate quantifiers dynamically during the solving, is defined. QCHR models exhibit state-of-the-art performances on a priori fixed alternation of quantifiers and outperforms previous QCSP approaches when the generation of quantifiers is dynamic.",article qcsp quantified constraint satisfaction problems framework shifted qchr quantified constraint handling rules framework enabling dynamic generation quantifiers access user defined constraints qcsp offers natural framework express pspace problems finite two players games define qcsp model alternation quantifiers must formerly known built dynamically even worst case occur overcome issue new qchr formalism allows generate quantifiers dynamically solving defined qchr models exhibit state art performances priori fixed alternation quantifiers outperforms previous qcsp approaches generation quantifiers dynamic
"We survey results on the formalization and independence of mathematical statements related to major open problems in computational complexity theory. Our primary focus is on recent findings concerning the (un)provability of complexity bounds within theories of bounded arithmetic. This includes the techniques employed and related open problems, such as the (non)existence of a feasible proof that P = NP.",survey results formalization independence mathematical statements related major open problems computational complexity theory primary focus recent findings concerning provability complexity bounds within theories bounded arithmetic includes techniques employed related open problems non existence feasible proof
"Temporal logics over finite traces have recently seen wide application in a number of areas, from business process modelling, monitoring and mining to planning and decision-making. However, real-life dynamic systems contain a degree of uncertainty which cannot be handled with classical logics. We thus propose a new probabilistic temporal logic over finite traces using superposition semantics, where all possible evolutions are possible, until observed. We study the properties of the logic and provide automata-based mechanisms for deriving probabilistic inferences from its formulas. We then study a fragment of the logic with better computational properties. Notably, formulas in this fragment can be discovered from event log data using off-the-shelf existing declarative process discovery techniques.",temporal logics finite traces recently seen wide application number areas business process modelling monitoring mining planning decision making however real life dynamic systems contain degree uncertainty handled classical logics thus propose new probabilistic temporal logic finite traces using superposition semantics possible evolutions possible observed study properties logic provide automata based mechanisms deriving probabilistic inferences formulas study fragment logic better computational properties notably formulas fragment discovered event log data using shelf existing declarative process discovery techniques
"Primal logic arose in access control; it has a remarkably efficient (linear time) decision procedure for its entailment problem. But primal logic is a general logic of information. In the realm of arbitrary items of information (infons), conjunction, disjunction, and implication may seem to correspond (set-theoretically) to union, intersection, and relative complementation. But, while infons are closed under union, they are not closed under intersection or relative complementation. It turns out that there is a systematic transformation of propositional intuitionistic calculi to the original (propositional) primal calculi; we call it flatting. We extend flatting to quantifier rules, obtaining arguably the right quantified primal logic (QPL). The QPL entailment problem is exponential-time complete, but it is polynomial-time complete in the case, of importance to applications (at least to access control), where the number of quantifiers is bounded.",primal logic arose access control remarkably efficient linear time decision procedure entailment problem primal logic general logic information realm arbitrary items information infons conjunction disjunction implication may seem correspond set theoretically union intersection relative complementation infons closed union closed intersection relative complementation turns systematic transformation propositional intuitionistic calculi original propositional primal calculi call flatting extend flatting quantifier rules obtaining arguably right quantified primal logic qpl qpl entailment problem exponential time complete polynomial time complete case importance applications least access control number quantifiers bounded
"Single-round multiway join algorithms first reshuffle data over many servers and then evaluate the query at hand in a parallel and communication-free way. A key question is whether a given distribution policy for the reshuffle is adequate for computing a given query. This property is referred to as parallel-correctness. Another key problem is to detect whether the data reshuffle step can be avoided when evaluating subsequent queries. The latter problem is referred to as transfer of parallel-correctness. This article extends the study of parallel-correctness and transfer of parallel-correctness of conjunctive queries to incorporate bag semantics. We provide semantical characterizations for both problems, obtain complexity bounds, and discuss the relationship with their set semantics counterparts. Finally, we revisit both problems under a modified distribution model that takes advantage of a linear order on compute nodes and obtain tight complexity bounds.",single round multiway join algorithms first reshuffle data many servers evaluate query hand parallel communication free way key question whether given distribution policy reshuffle adequate computing given query property referred parallel correctness another key problem detect whether data reshuffle step avoided evaluating subsequent queries latter problem referred transfer parallel correctness article extends study parallel correctness transfer parallel correctness conjunctive queries incorporate bag semantics provide semantical characterizations problems obtain complexity bounds discuss relationship set semantics counterparts finally revisit problems modified distribution model takes advantage linear order compute nodes obtain tight complexity bounds
"Over the years, interest in computational storage devices has been growing steadily. This is largely due to the rise of data-intensive applications, such as machine learning, online video distribution, astrophysics, and genomics. Moving compute operations closer to the data provides benefits in terms of scaling possibilities and energy efficiency. The development of computational storage devices has been limited by the need for specialized and complex hardware. In this work, we propose a portable Linux-based firmware framework for the development of NVMe computational storage devices. Our firmware runs on a variety of hardware platforms ranging from expensive FPGA solutions to inexpensive off-the-shelf single board computers. The firmware leverages the vast Linux software ecosystem to facilitate the development and prototyping of novel computational storage devices. We benchmark our firmware on multiple hardware platforms and demonstrate its versatility through several computational examples including a content-aware disk image search engine based on natural language processing and AI-driven image recognition.",years interest computational storage devices growing steadily largely due rise data intensive applications machine learning online video distribution astrophysics genomics moving compute operations closer data provides benefits terms scaling possibilities energy efficiency development computational storage devices limited need specialized complex hardware work propose portable linux based firmware framework development nvme computational storage devices firmware runs variety hardware platforms ranging expensive fpga solutions inexpensive shelf single board computers firmware leverages vast linux software ecosystem facilitate development prototyping novel computational storage devices benchmark firmware multiple hardware platforms demonstrate versatility several computational examples including content aware disk image search engine based natural language processing driven image recognition
"Kangaroo Mother Care (KMC) is a low-cost, evidence-based intervention with proven benefits to both infants and their families. It refers to holding the infant with the infant's chest skin against the caregiver's chest skin in an upright positionc[2]. Based on a report from the World Health Organizationc[3], KMC benefits are evidenced by a 32% reduction in neonatal mortality. It has been adopted as a standard of care for infants with low birth weights worldwide, especially in resource-constrained environments. Continuous monitoring of KMC practices is clinically important. In particular, the duration of KMC sessions and infant vital signs during KMC hold significant clinical values.",kangaroo mother care kmc low cost evidence based intervention proven benefits infants families refers holding infant infant chest skin caregiver chest skin upright positionc based report world health organizationc kmc benefits evidenced reduction neonatal mortality adopted standard care infants low birth weights worldwide especially resource constrained environments continuous monitoring kmc practices clinically important particular duration kmc sessions infant vital signs kmc hold significant clinical values
"Adult children often help their older parents maintain independence at home, and this informal caregiving can be overwhelming and challenging to balance with other aspects of life. Yet, adult children's requirements tend to be overlooked when designing technologies to foster aging in place. In this work, we use reflexive thematic analysis to study Reddit content to understand the experiences of adult caregivers and the issues that they face in supporting their parents. The pseudonymous data allowed us to explore three research questions: 1) What are the experiences of adult children as informal helpers for elderly parents? 2) How do adult children currently manage their role as informal helpers? and 3) How might digital technology facilitate communication between aging parents and adult children as informal helpers? Our findings lead to a number of design considerations for technologies that support adult children caring for parents who are aging in place, including: building common ground, establishing boundaries, addressing guilt, encouraging in-person communication, community support for adult children, and adapting to changes in role reciprocity.",adult children often help older parents maintain independence home informal caregiving overwhelming challenging balance aspects life yet adult children requirements tend overlooked designing technologies foster aging place work use reflexive thematic analysis study reddit content understand experiences adult caregivers issues face supporting parents pseudonymous data allowed explore three research questions experiences adult children informal helpers elderly parents adult children currently manage role informal helpers might digital technology facilitate communication aging parents adult children informal helpers findings lead number design considerations technologies support adult children caring parents aging place including building common ground establishing boundaries addressing guilt encouraging person communication community support adult children adapting changes role reciprocity
"The proliferation of unreliable news domains on the internet has had wide-reaching negative impacts on society. We introduce and evaluate interventions aimed at reducing traffic to unreliable news domains from search engines while maintaining traffic to reliable domains. We build these interventions on the principles of fairness (penalize sites for what is in their control), generality (label/fact-check agnostic), targeted (increase the cost of adversarial behavior), and scalability (works at webscale). We refine our methods on small-scale webdata as a testbed and then generalize the interventions to a large-scale webgraph containing 93.9M domains and 1.6B edges. We demonstrate that our methods penalize unreliable domains far more than reliable domains in both settings and we explore multiple avenues to mitigate unintended effects on both the small-scale and large-scale webgraph experiments. These results indicate the potential of our approach to reduce the spread of misinformation and foster a more reliable online information ecosystem. This research contributes to the development of targeted strategies to enhance the trustworthiness and quality of search engine results, ultimately benefiting users, and the broader digital community.",proliferation unreliable news domains internet wide reaching negative impacts society introduce evaluate interventions aimed reducing traffic unreliable news domains search engines maintaining traffic reliable domains build interventions principles fairness penalize sites control generality label fact check agnostic targeted increase cost adversarial behavior scalability works webscale refine methods small scale webdata testbed generalize interventions large scale webgraph containing domains edges demonstrate methods penalize unreliable domains far reliable domains settings explore multiple avenues mitigate unintended effects small scale large scale webgraph experiments results indicate potential approach reduce spread misinformation foster reliable online information ecosystem research contributes development targeted strategies enhance trustworthiness quality search engine results ultimately benefiting users broader digital community
A route to democratic digital governance.,route democratic digital governance
"We give a complete complexity classification for the problem of finding a solution to a given system of equations over a fixed finite monoid, given that a solution over a more restricted monoid exists. As a corollary, we obtain a complexity classification for the same problem over groups.",give complete complexity classification problem finding solution given system equations fixed finite monoid given solution restricted monoid exists corollary obtain complexity classification problem groups
"Generative AI offers great promise for effectively solving challenging ill-posed inverse problems, transforming the way we measure and infer the physical world around us, and enabling exciting new user-centric capabilities.",generative offers great promise effectively solving challenging ill posed inverse problems transforming way measure infer physical world around enabling exciting new user centric capabilities
"For the past half-billion years, evolution has produced a diversity of eyes and brains that work together to solve visual problems with remarkable efficiency and robustness. By reverse-engineering these systems, we can uncover powerful principles to build the next generation of computational cameras.",past half billion years evolution produced diversity eyes brains work together solve visual problems remarkable efficiency robustness reverse engineering systems uncover powerful principles build next generation computational cameras
"We develop a computational pipeline to facilitate the biomimetic design of winged seeds. Our approach leverages 3D scans of natural winged seeds to construct a bio-inspired design space by interpolating them with geodesic coordinates in the 3D diffeomorphism group. We formulate aerodynamic design tasks with probabilistic performance objectives and adapt a gradient-free optimizer to explore the design space and minimize the expectation of performance objectives efficiently and effectively. Our pipeline discovers novel winged seed designs that outperform natural counterparts in aerodynamic tasks, including long-distance dispersal and guided flight. We validate the physical fidelity of our pipeline by showcasing paper models of selected winged seeds in the design space and reporting their similar aerodynamic behaviors in simulation and reality.",develop computational pipeline facilitate biomimetic design winged seeds approach leverages scans natural winged seeds construct bio inspired design space interpolating geodesic coordinates diffeomorphism group formulate aerodynamic design tasks probabilistic performance objectives adapt gradient free optimizer explore design space minimize expectation performance objectives efficiently effectively pipeline discovers novel winged seed designs outperform natural counterparts aerodynamic tasks including long distance dispersal guided flight validate physical fidelity pipeline showcasing paper models selected winged seeds design space reporting similar aerodynamic behaviors simulation reality
"Quality patient-provider communication is critical to improve clinical care and patient outcomes. While progress has been made with communication skills training for clinicians, significant gaps exist in how to best monitor, measure, and evaluate the implementation of communication skills in the actual clinical setting. Advancements in ubiquitous technology and natural language processing make it possible to realize more objective, real-time assessment of clinical interactions and in turn provide more timely feedback to clinicians about their communication effectiveness. In this paper, we propose CommSense, a computational sensing framework that combines smartwatch audio and transcripts with natural language processing methods to measure selected ""best-practice'' communication metrics captured by wearable devices in the context of palliative care interactions, including understanding, empathy, presence, emotion, and clarity. We conducted a pilot study involving N=40 clinician participants, to test the technical feasibility and acceptability of CommSense in a simulated clinical setting. Our findings demonstrate that CommSense effectively captures most communication metrics and is well-received by both practicing clinicians and student trainees. Our study also highlights the potential for digital technology to enhance communication skills training for healthcare providers and students, ultimately resulting in more equitable delivery of healthcare and accessible, lower cost tools for training with the potential to improve patient outcomes.",quality patient provider communication critical improve clinical care patient outcomes progress made communication skills training clinicians significant gaps exist best monitor measure evaluate implementation communication skills actual clinical setting advancements ubiquitous technology natural language processing make possible realize objective real time assessment clinical interactions turn provide timely feedback clinicians communication effectiveness paper propose commsense computational sensing framework combines smartwatch audio transcripts natural language processing methods measure selected best practice communication metrics captured wearable devices context palliative care interactions including understanding empathy presence emotion clarity conducted pilot study involving clinician participants test technical feasibility acceptability commsense simulated clinical setting findings demonstrate commsense effectively captures communication metrics well received practicing clinicians student trainees study also highlights potential digital technology enhance communication skills training healthcare providers students ultimately resulting equitable delivery healthcare accessible lower cost tools training potential improve patient outcomes
"Conversational agents (CAs) that deliver proactive interventions can benefit users by reducing their cognitive workload and improving performance. However, little is known regarding how such interventions would impact users’ reflection on choices in voice-only decision-making tasks. We conducted a within-subjects experiment to evaluate the effect of CA’s feedback delivery strategy at three levels (no feedback, unsolicited and solicited feedback) and the impact on users’ likelihood of changing their choices in an interactive food ordering scenario. We discovered that in both feedback conditions the CA was perceived to be significantly more persuasive than in the baseline condition, while being perceived as significantly less confident. Interestingly, while unsolicited feedback was perceived as less appropriate than the baseline, both types of proactive feedback led participants to relisten and reconsider menu options significantly more often. Our results provide insights regarding the impact of proactive feedback on CA perception and user’s reflection in decision-making tasks, thereby paving a new way for designing proactive CAs.",conversational agents cas deliver proactive interventions benefit users reducing cognitive workload improving performance however little known regarding interventions would impact users reflection choices voice decision making tasks conducted within subjects experiment evaluate effect feedback delivery strategy three levels feedback unsolicited solicited feedback impact users likelihood changing choices interactive food ordering scenario discovered feedback conditions perceived significantly persuasive baseline condition perceived significantly less confident interestingly unsolicited feedback perceived less appropriate baseline types proactive feedback led participants relisten reconsider menu options significantly often results provide insights regarding impact proactive feedback perception user reflection decision making tasks thereby paving new way designing proactive cas
"This article addresses the problem of checking invariant properties for a large class of symbolic transition systems defined by a combination of SMT theories and quantifiers. State variables can be functions from an uninterpreted sort (finite but unbounded) to an interpreted sort, such as the integers under the theory of linear arithmetic. This formalism is very expressive and can be used for modeling parameterized systems, array-manipulating programs, and more. We propose two algorithms for finding universal inductive invariants for such systems. The first algorithm combines an IC3-style loop with a form of implicit predicate abstraction to construct an invariant in an incremental manner. The second algorithm constructs an under-approximation of the original problem and searches for a formula which is an inductive invariant for this case; then, the invariant is generalized to the original case and checked with a portfolio of techniques. We have implemented the two algorithms and conducted an extensive experimental evaluation, considering various benchmarks and different tools from the literature. As far as we know, our method is the first capable of handling in a large class of systems in a uniform way. The experiment shows that both algorithms are competitive with the state of the art.",article addresses problem checking invariant properties large class symbolic transition systems defined combination smt theories quantifiers state variables functions uninterpreted sort finite unbounded interpreted sort integers theory linear arithmetic formalism expressive used modeling parameterized systems array manipulating programs propose two algorithms finding universal inductive invariants systems first algorithm combines style loop form implicit predicate abstraction construct invariant incremental manner second algorithm constructs approximation original problem searches formula inductive invariant case invariant generalized original case checked portfolio techniques implemented two algorithms conducted extensive experimental evaluation considering various benchmarks different tools literature far know method first capable handling large class systems uniform way experiment shows algorithms competitive state art
"We prove that there are continuum-many axiomatic extensions of the full Lambek calculus with exchange that have the deductive interpolation property. Further, we extend this result to both classical and intuitionistic linear logic as well as their multiplicative-additive fragments. None of the logics we exhibit have the Craig interpolation property, but we show that the exhibited extensions of classical and intuitionistic linear logic all enjoy a guarded form of Craig interpolation. We also give continuum-many axiomatic extensions of classical linear logic without the deductive interpolation property.",prove continuum many axiomatic extensions full lambek calculus exchange deductive interpolation property extend result classical intuitionistic linear logic well multiplicative additive fragments none logics exhibit craig interpolation property show exhibited extensions classical intuitionistic linear logic enjoy guarded form craig interpolation also give continuum many axiomatic extensions classical linear logic without deductive interpolation property
"The search for alternative teaching–learning processes that attract more interest and involvement of young people has inspired the development of a game with a chatbot architecture based on interactive storytelling and multiple learning paths. Thus, we introduce in this article the GameBot ZoAm, developed for the Discord instant messaging and social platform. ZoAm offers a unique learning experience centered around storytelling, focusing on fundamental computing concepts and logical challenges that enhance computational thinking skills. Furthermore, the game also promotes an appreciation for Amazonian culture and folklore, with decision-making with human values. An action research study was conducted involving students from the last years of the end of elementary school. The research utilized a heuristic analysis based on the Gameplay Heuristics (PLAY) by Desurvire and Wiberg (ANO) and the evaluation model proposed by Korhonen and Koivisto (ANO) for mobile devices. The analysis employed a reduced and merged set of heuristics from these models, suited for the GameBot's context, focusing on (I) usability, (II) gameplay and immersion, and (III) mobility. Regarding the reliability coefficient used to evaluate the survey applied to students after playing the GameBot, Cronbach's Alpha and Guttman Lambda-6 (G6(smc)) coefficients were applied. These metrics were chosen to ensure the internal consistency and reliability of survey items, reflecting on how effectively the questions measured the focuses proposed by the heuristic analysis. The findings indicate that the game has the potential to facilitate the assimilation of the integrated concepts and sustain student interest throughout gameplay.",search alternative teaching learning processes attract interest involvement young people inspired development game chatbot architecture based interactive storytelling multiple learning paths thus introduce article gamebot zoam developed discord instant messaging social platform zoam offers unique learning experience centered around storytelling focusing fundamental computing concepts logical challenges enhance computational thinking skills furthermore game also promotes appreciation amazonian culture folklore decision making human values action research study conducted involving students last years end elementary school research utilized heuristic analysis based gameplay heuristics play desurvire wiberg ano evaluation model proposed korhonen koivisto ano mobile devices analysis employed reduced merged set heuristics models suited gamebot context focusing usability gameplay immersion iii mobility regarding reliability coefficient used evaluate survey applied students playing gamebot cronbach alpha guttman lambda smc coefficients applied metrics chosen ensure internal consistency reliability survey items reflecting effectively questions measured focuses proposed heuristic analysis findings indicate game potential facilitate assimilation integrated concepts sustain student interest throughout gameplay
"This teaching case applies design thinking to a large-scale client project in a technical and professional communication (TPC) class. Using the 5-step design thinking process (""empathize, define, ideate, prototype, test"") over 8 weeks, the students in an upper-division TPC course developed social media content and strategy for a statewide public relations campaign. The two authors, the instructing faculty and a senior student who served as project manager, illuminate how iterative design thinking, as a UX pedagogical practice, can help students set boundaries around ill-defined problems; mirror workplace collaboration to contribute to professional development; and build a toolkit for exercising agency and creativity as researchers, writers, and designers.",teaching case applies design thinking large scale client project technical professional communication tpc class using step design thinking process empathize define ideate prototype test weeks students upper division tpc course developed social media content strategy statewide public relations campaign two authors instructing faculty senior student served project manager illuminate iterative design thinking pedagogical practice help students set boundaries around ill defined problems mirror workplace collaboration contribute professional development build toolkit exercising agency creativity researchers writers designers
Software Developers and Collective Empathy---Can They Be Disposed to Care?,software developers collective empathy disposed care
It is shown that Holliday’s propositional Fundamental Logic is decidable in polynomial time and that first-order Fundamental Logic is decidable in double-exponential time. The proof also yields a double-exponential–time decision procedure for first-order orthologic.,shown holliday propositional fundamental logic decidable polynomial time first order fundamental logic decidable double exponential time proof also yields double exponential time decision procedure first order orthologic
"Illusion-knit fabrics reveal distinct patterns or images depending on the viewing angle. Artists have manually achieved this effect by exploiting ""microgeometry,"" i.e., small differences in stitch heights. However, past work in computational 3D knitting does not model or exploit designs based on stitch height variation. This paper establishes a foundation for exploring illusion knitting in the context of computational design and fabrication. We observe that the design space is highly constrained, elucidate these constraints, and derive strategies for developing effective, machine-knittable illusion patterns. We partially automate these strategies in a new interactive design tool that reduces difficult patterning tasks to familiar image editing tasks. Illusion patterns also uncover new fabrication challenges regarding mixed colorwork and texture; we describe new algorithms for mitigating fabrication failures and ensuring high-quality knit results.",illusion knit fabrics reveal distinct patterns images depending viewing angle artists manually achieved effect exploiting microgeometry small differences stitch heights however past work computational knitting model exploit designs based stitch height variation paper establishes foundation exploring illusion knitting context computational design fabrication observe design space highly constrained elucidate constraints derive strategies developing effective machine knittable illusion patterns partially automate strategies new interactive design tool reduces difficult patterning tasks familiar image editing tasks illusion patterns also uncover new fabrication challenges regarding mixed colorwork texture describe new algorithms mitigating fabrication failures ensuring high quality knit results
"While conventional cameras offer versatility for applications ranging from amateur photography to autonomous driving, computational cameras allow for domain-specific adaption. Cameras with co-designed optics and image processing algorithms enable high-dynamic-range image recovery, depth estimation, and hyperspectral imaging through optically encoding scene information that is otherwise undetected by conventional cameras. However, this optical encoding creates a challenging inverse reconstruction problem for conventional image recovery, and often lowers the overall photographic quality. Thus computational cameras with domain-specific optics have only been adopted in a few specialized applications where the captured information cannot be acquired in other ways. In this work, we investigate a method that combines two optical systems into one to tackle this challenge. We split the aperture of a conventional camera into two halves: one which applies an application-specific modulation to the incident light via a diffractive optical element to produce a coded image capture, and one which applies no modulation to produce a conventional image capture. Co-designing the phase modulation of the split aperture with a dual-pixel sensor allows us to simultaneously capture these coded and uncoded images without increasing physical or computational footprint. With an uncoded conventional image alongside the optically coded image in hand, we investigate image reconstruction methods that are conditioned on the conventional image, making it possible to eliminate artifacts and compute costs that existing methods struggle with. We assess the proposed method with 2-in-1 cameras for optical high-dynamic-range reconstruction, monocular depth estimation, and hyperspectral imaging, comparing favorably to all tested methods in all applications.",conventional cameras offer versatility applications ranging amateur photography autonomous driving computational cameras allow domain specific adaption cameras designed optics image processing algorithms enable high dynamic range image recovery depth estimation hyperspectral imaging optically encoding scene information otherwise undetected conventional cameras however optical encoding creates challenging inverse reconstruction problem conventional image recovery often lowers overall photographic quality thus computational cameras domain specific optics adopted specialized applications captured information acquired ways work investigate method combines two optical systems one tackle challenge split aperture conventional camera two halves one applies application specific modulation incident light via diffractive optical element produce coded image capture one applies modulation produce conventional image capture designing phase modulation split aperture dual pixel sensor allows simultaneously capture coded uncoded images without increasing physical computational footprint uncoded conventional image alongside optically coded image hand investigate image reconstruction methods conditioned conventional image making possible eliminate artifacts compute costs existing methods struggle assess proposed method cameras optical high dynamic range reconstruction monocular depth estimation hyperspectral imaging comparing favorably tested methods applications
"Random testing approaches work by generating inputs at random, or by selecting inputs randomly from some pre-defined operational profile. One long-standing question that arises in this and other testing contexts is as follows: When can we stop testing? At what point can we be certain that executing further tests in this manner will not explore previously untested (and potentially buggy) software behaviors? This is analogous to the question in Machine Learning, of how many training examples are required in order to infer an accurate model. In this paper we show how probabilistic approaches to answer this question in Machine Learning (arising from Computational Learning Theory) can be applied in our testing context. This enables us to produce an upper bound on the number of tests that are required to achieve a given level of adequacy. We are the first to enable this from only knowing the number of coverage targets (e.g. lines of code) in the source code, without needing to observe a sample test executions. We validate this bound on a large set of Java units, and an autonomous driving system.",random testing approaches work generating inputs random selecting inputs randomly pre defined operational profile one long standing question arises testing contexts follows stop testing point certain executing tests manner explore previously untested potentially buggy software behaviors analogous question machine learning many training examples required order infer accurate model paper show probabilistic approaches answer question machine learning arising computational learning theory applied testing context enables produce upper bound number tests required achieve given level adequacy first enable knowing number coverage targets lines code source code without needing observe sample test executions validate bound large set java units autonomous driving system
"In usability tests, the users are commonly asked to think aloud to let the evaluator listen in on their thoughts. Two variants of this procedure involve that the users either think aloud while using the tested product (concurrent thinking aloud, CTA) or after using it (retrospective thinking aloud, RTA). This study reviews the studies that compare CTA and RTA to investigate what is gained and lost by using one or the other variant in a usability test. A total of 29 studies, reporting from 42 comparisons of CTA and RTA, matched the inclusion criteria and were included in the meta-analyses. The main differences are that for CTA task time is longer, but total time shorter, whereas for RTA the users verbalize more explanations, problem formulations, and design recommendations. In addition, CTA users probably experience the evaluator’s presence as less disturbing than RTA users do.",usability tests users commonly asked think aloud let evaluator listen thoughts two variants procedure involve users either think aloud using tested product concurrent thinking aloud cta using retrospective thinking aloud rta study reviews studies compare cta rta investigate gained lost using one variant usability test total studies reporting comparisons cta rta matched inclusion criteria included meta analyses main differences cta task time longer total time shorter whereas rta users verbalize explanations problem formulations design recommendations addition cta users probably experience evaluator presence less disturbing rta users
"Time series data are widely used and provide a wealth of information for countless applications. However, some applications are faced with a limited amount of data, or the data cannot be used due to confidentiality concerns. To overcome these obstacles, time series can be generated synthetically. For example, electrocardiograms can be synthesized to make them available for building models to predict conditions such as cardiac arrhythmia without leaking patient information. Although many different approaches to time series synthesis have been proposed, evaluating the quality of synthetic time series data poses unique challenges and remains an open problem, as there is a lack of a clear definition of what constitutes a “good” synthesis. To this end, we present a comprehensive literature survey to identify different aspects of synthesis quality and their relationships. Based on this, we propose a definition of synthesis quality and a systematic evaluation procedure for assessing it. With this work, we aim to provide a common language and criteria for evaluating synthetic time series data. Our goal is to promote more rigorous and reproducible research in time series synthesis by enabling researchers and practitioners to generate high-quality synthetic time series data.",time series data widely used provide wealth information countless applications however applications faced limited amount data data used due confidentiality concerns overcome obstacles time series generated synthetically example electrocardiograms synthesized make available building models predict conditions cardiac arrhythmia without leaking patient information although many different approaches time series synthesis proposed evaluating quality synthetic time series data poses unique challenges remains open problem lack clear definition constitutes good synthesis end present comprehensive literature survey identify different aspects synthesis quality relationships based propose definition synthesis quality systematic evaluation procedure assessing work aim provide common language criteria evaluating synthetic time series data goal promote rigorous reproducible research time series synthesis enabling researchers practitioners generate high quality synthetic time series data
"In this article, we introduce a notion of backdoors to Reiter’s propositional default logic and study structural properties of it. Also we consider the problems of backdoor detection (parameterised by the solution size) as well as backdoor evaluation (parameterised by the size of the given backdoor) for various kinds of target classes (CNF, KROM, MONOTONE) and all SCHAEFER classes. Also, we study generalisations of HORN-formulas, namely QHORN, RHORN, as well as DUALHORN. For these classes, we also classify the computational complexity of the implication problem. We show that backdoor detection is fixed-parameter tractable for the considered target classes and prove a complete trichotomy for backdoor evaluation. The problems are either fixed-parameter tractable, para-DeltaP2-complete, or para-NP-complete, depending on the target class.",article introduce notion backdoors reiter propositional default logic study structural properties also consider problems backdoor detection parameterised solution size well backdoor evaluation parameterised size given backdoor various kinds target classes cnf krom monotone schaefer classes also study generalisations horn formulas namely qhorn rhorn well dualhorn classes also classify computational complexity implication problem show backdoor detection fixed parameter tractable considered target classes prove complete trichotomy backdoor evaluation problems either fixed parameter tractable deltap complete complete depending target class
"We introduce and study online conversion with switching costs, a family of online problems that capture emerging problems at the intersection of energy and sustainability. In this problem, an online player attempts to purchase (alternatively, sell) fractional shares of an asset during a fixed time horizon with length T. At each time step, a cost function (alternatively, price function) is revealed, and the player must irrevocably decide an amount of asset to convert. The player also incurs a switching cost whenever their decision changes in consecutive time steps, i.e., when they increase or decrease their purchasing amount. We introduce competitive (robust) threshold-based algorithms for both the minimization and maximization variants of this problem, and show they are optimal among deterministic online algorithms. We then propose learning-augmented algorithms that take advantage of untrusted black-box advice (such as predictions from a machine learning model) to achieve significantly better average-case performance without sacrificing worst-case competitive guarantees. Finally, we empirically evaluate our proposed algorithms using a carbon-aware EV charging case study, showing that our algorithms substantially improve on baseline methods for this problem.",introduce study online conversion switching costs family online problems capture emerging problems intersection energy sustainability problem online player attempts purchase alternatively sell fractional shares asset fixed time horizon length time step cost function alternatively price function revealed player must irrevocably decide amount asset convert player also incurs switching cost whenever decision changes consecutive time steps increase decrease purchasing amount introduce competitive robust threshold based algorithms minimization maximization variants problem show optimal among deterministic online algorithms propose learning augmented algorithms take advantage untrusted black box advice predictions machine learning model achieve significantly better average case performance without sacrificing worst case competitive guarantees finally empirically evaluate proposed algorithms using carbon aware charging case study showing algorithms substantially improve baseline methods problem
"We introduce and study the online pause and resume problem. In this problem, a player attempts to find the k lowest (alternatively, highest) prices in a sequence of fixed length T, which is revealed sequentially. At each time step, the player is presented with a price and decides whether to accept or reject it. The player incurs a switching cost whenever their decision changes in consecutive time steps, i.e., whenever they pause or resume purchasing. This online problem is motivated by the goal of carbon-aware load shifting, where a workload may be paused during periods of high carbon intensity and resumed during periods of low carbon intensity and incurs a cost when saving or restoring its state. It has strong connections to existing problems studied in the literature on online optimization, though it introduces unique technical challenges that prevent the direct application of existing algorithms. Extending prior work on threshold-based algorithms, we introduce double-threshold algorithms for both variants of this problem. We further show that the competitive ratios achieved by these algorithms are the best achievable by any deterministic online algorithm. Finally, we empirically validate our proposed algorithm through case studies on the application of carbon-aware load shifting using real carbon trace data and existing baseline algorithms.",introduce study online pause resume problem problem player attempts find lowest alternatively highest prices sequence fixed length revealed sequentially time step player presented price decides whether accept reject player incurs switching cost whenever decision changes consecutive time steps whenever pause resume purchasing online problem motivated goal carbon aware load shifting workload may paused periods high carbon intensity resumed periods low carbon intensity incurs cost saving restoring state strong connections existing problems studied literature online optimization though introduces unique technical challenges prevent direct application existing algorithms extending prior work threshold based algorithms introduce double threshold algorithms variants problem show competitive ratios achieved algorithms best achievable deterministic online algorithm finally empirically validate proposed algorithm case studies application carbon aware load shifting using real carbon trace data existing baseline algorithms
Routes toward more deliberation in technology development.,routes toward deliberation technology development
"Differentiable rendering methods promise the ability to optimize various parameters of three-dimensional (3D) scenes to achieve a desired result. However, lighting design has so far received little attention in this field. In this article, we introduce a method that enables continuous optimization of the arrangement of luminaires in a 3D scene via differentiable light tracing. Our experiments show two major issues when attempting to apply existing methods from differentiable path tracing to this problem: First, many rendering methods produce images, which restricts the ability of a designer to define lighting objectives to image space. Second, most previous methods are designed for scene geometry or material optimization and have not been extensively tested for the case of optimizing light sources. Currently available differentiable ray-tracing methods do not provide satisfactory performance, even on fairly basic test cases in our experience. In this article, we propose, to the best of our knowledge, a novel adjoint light tracing method that overcomes these challenges and enables gradient-based lighting design optimization in a view-independent (camera-free) way. Thus, we allow the user to paint illumination targets directly onto the 3D scene or use existing baked illumination data (e.g., light maps). Using modern ray-tracing hardware, we achieve interactive performance. We find light tracing advantageous over path tracing in this setting, as it naturally handles irregular geometry, resulting in less noise and improved optimization convergence. We compare our adjoint gradients to state-of-the-art image-based differentiable rendering methods. We also demonstrate that our gradient data works with various common optimization algorithms, providing good convergence behaviour. Qualitative comparisons with real-world scenes underline the practical applicability of our method.",differentiable rendering methods promise ability optimize various parameters three dimensional scenes achieve desired result however lighting design far received little attention field article introduce method enables continuous optimization arrangement luminaires scene via differentiable light tracing experiments show two major issues attempting apply existing methods differentiable path tracing problem first many rendering methods produce images restricts ability designer define lighting objectives image space second previous methods designed scene geometry material optimization extensively tested case optimizing light sources currently available differentiable ray tracing methods provide satisfactory performance even fairly basic test cases experience article propose best knowledge novel adjoint light tracing method overcomes challenges enables gradient based lighting design optimization view independent camera free way thus allow user paint illumination targets directly onto scene use existing baked illumination data light maps using modern ray tracing hardware achieve interactive performance find light tracing advantageous path tracing setting naturally handles irregular geometry resulting less noise improved optimization convergence compare adjoint gradients state art image based differentiable rendering methods also demonstrate gradient data works various common optimization algorithms providing good convergence behaviour qualitative comparisons real world scenes underline practical applicability method
"This paper presents a systematic literature review of professional development programs in computational thinking (CT). CT has emerged as an essential set of skills that everyone should develop to participate in a global society. However, there were no pre-service or in-service teacher programs to integrate CT into the K–12 classrooms until very recently. Thus, it is important to identify how educators and researchers address the challenges to prepare the next generation of students and what gaps persist in the current literature. We review existing work in this field from two perspectives: First, we analyze the learning outcomes, assessment methods, pedagogical approaches, and pedagogical tools used in the professional development programs in CT. Second, we examine how these programs assess the teachers’ knowledge and skills as outcomes. We used the technological pedagogical and content knowledge (TPACK) framework to characterize existing literature and identify possible gaps in the preparation of pre-service and in-service teachers in CT. Our results suggest that (1) existing evidence is limited to developed countries; (2) many studies are only focusing on teachers understanding the concepts but do not explore how the participants evaluate or create learning activities; (3) no studies look into classroom observations as part of the program, which limits our understanding to how these programs work; and (4) most programs use block-based programming languages as the tool to develop student CT. While block-based programming languages are used for introductory training programs, students are often expected to transfer their learning to more professional programming languages.",paper presents systematic literature review professional development programs computational thinking emerged essential set skills everyone develop participate global society however pre service service teacher programs integrate classrooms recently thus important identify educators researchers address challenges prepare next generation students gaps persist current literature review existing work field two perspectives first analyze learning outcomes assessment methods pedagogical approaches pedagogical tools used professional development programs second examine programs assess teachers knowledge skills outcomes used technological pedagogical content knowledge tpack framework characterize existing literature identify possible gaps preparation pre service service teachers results suggest existing evidence limited developed countries many studies focusing teachers understanding concepts explore participants evaluate create learning activities studies look classroom observations part program limits understanding programs work programs use block based programming languages tool develop student block based programming languages used introductory training programs students often expected transfer learning professional programming languages
"Separation logic’s compositionality and local reasoning properties have led to significant advances in scalable static analysis. But program analysis has new challenges—many programs displaycomputational effectsand, orthogonally, static analyzers must handleincorrectnesstoo. We present Outcome Separation Logic (OSL), a program logic that is sound for both correctness and incorrectness reasoning in programs with varying effects. OSL has a frame rule—just like separation logic—but uses different underlying assumptions that open up local reasoning to a larger class of properties than can be handled by any single existing logic.Building on this foundational theory, we also define symbolic execution algorithms that use bi-abduction to derive specifications for programs with effects. This involves a newtri-abductionprocedure to analyze programs whose execution branches due to effects such as nondeterministic or probabilistic choice. This work furthers the compositionality promised by separation logic by opening up the possibility for greater reuse of analysis tools across two dimensions: bug-finding vs verification in programs with varying effects.",separation logic compositionality local reasoning properties led significant advances scalable static analysis program analysis new challenges many programs displaycomputational effectsand orthogonally static analyzers must handleincorrectnesstoo present outcome separation logic osl program logic sound correctness incorrectness reasoning programs varying effects osl frame rule like separation logic uses different underlying assumptions open local reasoning larger class properties handled single existing logic building foundational theory also define symbolic execution algorithms use abduction derive specifications programs effects involves newtri abductionprocedure analyze programs whose execution branches due effects nondeterministic probabilistic choice work furthers compositionality promised separation logic opening possibility greater reuse analysis tools across two dimensions bug finding verification programs varying effects
"The problem arises from the lack of sufficient and comprehensive information about the necessary computer techniques. These techniques are crucial for developing information systems that assist doctors in diagnosing breast cancer, especially those related to positron emission tomography and computed tomography (PET/CT). Despite global efforts in breast cancer prevention and control, the scarcity of literature poses an obstacle to a complete understanding in this area of interest. The methodologies studied were systematic mapping and systematic literature review. For each article, the journal, conference, year of publication, dataset, breast cancer characteristics, PET/CT processing techniques, metrics and diagnostic yield results were identified. Sixty-four articles were analyzed, 44 (68.75%) belong to journals and 20 (31.25%) belong to the conference category. A total of 102 techniques were identified, which were distributed in preprocessing with 7 (6.86%), segmentation with 15 (14.71%), feature extraction with 15 (14.71%), and classification with 65 (63.73%). The techniques with the highest incidence identified in each stage are: Gaussian Filter, SLIC, Local Binary Pattern, and Support Vector Machine with 4, 2, 7, and 35 occurrences, respectively. Support Vector Machine is the predominant technique in the classification stage, due to the fact that Artificial Intelligence is emerging in medical image processing and health care to make expert systems increasingly intelligent and obtain favorable results.",problem arises lack sufficient comprehensive information necessary computer techniques techniques crucial developing information systems assist doctors diagnosing breast cancer especially related positron emission tomography computed tomography pet despite global efforts breast cancer prevention control scarcity literature poses obstacle complete understanding area interest methodologies studied systematic mapping systematic literature review article journal conference year publication dataset breast cancer characteristics pet processing techniques metrics diagnostic yield results identified sixty four articles analyzed belong journals belong conference category total techniques identified distributed preprocessing segmentation feature extraction classification techniques highest incidence identified stage gaussian filter slic local binary pattern support vector machine occurrences respectively support vector machine predominant technique classification stage due fact artificial intelligence emerging medical image processing health care make expert systems increasingly intelligent obtain favorable results
"Science and technology journalists today face challenges in finding newsworthy leads due to increased workloads, reduced resources, and expanding scientific publishing ecosystems. Given this context, we explore computational methods to aid these journalists' news discovery in terms of their agency and time-efficiency. We prototyped three computational information subsidies into an interactive tool that we used as a probe to better understand how such a tool may offer utility or more broadly shape the practices of professional science journalists. Our findings highlight central considerations around science journalists' user agency, contexts of use, and professional responsibility that such tools can influence and could account for in design. Based on this, we suggest design opportunities for enhancing and extending user agency over the longer-term; incorporating contextual, personal and collaborative notions of newsworthiness; and leveraging flexible interfaces and generative models. Overall, our findings contribute a richer view of the sociotechnical system around computational news discovery tools, and suggest ways to improve such tools to better support the practices of science journalists.",science technology journalists today face challenges finding newsworthy leads due increased workloads reduced resources expanding scientific publishing ecosystems given context explore computational methods aid journalists news discovery terms agency time efficiency prototyped three computational information subsidies interactive tool used probe better understand tool may offer utility broadly shape practices professional science journalists findings highlight central considerations around science journalists user agency contexts use professional responsibility tools influence could account design based suggest design opportunities enhancing extending user agency longer term incorporating contextual personal collaborative notions newsworthiness leveraging flexible interfaces generative models overall findings contribute richer view sociotechnical system around computational news discovery tools suggest ways improve tools better support practices science journalists
"Border Theory suggests individuals create borders to manage the transitions between work and family (or, more generally, life) domains. The degree of separation or integration of domains across borders has an impact on the balance between work and life. Previous studies have shown individuals who perceive balance between work and life domains tend to be more satisfied with their lives, reporting higher physical and mental health. At times of crisis, such as during a pandemic, borders can be disrupted, affecting work-life balance and leading to a short- or long-term negative impact on well-being. Border theory provides a systematic lens through which to study these changes. However, changes cannot be studied using interviews or diaries as these are not at the scale required when societal disruptions occur. In this paper, we explore the feasibility of using a computational linguistic approach to operationalize border theory at scale, using readily available social media data. In particular, we make two main contributions. First, we design metrics to measure key characteristics of borders. This involves the application of a transformer-based topic modeling technique, BERTopic, to detect topics from social media data. Second, we apply this operationalization to a case study of around a million tweets posted by nearly two hundred teachers and journalists in the UK from the beginning of 2019 to the end of 2022. In so doing, we longitudinally study and compare the changes in borders between work and life before, during, and after COVID-19 lockdown periods.",border theory suggests individuals create borders manage transitions work family generally life domains degree separation integration domains across borders impact balance work life previous studies shown individuals perceive balance work life domains tend satisfied lives reporting higher physical mental health times crisis pandemic borders disrupted affecting work life balance leading short long term negative impact well border theory provides systematic lens study changes however changes studied using interviews diaries scale required societal disruptions occur paper explore feasibility using computational linguistic approach operationalize border theory scale using readily available social media data particular make two main contributions first design metrics measure key characteristics borders involves application transformer based topic modeling technique bertopic detect topics social media data second apply operationalization case study around million tweets posted nearly two hundred teachers journalists beginning end longitudinally study compare changes borders work life covid lockdown periods
"Undoing computations of a concurrent system is beneficial in many situations, such as in reversible debugging of multi-threaded programs and in recovery from errors due to optimistic execution in parallel discrete event simulation. A number of approaches have been proposed for how to reverse formal models of concurrent computation, including process calculi such as CCS, languages like Erlang, and abstract models such as prime event structures and occurrence nets. However, it has not been settled as to what properties a reversible system should enjoy, nor how the various properties that have been suggested, such as the parabolic lemma and the causal-consistency property, are related. We contribute to a solution to these issues by using a generic labelled transition system equipped with a relation capturing whether transitions are independent to explore the implications between various reversibility properties. In particular, we show how all properties we consider are derivable from a set of axioms. Our intention is that when establishing properties of some formalism, it will be easier to verify the axioms rather than proving properties such as the parabolic lemma directly. We also introduce two new properties related to causal-consistent reversibility, namely causal liveness and causal safety, stating, respectively, that an action can be undone if (causal liveness) and only if (causal safety) it is independent from all of the following actions. These properties come in three flavours: defined in terms of independent transitions, independent events, or via an ordering on events. Both causal liveness and causal safety are derivable from our axioms.",undoing computations concurrent system beneficial many situations reversible debugging multi threaded programs recovery errors due optimistic execution parallel discrete event simulation number approaches proposed reverse formal models concurrent computation including process calculi ccs languages like erlang abstract models prime event structures occurrence nets however settled properties reversible system enjoy various properties suggested parabolic lemma causal consistency property related contribute solution issues using generic labelled transition system equipped relation capturing whether transitions independent explore implications various reversibility properties particular show properties consider derivable set axioms intention establishing properties formalism easier verify axioms rather proving properties parabolic lemma directly also introduce two new properties related causal consistent reversibility namely causal liveness causal safety stating respectively action undone causal liveness causal safety independent following actions properties come three flavours defined terms independent transitions independent events via ordering events causal liveness causal safety derivable axioms
"We study the existence of finite characterisations for modal formulas. A finite characterisation of a modal formula φ is a finite collection of positive and negative examples that distinguishes φ from every other, non-equivalent modal formula, where an example is a finite pointed Kripke structure. This definition can be restricted to specific frame classes and to fragments of the modal language: a modal fragment ℒ admits finite characterisations with respect to a frame class ℱ if every formula φ ∈ ℒ has a finite characterisation with respect to ℒ consisting of examples that are based on frames in ℱ. Finite characterisations are useful for illustration, interactive specification and debugging of formal specifications, and their existence is a precondition for exact learnability with membership queries. We show that the full modal language admits finite characterisations with respect to a frame class ℱ only when the modal logic of ℱ is locally tabular. We then study which modal fragments, freely generated by some set of connectives, admit finite characterisations. Our main result is that the positive modal language without the truth-constants ⊤ and ⊥ admits finite characterisations w.r.t. the class of all frames. This result is essentially optimal: finite characterisability fails when the language is extended with the truth constant ⊤ or ⊥ or with all but very limited forms of negation.",study existence finite characterisations modal formulas finite characterisation modal formula finite collection positive negative examples distinguishes every non equivalent modal formula example finite pointed kripke structure definition restricted specific frame classes fragments modal language modal fragment admits finite characterisations respect frame class every formula finite characterisation respect consisting examples based frames finite characterisations useful illustration interactive specification debugging formal specifications existence precondition exact learnability membership queries show full modal language admits finite characterisations respect frame class modal logic locally tabular study modal fragments freely generated set connectives admit finite characterisations main result positive modal language without truth constants admits finite characterisations class frames result essentially optimal finite characterisability fails language extended truth constant limited forms negation
"Public sector institutions that consult citizens to inform decision-making face the challenge of evaluating the contributions made by citizens. This evaluation has important democratic implications but at the same time, consumes substantial human resources. However, until now the use of artificial intelligence such as computer-supported text analysis has remained an under-studied solution to this problem. We identify three generic tasks in the evaluation process that could benefit from natural language processing (NLP). Based on a systematic literature search in two databases on computational linguistics and digital government, we provide a detailed review of existing methods and their performance. While some promising approaches exist, for instance to group data thematically and to detect arguments and opinions, we show that there remain important challenges before these could offer any reliable support in practice. These include the quality of results, the applicability to non-English language corpuses and making algorithmic models available to practitioners through software. We discuss a number of avenues that future research should pursue that can ultimately lead to solutions for practice. The most promising of these bring in the expertise of human evaluators, for example through active learning approaches or interactive topic modeling.",public sector institutions consult citizens inform decision making face challenge evaluating contributions made citizens evaluation important democratic implications time consumes substantial human resources however use artificial intelligence computer supported text analysis remained studied solution problem identify three generic tasks evaluation process could benefit natural language processing nlp based systematic literature search two databases computational linguistics digital government provide detailed review existing methods performance promising approaches exist instance group data thematically detect arguments opinions show remain important challenges could offer reliable support practice include quality results applicability non english language corpuses making algorithmic models available practitioners software discuss number avenues future research pursue ultimately lead solutions practice promising bring expertise human evaluators example active learning approaches interactive topic modeling
"VerbNet is a lexical resource for verbs that has many applications in natural language processing tasks, especially ones that require information about both the syntactic behavior and the semantics of verbs. This article presents an attempt to construct the first version of a Thai VerbNet corpus via data enrichment of the existing lexical resource. This corpus contains the annotation at both the syntactic and semantic levels, where verbs are tagged with frames within the verb class hierarchy and their arguments are labeled with the semantic role. We discuss the technical aspect of the construction process of Thai VerbNet and survey different semantic role labeling methods to make this process fully automatic. We also investigate the linguistic aspect of the computed verb classes and the results show the potential in assisting semantic classification and analysis. At the current stage, we have built the verb class hierarchy consisting of 28 verb classes from 112 unique concept frames over 490 unique verbs using our association rule learning method on Thai verbs.",verbnet lexical resource verbs many applications natural language processing tasks especially ones require information syntactic behavior semantics verbs article presents attempt construct first version thai verbnet corpus via data enrichment existing lexical resource corpus contains annotation syntactic semantic levels verbs tagged frames within verb class hierarchy arguments labeled semantic role discuss technical aspect construction process thai verbnet survey different semantic role labeling methods make process fully automatic also investigate linguistic aspect computed verb classes results show potential assisting semantic classification analysis current stage built verb class hierarchy consisting verb classes unique concept frames unique verbs using association rule learning method thai verbs
"Disruptive behavior is a prevalent threat to constructive online engagement. Covert behaviors, such as trolling, are especially challenging to detect automatically, because they utilize deceptive strategies to manipulate conversation. We illustrate a novel approach to their detection: analyzing conversational structures instead of focusing only on messages in isolation. Building on conversation analysis, we demonstrate that (1) conversational actions and their norms provide concepts for a deeper understanding of covert disruption, and that (2) machine learning, natural language processing and structural analysis of conversation can complement message-level features to create models that surpass earlier approaches to trolling detection. Our models, developed for detecting overt (aggression) as well as covert (trolling) behaviors using prior studies’ message-level features and new conversational action features, achieved high accuracies (0.90 and 0.92, respectively). The findings offer a theoretically grounded approach to computationally analyzing social media interaction and novel methods for effectively detecting covert disruptive conversations online.",disruptive behavior prevalent threat constructive online engagement covert behaviors trolling especially challenging detect automatically utilize deceptive strategies manipulate conversation illustrate novel approach detection analyzing conversational structures instead focusing messages isolation building conversation analysis demonstrate conversational actions norms provide concepts deeper understanding covert disruption machine learning natural language processing structural analysis conversation complement message level features create models surpass earlier approaches trolling detection models developed detecting overt aggression well covert trolling behaviors using prior studies message level features new conversational action features achieved high accuracies respectively findings offer theoretically grounded approach computationally analyzing social media interaction novel methods effectively detecting covert disruptive conversations online
"Wearable sensor-based human activity recognition (HAR) has gained significant attention due to the widespread use of smart wearable devices. However, variations in different subjects can cause a domain shift that impedes the scaling of the recognition model. Unsupervised domain adaptation has been proposed as a solution to recognize activities in new, unlabeled target domains by training the source and target data together. However, the need for accessing source data raises privacy concerns. Source-free domain adaptation has emerged as a practical setting, where only a pre-trained source model is provided for the unlabeled target domain. This setup aligns with the need for personalized activity model adaptation on target local devices. As the edge devices are resource-constrained with limited memory, it is crucial to take the computational efficiency, i.e., memory cost into consideration. In this paper, we develop a source-free domain adaptation framework for wearable sensor-based HAR, with a focus on computational efficiency for target edge devices. Firstly, we design a lightweight add-on module called adapter to adapt the frozen pre-trained model to the unlabeled target domain. Secondly, to optimize the adapter, we adopt a simple yet effective model adaptation method that leverages local representation similarity and prediction consistency. Additionally, we design a set of sample selection optimization strategies to select samples effective for adaptation and further enhance computational efficiency while maintaining adaptation performance. Our extensive experiments on three datasets demonstrate that our method achieves comparable recognition accuracy to the state-of-the-art source free domain adaptation methods with fewer than 1% of the parameters updated and saves up to 4.99X memory cost.",wearable sensor based human activity recognition har gained significant attention due widespread use smart wearable devices however variations different subjects cause domain shift impedes scaling recognition model unsupervised domain adaptation proposed solution recognize activities new unlabeled target domains training source target data together however need accessing source data raises privacy concerns source free domain adaptation emerged practical setting pre trained source model provided unlabeled target domain setup aligns need personalized activity model adaptation target local devices edge devices resource constrained limited memory crucial take computational efficiency memory cost consideration paper develop source free domain adaptation framework wearable sensor based har focus computational efficiency target edge devices firstly design lightweight add module called adapter adapt frozen pre trained model unlabeled target domain secondly optimize adapter adopt simple yet effective model adaptation method leverages local representation similarity prediction consistency additionally design set sample selection optimization strategies select samples effective adaptation enhance computational efficiency maintaining adaptation performance extensive experiments three datasets demonstrate method achieves comparable recognition accuracy state art source free domain adaptation methods fewer parameters updated saves memory cost
"The increasing prevalence of data breaches necessitates robust data protection measures in computational tasks. Secure computation outsourcing (SCO) presents a viable solution by safeguarding the confidentiality of inputs and outputs in data processing without disclosure. Nonetheless, this approach assumes the existence of a trustworthy coordinator to orchestrate and oversee the process, typically implying that data owners must fulfill this role themselves. In this paper, we consider secure delegated data processing (SDDP), an expanded data processing scenario wherein data owners simply delegate their data to SDDP providers for subsequent value mining or other downstream applications, eliminating the necessary involvement of data owners or trusted entities to dive into data processing deeply. However, general-purpose SDDP poses significant challenges in permitting the discretionary execution of computational tasks by SDDP providers on sensitive data while ensuring confidentiality. Existing approaches are insufficient to support SDDP in either efficiency or universality. To tackle this issue, we propose TGCB, a TEE-based General-purpose Computational Backend, designed to endow general-purpose computation with SDDP capabilities from an engineering perspective, powered by TEE-based code integrity and data confidentiality. Central to TGCB is the Encryption Programming Language (EPL) that defines computational tasks in SDDP. Specifically, SDDP providers can express arbitrary computable functions as EPL scripts, processed by TGCB's interfaces, securely interpreted and executed in TEE, ensuring data confidentiality throughout the process. As a universal computational backend, TGCB extensively bolsters data security in existing general-purpose computational tasks, allowing data owners to leverage SDDP without privacy concerns.",increasing prevalence data breaches necessitates robust data protection measures computational tasks secure computation outsourcing sco presents viable solution safeguarding confidentiality inputs outputs data processing without disclosure nonetheless approach assumes existence trustworthy coordinator orchestrate oversee process typically implying data owners must fulfill role paper consider secure delegated data processing sddp expanded data processing scenario wherein data owners simply delegate data sddp providers subsequent value mining downstream applications eliminating necessary involvement data owners trusted entities dive data processing deeply however general purpose sddp poses significant challenges permitting discretionary execution computational tasks sddp providers sensitive data ensuring confidentiality existing approaches insufficient support sddp either efficiency universality tackle issue propose tgcb tee based general purpose computational backend designed endow general purpose computation sddp capabilities engineering perspective powered tee based code integrity data confidentiality central tgcb encryption programming language epl defines computational tasks sddp specifically sddp providers express arbitrary computable functions epl scripts processed tgcb interfaces securely interpreted executed tee ensuring data confidentiality throughout process universal computational backend tgcb extensively bolsters data security existing general purpose computational tasks allowing data owners leverage sddp without privacy concerns
"We introduce and study the online pause and resume problem. In this problem, a player attempts to find the k lowest (alternatively, highest) prices in a sequence of fixed length T, which is revealed sequentially. At each time step, the player is presented with a price and decides whether to accept or reject it. The player incurs aswitching cost whenever their decision changes in consecutive time steps, i.e., whenever they pause or resume purchasing. This online problem is motivated by the goal of carbon-aware load shifting, where a workload may be paused during periods of high carbon intensity and resumed during periods of low carbon intensity and incurs a cost when saving or restoring its state. It has strong connections to existing problems studied in the literature on online optimization, though it introduces unique technical challenges that prevent the direct application of existing algorithms. Extending prior work on threshold-based algorithms, we introducedouble-threshold algorithms for both the minimization and maximization variants of this problem. We further show that the competitive ratios achieved by these algorithms are the best achievable by any deterministic online algorithm. Finally, we empirically validate our proposed algorithm through case studies on the application of carbon-aware load shifting using real carbon trace data and existing baseline algorithms.",introduce study online pause resume problem problem player attempts find lowest alternatively highest prices sequence fixed length revealed sequentially time step player presented price decides whether accept reject player incurs aswitching cost whenever decision changes consecutive time steps whenever pause resume purchasing online problem motivated goal carbon aware load shifting workload may paused periods high carbon intensity resumed periods low carbon intensity incurs cost saving restoring state strong connections existing problems studied literature online optimization though introduces unique technical challenges prevent direct application existing algorithms extending prior work threshold based algorithms introducedouble threshold algorithms minimization maximization variants problem show competitive ratios achieved algorithms best achievable deterministic online algorithm finally empirically validate proposed algorithm case studies application carbon aware load shifting using real carbon trace data existing baseline algorithms
"When you use the most popular computational methods for biological data analysis, have you checked whether their models are reasonable in your settings?",use popular computational methods biological data analysis checked whether models reasonable settings
"Fashion recommendation is a key research field in computational fashion research and has attracted considerable interest in the computer vision, multimedia, and information retrieval communities in recent years. Due to the great demand for applications, various fashion recommendation tasks, such as personalized fashion product recommendation, complementary (mix-and-match) recommendation, and outfit recommendation, have been posed and explored in the literature. The continuing research attention and advances impel us to look back and in-depth into the field for a better understanding. In this article, we comprehensively review recent research efforts on fashion recommendation from a technological perspective. We first introduce fashion recommendation at a macro level and analyze its characteristics and differences with general recommendation tasks. We then clearly categorize different fashion recommendation efforts into several sub-tasks and focus on each sub-task in terms of its problem formulation, research focus, state-of-the-art methods, and limitations. We also summarize the datasets proposed in the literature for use in fashion recommendation studies to give readers a brief illustration. Finally, we discuss several promising directions for future research in this field. Overall, this survey systematically reviews the development of fashion recommendation research. It also discusses the current limitations and gaps between academic research and the real needs of the fashion industry. In the process, we offer a deep insight into how the fashion industry could benefit from the computational technologies of fashion recommendation.",fashion recommendation key research field computational fashion research attracted considerable interest computer vision multimedia information retrieval communities recent years due great demand applications various fashion recommendation tasks personalized fashion product recommendation complementary mix match recommendation outfit recommendation posed explored literature continuing research attention advances impel look back depth field better understanding article comprehensively review recent research efforts fashion recommendation technological perspective first introduce fashion recommendation macro level analyze characteristics differences general recommendation tasks clearly categorize different fashion recommendation efforts several sub tasks focus sub task terms problem formulation research focus state art methods limitations also summarize datasets proposed literature use fashion recommendation studies give readers brief illustration finally discuss several promising directions future research field overall survey systematically reviews development fashion recommendation research also discusses current limitations gaps academic research real needs fashion industry process offer deep insight fashion industry could benefit computational technologies fashion recommendation
"This article revisits soundness and completeness of proof systems for proving that sets of states in infinite-state labeled transition systems satisfy formulas in the modal mu-calculus in order to develop proof techniques that permit the seamless inclusion of new features in this logic. Our approach relies on novel results in lattice theory, which give constructive characterizations of both greatest and least fixpoints of monotonic functions over complete lattices. We show how these results may be used to reason about the sound and complete tableau method for this problem due to Bradfield and Stirling. We also show how the flexibility of our lattice-theoretic basis simplifies reasoning about tableau-based proof strategies for alternative classes of systems. In particular, we extend the modal mu-calculus with timed modalities, and prove that the resulting tableau method is sound and complete for timed transition systems.",article revisits soundness completeness proof systems proving sets states infinite state labeled transition systems satisfy formulas modal calculus order develop proof techniques permit seamless inclusion new features logic approach relies novel results lattice theory give constructive characterizations greatest least fixpoints monotonic functions complete lattices show results may used reason sound complete tableau method problem due bradfield stirling also show flexibility lattice theoretic basis simplifies reasoning tableau based proof strategies alternative classes systems particular extend modal calculus timed modalities prove resulting tableau method sound complete timed transition systems
"After almost a decade of research, development of more efficient imprecise computational blocks is still a major concern in imprecise computing domain. There are many instances of the introduced imprecise components of different types, while their main difference is that they propose different precision-cost-performance trade-offs. In this paper, a novel comprehensive model for the imprecise components is introduced, which can be exploited to cover a wide range of precision-cost-performance trade-offs, for different types of imprecise components. The model helps to find the suitable imprecise component based on any desired error criterion. Therefore, the most significant advantage of the proposed model is that it can be simply exploited for design space exploration of different imprecise components to extract the suitable components, with the desired precision-cost-performance trade-off for any specific application. To demonstrate the efficiency of the proposed model, two novel families of Lowest-cost Imprecise Adders (LIAs) and Lowest-cost Imprecise Multipliers (LIMs) are introduced in the paper, which are systematically extracted based on exploration of the design space provided by the proposed model. A wide range of simulation and synthesis results are also presented in the paper to prove the comparable efficiency of the systematically extracted LIA/LIM structures with respect to the most efficient existing human-made imprecise components both individually and in a Multiply-Accumulate application.",almost decade research development efficient imprecise computational blocks still major concern imprecise computing domain many instances introduced imprecise components different types main difference propose different precision cost performance trade offs paper novel comprehensive model imprecise components introduced exploited cover wide range precision cost performance trade offs different types imprecise components model helps find suitable imprecise component based desired error criterion therefore significant advantage proposed model simply exploited design space exploration different imprecise components extract suitable components desired precision cost performance trade specific application demonstrate efficiency proposed model two novel families lowest cost imprecise adders lias lowest cost imprecise multipliers lims introduced paper systematically extracted based exploration design space provided proposed model wide range simulation synthesis results also presented paper prove comparable efficiency systematically extracted lia lim structures respect efficient existing human made imprecise components individually multiply accumulate application
"Computational thinking (CT) is playing an increasingly relevant role within disciplinary teaching in elementary school, particularly in science. However, many teachers are unfamiliar with CT, either because their education occurred before the popularization of CT or because CT instruction was not included in their pre-service coursework. For these teachers, CT professional development (PD) becomes a primary mechanism to close their CT knowledge gap. While CT PD has demonstrated success at increasing teacher's CT understanding, researchers have reported varied outcomes in supporting teachers to write CT-integrated lesson plans. To explore how we might support teachers to integrate CT into elementary science, we employed design-based research (DBR) in a dual-track design of in-class CT instruction for pre-service undergraduates within an elementary science methods class paired with a collaborative, multi-month PD opportunity for pre- and in-service teachers. In this article, we reflect on our 5-year period of DBR and present our design insights and implications for CT instruction and curriculum design from each iteration. Our findings on best practices will inform both teacher educators and PD providers within CT education. Our work will also be of interest to researchers considering DBR for technology-based educational projects.",computational thinking playing increasingly relevant role within disciplinary teaching elementary school particularly science however many teachers unfamiliar either education occurred popularization instruction included pre service coursework teachers professional development becomes primary mechanism close knowledge gap demonstrated success increasing teacher understanding researchers reported varied outcomes supporting teachers write integrated lesson plans explore might support teachers integrate elementary science employed design based research dbr dual track design class instruction pre service undergraduates within elementary science methods class paired collaborative multi month opportunity pre service teachers article reflect year period dbr present design insights implications instruction curriculum design iteration findings best practices inform teacher educators providers within education work also interest researchers considering dbr technology based educational projects
"Mixed Reality (MR) systems display content freely in space, and present nearly arbitrary amounts of information, enabling ubiquitous access to digital information. This approach, however, introduces clutter and distraction if too much virtual content is shown. We present BlendMR, an optimization-based MR system that blends virtual content onto the physical objects in users’ environments to serve as ambient information displays. Our approach takes existing 2D applications and meshes of physical objects as input. It analyses the geometry of the physical objects and identifies regions that are suitable hosts for virtual elements. Using a novel integer programming formulation, our approach then optimally maps selected contents of the 2D applications onto the object, optimizing for factors such as importance and hierarchy of information, viewing angle, and geometric distortion. We evaluate BlendMR by comparing it to a 2D window baseline. Study results show that BlendMR decreases clutter and distraction, and is preferred by users. We demonstrate the applicability of BlendMR in a series of results and usage scenarios.",mixed reality systems display content freely space present nearly arbitrary amounts information enabling ubiquitous access digital information approach however introduces clutter distraction much virtual content shown present blendmr optimization based system blends virtual content onto physical objects users environments serve ambient information displays approach takes existing applications meshes physical objects input analyses geometry physical objects identifies regions suitable hosts virtual elements using novel integer programming formulation approach optimally maps selected contents applications onto object optimizing factors importance hierarchy information viewing angle geometric distortion evaluate blendmr comparing window baseline study results show blendmr decreases clutter distraction preferred users demonstrate applicability blendmr series results usage scenarios
"Computational modeling has become a widespread approach for studying real-world phenomena by using different modeling perspectives, in particular, the microscopic point of view concentrates on the behavior of the single components and their interactions from which the global system evolution emerges, while the macroscopic point of view represents the system’s overall behavior abstracting as much as possible from that of the single components. The preferred point of view depends on the effort required to develop the model, on the detail level of the available information about the system to be modeled, and on the type of measures that are of interest to the modeler; each point of view may lead to a different modeling language and simulation paradigm. An approach adequate for the microscopic point of view is Agent-Based Modeling and Simulation, which has gained popularity in the last few decades but lacks a formal definition common to the different tools supporting it. This may lead to modeling mistakes and wrong interpretation of the results, especially when comparing models of the same system developed according to different points of view. The aim of the work described in this paper is to provide a common compositional modeling language from which both a macro and a micro simulation model can be automatically derived: these models are coherent by construction and may be studied through different simulation approaches and tools. A framework is thus proposed in which a model can be composed using a Petri Net formalism and then studied through both an Agent-Based Simulation and a classical Stochastic Simulation Algorithm, depending on the study goal.",computational modeling become widespread approach studying real world phenomena using different modeling perspectives particular microscopic point view concentrates behavior single components interactions global system evolution emerges macroscopic point view represents system overall behavior abstracting much possible single components preferred point view depends effort required develop model detail level available information system modeled type measures interest modeler point view may lead different modeling language simulation paradigm approach adequate microscopic point view agent based modeling simulation gained popularity last decades lacks formal definition common different tools supporting may lead modeling mistakes wrong interpretation results especially comparing models system developed according different points view aim work described paper provide common compositional modeling language macro micro simulation model automatically derived models coherent construction may studied different simulation approaches tools framework thus proposed model composed using petri net formalism studied agent based simulation classical stochastic simulation algorithm depending study goal
"Six recipients have received the ACM SIGHPC Computational and Data Science Fellowships for 2023. These highly competitive fellowships, funded exclusively by SIGHPC, are awarded after a rigorous merit-review and intended to increase the diversity of students pursuing graduate degrees in data science and computational science.",six recipients received acm sighpc computational data science fellowships highly competitive fellowships funded exclusively sighpc awarded rigorous merit review intended increase diversity students pursuing graduate degrees data science computational science
"Just as other disciplines, the humanities explore how computational research approaches and tools can meaningfully contribute to scholarly knowledge production. Building on related work from the areas of CSCW and HCI, we approach the design of computational tools through the analytical lens of 'human-AI collaboration.' Such work investigates how human competencies and computational capabilities can be effectively and meaningfully combined. However, there is no generalizable concept of what constitutes 'meaningful' human-AI collaboration. In terms of genuinely human competencies, we consider criticality and reflection as guiding principles of scholarly knowledge production and as deeply embedded in the methodologies and practices of the humanities. Although (designing for) reflection is a recurring topic in CSCW and HCI discourses, it has not been centered in work on human-AI collaboration. We posit that integrating both concepts is a viable approach to supporting 'meaningful' human-AI collaboration in the humanities and other qualitative, interpretivist, and hermeneutic research areas. Our research, thus, is guided by the question of how critical reflection can be enabled in human-AI collaboration. We address this question with a use case that centers on computer vision (CV) tools for art historical image retrieval. Specifically, we conducted a qualitative interview study with art historians to explore a) what potentials and affordances art historians ascribe to human-AI collaboration and CV in particular, and b) in what ways art historians conceptualize critical reflection in the context of human-AI collaboration. We extended the interviews with a think-aloud software exploration. We observed and recorded participants' interaction with a ready-to-use CV tool in a possible research scenario. We found that critical reflection, indeed, constitutes a core prerequisite for 'meaningful' human-AI collaboration in humanities research contexts. However, we observed that critical reflection was not fully realized during interaction with the CV tool. We interpret this divergence as supporting our hypothesis that computational tools need to be intentionally designed in such a way that they actively scaffold and support critical reflection during interaction. Based on our findings, we suggest four empirically grounded design implications for 'critical-reflective human-AI collaboration': supporting reflection on the basis of transparency, foregrounding epistemic presumptions, emphasizing the situatedness of data, and strengthening interpretability through contextualized explanations.",disciplines humanities explore computational research approaches tools meaningfully contribute scholarly knowledge production building related work areas cscw hci approach design computational tools analytical lens human collaboration work investigates human competencies computational capabilities effectively meaningfully combined however generalizable concept constitutes meaningful human collaboration terms genuinely human competencies consider criticality reflection guiding principles scholarly knowledge production deeply embedded methodologies practices humanities although designing reflection recurring topic cscw hci discourses centered work human collaboration posit integrating concepts viable approach supporting meaningful human collaboration humanities qualitative interpretivist hermeneutic research areas research thus guided question critical reflection enabled human collaboration address question use case centers computer vision tools art historical image retrieval specifically conducted qualitative interview study art historians explore potentials affordances art historians ascribe human collaboration particular ways art historians conceptualize critical reflection context human collaboration extended interviews think aloud software exploration observed recorded participants interaction ready use tool possible research scenario found critical reflection indeed constitutes core prerequisite meaningful human collaboration humanities research contexts however observed critical reflection fully realized interaction tool interpret divergence supporting hypothesis computational tools need intentionally designed way actively scaffold support critical reflection interaction based findings suggest four empirically grounded design implications critical reflective human collaboration supporting reflection basis transparency foregrounding epistemic presumptions emphasizing situatedness data strengthening interpretability contextualized explanations
"This paper examines craft's foundational relations to materials, techniques, and collaborative modes of teaching and learning, and these can be called upon to strengthen and extend computational craft as practiced in fields like CSCW and HCI. Drawing from literature in HCI, craft studies and Science and Technology Studies (STS), we explore craft's modern formation at the dawn of the Industrial Revolution across three formative sites: Scandinavian Slöyd, British Arts and Crafts, and Japanese Mingei. From this review we identify three key (and still evolving) features: craft's accountabilities to natural materials and local ecologies; craft's holistic ways of making with 'head, heart, and hand'; and craft's distinctly collaborative and embodied styles of teaching and learning. We then show how these lessons can be applied to contemporary practices and pedagogies of computational making. We argue that doing so can help to rebalance computation's ecological ties and relations, recenter its practice on a sensorially rich and 'whole-self' concept of making, and support more collaborative modes of teaching and learning that are inclusive, relational, and heterogeneous.",paper examines craft foundational relations materials techniques collaborative modes teaching learning called upon strengthen extend computational craft practiced fields like cscw hci drawing literature hci craft studies science technology studies sts explore craft modern formation dawn industrial revolution across three formative sites scandinavian british arts crafts japanese mingei review identify three key still evolving features craft accountabilities natural materials local ecologies craft holistic ways making head heart hand craft distinctly collaborative embodied styles teaching learning show lessons applied contemporary practices pedagogies computational making argue help rebalance computation ecological ties relations recenter practice sensorially rich whole self concept making support collaborative modes teaching learning inclusive relational heterogeneous
"Despite the benefits of expert interaction techniques, many users do not learn them and continue to use novice ones. This article aims at better understanding if, when and how users decide to learn and ultimately adopt expert interaction techniques. This dynamic learning process is a complex skill-acquisition and decision-making problem. We first present and compare three generic benchmark models, inspired by the neuroscience literature, to explain and predict the learning process for shortcut adoption. Results show that they do not account for the complexity of users’ behavior. We then introduce a dedicated model, Transition, combining five cognitive mechanisms: implicit and explicit learning, decay, planning and perseveration. Results show that our model outperforms the three benchmark models both in terms of model fitting and model simulation. Finally, a post-analysis shows that each of the five mechanisms contribute to goodness-of-fit, but the role of perseveration is unclear regarding model simulation.",despite benefits expert interaction techniques many users learn continue use novice ones article aims better understanding users decide learn ultimately adopt expert interaction techniques dynamic learning process complex skill acquisition decision making problem first present compare three generic benchmark models inspired neuroscience literature explain predict learning process shortcut adoption results show account complexity users behavior introduce dedicated model transition combining five cognitive mechanisms implicit explicit learning decay planning perseveration results show model outperforms three benchmark models terms model fitting model simulation finally post analysis shows five mechanisms contribute goodness fit role perseveration unclear regarding model simulation
"IFISS is an established MATLAB finite element software package for studying strategies for solving partial differential equations (PDEs). IFISS3D is a new add-on toolbox that extends IFISS capabilities for elliptic PDEs from two to three space dimensions. The open-source MATLAB framework provides a computational laboratory for experimentation and exploration of finite element approximation and error estimation, as well as iterative solvers. The package is designed to be useful as a teaching tool for instructors and students who want to learn about state-of-the-art finite element methodology. It will also be useful for researchers as a source of reproducible test matrices of arbitrarily large dimension.",ifiss established matlab finite element software package studying strategies solving partial differential equations pdes ifiss new add toolbox extends ifiss capabilities elliptic pdes two three space dimensions open source matlab framework provides computational laboratory experimentation exploration finite element approximation error estimation well iterative solvers package designed useful teaching tool instructors students want learn state art finite element methodology also useful researchers source reproducible test matrices arbitrarily large dimension
"In genome analysis, it is often important to identify variants from a reference genome. However, identifying variants that occur with low frequency can be challenging, as it is computationally intensive to do so accurately. LoFreq is a widely used program that is adept at identifying low-frequency variants. This article presents a design framework for an FPGA-based accelerator for LoFreq. In particular, this accelerator is targeted at virus analysis, which is particularly challenging, compared to human genome analysis, as the characteristics of the data to be analyzed are fundamentally different. Across the design space, this accelerator can achieve up to 120× speedups on the core computation of LoFreq and speedups of up to 51.7× across the entire program.",genome analysis often important identify variants reference genome however identifying variants occur low frequency challenging computationally intensive accurately lofreq widely used program adept identifying low frequency variants article presents design framework fpga based accelerator lofreq particular accelerator targeted virus analysis particularly challenging compared human genome analysis characteristics data analyzed fundamentally different across design space accelerator achieve speedups core computation lofreq speedups across entire program
"Despite the recent trend of computational origami for human-computer interaction (HCI) and digital fabrication, it is still difficult for designers to complete a series of design, simulation, and fabrication of objects leveraging computational origami theory. In this paper, we propose Crane, an integrated origami design platform implemented with Grasshopper. With this platform, users can seamlessly (1) design the 2D and 3D crease pattern, (2) simulate 3D folding transformation from the given crease pattern, (3) inversely find a new pattern under design constraints, (4) thicken the 2D pattern into a 3D volume along with the appropriate hinge structures for different fabrication methods, and (5) optionally connect the resulting design to other Rhinoceros or Grasshopper plugins for post-processes. To help understand how to use our system and demonstrate its feasibility, we showed three examples of origami products designed using our system. We also reported user feedback from the workshop as an evaluation.",despite recent trend computational origami human computer interaction hci digital fabrication still difficult designers complete series design simulation fabrication objects leveraging computational origami theory paper propose crane integrated origami design platform implemented grasshopper platform users seamlessly design crease pattern simulate folding transformation given crease pattern inversely find new pattern design constraints thicken pattern volume along appropriate hinge structures different fabrication methods optionally connect resulting design rhinoceros grasshopper plugins post processes help understand use system demonstrate feasibility showed three examples origami products designed using system also reported user feedback workshop evaluation
"Computational media describes a vision of software, which, in contrast to application-centric software, is (1) malleable, so users can modify existing functionality, (2) computable, so users can run custom code, (3) distributable, so users can open documents across different devices, and (4) shareable, so users can easily share and collaborate on documents. Over the last ten years, the Webstrates and Codestrates projects aimed at realizing this vision of computational media. Webstrates is a server application that synchronizes the DOM of websites. Codestrates builds on top of Webstrates and adds an authoring environment, which blurs the use and development of applications. Grounded in a chronology of the development of Webstrates and Codestrates, we present eight tensions that we needed to balance during their development. We use these tensions as an analytical lens in three case studies and a game challenge in which participants created games using Codestrates. We discuss the results of the game challenge based on these tensions and present key takeaways for six of them. Finally, we present six lessons learned from our endeavor to realize the vision of computational media, demonstrating the balancing act of weighing the vision against the pragmatics of implementing a working system.",computational media describes vision software contrast application centric software malleable users modify existing functionality computable users run custom code distributable users open documents across different devices shareable users easily share collaborate documents last ten years webstrates codestrates projects aimed realizing vision computational media webstrates server application synchronizes dom websites codestrates builds top webstrates adds authoring environment blurs use development applications grounded chronology development webstrates codestrates present eight tensions needed balance development use tensions analytical lens three case studies game challenge participants created games using codestrates discuss results game challenge based tensions present key takeaways six finally present six lessons learned endeavor realize vision computational media demonstrating balancing act weighing vision pragmatics implementing working system
"Using the association rules among both data mining (DM) and Artificial Intelligence (AI) technology, this paper proposes concepts and techniques for enhancing the curriculum environment based on the modernization of the educational field curriculum scheme of Ideological and Political Education (IPE) classes in universities and colleges. The study analyze the inter-subjectivity concept to the model development from the conceptual level, the benchmark value of the whole setting, multidimensional spacetime, and the leading importance of the Internet governance idea to the reformation of IPE in the colleges and universities. The findings assess the issues with IPE transformation and development in colleges and universities and suggest solutions, as well as four mechanisms of ""methodical design underlined in the overview, team-work cooperation of team development, augmentation of environmental development, and double endorsement of quality management”. The concept of developing a” three-stage full atmosphere"" network IPE architecture for universities and colleges is presented as one of the remedies to enhance the efficiency of the IPE in universities. This idea is based on experience summary, theoretical analysis, and empirical study. The timeliness concerns of IPE transformation in universities and colleges are resolved by using in-depth comprehension and evaluation of the necessary knowledge of DM-AI, which is then used to apply the appropriate DM-AI methodologies. This allows the relevant administrators to quickly learn the crucial data in complicated problems and suggest solutions for subsequent decision-making.",using association rules among data mining artificial intelligence technology paper proposes concepts techniques enhancing curriculum environment based modernization educational field curriculum scheme ideological political education ipe classes universities colleges study analyze inter subjectivity concept model development conceptual level benchmark value whole setting multidimensional spacetime leading importance internet governance idea reformation ipe colleges universities findings assess issues ipe transformation development colleges universities suggest solutions well four mechanisms methodical design underlined overview team work cooperation team development augmentation environmental development double endorsement quality management concept developing three stage full atmosphere network ipe architecture universities colleges presented one remedies enhance efficiency ipe universities idea based experience summary theoretical analysis empirical study timeliness concerns ipe transformation universities colleges resolved using depth comprehension evaluation necessary knowledge used apply appropriate methodologies allows relevant administrators quickly learn crucial data complicated problems suggest solutions subsequent decision making
"Tattoos are a highly popular medium, with both artistic and medical applications. Although the mechanical process of tattoo application has evolved historically, the results are reliant on the artisanal skill of the artist. This can be especially challenging for some skin tones, or in cases where artists lack experience. We provide the first systematic overview of tattooing as a computational fabrication technique. We built an automated tattooing rig and a recipe for the creation of silicone sheets mimicking realistic skin tones, which allowed us to create an accurate model predicting tattoo appearance. This enables several exciting applications including tattoo previewing, color retargeting, novel ink spectra optimization, color-accurate prosthetics, and more.",tattoos highly popular medium artistic medical applications although mechanical process tattoo application evolved historically results reliant artisanal skill artist especially challenging skin tones cases artists lack experience provide first systematic overview tattooing computational fabrication technique built automated tattooing rig recipe creation silicone sheets mimicking realistic skin tones allowed create accurate model predicting tattoo appearance enables several exciting applications including tattoo previewing color retargeting novel ink spectra optimization color accurate prosthetics
"Long exposure photography produces stunning imagery, representing moving elements in a scene with motion-blur. It is generally employed in two modalities, producing either a foreground or a background blur effect. Foreground blur images are traditionally captured on a tripod-mounted camera and portray blurred moving foreground elements, such as silky water or light trails, over a perfectly sharp background landscape. Background blur images, also called panning photography, are captured while the camera is tracking a moving subject, to produce an image of a sharp subject over a background blurred by relative motion. Both techniques are notoriously challenging and require additional equipment and advanced skills. In this paper, we describe a computational burst photography system that operates in a hand-held smartphone camera app, and achieves these effects fully automatically, at the tap of the shutter button. Our approach first detects and segments the salient subject. We track the scene motion over multiple frames and align the images in order to preserve desired sharpness and to produce aesthetically pleasing motion streaks. We capture an under-exposed burst and select the subset of input frames that will produce blur trails of controlled length, regardless of scene or camera motion velocity. We predict inter-frame motion and synthesize motion-blur to fill the temporal gaps between the input frames. Finally, we composite the blurred image with the sharp regular exposure to protect the sharpness of faces or areas of the scene that are barely moving, and produce a final high resolution and high dynamic range (HDR) photograph. Our system democratizes a capability previously reserved to professionals, and makes this creative style accessible to most casual photographers.",long exposure photography produces stunning imagery representing moving elements scene motion blur generally employed two modalities producing either foreground background blur effect foreground blur images traditionally captured tripod mounted camera portray blurred moving foreground elements silky water light trails perfectly sharp background landscape background blur images also called panning photography captured camera tracking moving subject produce image sharp subject background blurred relative motion techniques notoriously challenging require additional equipment advanced skills paper describe computational burst photography system operates hand held smartphone camera app achieves effects fully automatically tap shutter button approach first detects segments salient subject track scene motion multiple frames align images order preserve desired sharpness produce aesthetically pleasing motion streaks capture exposed burst select subset input frames produce blur trails controlled length regardless scene camera motion velocity predict inter frame motion synthesize motion blur fill temporal gaps input frames finally composite blurred image sharp regular exposure protect sharpness faces areas scene barely moving produce final high resolution high dynamic range hdr photograph system democratizes capability previously reserved professionals makes creative style accessible casual photographers
"Satisfiability Modulo Theories (SMT) refers to the problem of deciding the satisfiability of a formula with respect to certain background first-order theories. In this article, we focus on Satisfiablity Modulo Integer Arithmetic, which is referred to as SMT(IA), including both linear and non-linear integer arithmetic theories. Dominant approaches to SMT rely on calling a CDCL-based SAT solver, either in a lazy or eager flavour. Local search, a competitive approach to solving combinatorial problems including SAT, however, has not been well studied for SMT. We develop the first local-search algorithm for SMT(IA) by directly operating on variables, breaking through the traditional framework. We propose a local-search framework by considering the distinctions between Boolean and integer variables. Moreover, we design a novel operator and scoring functions tailored for integer arithmetic, as well as a two-level operation selection heuristic. Putting these together, we develop a local search SMT(IA) solver called LocalSMT. Experiments are carried out to evaluate LocalSMT on benchmark sets from SMT-LIB. The results show that LocalSMT is competitive and complementary with state-of-the-art SMT solvers, and performs particularly well on those formulae with only integer variables. A simple sequential portfolio with Z3 improves the state-of-the-art on satisfiable benchmark sets from SMT-LIB.",satisfiability modulo theories smt refers problem deciding satisfiability formula respect certain background first order theories article focus satisfiablity modulo integer arithmetic referred smt including linear non linear integer arithmetic theories dominant approaches smt rely calling cdcl based sat solver either lazy eager flavour local search competitive approach solving combinatorial problems including sat however well studied smt develop first local search algorithm smt directly operating variables breaking traditional framework propose local search framework considering distinctions boolean integer variables moreover design novel operator scoring functions tailored integer arithmetic well two level operation selection heuristic putting together develop local search smt solver called localsmt experiments carried evaluate localsmt benchmark sets smt lib results show localsmt competitive complementary state art smt solvers performs particularly well formulae integer variables simple sequential portfolio improves state art satisfiable benchmark sets smt lib
"First-order logic (FO) can express many algorithmic problems on graphs, such as the independent set and dominating set problem parameterized by solution size. However, FO cannot express the very simple algorithmic question whether two vertices are connected. We enrich FO with connectivity predicates that are tailored to express algorithmic graph problems that are commonly studied in parameterized algorithmics. By adding the atomic predicates connk(x,y,z_1,..., zk) that hold true in a graph if there exists a path between (the valuations of)xandyafter (the valuations of)z1,..., zkhave been deleted, we obtainseparator logicFO + conn. We show that separator logic can express many interesting problems, such as the feedback vertex set problem and elimination distance problems to first-order definable classes. Denote by FO + connkthe fragment of separator logic that is restricted to connectivity predicates with at mostk + 2variables (that is, at mostkdeletions), we show that FO + connk + 1is strictly more expressive than FO + connkfor allk ≥ 0. We then study the limitations of separator logic and prove that it cannot express planarity, and, in particular, not the disjoint paths problem. We obtain the strongerdisjoint-paths logicFO + DP by adding the atomic predicates disjoint-pathsk[(x1, y1),..., (xk, yk) that evaluate to true if there are internally vertex-disjoint paths between (the valuations of)xiandyifor all 1 ≤ i ≤ k. Disjoint-paths logic can express the disjoint paths problem, the problem of (topological) minor containment, the problem of hitting (topological) minors, and many more. Again, we show that the fragments FO + DPkthat use predicates for at mostkdisjoint paths form a strict hierarchy of expressiveness. Finally, we compare the expressive power of the new logics with that of transitive-closure logics and monadic second-order logic.",first order logic express many algorithmic problems graphs independent set dominating set problem parameterized solution size however express simple algorithmic question whether two vertices connected enrich connectivity predicates tailored express algorithmic graph problems commonly studied parameterized algorithmics adding atomic predicates connk hold true graph exists path valuations xandyafter valuations zkhave deleted obtainseparator logicfo conn show separator logic express many interesting problems feedback vertex set problem elimination distance problems first order definable classes denote connkthe fragment separator logic restricted connectivity predicates mostk variables mostkdeletions show connk strictly expressive connkfor allk study limitations separator logic prove express planarity particular disjoint paths problem obtain strongerdisjoint paths logicfo adding atomic predicates disjoint pathsk evaluate true internally vertex disjoint paths valuations xiandyifor disjoint paths logic express disjoint paths problem problem topological minor containment problem hitting topological minors many show fragments dpkthat use predicates mostkdisjoint paths form strict hierarchy expressiveness finally compare expressive power new logics transitive closure logics monadic second order logic
Enabling researchers to leverage systems to overcome the limits of human cognitive capacity.,enabling researchers leverage systems overcome limits human cognitive capacity
"Fog computing is a paradigm that allows the provisioning of computational resources and services at the edge of the network, closer to the end devices and users, complementing cloud computing. The heterogeneity and large number of devices are challenges to obtaining optimized resource allocation in this environment. Over time, some surveys have been presented on resource management in fog computing. However, they now lack a broader and deeper view about this subject, considering the recent publications. This article presents a systematic literature review with a focus on resource allocation for fog computing, and in a more comprehensive way than the existing works. The survey is based on 108 selected publications from 2012 to 2022. The analysis has exposed their main techniques, metrics used, evaluation tools, virtualization methods, architecture, and domains where the proposed solutions were applied. The results show an updated and comprehensive view about resource allocation in fog computing. The main challenges and open research questions are discussed, and a new fog computing resource management cycle is proposed.",fog computing paradigm allows provisioning computational resources services edge network closer end devices users complementing cloud computing heterogeneity large number devices challenges obtaining optimized resource allocation environment time surveys presented resource management fog computing however lack broader deeper view subject considering recent publications article presents systematic literature review focus resource allocation fog computing comprehensive way existing works survey based selected publications analysis exposed main techniques metrics used evaluation tools virtualization methods architecture domains proposed solutions applied results show updated comprehensive view resource allocation fog computing main challenges open research questions discussed new fog computing resource management cycle proposed
"Researchers worldwide have become increasingly interested in developing computational approaches to handle challenges facing electric vehicles (EVs) in recent years. This article examines the challenges and future potential of computational approaches for problems such as EV routing, EV charging scheduling (EVCS), EV charging station (CS) placement, CS sizing, and energy or load management. In addition, a summary of the fundamental mathematical models employed to solve the EV computational problems is presented. We cover recent work on computational solutions for various EV problems utilizing single and coupled mathematical models. Finally, we also examine potential research avenues that researchers could pursue to realize the objective of environment-friendly transportation and smart grids (SGs).",researchers worldwide become increasingly interested developing computational approaches handle challenges facing electric vehicles evs recent years article examines challenges future potential computational approaches problems routing charging scheduling evcs charging station placement sizing energy load management addition summary fundamental mathematical models employed solve computational problems presented cover recent work computational solutions various problems utilizing single coupled mathematical models finally also examine potential research avenues researchers could pursue realize objective environment friendly transportation smart grids sgs
"Autonomous systems that can assist humans with increasingly complex tasks are becoming ubiquitous. Moreover, it has been established that a human’s decision to rely on such systems is a function of both their trust in the system and their own self-confidence as it relates to executing the task of interest. Given that both under- and over-reliance on automation can pose significant risks to humans, there is motivation for developing autonomous systems that could appropriately calibrate a human’s trust or self-confidence to achieve proper reliance behavior. In this article, a computational model of coupled human trust and self-confidence dynamics is proposed. The dynamics are modeled as a partially observable Markov decision process without a reward function (POMDP/R) that leverages behavioral and self-report data as observations for estimation of these cognitive states. The model is trained and validated using data collected from 340 participants. Analysis of the transition probabilities shows that the proposed model captures the probabilistic relationship between trust, self-confidence, and reliance for all discrete combinations of high and low trust and self-confidence. The use of the proposed model to design an optimal policy to facilitate trust and self-confidence calibration is a goal of future work.",autonomous systems assist humans increasingly complex tasks becoming ubiquitous moreover established human decision rely systems function trust system self confidence relates executing task interest given reliance automation pose significant risks humans motivation developing autonomous systems could appropriately calibrate human trust self confidence achieve proper reliance behavior article computational model coupled human trust self confidence dynamics proposed dynamics modeled partially observable markov decision process without reward function pomdp leverages behavioral self report data observations estimation cognitive states model trained validated using data collected participants analysis transition probabilities shows proposed model captures probabilistic relationship trust self confidence reliance discrete combinations high low trust self confidence use proposed model design optimal policy facilitate trust self confidence calibration goal future work
"With the implementation of China's reform and opening up and China's accession to the World Trade Organization, Chinese business Korean translation plays an increasingly important role in international trade activities. Although the study of business Korean has aroused the strong research interest of scholars, its research focus is limited to word selection, grammatical transformation and various specific translation techniques, which is far from systematic. Good business translation should also involve factors such as the translator's role, cross-culture, and the client of translation. However, how to combine corpus with language teaching, make corpus enter the classroom and get practical application in daily language teaching is still in the exploratory stage. Most of the existing translation theories are tailored for literary translation, and business translation is a branch of non-literary translation, so these theories are difficult to guide business translation. Combined with the research results of translation studies and examples in business Korean, this paper discusses the characteristics and translation of business Korean as a special Korean, so as to provide reference for translation or related learners, and also want to attract more scholars to pay attention to business Korean and its translation.",implementation china reform opening china accession world trade organization chinese business korean translation plays increasingly important role international trade activities although study business korean aroused strong research interest scholars research focus limited word selection grammatical transformation various specific translation techniques far systematic good business translation also involve factors translator role cross culture client translation however combine corpus language teaching make corpus enter classroom get practical application daily language teaching still exploratory stage existing translation theories tailored literary translation business translation branch non literary translation theories difficult guide business translation combined research results translation studies examples business korean paper discusses characteristics translation business korean special korean provide reference translation related learners also want attract scholars pay attention business korean translation
"Argumentation is a well-established formalism dealing with conflicting information by generating and comparing arguments. It has been playing a major role in AI for decades. In logic-based argumentation, we explore the internal structure of an argument. Informally, a set of formulas is the support for a given claim if it is consistent, subset-minimal, and implies the claim. In such a case, the pair of the support and the claim together is called an argument. In this article, we study the propositional variants of the following three computational tasks studied in argumentation: ARG (exists a support for a given claim with respect to a given set of formulas), ARG-Check (is a given set a support for a given claim), and ARG-Rel (similarly as ARG plus requiring an additionally given formula to be contained in the support). ARG-Check is complete for the complexity class DP, and the other two problems are known to be complete for the second level of the polynomial hierarchy (Creignou et al. 2014 and Parson et al., 2003) and, accordingly, are highly intractable. Analyzing the reason for this intractability, we perform a two-dimensional classification: First, we consider all possible propositional fragments of the problem within Schaefer’s framework (STOC 1978) and then study different parameterizations for each of the fragments. We identify a list of reasonable structural parameters (size of the claim, support, knowledge base) that are connected to the aforementioned decision problems. Eventually, we thoroughly draw a fine border of parameterized intractability for each of the problems showing where the problems are fixed-parameter tractable and when this exactly stops. Surprisingly, several cases are of very high intractability (para-NP and beyond).",argumentation well established formalism dealing conflicting information generating comparing arguments playing major role decades logic based argumentation explore internal structure argument informally set formulas support given claim consistent subset minimal implies claim case pair support claim together called argument article study propositional variants following three computational tasks studied argumentation arg exists support given claim respect given set formulas arg check given set support given claim arg rel similarly arg plus requiring additionally given formula contained support arg check complete complexity class two problems known complete second level polynomial hierarchy creignou parson accordingly highly intractable analyzing reason intractability perform two dimensional classification first consider possible propositional fragments problem within schaefer framework stoc study different parameterizations fragments identify list reasonable structural parameters size claim support knowledge base connected aforementioned decision problems eventually thoroughly draw fine border parameterized intractability problems showing problems fixed parameter tractable exactly stops surprisingly several cases high intractability beyond
"Several forms of iterable belief change exist, differing in the kind of change and its strength: some operators introduce formulae, others remove them; some add formulae unconditionally, others only as additions to the previous beliefs; some only relative to the current situation, others in all possible cases. A sequence of changes may involve several of them: for example, the first step is a revision, the second a contraction and the third a refinement of the previous beliefs. The ten operators considered in this article are shown to be all reducible to three: lexicographic revision, refinement, and severe withdrawal. In turn, these three can be expressed in terms of lexicographic revision at the cost of restructuring the sequence. This restructuring needs not to be done explicitly: an algorithm that works on the original sequence is shown. The complexity of mixed sequences of belief change operators is also analyzed. Most of them require only a polynomial number of calls to a satisfiability checker, some are even easier.",several forms iterable belief change exist differing kind change strength operators introduce formulae others remove add formulae unconditionally others additions previous beliefs relative current situation others possible cases sequence changes may involve several example first step revision second contraction third refinement previous beliefs ten operators considered article shown reducible three lexicographic revision refinement severe withdrawal turn three expressed terms lexicographic revision cost restructuring sequence restructuring needs done explicitly algorithm works original sequence shown complexity mixed sequences belief change operators also analyzed require polynomial number calls satisfiability checker even easier
"Data-driven ontology-based knowledge (OK) presentation and computational linguistics for evolving semantic Asian social networks (ASNs) can make one of the most important platforms that provide robust and real-time data mapping in massive access across the heterogeneous big data sources in the web that is named OK-ASN. It benefits from computational intelligence, web-of-things (WoT) architecture, semantic features, statistical learning and pattern recognition, database management, computer vision, cyber-security, and language processing. OK-ASN is a critical strategy for WoT big data mining and enterprises from social media to medical and industrial sectors.",data driven ontology based knowledge presentation computational linguistics evolving semantic asian social networks asns make one important platforms provide robust real time data mapping massive access across heterogeneous big data sources web named asn benefits computational intelligence web things wot architecture semantic features statistical learning pattern recognition database management computer vision cyber security language processing asn critical strategy wot big data mining enterprises social media medical industrial sectors
"We investigate the optimal aesthetic location and size of a single dominant salient region in a photographic image. Existing algorithms for photographic composition do not take full account of the spatial positioning or sizes of these salient regions. We present a set of experiments to assess aesthetic preferences, inspired by theories of centeredness, principal lines, and Rule-of-Thirds. Our experimental results show a clear preference for the salient region to be centered in the image and that there is a preferred size of non-salient border around this salient region. We thus propose a novel image cropping mechanism for images containing a single salient region to achieve the best aesthetic balance. Our results show that the Rule-of-Thirds guideline is not generally valid but also allow us to hypothesize in which situations it is useful and in which it is inappropriate.",investigate optimal aesthetic location size single dominant salient region photographic image existing algorithms photographic composition take full account spatial positioning sizes salient regions present set experiments assess aesthetic preferences inspired theories centeredness principal lines rule thirds experimental results show clear preference salient region centered image preferred size non salient border around salient region thus propose novel image cropping mechanism images containing single salient region achieve best aesthetic balance results show rule thirds guideline generally valid also allow hypothesize situations useful inappropriate
"Proofs in propositional logic are typically presented as trees of derived formulas or, alternatively, as directed acyclic graphs of derived formulas. This distinction between tree-like vs. dag-like structure is particularly relevant when making quantitative considerations regarding, for example, proof size. Here we analyze a more general type of structural restriction for proofs in rule-based proof systems. In this definition, proofs are directed graphs of derived formulas in which cycles are allowed as long as every formula is derived at least as many times as it is required as a premise. We call such proofs “circular”. We show that, for all sets of standard inference rules with single or multiple conclusions, circular proofs are sound. We start the study of the proof complexity of circular proofs at Circular Resolution, the circular version of Resolution. We immediately see that Circular Resolution is stronger than dag-like Resolution since, as we show, the propositional encoding of the pigeonhole principle has circular Resolution proofs of polynomial size. Furthermore, for derivations of clauses from clauses, we show that Circular Resolution is, surprisingly, equivalent to Sherali-Adams, a proof system for reasoning through polynomial inequalities that has linear programming at its base. As corollaries we get: (1) polynomial-time (LP-based) algorithms that find Circular Resolution proofs of constant width, (2) examples that separate Circular from dag-like Resolution, such as the pigeonhole principle and its variants, and (3) exponentially hard cases for Circular Resolution. Contrary to the case of Circular Resolution, for Frege we show that circular proofs can be converted into tree-like proofs with at most polynomial overhead.",proofs propositional logic typically presented trees derived formulas alternatively directed acyclic graphs derived formulas distinction tree like dag like structure particularly relevant making quantitative considerations regarding example proof size analyze general type structural restriction proofs rule based proof systems definition proofs directed graphs derived formulas cycles allowed long every formula derived least many times required premise call proofs circular show sets standard inference rules single multiple conclusions circular proofs sound start study proof complexity circular proofs circular resolution circular version resolution immediately see circular resolution stronger dag like resolution since show propositional encoding pigeonhole principle circular resolution proofs polynomial size furthermore derivations clauses clauses show circular resolution surprisingly equivalent sherali adams proof system reasoning polynomial inequalities linear programming base corollaries get polynomial time based algorithms find circular resolution proofs constant width examples separate circular dag like resolution pigeonhole principle variants exponentially hard cases circular resolution contrary case circular resolution frege show circular proofs converted tree like proofs polynomial overhead
"Eager equality for algebraic expressions over partial algebras distinguishes or separates terms only if both have defined values and they are different. We consider arithmetical algebras with division as a partial operator, called meadows, and focus on algebras of rational numbers. To study eager equality, we use common meadows, which are totalisations of partial meadows by means of absorptive elements. An axiomatisation of common meadows is the basis of an axiomatisation of eager equality as a predicate on a common meadow. Applied to the rational numbers, we prove completeness and decidability of the equational theory of eager equality. To situate eager equality theoretically, we consider two other partial equalities of increasing strictness: Kleene equality, which is equivalent to the native equality of common meadows, and one we call cautious equality. Our methods of analysis for eager equality are quite general, and so we apply them to these two other partial equalities; and, in addition to common meadows, we use three other kinds of algebra designed to totalise division. In summary, we are able to compare 13 forms of equality for the partial meadow of rational numbers. We focus on the decidability of the equational theories of these equalities. We show that for the four total algebras, eager and cautious equality are decidable. We also show that for others the Diophantine Problem over the rationals is one-one computably reducible to their equational theories. The Diophantine Problem for rationals is a longstanding open problem. Thus, eager equality has substantially less complex semantics.",eager equality algebraic expressions partial algebras distinguishes separates terms defined values different consider arithmetical algebras division partial operator called meadows focus algebras rational numbers study eager equality use common meadows totalisations partial meadows means absorptive elements axiomatisation common meadows basis axiomatisation eager equality predicate common meadow applied rational numbers prove completeness decidability equational theory eager equality situate eager equality theoretically consider two partial equalities increasing strictness kleene equality equivalent native equality common meadows one call cautious equality methods analysis eager equality quite general apply two partial equalities addition common meadows use three kinds algebra designed totalise division summary able compare forms equality partial meadow rational numbers focus decidability equational theories equalities show four total algebras eager cautious equality decidable also show others diophantine problem rationals one one computably reducible equational theories diophantine problem rationals longstanding open problem thus eager equality substantially less complex semantics
"In 2018 I received a SIGCSE Special Projects award to fund creation of computational thinking and artificial intelligence curriculum materials for students in grades 3-5 using my Calypso for Cozmo robot programming framework (https://Calypso.software) and the Cozmo robot from Anki. The work was done in collaboration with the New Brighton Area School District in New Brighton, Pennsylvania. The project succeeded, then failed, and may now be reborn again.",received sigcse special projects award fund creation computational thinking artificial intelligence curriculum materials students grades using calypso cozmo robot programming framework https calypso software cozmo robot anki work done collaboration new brighton area school district new brighton pennsylvania project succeeded failed may reborn
"""Once upon a time on Tralfamdore there were creatures who weren't anything like machines...And these poor creatures were obsessed by the idea that everything that existed had to have a purpose...And the machines did everything so expertly that they were finally given the job of finding out what the highest purpose of the creatures could be...The machines reported that in all honesty that the creatures couldn't really be said to have any purpose...The creatures thereupon began slaying each other...And they discovered that they weren't even very good at slaying. So they turned that job over to the machines, too. And the machines finished up the job in less time than it takes to say, 'Tralfamadore'"" [45].",upon time tralfamdore creatures anything like machines poor creatures obsessed idea everything existed purpose machines everything expertly finally given job finding highest purpose creatures could machines reported honesty creatures really said purpose creatures thereupon began slaying discovered even good slaying turned job machines machines finished job less time takes say tralfamadore
"""[We have] really finally started to look at why we build things the way we build them, which is very archaic. I mean, the way we built structures and roads is ancient, you know, we're finally questioning do we have to do it this way? Like we're doing it just because that's the way we did it before. But then how we run them. No one has asked that same question. The way that we influence the people within these cities still stems back to feudal and pre feudal systems. I am the magnificent Lord, and I know how you should behave. So do this, or I will punish you. carry someone from A to B to make the right choices? What if we applied that at a civic level? What if we applied that in cities, because for the most part, [directing behavior takes place via] negative reinforcement. Do this, [you get a] fine, do this [you face] incarceration, but there's not a lot of positive design in civic or urban environments. I mean, there's very little, if any, at all. And so that's where I jumped in and really became fascinated in merging these ideas of art, entertainment and video games with this, behavioral activation, incentivizing the lay citizen to make good choices based on things they don't even know we're doing."" IoT program for parks and recreation for cities. Their slide deck was very focused on 'this is how much you could save as a city. This is how you could respond faster to vandalism or requisite services. This is how you can monitor if it's being used when it's not supposed to or without permits'. And it was so authoritarian. I mean, it was really genuinely like, you know, you're not using the way we told you to. And it took me about 30 seconds. I just said what if that exact same technology was just reused and reframed and said hey, the park is too busy considering COVID restrictions. You shouldn't go with your family. Here's a park less than half a mile down the street that's completely empty and probably safer for younger children. The exact same technology for a completely different purpose.""},",really finally started look build things way build archaic mean way built structures roads ancient know finally questioning way like way run one asked question way influence people within cities still stems back feudal pre feudal systems magnificent lord know behave punish carry someone make right choices applied civic level applied cities part directing behavior takes place via negative reinforcement get fine face incarceration lot positive design civic urban environments mean little jumped really became fascinated merging ideas art entertainment video games behavioral activation incentivizing lay citizen make good choices based things even know iot program parks recreation cities slide deck focused much could save city could respond faster vandalism requisite services monitor used supposed without permits authoritarian mean really genuinely like know using way told took seconds said exact technology reused reframed said hey park busy considering covid restrictions family park less half mile street completely empty probably safer younger children exact technology completely different purpose
"Although various dynamic or temporal logics have been proposed to verify quantum protocols and systems, these two viewpoints have not been studied comprehensively enough. We propose Linear Temporal Quantum Logic (LTQL), a linear temporal extension of quantum logic with a quantum implication, and extend it to Dynamic Linear Temporal Quantum Logic (DLTQL). This logic has temporal operators to express transitions by unitary operators (quantum gates) and dynamic ones to express those by projections (projective measurement). We then prove some logical properties of the relationship between these two transitions expressed by LTQL and DLTQL. A drawback in applying LTQL to the verification of quantum protocols is that these logics cannot express the future operator in linear temporal logic. We propose a way to mitigate this drawback by using a translation from (D)LTQL to Linear Temporal Modal Logic (LTML) and a simulation. This translation reduces the satisfiability problem of (D)LTQL formulas to that of LTML with the classical semantics over quantum states.",although various dynamic temporal logics proposed verify quantum protocols systems two viewpoints studied comprehensively enough propose linear temporal quantum logic ltql linear temporal extension quantum logic quantum implication extend dynamic linear temporal quantum logic dltql logic temporal operators express transitions unitary operators quantum gates dynamic ones express projections projective measurement prove logical properties relationship two transitions expressed ltql dltql drawback applying ltql verification quantum protocols logics express future operator linear temporal logic propose way mitigate drawback using translation ltql linear temporal modal logic ltml simulation translation reduces satisfiability problem ltql formulas ltml classical semantics quantum states
"To provide practice and assessment of computational thinking, we need specific problems students can solve. There are many such problems, but they are hard to find. Learning environments and assessments often use only specific types of problems and thus do not cover computational thinking in its whole scope. We provide an extensive catalog of well-structured computational thinking problem sets together with a systematic encoding of their features. Based on this encoding, we propose a four-level taxonomy that provides an organization of a wide variety of problems. The catalog, taxonomy, and problem features are useful for content authors, designers of learning environments, and researchers studying computational thinking.",provide practice assessment computational thinking need specific problems students solve many problems hard find learning environments assessments often use specific types problems thus cover computational thinking whole scope provide extensive catalog well structured computational thinking problem sets together systematic encoding features based encoding propose four level taxonomy provides organization wide variety problems catalog taxonomy problem features useful content authors designers learning environments researchers studying computational thinking
"Applying artificial intelligence to Chinese language translation in computational linguistics is of practical significance for economic boosts and cultural exchanges. In the present work, the bi-directional long short-term memory (BiLSTM) network is employed to extract Chinese text features regarding the overlapping semantic roles in Chinese language translation and hard-to-converge training of high-dimensional text word vectors in text classification during translation. In addition, AlexNet is optimized to extract the local features of the text and meanwhile update and learn network parameters in the deep network. Then, the attention mechanism is introduced to build a forecasting algorithm of Chinese language translation based on BiLSTM and improved AlexNet. Last, the forecasting algorithm is simulated to validate its performance. Some state-of-the-art algorithms are selected for a comparative experiment, including long short-term memory, regions with convolutional neural network features, AlexNet, and support vector machine. Results demonstrate that the forecasting algorithm proposed here can achieve a feature identification accuracy of 90.55%, at least an improvement of 4.24% over other algorithms. In addition, it provides an area under the curve of above 90%, a training duration of about 54.21 seconds, and a test duration of about 19.07 seconds. Regarding the performance of Chinese language translation, the algorithm proposed here provides a bilingual evaluation understudy (BLEU) value of 28.21 on the training set, with a performance gain ratio reaching 111.55%; on the test set, its BLEU reaches 40.45, with a performance gain ratio of 129.80%. Hence, this forecasting algorithm is notably superior to other algorithms, which can enhance the machine translation performance. Through experiments, the Chinese language translation algorithm constructed here improves translation performance while ensuring a high correct identification rate, providing experimental references for the later intelligent development of Chinese language translation in computational linguistics.",applying artificial intelligence chinese language translation computational linguistics practical significance economic boosts cultural exchanges present work directional long short term memory bilstm network employed extract chinese text features regarding overlapping semantic roles chinese language translation hard converge training high dimensional text word vectors text classification translation addition alexnet optimized extract local features text meanwhile update learn network parameters deep network attention mechanism introduced build forecasting algorithm chinese language translation based bilstm improved alexnet last forecasting algorithm simulated validate performance state art algorithms selected comparative experiment including long short term memory regions convolutional neural network features alexnet support vector machine results demonstrate forecasting algorithm proposed achieve feature identification accuracy least improvement algorithms addition provides area curve training duration seconds test duration seconds regarding performance chinese language translation algorithm proposed provides bilingual evaluation understudy bleu value training set performance gain ratio reaching test set bleu reaches performance gain ratio hence forecasting algorithm notably superior algorithms enhance machine translation performance experiments chinese language translation algorithm constructed improves translation performance ensuring high correct identification rate providing experimental references later intelligent development chinese language translation computational linguistics
"Using computational notebooks (e.g., Jupyter Notebook), data scientists rationalize their exploratory data analysis (EDA) based on their prior experience and external knowledge, such as online examples. For novices or data scientists who lack specific knowledge about the dataset or problem to investigate, effectively obtaining and understanding the external information is critical to carrying out EDA. This article presents EDAssistant, a JupyterLab extension that supports EDA with in situ search of example notebooks and recommendation of useful APIs, powered by novel interactive visualization of search results. The code search and recommendation are enabled by advanced machine learning models, trained on a large corpus of EDA notebooks collected online. A user study is conducted to investigate both EDAssistant and data scientists’ current practice (i.e., using external search engines). The results demonstrate the effectiveness and usefulness of EDAssistant, and participants appreciated its smooth and in-context support of EDA. We also report several design implications regarding code recommendation tools.",using computational notebooks jupyter notebook data scientists rationalize exploratory data analysis eda based prior experience external knowledge online examples novices data scientists lack specific knowledge dataset problem investigate effectively obtaining understanding external information critical carrying eda article presents edassistant jupyterlab extension supports eda situ search example notebooks recommendation useful apis powered novel interactive visualization search results code search recommendation enabled advanced machine learning models trained large corpus eda notebooks collected online user study conducted investigate edassistant data scientists current practice using external search engines results demonstrate effectiveness usefulness edassistant participants appreciated smooth context support eda also report several design implications regarding code recommendation tools
"Over the last 20 years, human interaction with robot swarms has been investigated as a means to mitigate problems associated with the control and coordination of such swarms by either human teleoperation or completely autonomous swarms. Ongoing research seeks to characterize those situations in which such interaction is both viable and preferable. In this article, we contribute to this effort by giving the first computational complexity analyses of problems associated with algorithm, environmental influence, and leader selection methods for the control of swarms performing distributed construction tasks. These analyses are done relative to a simple model in which swarms of deterministic finite-state robots operate in a synchronous error-free manner in 2D grid-based environments. We show that all three of our problems are polynomial-time intractable in general and remain intractable under a number of plausible restrictions (both individually and in many combinations) on robot controllers, environments, target structures, and sequences of swarm control commands. We also give the first restrictions relative to which these problems are tractable, as well as discussions of the implications of our results for both the design and deployment of swarm control assistance software tools and the human control of swarms.",last years human interaction robot swarms investigated means mitigate problems associated control coordination swarms either human teleoperation completely autonomous swarms ongoing research seeks characterize situations interaction viable preferable article contribute effort giving first computational complexity analyses problems associated algorithm environmental influence leader selection methods control swarms performing distributed construction tasks analyses done relative simple model swarms deterministic finite state robots operate synchronous error free manner grid based environments show three problems polynomial time intractable general remain intractable number plausible restrictions individually many combinations robot controllers environments target structures sequences swarm control commands also give first restrictions relative problems tractable well discussions implications results design deployment swarm control assistance software tools human control swarms
"In this article, we consider Answer Set Programming (ASP). It is a declarative problem solving paradigm that can be used to encode a problem as a logic program whose answer sets correspond to the solutions of the problem. It has been widely applied in various domains in AI and beyond. Given that answer sets are supposed to yield solutions to the original problem, the question of “why a set of atoms is an answer set” becomes important for both semantics understanding and program debugging. It has been well investigated for normal logic programs. However, for the class of disjunctive logic programs, which is a substantial extension of that of normal logic programs, this question has not been addressed much. In this article, we propose a notion of reduct for disjunctive logic programs and show how it can provide answers to the aforementioned question. First, we show that for each answer set, its reduct provides a resolution proof for each atom in it. We then further consider minimal sets of rules that will be sufficient to provide resolution proofs for sets of atoms. Such sets of rules will be called witnesses and are the focus of this article. We study complexity issues of computing various witnesses and provide algorithms for computing them. In particular, we show that the problem is tractable for normal and headcycle-free disjunctive logic programs, but intractable for general disjunctive logic programs. We also conducted some experiments and found that for many well-known ASP and SAT benchmarks, computing a minimal witness for an atom of an answer set is often feasible.",article consider answer set programming asp declarative problem solving paradigm used encode problem logic program whose answer sets correspond solutions problem widely applied various domains beyond given answer sets supposed yield solutions original problem question set atoms answer set becomes important semantics understanding program debugging well investigated normal logic programs however class disjunctive logic programs substantial extension normal logic programs question addressed much article propose notion reduct disjunctive logic programs show provide answers aforementioned question first show answer set reduct provides resolution proof atom consider minimal sets rules sufficient provide resolution proofs sets atoms sets rules called witnesses focus article study complexity issues computing various witnesses provide algorithms computing particular show problem tractable normal headcycle free disjunctive logic programs intractable general disjunctive logic programs also conducted experiments found many well known asp sat benchmarks computing minimal witness atom answer set often feasible
Advances in artificial intelligence permit computers to converse with humans in seemingly realistic ways.,advances artificial intelligence permit computers converse humans seemingly realistic ways
"The article discusses temporal information systems (TISs) that add the dimension of time to complete or incomplete information systems. Through TISs, one can accommodate the possibility of domains or attribute values for objects changing with time or the availability of currently missing information with time. Different patterns of flow of information give different TISs. The corresponding logics with sound and complete axiomatization are presented.",article discusses temporal information systems tiss add dimension time complete incomplete information systems tiss one accommodate possibility domains attribute values objects changing time availability currently missing information time different patterns flow information give different tiss corresponding logics sound complete axiomatization presented
"We develop a doubly exponential decision procedure for the satisfiability problem ofguarded separation logic—a novel fragment of separation logic featuring user-supplied inductive predicates, Boolean connectives, and separating connectives, including restricted (guarded) versions of negation, magic wand, and septraction. Moreover, we show that dropping the guards for any of the preceding connectives leads to an undecidable fragment.We further apply our decision procedure to reason aboutentailmentsin the popular symbolic heap fragment of separation logic. In particular, we obtain a doubly exponential decision procedure for entailments between (quantifier-free) symbolic heaps with inductive predicate definitions of bounded treewidth (SLbtw)—one of the most expressive decidable fragments of separation logic. Together with the recently shown2ExpTime-hardness for entailments in said fragment, we conclude that the entailment problem forSLbtwis2ExpTime-complete—thereby closing a previously open complexity gap.",develop doubly exponential decision procedure satisfiability problem ofguarded separation logic novel fragment separation logic featuring user supplied inductive predicates boolean connectives separating connectives including restricted guarded versions negation magic wand septraction moreover show dropping guards preceding connectives leads undecidable fragment apply decision procedure reason aboutentailmentsin popular symbolic heap fragment separation logic particular obtain doubly exponential decision procedure entailments quantifier free symbolic heaps inductive predicate definitions bounded treewidth slbtw one expressive decidable fragments separation logic together recently shown exptime hardness entailments said fragment conclude entailment problem forslbtwis exptime complete thereby closing previously open complexity gap
"Optimized SAT solvers not only preprocess the clause set, they also transform it during solving as inprocessing. Some preprocessing techniques have been generalized to first-order logic with equality. In this article, we port inprocessing techniques to work with superposition, a leading first-order proof calculus, and we strengthen known preprocessing techniques. Specifically, we look into elimination of hidden literals, variables (predicates), and blocked clauses. Our evaluation using the Zipperposition prover confirms that the new techniques usefully supplement the existing superposition machinery.",optimized sat solvers preprocess clause set also transform solving inprocessing preprocessing techniques generalized first order logic equality article port inprocessing techniques work superposition leading first order proof calculus strengthen known preprocessing techniques specifically look elimination hidden literals variables predicates blocked clauses evaluation using zipperposition prover confirms new techniques usefully supplement existing superposition machinery
"Within knowledge representation in artificial intelligence, a first-order ontology is a theory in first-order logic that axiomatizes the concepts in some domain. Ontology verification is concerned with the relationship between the intended models of an ontology and the models of the axiomatization of the ontology. In particular, we want to characterize the models of an ontology up to isomorphism and determine whether or not these models are equivalent to the intended models of the ontology. Unfortunately, it can be quite difficult to characterize the models of an ontology up to isomorphism. In the first half of this article, we review the different metalogical relationships between first-order theories and identify which relationship is needed for ontology verification. In particular, we will demonstrate that the notion of logical synonymy is needed to specify a representation theorem for the class of models of one first-order ontology with respect to another. In the second half of the article, we discuss the notion of reducible theories and show we can specify representation theorems by which models are constructed by amalgamating models of the constituent ontologies.",within knowledge representation artificial intelligence first order ontology theory first order logic axiomatizes concepts domain ontology verification concerned relationship intended models ontology models axiomatization ontology particular want characterize models ontology isomorphism determine whether models equivalent intended models ontology unfortunately quite difficult characterize models ontology isomorphism first half article review different metalogical relationships first order theories identify relationship needed ontology verification particular demonstrate notion logical synonymy needed specify representation theorem class models one first order ontology respect another second half article discuss notion reducible theories show specify representation theorems models constructed amalgamating models constituent ontologies
"Computational techniques offer a means to overcome the amplified complexity and resource-intensity of qualitative research on online communities. However, we lack an understanding of how these techniques are integrated by researchers in practice, and how to address concerns about researcher agency in the qualitative research process. To explore this gap, we deployed the Computational Thematic Analysis Toolkit to a team of public health researchers, and compared their analysis to a team working with traditional tools and methods. Each team independently conducted a thematic analysis of a corpus of comments from Canadian news sites to understand discourses around vaccine hesitancy. We then compared the analyses to investigate how computational techniques may have influenced their research process and outcomes. We found that the toolkit provided access to advanced computational techniques for researchers without programming expertise, facilitated their interaction and interpretation of the data, but also found that it influenced how they approached their thematic analysis.",computational techniques offer means overcome amplified complexity resource intensity qualitative research online communities however lack understanding techniques integrated researchers practice address concerns researcher agency qualitative research process explore gap deployed computational thematic analysis toolkit team public health researchers compared analysis team working traditional tools methods team independently conducted thematic analysis corpus comments canadian news sites understand discourses around vaccine hesitancy compared analyses investigate computational techniques may influenced research process outcomes found toolkit provided access advanced computational techniques researchers without programming expertise facilitated interaction interpretation data also found influenced approached thematic analysis
"With the advent of a new year and the impending end of my term as SIGCAS Chair it seems just about the right time to share some thoughts on SIGCAS, its mission and where we all stand in relation to it. I'd love to hear your thoughts and reactions to this, either directly via email, or on the SIGCAS-Talk distribution list or our Discord server.",advent new year impending end term sigcas chair seems right time share thoughts sigcas mission stand relation love hear thoughts reactions either directly via email sigcas talk distribution list discord server
"High-performance computing scientists are producing unprecedented volumes of data that take a long time to load for analysis. However, many analyses only require loading in the data containing particular features of interest and scientists have many approaches for identifying these features. Therefore, if scientists store information (descriptive metadata) about these identified features, then for subsequent analyses they can use this information to only read in the data containing these features. This can greatly reduce the amount of data that scientists have to read in, thereby accelerating analysis. Despite the potential benefits of descriptive metadata management, no prior work has created a descriptive metadata system that can help scientists working with a wide range of applications and analyses to restrict their reads to data containing features of interest. In this article, we present EMPRESS, the first such solution. EMPRESS offers all of the features needed to help accelerate discovery: It can accelerate analysis by up to 300 ×, supports a wide range of applications and analyses, is high-performing, is highly scalable, and requires minimal storage space. In addition, EMPRESS offers features required for a production-oriented system: scalable metadata consistency techniques, flexible system configurations, fault tolerance as a service, and portability.",high performance computing scientists producing unprecedented volumes data take long time load analysis however many analyses require loading data containing particular features interest scientists many approaches identifying features therefore scientists store information descriptive metadata identified features subsequent analyses use information read data containing features greatly reduce amount data scientists read thereby accelerating analysis despite potential benefits descriptive metadata management prior work created descriptive metadata system help scientists working wide range applications analyses restrict reads data containing features interest article present empress first solution empress offers features needed help accelerate discovery accelerate analysis supports wide range applications analyses high performing highly scalable requires minimal storage space addition empress offers features required production oriented system scalable metadata consistency techniques flexible system configurations fault tolerance service portability
Optical imaging technologies hold powerful potential in healthcare.,optical imaging technologies hold powerful potential healthcare
"In the past few decades,artificial intelligence (AI)technology has experienced swift developments, changing everyone’s daily life and profoundly altering the course of human society. The intention behind developing AI was and is to benefit humans by reducing labor, increasing everyday conveniences, and promoting social good. However, recent research and AI applications indicate that AI can cause unintentional harm to humans by, for example, making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against a group or groups. Consequently, trustworthy AI has recently garnered increased attention regarding the need to avoid the adverse effects that AI could bring to people, so people can fully trust and live in harmony with AI technologies.A tremendous amount of research on trustworthy AI has been conducted and witnessed in recent years. In this survey, we present a comprehensive appraisal of trustworthy AI from a computational perspective to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex subject, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety &amp; Robustness, (ii) Nondiscrimination &amp; Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability &amp; Auditability, and (vi) Environmental Well-being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.",past decades artificial intelligence technology experienced swift developments changing everyone daily life profoundly altering course human society intention behind developing benefit humans reducing labor increasing everyday conveniences promoting social good however recent research applications indicate cause unintentional harm humans example making unreliable decisions safety critical scenarios undermining fairness inadvertently discriminating group groups consequently trustworthy recently garnered increased attention regarding need avoid adverse effects could bring people people fully trust live harmony technologies tremendous amount research trustworthy conducted witnessed recent years survey present comprehensive appraisal trustworthy computational perspective help readers understand latest technologies achieving trustworthy trustworthy large complex subject involving various dimensions work focus six crucial dimensions achieving trustworthy safety amp robustness nondiscrimination amp fairness iii explainability privacy accountability amp auditability environmental well dimension review recent related technologies according taxonomy summarize applications real world systems also discuss accordant conflicting interactions among different dimensions discuss potential aspects trustworthy investigate future
"This paper reports on empirical work conducted to study perceptions of unfair treatment caused by automated computational systems. While the pervasiveness of algorithmic bias has been widely acknowledged, and perceptions of fairness are commonly studied in Human Computer Interaction, there is a lack of research on how unfair treatment by automated computational systems is experienced by users from disadvantaged and marginalised backgrounds. There is a need for more diversification in terms of the investigated users, domains, and tasks, and regarding the strategies that users employ to reduce harm. To unpack these issues, we ran a prescreened survey of 663 participants, oversampling those with at-risk characteristics. We collected occurrences and types of conflicts regarding unfair and discriminatory treatment and systems, as well as the actions taken towards resolving these situations. Drawing on intersectional research, we combine qualitative and quantitative approaches in order to highlight the nuances around power and privilege in the perceptions of automated computational systems. Among our participants, we discuss experiences of computational essentialism, attribute-based exclusion, and expected harm. We derive suggestions to address these perceptions of unfairness as they occur.",paper reports empirical work conducted study perceptions unfair treatment caused automated computational systems pervasiveness algorithmic bias widely acknowledged perceptions fairness commonly studied human computer interaction lack research unfair treatment automated computational systems experienced users disadvantaged marginalised backgrounds need diversification terms investigated users domains tasks regarding strategies users employ reduce harm unpack issues ran prescreened survey participants oversampling risk characteristics collected occurrences types conflicts regarding unfair discriminatory treatment systems well actions taken towards resolving situations drawing intersectional research combine qualitative quantitative approaches order highlight nuances around power privilege perceptions automated computational systems among participants discuss experiences computational essentialism attribute based exclusion expected harm derive suggestions address perceptions unfairness occur
"The 15th International Workshop on Computational Transportation Science (IWCTS 2022) is particularly timely given the prominence of human mobility data, such as probe data from cell phones and connected automated vehicles, volunteered geographic information, and other sensing data. This unprecedented access to sensing data of mobility, and of integration of this analytics into smart cities management has led to innovations in intelligent transportation systems, building information management, and urban planning. Due to the scale of the data, these developments are deeply computational.",international workshop computational transportation science iwcts particularly timely given prominence human mobility data probe data cell phones connected automated vehicles volunteered geographic information sensing data unprecedented access sensing data mobility integration analytics smart cities management led innovations intelligent transportation systems building information management urban planning due scale data developments deeply computational
"Computational support for learning in the domain of esports has seen a great deal of attention in recent years as an effective means of helping players learn and reap the benefits of play. However, previous work has not examined the tools from a learning theory perspective to assess if learning is prompted and supported in the right place and time. As a first step towards addressing this gap, this paper presents the results of two studies: a review of existing computational tools, and an online survey of esports' players' learning needs supplemented with qualitative interviews. Using Zimmerman's Cyclical Phase Model of Self-Regulated Learning as a lens, we identify patterns in the types of support offered by existing tools and players' support interests during different learning phases. We identify 11 opportunities for future research and development to better support self-regulated learning in esports.",computational support learning domain esports seen great deal attention recent years effective means helping players learn reap benefits play however previous work examined tools learning theory perspective assess learning prompted supported right place time first step towards addressing gap paper presents results two studies review existing computational tools online survey esports players learning needs supplemented qualitative interviews using zimmerman cyclical phase model self regulated learning lens identify patterns types support offered existing tools players support interests different learning phases identify opportunities future research development better support self regulated learning esports
"Bergstra and Klop have shown thatbisimilarityhas afiniteequational axiomatisation over ACP/CCS extended with the binaryleftandcommunication mergeoperators. Moller proved that auxiliary operators arenecessaryto obtain a finite axiomatisation of bisimilarity over CCS, and Aceto et al. showed that this remains true whenHennessy’s mergeis added to that language. These results raise the question of whether there isoneauxiliarybinaryoperator whose addition to CCS leads to a finite axiomatisation of bisimilarity. We contribute to answering this question in the simplified setting of the recursion-, relabelling-, and restriction-free fragment of CCS. We formulate three natural assumptions pertaining to the operational semantics of auxiliary operators and their relationship to parallel composition and prove that an auxiliary binary operator facilitating a finite axiomatisation of bisimilarity in the simplified setting cannot satisfy all three assumptions.",bergstra klop shown thatbisimilarityhas afiniteequational axiomatisation acp ccs extended binaryleftandcommunication mergeoperators moller proved auxiliary operators arenecessaryto obtain finite axiomatisation bisimilarity ccs aceto showed remains true whenhennessy mergeis added language results raise question whether isoneauxiliarybinaryoperator whose addition ccs leads finite axiomatisation bisimilarity contribute answering question simplified setting recursion relabelling restriction free fragment ccs formulate three natural assumptions pertaining operational semantics auxiliary operators relationship parallel composition prove auxiliary binary operator facilitating finite axiomatisation bisimilarity simplified setting satisfy three assumptions
"In this article, we study several aspects of the intersections of algorithmically random closed sets. First, we answer a question of Cenzer and Weber, showing that the operation of intersecting relatively random closed sets (random with respect to certain underlying measures induced by Bernoulli measures on the space of codes of closed sets), which preserves randomness, can be inverted: a random closed set of the appropriate type can be obtained as the intersection of two relatively random closed sets. We then extend the Cenzer/Weber analysis to the intersection of multiple random closed sets, identifying the Bernoulli measures with respect to which the intersection of relatively random closed sets can be non-empty. We lastly apply our analysis to provide a characterization of the effective Hausdorff dimension of sequences in terms of the degree of intersectability of random closed sets that contain them.",article study several aspects intersections algorithmically random closed sets first answer question cenzer weber showing operation intersecting relatively random closed sets random respect certain underlying measures induced bernoulli measures space codes closed sets preserves randomness inverted random closed set appropriate type obtained intersection two relatively random closed sets extend cenzer weber analysis intersection multiple random closed sets identifying bernoulli measures respect intersection relatively random closed sets non empty lastly apply analysis provide characterization effective hausdorff dimension sequences terms degree intersectability random closed sets contain
"This article presents a family of computational storage drives (CSDs) and demonstrates their performance and power improvements due to in-storage processing (ISP) when running big data analytics applications. CSDs are an emerging class of solid state drives that are capable of running user code while minimizing data transfer time and energy. Applications that can benefit from in situ processing include distributed training, distributed inferencing, and databases. To achieve the full advantage of the proposed ISP architecture, we propose software solutions for workload balancing before and at runtime for training and inferencing applications. Other applications such as sharding-based databases can readily take advantage of our ISP structure without additional tooling. Experimental results on different capacity and form factors of CSDs show up to 3.1× speedup in processing while reducing the energy consumption and data transfer by up to 67% and 68%, respectively, compared to regular enterprise solid state drives.",article presents family computational storage drives csds demonstrates performance power improvements due storage processing isp running big data analytics applications csds emerging class solid state drives capable running user code minimizing data transfer time energy applications benefit situ processing include distributed training distributed inferencing databases achieve full advantage proposed isp architecture propose software solutions workload balancing runtime training inferencing applications applications sharding based databases readily take advantage isp structure without additional tooling experimental results different capacity form factors csds show speedup processing reducing energy consumption data transfer respectively compared regular enterprise solid state drives
"This article provides a summary and review of ""Make It Meaningful: Taking Learning Design from Instructional to Transformational"" by Dr. Clark Quinn. The book consists of a section on principles and a section on practices for designing learning experiences that engage learners intellectually and emotionally. The book provides a primer and a playbook for educators to reflect upon and improve their personal practice.",article provides summary review make meaningful taking learning design instructional transformational clark quinn book consists section principles section practices designing learning experiences engage learners intellectually emotionally book provides primer playbook educators reflect upon improve personal practice
"After a highly competitive merit-review process, we are pleased to announce the eleven winners of our Computational and Data Science Fellowship for 2022. The fellowships are highly competitive and awarded after a rigorous merit review. The program, previously funded by Intel, is now funded exclusively by SIGHPC.",highly competitive merit review process pleased announce eleven winners computational data science fellowship fellowships highly competitive awarded rigorous merit review program previously funded intel funded exclusively sighpc
"This paper takes a landscape view of archives practice now operating in a sea of human digital behavior, interacting with computational systems embedded in real and virtual life, part of our complex global digital ecosystem driving cultural and social change. We envision a new computational archives framework, designed to be user-centric, in ways that integrate traditional archival practice into an overarching computational framework incorporating structured and unstructured data, computational tools,AI (artificial intelligence), ML (machine learning), robotics, and automation intended to aid in management and public engagement with physical, digitized, and born-digital documents. Set in a networked environment of increasing computing power, this “more than human” system derives from the latest computing advances fromNLP (natural language processing)and image recognition to artificial neural networks. We envision an archives system that is at once complex and integrated into a new inclusive and diverse cultural fabric. This paper covers general issues that have been accelerated by the Covid-19 pandemic, together with two institutional case studies.",paper takes landscape view archives practice operating human digital behavior interacting computational systems embedded real virtual life part complex global digital ecosystem driving cultural social change envision new computational archives framework designed user centric ways integrate traditional archival practice overarching computational framework incorporating structured unstructured data computational tools artificial intelligence machine learning robotics automation intended aid management public engagement physical digitized born digital documents set networked environment increasing computing power human system derives latest computing advances fromnlp natural language processing image recognition artificial neural networks envision archives system complex integrated new inclusive diverse cultural fabric paper covers general issues accelerated covid pandemic together two institutional case studies
"This investigation focuses on the application of computational intelligence to the security of Digital Twins (DTs) graphic data of the Cyber-physical System (CPS). The intricate and diverse physical space of CPS in the smart city is mapped in virtual space to construct the DTs CPS in the smart city. Besides, Differential Privacy Frequent Subgraph-Big Multigraph (DPFS-BM) is employed to ensure data privacy security. Moreover, the analysis and prediction model for the DTs big graphic data (BGD) in the CPS is built based on Differential Privacy-AlexNet (DP-AlexNet). Alexnet successfully solves the gradient dispersion problem of the Sigmoid function of deep network structures. Finally, the comparative analysis approach is utilized to verify the performance of the model reported here by comparing it with Long Short-Term Memory, Convolutional Neural Network, Recurrent Neural Network, original AlexNet, and Multi-Layer Perceptron in a simulation experiment. Through the comparison in the root mean square error, the mean absolute error, the mean absolute percentage error, training time, and test time, the model proposed here outperforms other models regarding errors, time delay, and time consumption. In the same environment, the system performs better with multi-hop paths, extra relays, and a high fading index; in that case, the outage probability is minimal. Therefore, the DP-AlexNet model is suitable for processing BGD. Moreover, its speed acceleration is more apparent than that of other models, with a higher SpeedUp indicator. The research effectively combines data mining and data security, which is of significant value for optimizing the privacy protection technology of frequent subgraph mining on a single multi-graph. Besides, the constructed DTs of CPS can provide excellent accuracy and a prominent acceleration effect on the premise of low errors. In addition, the model reported here can provide reference for the intelligent and digital development of smart cities.",investigation focuses application computational intelligence security digital twins dts graphic data cyber physical system cps intricate diverse physical space cps smart city mapped virtual space construct dts cps smart city besides differential privacy frequent subgraph big multigraph dpfs employed ensure data privacy security moreover analysis prediction model dts big graphic data bgd cps built based differential privacy alexnet alexnet alexnet successfully solves gradient dispersion problem sigmoid function deep network structures finally comparative analysis approach utilized verify performance model reported comparing long short term memory convolutional neural network recurrent neural network original alexnet multi layer perceptron simulation experiment comparison root mean square error mean absolute error mean absolute percentage error training time test time model proposed outperforms models regarding errors time delay time consumption environment system performs better multi hop paths extra relays high fading index case outage probability minimal therefore alexnet model suitable processing bgd moreover speed acceleration apparent models higher speedup indicator research effectively combines data mining data security significant value optimizing privacy protection technology frequent subgraph mining single multi graph besides constructed dts cps provide excellent accuracy prominent acceleration effect premise low errors addition model reported provide reference intelligent digital development smart cities
"In the mirror cup and saucer art created by artists Yul Cho and Sang-Ha Cho, part of the saucer is directly visible to the viewer, while the other part of the saucer is occluded and can only be seen as a reflection through a mirror cup. Thus, viewers see an image directly on the saucer and another image on the mirror cup; however, the existing art design is limited to wavelike saucers. In this work, we propose a general computational framework for mirror cup and saucer art design. As input, we take from the user one image for the direct view, one image for the reflected view, and the base shape of the saucer. Our algorithm then generates a suitable saucer shape by deforming the input shape. We formulate this problem as a constrained optimization for the saucer surface. Our framework solves for the fine geometry details on the base shape along with its texture, such that when a mirror cup is placed on the saucer, the user-specified images are observed as direct and reflected views. Through extensive experiments, we demonstrate the effectiveness of our framework and the great design flexibility that it offers to users. We further validate the produced art pieces by fabricating the colored saucers using three-dimensional printing.",mirror cup saucer art created artists yul cho sang cho part saucer directly visible viewer part saucer occluded seen reflection mirror cup thus viewers see image directly saucer another image mirror cup however existing art design limited wavelike saucers work propose general computational framework mirror cup saucer art design input take user one image direct view one image reflected view base shape saucer algorithm generates suitable saucer shape deforming input shape formulate problem constrained optimization saucer surface framework solves fine geometry details base shape along texture mirror cup placed saucer user specified images observed direct reflected views extensive experiments demonstrate effectiveness framework great design flexibility offers users validate produced art pieces fabricating colored saucers using three dimensional printing
Incorporating data thinking into computer science education.,incorporating data thinking computer science education
"This work proposes a novel generative design tool for passive grippers---robot end effectors that have no additional actuation and instead leverage the existing degrees of freedom in a robotic arm to perform grasping tasks. Passive grippers are used because they offer interesting trade-offs between cost and capabilities. However, existing designs are limited in the types of shapes that can be grasped. This work proposes to use rapid-manufacturing and design optimization to expand the space of shapes that can be passively grasped. Our novel generative design algorithm takes in an object and its positioning with respect to a robotic arm and generates a 3D printable passive gripper that can stably pick the object up. To achieve this, we address the key challenge of jointly optimizing the shape and the insert trajectory to ensure a passively stable grasp. We evaluate our method on a testing suite of 22 objects (23 experiments), all of which were evaluated with physical experiments to bridge the virtual-to-real gap. Code and data are at https://homes.cs.washington.edu/~milink/passive-gripper/",work proposes novel generative design tool passive grippers robot end effectors additional actuation instead leverage existing degrees freedom robotic arm perform grasping tasks passive grippers used offer interesting trade offs cost capabilities however existing designs limited types shapes grasped work proposes use rapid manufacturing design optimization expand space shapes passively grasped novel generative design algorithm takes object positioning respect robotic arm generates printable passive gripper stably pick object achieve address key challenge jointly optimizing shape insert trajectory ensure passively stable grasp evaluate method testing suite objects experiments evaluated physical experiments bridge virtual real gap code data https homes washington edu milink passive gripper
"We propose a method for computing a sewing pattern of a given 3D garment model. Our algorithm segments an input 3D garment shape into patches and computes their 2D parameterization, resulting in pattern pieces that can be cut out of fabric and sewn together to manufacture the garment. Unlike the general state-of-the-art approaches for surface cutting and flattening, our method explicitly targets garment fabrication. It accounts for the unique properties and constraints of tailoring, such as seam symmetry, the usage of darts, fabric grain alignment, and a flattening distortion measure that models woven fabric deformation, respecting its anisotropic behavior. We bootstrap a recent patch layout approach developed for quadrilateral remeshing and adapt it to the purpose of computational pattern making, ensuring that the deformation of each pattern piece stays within prescribed bounds of cloth stress. While our algorithm can automatically produce the sewing patterns, it is fast enough to admit user input to creatively iterate on the pattern design. Our method can take several target poses of the 3D garment into account and integrate them into the sewing pattern design. We demonstrate results on both skintight and loose garments, showcasing the versatile application possibilities of our approach.",propose method computing sewing pattern given garment model algorithm segments input garment shape patches computes parameterization resulting pattern pieces cut fabric sewn together manufacture garment unlike general state art approaches surface cutting flattening method explicitly targets garment fabrication accounts unique properties constraints tailoring seam symmetry usage darts fabric grain alignment flattening distortion measure models woven fabric deformation respecting anisotropic behavior bootstrap recent patch layout approach developed quadrilateral remeshing adapt purpose computational pattern making ensuring deformation pattern piece stays within prescribed bounds cloth stress algorithm automatically produce sewing patterns fast enough admit user input creatively iterate pattern design method take several target poses garment account integrate sewing pattern design demonstrate results skintight loose garments showcasing versatile application possibilities approach
"The use of design thinking (DT) as a pedagogical and problem-solving strategy has been gaining interest in technical and professional communication (TPC) for years, and Jason Tham'sDesign Thinking in Technical Communicationis the best and most comprehensive statement on this topic that our discipline has created yet. The book first overviews its central concepts (DT and ""making""), then illustrates very concretely how those concepts can improve pedagogy, social advocacy, and collaboration in TPC. All the book's chapters (except the conclusion and first chapter) contain empirical elements, which Tham uses to support his points.",use design thinking pedagogical problem solving strategy gaining interest technical professional communication tpc years jason tham sdesign thinking technical communicationis best comprehensive statement topic discipline created yet book first overviews central concepts making illustrates concretely concepts improve pedagogy social advocacy collaboration tpc book chapters except conclusion first chapter contain empirical elements tham uses support points
"Today's Hybrid Transactional and Analytical Processing (HTAP) systems, tackle the ever-growing data in combination with a mixture of transactional and analytical workloads. While optimizing for aspects such as data freshness and performance isolation, they build on the traditional data-to-code principle and may trigger massive cold data transfers that impair the overall performance and scalability. Firstly, in this paper we show that Near-Data Processing (NDP) naturally fits in the HTAP design space. Secondly, we propose an NDP database architecture, allowing transactionally consistent in-situ executions of analytical operations in HTAP settings. We evaluate the proposed architecture in state-of-the-art key/value-stores and multi-versioned DBMS. In contrast to traditional setups, our approach yields robust, resource- and cost-efficient performance.",today hybrid transactional analytical processing htap systems tackle ever growing data combination mixture transactional analytical workloads optimizing aspects data freshness performance isolation build traditional data code principle may trigger massive cold data transfers impair overall performance scalability firstly paper show near data processing ndp naturally fits htap design space secondly propose ndp database architecture allowing transactionally consistent situ executions analytical operations htap settings evaluate proposed architecture state art key value stores multi versioned dbms contrast traditional setups approach yields robust resource cost efficient performance
"We describe a general and safe computational framework that provides integer programming results with the degree of certainty that is required for machine-assisted proofs of mathematical theorems. At its core, the framework relies on a rational branch-and-bound certificate produced by an exact integer programming solver, SCIP, in order to circumvent floating-point round-off errors present in most state-of-the-art solvers for mixed-integer programs. The resulting certificates are self-contained and checker software exists that can verify their correctness independently of the integer programming solver used to produce the certificate. This acts as a safeguard against programming errors that may be present in complex solver software. The viability of this approach is tested by applying it to finite cases of Chvátal’s conjecture, a long-standing open question in extremal combinatorics. We take particular care to verify also the correctness of the input for this specific problem, using the Coq formal proof assistant. As a result, we are able to provide the first machine-assisted proof that Chvátal’s conjecture holds for all downsets whose union of sets contains seven elements or less.",describe general safe computational framework provides integer programming results degree certainty required machine assisted proofs mathematical theorems core framework relies rational branch bound certificate produced exact integer programming solver scip order circumvent floating point round errors present state art solvers mixed integer programs resulting certificates self contained checker software exists verify correctness independently integer programming solver used produce certificate acts safeguard programming errors may present complex solver software viability approach tested applying finite cases chvátal conjecture long standing open question extremal combinatorics take particular care verify also correctness input specific problem using coq formal proof assistant result able provide first machine assisted proof chvátal conjecture holds downsets whose union sets contains seven elements less
Improving the peer review process in a scientific manner shows promise.,improving peer review process scientific manner shows promise
Standardizing computational reuse and portability with the Common Workflow Language.,standardizing computational reuse portability common workflow language
"This article presents Computational SRAM (C-SRAM) solution combining In- and Near-Memory Computing approaches. It allows performing arithmetic, logic, and complex memory operations inside or next to the memory without transferring data over the system bus, leading to significant energy reduction. Operations are performed on large vectors of data occupying the entire physical row of C-SRAM array, leading to high performance gains. We introduce the C-SRAM solution in this article as an integrated vector processing unit to be used by a scalar processor as an energy-efficient and high performing co-processor. We detail the C-SRAM system design on different levels: (i) circuit design and silicon proof of concept, (ii) system interface and instruction set architecture, and (iii) high-level software programming and simulation. Experimental results on two complete memory-bound applications, AES and MobileNetV2, show that the C-SRAM implementation achieves up to 70× timing speedup and 37× energy reduction compared to scalar architecture, and up to 17× timing speedup and 5× energy reduction compared to SIMD architecture.",article presents computational sram sram solution combining near memory computing approaches allows performing arithmetic logic complex memory operations inside next memory without transferring data system bus leading significant energy reduction operations performed large vectors data occupying entire physical row sram array leading high performance gains introduce sram solution article integrated vector processing unit used scalar processor energy efficient high performing processor detail sram system design different levels circuit design silicon proof concept system interface instruction set architecture iii high level software programming simulation experimental results two complete memory bound applications aes mobilenetv show sram implementation achieves timing speedup energy reduction compared scalar architecture timing speedup energy reduction compared simd architecture
"We consider the operation of sum on Kripke frames, where a family of frames-summands is indexed by elements of another frame. In many cases, the modal logic of sums inherits the finite model property and decidability from the modal logic of summands [Babenyshev and Rybakov2010; Shapirovsky2018]. In this paper we show that, under a general condition, the satisfiability problem on sums is polynomial space Turing reducible to the satisfiability problem on summands. In particular, for many modal logics decidability in PSpace is an immediate corollary from the semantic characterization of the logic.",consider operation sum kripke frames family frames summands indexed elements another frame many cases modal logic sums inherits finite model property decidability modal logic summands babenyshev rybakov shapirovsky paper show general condition satisfiability problem sums polynomial space turing reducible satisfiability problem summands particular many modal logics decidability pspace immediate corollary semantic characterization logic
"As a historical and ornamental building element, muqarnas are widely found among the entrances of madrasas, mosques, and hans in Anatolian Seljuk architecture. In Kayseri (Turkey), muqarnas structures are characterized by symmetrical distribution of patterned geometric layers that presents computational rules for the design and construction of these ornamental structures. The presented research focuses on 12 unique muqarnas structures that are analyzed through a computational methodology combining photogrammetry, three-dimensional modeling, symmetry, and graph theory. The computational analysis shows that Seljukid muqarnas exhibit patterned branching of the symmetry axis between layers radiating from their geometric center. Using the modeled samples, the article analyzes inherent symmetry rules and growth patterns while offering a novel way of studying, modeling, and categorizing muqarnas.",historical ornamental building element muqarnas widely found among entrances madrasas mosques hans anatolian seljuk architecture kayseri turkey muqarnas structures characterized symmetrical distribution patterned geometric layers presents computational rules design construction ornamental structures presented research focuses unique muqarnas structures analyzed computational methodology combining photogrammetry three dimensional modeling symmetry graph theory computational analysis shows seljukid muqarnas exhibit patterned branching symmetry axis layers radiating geometric center using modeled samples article analyzes inherent symmetry rules growth patterns offering novel way studying modeling categorizing muqarnas
"Birkedal et al. recently introduced dependent right adjoints as an important class of (non-fibered) modalities in type theory. We observe that several aspects of their calculus are left underdeveloped and that it cannot serve as an internal language. We resolve these problems by assuming that the modal context operator is a parametric right adjoint. We show that this hitherto unrecognized structure is common. Based on these discoveries we present a new well-behaved Fitch-style multimodal type theory, which can be used as an internal language. Finally, we apply this syntax to guarded recursion and parametricity.",birkedal recently introduced dependent right adjoints important class non fibered modalities type theory observe several aspects calculus left underdeveloped serve internal language resolve problems assuming modal context operator parametric right adjoint show hitherto unrecognized structure common based discoveries present new well behaved fitch style multimodal type theory used internal language finally apply syntax guarded recursion parametricity
"Distributed quantum systems and especially the Quantum Internet have the ever-increasing potential to fully demonstrate the power of quantum computation. This is particularly true given that developing a general-purpose quantum computer is much more difficult than connecting many small quantum devices. One major challenge of implementing distributed quantum systems is programming them and verifying their correctness. In this paper, we propose a CSP-like distributed programming language to facilitate the specification and verification of such systems. After presenting its operational and denotational semantics, we develop a Hoare-style logic for distributed quantum programs and establish its soundness and (relative) completeness with respect to both partial and total correctness. The effectiveness of the logic is demonstrated by its applications in the verification of quantum teleportation and local implementation of non-local CNOT gates, two important algorithms widely used in distributed quantum systems.",distributed quantum systems especially quantum internet ever increasing potential fully demonstrate power quantum computation particularly true given developing general purpose quantum computer much difficult connecting many small quantum devices one major challenge implementing distributed quantum systems programming verifying correctness paper propose csp like distributed programming language facilitate specification verification systems presenting operational denotational semantics develop hoare style logic distributed quantum programs establish soundness relative completeness respect partial total correctness effectiveness logic demonstrated applications verification quantum teleportation local implementation non local cnot gates two important algorithms widely used distributed quantum systems
"It was a typical Friday morning. I had finished breakfast and moved on to my new practice of spending 15 minutes following a Shaolin breathing and movement exercise that I hoped would increase my flexibility and calm in my body. With just about 1 minute left in the routine it shifts to some massage movements. The first is to rub your hands to relax then from the particular poses they have been in. I started to rub my right hand with my left. All of a sudden, I had this eerie sensation that someone else was touching my hand and arm. I looked around. No one was in the room. The feeling persisted. I spoke aloud. ""Stop it; your scaring me."" I felt dizzy and nauseous. I went to grab the remote in order to turn off the TV. ""What; it was right here a minute ago."" At this point, I couldn't care about the TV. I decided I needed to lie down. I made my way up the first flight of stairs in our split-level house and for reasons that I could not articulate I felt uncertain that I could not make it upstairs. I called out tentatively to my husband who was working in the study and responded, ""Is everything ok?"". My response, ""I don't know"".",typical friday morning finished breakfast moved new practice spending minutes following shaolin breathing movement exercise hoped would increase flexibility calm body minute left routine shifts massage movements first rub hands relax particular poses started rub right hand left sudden eerie sensation someone else touching hand arm looked around one room feeling persisted spoke aloud stop scaring felt dizzy nauseous went grab remote order turn right minute ago point care decided needed lie made way first flight stairs split level house reasons could articulate felt uncertain could make upstairs called tentatively husband working study responded everything response know
"Despite the widespread adoption of computational notebooks, little is known about best practices for their usage in collaborative contexts. In this paper, we fill this gap by eliciting a catalog of best practices for collaborative data science with computational notebooks. With this aim, we first look for best practices through a multivocal literature review. Then, we conduct interviews with professional data scientists to assess their awareness of these best practices. Finally, we assess the adoption of best practices through the analysis of 1,380 Jupyter notebooks retrieved from the Kaggle platform. Findings reveal that experts are mostly aware of the best practices and tend to adopt them in their daily work. Nonetheless, they do not consistently follow all the recommendations as, depending on specific contexts, some are deemed unfeasible or counterproductive due to the lack of proper tool support. As such, we envision the design of notebook solutions that allow data scientists not to have to prioritize exploration and rapid prototyping over writing code of quality.",despite widespread adoption computational notebooks little known best practices usage collaborative contexts paper fill gap eliciting catalog best practices collaborative data science computational notebooks aim first look best practices multivocal literature review conduct interviews professional data scientists assess awareness best practices finally assess adoption best practices analysis jupyter notebooks retrieved kaggle platform findings reveal experts mostly aware best practices tend adopt daily work nonetheless consistently follow recommendations depending specific contexts deemed unfeasible counterproductive due lack proper tool support envision design notebook solutions allow data scientists prioritize exploration rapid prototyping writing code quality
"Co-creative proccesses between people can be characterized by rich dialogue that carries each person's ideas into the collaborative space. When people co-create an artifact that is both technical and aesthetic, their dialogue reflects the interplay between these two dimensions. However, the dialogue mechanisms that express this interplay and the extent to which they are related to outcomes, such as peer satisfaction, are not well understood. This paper reports on a study of 68 high school learner dyads' textual dialogues as they create music by writing code together in a digital learning environment for musical remixing. We report on a novel dialogue taxonomy built to capture the technical and aesthetic dimensions of learners' collaborative dialogues. We identified dialogue act n-grams (sequences of length 1, 2, or 3) that are present within the corpus and discovered five significant n-gram predictors for whether a learner felt satisfied with their partner during the collaboration. The learner was more likely to report higher satisfaction with their partner when the learner frequently acknowledges their partner, exchanges positive feedback with their partner, and their partner proposes an idea and elaborates on the idea. In contrast, the learner is more likely to report lower satisfaction with their partner when the learner frequently accepts back-to-back proposals from their partner and when the partner responds to the learner's statements with positive feedback. This work advances understanding of collaborative dialogue within co-creative domains and suggests dialogue strategies that may be helpful to foster co-creativity as learners collaborate to produce a creative artifact. The findings also suggest important areas of focus for intelligent or adaptive systems that aim to support learners during the co-creative process.",creative proccesses people characterized rich dialogue carries person ideas collaborative space people create artifact technical aesthetic dialogue reflects interplay two dimensions however dialogue mechanisms express interplay extent related outcomes peer satisfaction well understood paper reports study high school learner dyads textual dialogues create music writing code together digital learning environment musical remixing report novel dialogue taxonomy built capture technical aesthetic dimensions learners collaborative dialogues identified dialogue act grams sequences length present within corpus discovered five significant gram predictors whether learner felt satisfied partner collaboration learner likely report higher satisfaction partner learner frequently acknowledges partner exchanges positive feedback partner partner proposes idea elaborates idea contrast learner likely report lower satisfaction partner learner frequently accepts back back proposals partner partner responds learner statements positive feedback work advances understanding collaborative dialogue within creative domains suggests dialogue strategies may helpful foster creativity learners collaborate produce creative artifact findings also suggest important areas focus intelligent adaptive systems aim support learners creative process
"Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",experimental results often plotted dimensional graphical plots aka graphs scientific domains depicting dependent versus independent variables aid visual analysis processes repeatedly performing laboratory experiments consumes significant time resources motivating need computational estimation goals estimate graph obtained experiment given input conditions estimate conditions would lead desired graph existing estimation approaches often meet accuracy efficiency needs targeted applications develop computational estimation approach called autodomainmine integrates clustering classification complex scientific data framework automate classical learning methods scientists knowledge discovered thereby database existing experiments serves basis estimation challenges include preserving domain semantics clustering finding matching strategies classification striking good balance elaboration conciseness displaying estimation results based needs targeted users deriving objective measures capture subjective user interests challenges addressed work autodomainmine approach used build computational estimation system rigorously evaluated real data materials science evaluation confirms autodomainmine provides desired accuracy efficiency computational estimation extendable science engineering domains proved adaptation sub processes within fields bioinformatics nanotechnology
"This article presents a re-classification of information seeking (IS) tasks, concepts, and algorithms. The proposed taxonomy provides new dimensions to look into information seeking tasks and methods. The new dimensions include number of search iterations, search goal types, and procedures to reach these goals. Differences along these dimensions for the information seeking tasks call for suitable computational solutions. The article then reviews machine learning solutions that match each new category. The article ends with a review of evaluation campaigns for IS systems.",article presents classification information seeking tasks concepts algorithms proposed taxonomy provides new dimensions look information seeking tasks methods new dimensions include number search iterations search goal types procedures reach goals differences along dimensions information seeking tasks call suitable computational solutions article reviews machine learning solutions match new category article ends review evaluation campaigns systems
"We give an overview of theoretical and practical aspects of finding a simple polygon of minimum (Min-Area) or maximum (Max-Area) possible area for a given set ofnpoints in the plane. Both problems are known to beNP-hard and were the subject of the 2019 Computational Geometry Challenge, which presented the quest of finding good solutions to more than 200 instances, ranging fromn= 10 all the way ton= 1, 000, 000.",give overview theoretical practical aspects finding simple polygon minimum min area maximum max area possible area given set ofnpoints plane problems known benp hard subject computational geometry challenge presented quest finding good solutions instances ranging fromn way ton
"This paper presents a computational framework for modeling biobehavioral rhythms - the repeating cycles of physiological, psychological, social, and environmental events - from mobile and wearable data streams. The framework incorporates four main components: mobile data processing, rhythm discovery, rhythm modeling, and machine learning. We evaluate the framework with two case studies using datasets of smartphone, Fitbit, and OURA smart ring to evaluate the framework’s ability to (1) detect cyclic biobehavior, (2) model commonality and differences in rhythms of human participants in the sample datasets, and (3) predict their health and readiness status using models of biobehavioral rhythms. Our evaluation demonstrates the framework’s ability to generate new knowledge and findings through rigorous micro- and macro-level modeling of human rhythms from mobile and wearable data streams collected in the wild and using them to assess and predict different life and health outcomes.",paper presents computational framework modeling biobehavioral rhythms repeating cycles physiological psychological social environmental events mobile wearable data streams framework incorporates four main components mobile data processing rhythm discovery rhythm modeling machine learning evaluate framework two case studies using datasets smartphone fitbit oura smart ring evaluate framework ability detect cyclic biobehavior model commonality differences rhythms human participants sample datasets predict health readiness status using models biobehavioral rhythms evaluation demonstrates framework ability generate new knowledge findings rigorous micro macro level modeling human rhythms mobile wearable data streams collected wild using assess predict different life health outcomes
"There is little empirical research related to how elementary students develop computational thinking (CT) and how they apply CT in problem-solving. To address this gap in knowledge, this study made use of learning trajectories (LTs; hypothesized learning goals, progressions, and activities) in CT concept areas such as sequence, repetition, conditionals, and decomposition to better understand students’ CT. This study implemented eight math-CT integrated lessons aligned to U.S. national mathematics education standards and the LTs with third- and fourth-grade students. This basic interpretive qualitative study aimed at gaining a deeper understanding of elementary students’ CT by having students express and articulate their CT in cognitive interviews. Participants’ (n= 22) CT articulation was examined usinga prioricodes translated verbatim from the learning goals in the LTs and was mapped to the learning goals in the LTs. Results revealed a range of students’ CT in problem-solving, such as using precise and complete problem-solving instructions, recognizing repeating patterns, and decomposing arithmetic problems. By collecting empirical data on how students expressed and articulated their CT, this study makes theoretical contributions by generating initial empirical evidence to support the hypothesized learning goals and progressions in the LTs. This article also discusses the implications for integrated CT instruction and assessments at the elementary level.",little empirical research related elementary students develop computational thinking apply problem solving address gap knowledge study made use learning trajectories lts hypothesized learning goals progressions activities concept areas sequence repetition conditionals decomposition better understand students study implemented eight math integrated lessons aligned national mathematics education standards lts third fourth grade students basic interpretive qualitative study aimed gaining deeper understanding elementary students students express articulate cognitive interviews participants articulation examined usinga prioricodes translated verbatim learning goals lts mapped learning goals lts results revealed range students problem solving using precise complete problem solving instructions recognizing repeating patterns decomposing arithmetic problems collecting empirical data students expressed articulated study makes theoretical contributions generating initial empirical evidence support hypothesized learning goals progressions lts article also discusses implications integrated instruction assessments elementary level
"The purpose of this study is to examine the role of Minecraft-based coding activities on computational thinking (CT) of middle school students. In the study, CT was conceptualized so that it encapsulates not only the knowledge of computational concepts (e.g., loops and conditionals) but also the use of CT practices (e.g., testing and debugging). Data were collected using a combination of knowledge of computational concepts tests, the Minecraft-based coding artifacts, and one-on-one student interviews focusing on the processes of developing computational artifacts. The participants were 20 fifth-grade middle school students from a low-income public school with very limited (if none) formal computer programming experiences before the study. The Minecraft-based coding activities were designed and implemented as an instructional program to last 6 weeks. The results of the study showed a statistically significant increase in students’ knowledge of computational concepts. Based on the analysis of the students’ final coding artifacts, we identified that students mostly used the concepts of sequences, events, loops, and parallelism correctly, whereas variables, operators, and conditionals appeared to be the least successfully used concepts. The qualitative analysis of the artifact-based interviews showed that students employed the CT practices of testing and debugging most of the time while developing an artifact through coding. In contrast, the least resorted CT practice appeared to be reusing and remixing.",purpose study examine role minecraft based coding activities computational thinking middle school students study conceptualized encapsulates knowledge computational concepts loops conditionals also use practices testing debugging data collected using combination knowledge computational concepts tests minecraft based coding artifacts one one student interviews focusing processes developing computational artifacts participants fifth grade middle school students low income public school limited none formal computer programming experiences study minecraft based coding activities designed implemented instructional program last weeks results study showed statistically significant increase students knowledge computational concepts based analysis students final coding artifacts identified students mostly used concepts sequences events loops parallelism correctly whereas variables operators conditionals appeared least successfully used concepts qualitative analysis artifact based interviews showed students employed practices testing debugging time developing artifact coding contrast least resorted practice appeared reusing remixing
"Computational notebooks allow data scientists to express their ideas through a combination of code and documentation. However, data scientists often pay attention only to the code, and neglect creating or updating their documentation during quick iterations. Inspired by human documentation practices learned from 80 highly-voted Kaggle notebooks, we design and implement Themisto, an automated documentation generation system to explore how human-centered AI systems can support human data scientists in the machine learning code documentation scenario. Themisto facilitates the creation of documentation via three approaches: a deep-learning-based approach to generate documentation for source code, a query-based approach to retrieve online API documentation for source code, and a user prompt approach to nudge users to write documentation. We evaluated Themisto in a within-subjects experiment with 24 data science practitioners, and found that automated documentation generation techniques reduced the time for writing documentation, reminded participants to document code they would have ignored, and improved participants’ satisfaction with their computational notebook.",computational notebooks allow data scientists express ideas combination code documentation however data scientists often pay attention code neglect creating updating documentation quick iterations inspired human documentation practices learned highly voted kaggle notebooks design implement themisto automated documentation generation system explore human centered systems support human data scientists machine learning code documentation scenario themisto facilitates creation documentation via three approaches deep learning based approach generate documentation source code query based approach retrieve online api documentation source code user prompt approach nudge users write documentation evaluated themisto within subjects experiment data science practitioners found automated documentation generation techniques reduced time writing documentation reminded participants document code would ignored improved participants satisfaction computational notebook
"As online communities have grown, Computational Social Science has rapidly developed new techniques to study them. However, these techniques require researchers to become experts in a wide variety of tools in addition to qualitative and computational research methods. Studying online communities also requires researchers to constantly navigate highly contextual ethical and transparency considerations when engaging with data, such as respecting their members' privacy when discussing sensitive or stigmatized topics. To overcome these challenges, we developed the Computational Thematic Analysis Toolkit, a modular software package that supports analysis of online communities by combining aspects of reflexive thematic analysis with computational techniques. Our toolkit demonstrates how common analysis tasks like data collection, cleaning and filtering, modelling and sampling, and coding can be implemented within a single visual interface, and how that interface can encourage researchers to manage ethical and transparency considerations throughout their research process.",online communities grown computational social science rapidly developed new techniques study however techniques require researchers become experts wide variety tools addition qualitative computational research methods studying online communities also requires researchers constantly navigate highly contextual ethical transparency considerations engaging data respecting members privacy discussing sensitive stigmatized topics overcome challenges developed computational thematic analysis toolkit modular software package supports analysis online communities combining aspects reflexive thematic analysis computational techniques toolkit demonstrates common analysis tasks like data collection cleaning filtering modelling sampling coding implemented within single visual interface interface encourage researchers manage ethical transparency considerations throughout research process
"Colour vision deficiency is a common visual impairment that cannot be compensated for using optical lenses in traditional glasses, and currently remains untreatable. In our work, we report on research on Computational Glasses for compensating colour vision deficiency. While existing research only showed corrected images within the periphery or as an indirect aid, Computational Glasses build on modified standard optical see-through head-mounted displays and directly modulate the user’s vision, consequently adapting their perception of colours. In this work, we present an exhaustive literature review of colour vision deficiency compensation and subsequent findings; several prototypes with varying advantages—from well-controlled bench prototypes to less controlled but higher application portable prototypes; and a series of studies evaluating our approach starting with proving its efficacy, comparing to the state-of-the-art, and extending beyond static lab prototypes looking at real world applicability. Finally, we evaluated directions for future compensation methods for computational glasses.",colour vision deficiency common visual impairment compensated using optical lenses traditional glasses currently remains untreatable work report research computational glasses compensating colour vision deficiency existing research showed corrected images within periphery indirect aid computational glasses build modified standard optical see head mounted displays directly modulate user vision consequently adapting perception colours work present exhaustive literature review colour vision deficiency compensation subsequent findings several prototypes varying advantages well controlled bench prototypes less controlled higher application portable prototypes series studies evaluating approach starting proving efficacy comparing state art extending beyond static lab prototypes looking real world applicability finally evaluated directions future compensation methods computational glasses
"While most approaches in formal methods address system correctness, ensuring robustness has remained a challenge. In this article, we present and study the logic rLTL, which provides a means to formally reason about both correctness and robustness in system design. Furthermore, we identify a large fragment of rLTL for which the verification problem can be efficiently solved, i.e., verification can be done by using an automaton, recognizing the behaviors described by the rLTL formula φ, of size at most O(3 |φ |), where |φ | is the length of φ. This result improves upon the previously known bound of O(5|φ |) for rLTL verification and is closer to the LTL bound of O(2|φ |). The usefulness of this fragment is demonstrated by a number of case studies showing its practical significance in terms of expressiveness, the ability to describe robustness, and the fine-grained information that rLTL brings to the process of system verification. Moreover, these advantages come at a low computational overhead with respect to LTL verification.",approaches formal methods address system correctness ensuring robustness remained challenge article present study logic rltl provides means formally reason correctness robustness system design furthermore identify large fragment rltl verification problem efficiently solved verification done using automaton recognizing behaviors described rltl formula size length result improves upon previously known bound rltl verification closer ltl bound usefulness fragment demonstrated number case studies showing practical significance terms expressiveness ability describe robustness fine grained information rltl brings process system verification moreover advantages come low computational overhead respect ltl verification
"This paper introduces a functional term calculus, called pn, that captures the essence of the operational semantics of Intuitionistic Linear Logic Proof-Nets with a faithful degree of granularity, both statically and dynamically. On the static side, we identify an equivalence relation on pn-terms which is sound and complete with respect to the classical notion of structural equivalence for proof-nets. On the dynamic side, we show that every single (exponential) step in the term calculus translates to a different single (exponential) step in the graphical formalism, thus capturing the original Girard’s granularity of proof-nets but on the level of terms. We also show some fundamental properties of the calculus such as confluence, strong normalization, preservation of β-strong normalization and the existence of a strong bisimulation that captures pairs of pn-terms having the same graph reduction.",paper introduces functional term calculus called captures essence operational semantics intuitionistic linear logic proof nets faithful degree granularity statically dynamically static side identify equivalence relation terms sound complete respect classical notion structural equivalence proof nets dynamic side show every single exponential step term calculus translates different single exponential step graphical formalism thus capturing original girard granularity proof nets level terms also show fundamental properties calculus confluence strong normalization preservation strong normalization existence strong bisimulation captures pairs terms graph reduction
"Computational geometry and topology are huge branches of mathematics. Focussing on concepts that lead to computation is one strategy to provide a concrete conceptual basis for ideas that hold in a more general context. Indeed, this short book gives an introduction to a surprisingly broad range of ideas that can serve as a good introduction to geometry and topology (even broadly conceived) for undergraduates.",computational geometry topology huge branches mathematics focussing concepts lead computation one strategy provide concrete conceptual basis ideas hold general context indeed short book gives introduction surprisingly broad range ideas serve good introduction geometry topology even broadly conceived undergraduates
"Computational technologies have revolutionized the archival sciences field, prompting new approaches to process the extensive data in these collections. Automatic speech recognition and natural language processing create unique possibilities for analysis of oral history (OH) interviews, where otherwise the transcription and analysis of the full recording would be too time consuming. However, many oral historians note the loss of aural information when converting the speech into text, pointing out the relevance of subjective cues for a full understanding of the interviewee narrative. In this article, we explore various computational technologies for social signal processing and their potential application space in OH archives, as well as neighboring domains where qualitative studies is a frequently used method. We also highlight the latest developments in key technologies for multimedia archiving practices such as natural language processing and automatic speech recognition. We discuss the analysis of both visual (body language and facial expressions), and non-visual cues (paralinguistics, breathing, and heart rate), stating the specific challenges introduced by the characteristics of OH collections. We argue that applying social signal processing to OH archives will have a wider influence than solely OH practices, bringing benefits for various fields from humanities to computer sciences, as well as to archival sciences. Looking at human emotions and somatic reactions on extensive interview collections would give scholars from multiple fields the opportunity to focus on feelings, mood, culture, and subjective experiences expressed in these interviews on a larger scale.",computational technologies revolutionized archival sciences field prompting new approaches process extensive data collections automatic speech recognition natural language processing create unique possibilities analysis oral history interviews otherwise transcription analysis full recording would time consuming however many oral historians note loss aural information converting speech text pointing relevance subjective cues full understanding interviewee narrative article explore various computational technologies social signal processing potential application space archives well neighboring domains qualitative studies frequently used method also highlight latest developments key technologies multimedia archiving practices natural language processing automatic speech recognition discuss analysis visual body language facial expressions non visual cues paralinguistics breathing heart rate stating specific challenges introduced characteristics collections argue applying social signal processing archives wider influence solely practices bringing benefits various fields humanities computer sciences well archival sciences looking human emotions somatic reactions extensive interview collections would give scholars multiple fields opportunity focus feelings mood culture subjective experiences expressed interviews larger scale
"We present an interactive design system for knitting that allows users to create template patterns that can be fabricated using an industrial knitting machine. Our interactive design tool is novel in that it allows direct control of key knitting design axes we have identified in our formative study and does so consistently across the variations of an input parametric template geometry. This is achieved with two key technical advances. First, we present an interactive meshing tool that lets users build a coarse quadrilateral mesh that adheres to their knit design guidelines. This solution ensures consistency across the parameter space for further customization over shape variations and avoids helices, promoting knittability. Second, we lift and formalize low-level machine knitting constraints to the level of this coarse quad mesh. This enables us to not only guarantee hand- and machine-knittability, but also provides automatic design assistance through auto-completion and suggestions. We show the capabilities through a set of fabricated examples that illustrate the effectiveness of our approach in creating a wide variety of objects and interactively exploring the space of design variations.",present interactive design system knitting allows users create template patterns fabricated using industrial knitting machine interactive design tool novel allows direct control key knitting design axes identified formative study consistently across variations input parametric template geometry achieved two key technical advances first present interactive meshing tool lets users build coarse quadrilateral mesh adheres knit design guidelines solution ensures consistency across parameter space customization shape variations avoids helices promoting knittability second lift formalize low level machine knitting constraints level coarse quad mesh enables guarantee hand machine knittability also provides automatic design assistance auto completion suggestions show capabilities set fabricated examples illustrate effectiveness approach creating wide variety objects interactively exploring space design variations
"Humans assume different production roles in a workspace. On one hand, humans design workplans to complete tasks as efficiently as possible in order to improve productivity. On the other hand, a nice workspace is essential to facilitate teamwork. In this way, workspace design and workplan design complement each other. Inspired by such observations, we propose an automatic approach to jointly design a workspace and a workplan. Taking staff properties, a space, and work equipment as input, our approach jointly optimizes a workspace and a workplan, considering performance factors such as time efficiency and congestion avoidance, as well as workload factors such as walk effort, turn effort, and workload balances. To enable exploration of design trade-offs, our approach generates a set of Pareto-optimal design solutions with strengths on different objectives, which can be adopted for different work scenarios. We apply our approach to synthesize workspaces and workplans for different workplaces such as a fast food kitchen and a supermarket. We also extend our approach to incorporate other common work considerations such as dynamic work demands and accommodating staff members with different physical capabilities. Evaluation experiments with simulations validate the efficacy of our approach for synthesizing effective workspaces and workplans.",humans assume different production roles workspace one hand humans design workplans complete tasks efficiently possible order improve productivity hand nice workspace essential facilitate teamwork way workspace design workplan design complement inspired observations propose automatic approach jointly design workspace workplan taking staff properties space work equipment input approach jointly optimizes workspace workplan considering performance factors time efficiency congestion avoidance well workload factors walk effort turn effort workload balances enable exploration design trade offs approach generates set pareto optimal design solutions strengths different objectives adopted different work scenarios apply approach synthesize workspaces workplans different workplaces fast food kitchen supermarket also extend approach incorporate common work considerations dynamic work demands accommodating staff members different physical capabilities evaluation experiments simulations validate efficacy approach synthesizing effective workspaces workplans
"On December 9, 2021, Apache announced a zero-day vulnerability in Apache Log4j [3]. As the news broke, tech companies and computing administrators scrambled to push out recommended mitigations. If you are reading this, chances are you already heard about this vulnerability and have made sure to update all of your systems. It is estimated that millions of companies are vulnerable to data breach through exploits of this widely used logging software. When millions of companies are vulnerable to breach it means that lots of people are going to have their data stolen and will suffer financial losses.",december apache announced zero day vulnerability apache log news broke tech companies computing administrators scrambled push recommended mitigations reading chances already heard vulnerability made sure update systems estimated millions companies vulnerable data breach exploits widely used logging software millions companies vulnerable breach means lots people going data stolen suffer financial losses
"As a critical task in intelligent traffic systems, traffic prediction has received a large amount of attention in the past few decades. The early efforts mainly model traffic prediction as the time-series mining problem, in which the spatial dependence has been largely ignored. As the rapid development of deep learning, some attempts have been made in modeling traffic prediction as the spatio-temporal data mining problem in a road network, in which deep learning techniques can be adopted for modeling the spatial and temporal dependencies simultaneously. Despite the success, the spatial and temporal dependencies are only modeled in a regionless network without considering the underlying hierarchical regional structure of the spatial nodes, which is an important structure naturally existing in the real-world road network. Apart from the challenge of modeling the spatial and temporal dependencies like the existing studies, the extra challenge caused by considering the hierarchical regional structure of the road network lies in simultaneously modeling the spatial and temporal dependencies between nodes and regions and the spatial and temporal dependencies between regions. To this end, this article proposes a new Temporal Hierarchical Graph Attention Network (TH-GAT). The main idea lies in augmenting the original road network into a region-augmented network, in which the hierarchical regional structure can be modeled. Based on the region-augmented network, the region-aware spatial dependence model and the region-aware temporal dependence model can be constructed, which are two main components of the proposed TH-GAT model. In addition, in the region-aware spatial dependence model, the graph attention network is adopted, in which the importance of a node to another node, of a node to a region, of a region to a node, and of a region to another region, can be captured automatically by means of the attention coefficients. Extensive experiments are conducted on two real-world traffic datasets, and the results have confirmed the superiority of the proposed TH-GAT model.",critical task intelligent traffic systems traffic prediction received large amount attention past decades early efforts mainly model traffic prediction time series mining problem spatial dependence largely ignored rapid development deep learning attempts made modeling traffic prediction spatio temporal data mining problem road network deep learning techniques adopted modeling spatial temporal dependencies simultaneously despite success spatial temporal dependencies modeled regionless network without considering underlying hierarchical regional structure spatial nodes important structure naturally existing real world road network apart challenge modeling spatial temporal dependencies like existing studies extra challenge caused considering hierarchical regional structure road network lies simultaneously modeling spatial temporal dependencies nodes regions spatial temporal dependencies regions end article proposes new temporal hierarchical graph attention network gat main idea lies augmenting original road network region augmented network hierarchical regional structure modeled based region augmented network region aware spatial dependence model region aware temporal dependence model constructed two main components proposed gat model addition region aware spatial dependence model graph attention network adopted importance node another node node region region node region another region captured automatically means attention coefficients extensive experiments conducted two real world traffic datasets results confirmed superiority proposed gat model
"Valued constraint satisfaction problems (VCSPs) are a large class of combinatorial optimisation problems. The computational complexity of VCSPs depends on the set of allowed cost functions in the input. Recently, the computational complexity of all VCSPs for finite sets of cost functions over finite domains has been classified. Many natural optimisation problems, however, cannot be formulated as VCSPs over a finite domain. We initiate the systematic investigation of the complexity of infinite-domain VCSPs with piecewise linear homogeneous cost functions. Such VCSPs can be solved in polynomial time if the cost functions are improved by fully symmetric fractional operations of all arities. We show this by reducing the problem to a finite-domain VCSP which can be solved using the basic linear program relaxation. It follows that VCSPs for submodular PLH cost functions can be solved in polynomial time; in fact, we show that submodular PLH functions form a maximally tractable class of PLH cost functions.",valued constraint satisfaction problems vcsps large class combinatorial optimisation problems computational complexity vcsps depends set allowed cost functions input recently computational complexity vcsps finite sets cost functions finite domains classified many natural optimisation problems however formulated vcsps finite domain initiate systematic investigation complexity infinite domain vcsps piecewise linear homogeneous cost functions vcsps solved polynomial time cost functions improved fully symmetric fractional operations arities show reducing problem finite domain vcsp solved using basic linear program relaxation follows vcsps submodular plh cost functions solved polynomial time fact show submodular plh functions form maximally tractable class plh cost functions
"Fixed-point logic with rank (FPR) is an extension of fixed-point logic with counting (FPC) with operators for computing the rank of a matrix over a finit field. The expressive power of FPR properly extends that of FPC and is contained in P, but it is not known if that containment is proper. We give a circuit characterization for FPR in terms of families of symmetric circuits with rank gates, along the lines of that for FPC given by Anderson and Dawar in 2017. This requires the development of a broad framework of circuits in which the individual gates compute functions that are not symmetric (i.e., invariant under all permutations of their inputs). This framework also necessitates the development of novel techniques to prove the equivalence of circuits and logic. Both the framework and the techniques are of greater generality than the main result.",fixed point logic rank fpr extension fixed point logic counting fpc operators computing rank matrix finit field expressive power fpr properly extends fpc contained known containment proper give circuit characterization fpr terms families symmetric circuits rank gates along lines fpc given anderson dawar requires development broad framework circuits individual gates compute functions symmetric invariant permutations inputs framework also necessitates development novel techniques prove equivalence circuits logic framework techniques greater generality main result
Professionals practice a form of computational thinking that is significantly more advanced than popular descriptions suggest.,professionals practice form computational thinking significantly advanced popular descriptions suggest
"We present an approach for covert visual updates by leveraging change blindness with computationally generated morphed images. To clarify the design parameters for intentionally suppressing change detection with morphing visuals, we investigated the visual change detection in three temporal behaviors: visual blank, eye-blink, and step-sequential changes. The results showed a robust trend of change blindness with a blank of more than 33.3 ms and with eye blink. Our sequential change study revealed that participants did not recognize changes until an average of 57% morphing toward another face in small change steps. In addition, changes went unnoticed until the end of morphing in more than 10% of all trials. Our findings should contribute to the design of covert visual updates without consuming users’ attention by leveraging change blindness with computational visual morphing.",present approach covert visual updates leveraging change blindness computationally generated morphed images clarify design parameters intentionally suppressing change detection morphing visuals investigated visual change detection three temporal behaviors visual blank eye blink step sequential changes results showed robust trend change blindness blank eye blink sequential change study revealed participants recognize changes average morphing toward another face small change steps addition changes went unnoticed end morphing trials findings contribute design covert visual updates without consuming users attention leveraging change blindness computational visual morphing
"We advocate the use of de Bruijn’s universal abstraction\lambda {\mathord \infty }{}{}{}for the quantification of schematic variables in the predicative setting, and we present a typed\lambda {}{}{}{}-calculus featuring the quantifier\lambda {\mathord \infty }{}{}{}accompanied by other practically useful constructions like explicit substitutions and expected type annotations. Our calculus stands just on two notions, i.e., bound rt-reduction and parametric validity, and has the expressive power of\lambda \mathord \rightarrow. Thus, while not aiming at being a logical framework by itself, it does enjoy many desired invariants of logical frameworks including confluence of reduction, strong normalization, preservation of type by reduction, decidability, correctness of types and uniqueness of types up to conversion. This calculus belongs to the\lambda \deltafamily of formal systems, which borrow some features from the pure type systems and some from the languages of the Automath tradition, but stand outside both families. In particular, our calculus includes and evolves two earlier systems of this family. Moreover, a machine-checked specification of its theory is available.",advocate use bruijn universal abstraction lambda mathord infty quantification schematic variables predicative setting present typed lambda calculus featuring quantifier lambda mathord infty accompanied practically useful constructions like explicit substitutions expected type annotations calculus stands two notions bound reduction parametric validity expressive power lambda mathord rightarrow thus aiming logical framework enjoy many desired invariants logical frameworks including confluence reduction strong normalization preservation type reduction decidability correctness types uniqueness types conversion calculus belongs lambda deltafamily formal systems borrow features pure type systems languages automath tradition stand outside families particular calculus includes evolves two earlier systems family moreover machine checked specification theory available
"Opioids have been shown to temporarily reduce the severity of pain when prescribed for medical purposes. However, opioid analgesics can also lead to severe adverse physical and psychological effects or even death through misuse, abuse, short- or long-term addiction, and one-time or recurrent overdose. Dynamic computational models and simulations can offer great potential to interpret the complex interaction of the drivers of the opioid crisis and assess intervention strategies. This study surveys existing studies of dynamic computational models and simulations addressing the opioid crisis and provides an overview of the state-of-the-art of dynamic computational models and simulations of the opioid crisis. This review gives a detailed analysis of existing modeling techniques, model conceptualization and formulation, and the policy interventions they suggest. It also explores the data sources they used and the study population they represented. Based on this analysis, direction and opportunities for future dynamic computational models for addressing the opioid crisis are suggested.",opioids shown temporarily reduce severity pain prescribed medical purposes however opioid analgesics also lead severe adverse physical psychological effects even death misuse abuse short long term addiction one time recurrent overdose dynamic computational models simulations offer great potential interpret complex interaction drivers opioid crisis assess intervention strategies study surveys existing studies dynamic computational models simulations addressing opioid crisis provides overview state art dynamic computational models simulations opioid crisis review gives detailed analysis existing modeling techniques model conceptualization formulation policy interventions suggest also explores data sources used study population represented based analysis direction opportunities future dynamic computational models addressing opioid crisis suggested
"Synchrony, the natural time-dependence of behavior in human interaction, is a pervasive feature of communication. However, most studies of synchrony have focused on dyadic interaction. In the current work, we explore synchrony in three-person teams using immersive virtual reality. Participants spent about two hours collaborating on four separate design tasks. The tracking data from the VR system allowed precise measurement of head and hand movements, facilitating calculation of synchrony. Results replicated previous work that found nonverbal synchrony in dyads in immersive VR. Moreover, we manipulated the context of the task environment, an informal garage or a traditional conference room. The environment for the task influenced synchrony, with higher levels occurring in the conference room than the garage. We also explored different methods of extending synchrony from dyads to triads, and explore the relationship of synchrony to turn taking and gaze. This paper provides theoretical insights about nonverbal synchrony and how design work functions in triads and provides suggestions for designers of VR to support good collaboration.",synchrony natural time dependence behavior human interaction pervasive feature communication however studies synchrony focused dyadic interaction current work explore synchrony three person teams using immersive virtual reality participants spent two hours collaborating four separate design tasks tracking data system allowed precise measurement head hand movements facilitating calculation synchrony results replicated previous work found nonverbal synchrony dyads immersive moreover manipulated context task environment informal garage traditional conference room environment task influenced synchrony higher levels occurring conference room garage also explored different methods extending synchrony dyads triads explore relationship synchrony turn taking gaze paper provides theoretical insights nonverbal synchrony design work functions triads provides suggestions designers support good collaboration
"In the era of big data and artificial intelligence, online risk detection has become a popular research topic. From detecting online harassment to the sexual predation of youth, the state-of-the-art in computational risk detection has the potential to protect particularly vulnerable populations from online victimization. Yet, this is a high-risk, high-reward endeavor that requires a systematic and human-centered approach to synthesize disparate bodies of research across different application domains, so that we can identify best practices, potential gaps, and set a strategic research agenda for leveraging these approaches in a way that betters society. Therefore, we conducted a comprehensive literature review to analyze 73 peer-reviewed articles on computational approaches utilizing text or meta-data/multimedia for online sexual risk detection. We identified sexual grooming (75%), sex trafficking (12%), and sexual harassment and/or abuse (12%) as the three types of sexual risk detection present in the extant literature. Furthermore, we found that the majority (93%) of this work has focused on identifying sexual predators after-the-fact, rather than taking more nuanced approaches to identify potential victims and problematic patterns that could be used to prevent victimization before it occurs. Many studies rely on public datasets (82%) and third-party annotators (33%) to establish ground truth and train their algorithms. Finally, the majority of this work (78%) mostly focused on algorithmic performance evaluation of their model and rarely (4%) evaluate these systems with real users. Thus, we urge computational risk detection researchers to integrate more human-centered approaches to both developing and evaluating sexual risk detection algorithms to ensure the broader societal impacts of this important work.",big data artificial intelligence online risk detection become popular research topic detecting online harassment sexual predation youth state art computational risk detection potential protect particularly vulnerable populations online victimization yet high risk high reward endeavor requires systematic human centered approach synthesize disparate bodies research across different application domains identify best practices potential gaps set strategic research agenda leveraging approaches way betters society therefore conducted comprehensive literature review analyze peer reviewed articles computational approaches utilizing text meta data multimedia online sexual risk detection identified sexual grooming sex trafficking sexual harassment abuse three types sexual risk detection present extant literature furthermore found majority work focused identifying sexual predators fact rather taking nuanced approaches identify potential victims problematic patterns could used prevent victimization occurs many studies rely public datasets third party annotators establish ground truth train algorithms finally majority work mostly focused algorithmic performance evaluation model rarely evaluate systems real users thus urge computational risk detection researchers integrate human centered approaches developing evaluating sexual risk detection algorithms ensure broader societal impacts important work
"Sustainable HCI (SHCI) researchers have historically looked to small and urban farmers to help situate and extend notions of sustainability within economic, social, and political frameworks. In the face of climate change and the Anthropocene, however, we ask how designing like the alternative farming practices of small and urban farmers might open up new, ecological approaches to agricultural technology. We conducted ethnographic field work with small farmers and their community in Indiana and show how they are challenging ""agrilogistics,"" defined by philosopher Timothy Morton as a strict separation of nature and culture in food production, a separation, he argues, which underlies the substantial agricultural contributions to climate change. Our ethnography led us to suggest new possibilities for design of agricultural technology that support ecological thinking and caring for more-than-human actors through visceral imaginaries, posthuman storytelling, and engaging curiosity, possibilities which may offer ways to disentangle agricultural technology from agrilogistic paradigms.?",sustainable hci shci researchers historically looked small urban farmers help situate extend notions sustainability within economic social political frameworks face climate change anthropocene however ask designing like alternative farming practices small urban farmers might open new ecological approaches agricultural technology conducted ethnographic field work small farmers community indiana show challenging agrilogistics defined philosopher timothy morton strict separation nature culture food production separation argues underlies substantial agricultural contributions climate change ethnography led suggest new possibilities design agricultural technology support ecological thinking caring human actors visceral imaginaries posthuman storytelling engaging curiosity possibilities may offer ways disentangle agricultural technology agrilogistic paradigms
"This article presents a method for designing planar multistable compliant structures. Given a sequence of desired stable states and the corresponding poses of the structure, we identify the topology and geometric realization of a mechanism—consisting of bars and joints—that is able to physically reproduce the desired multistable behavior. In order to solve this problem efficiently, we build on insights from minimally rigid graph theory to identify simple but effective topologies for the mechanism. We then optimize its geometric parameters, such as joint positions and bar lengths, to obtain correct transitions between the given poses. Simultaneously, we ensure adequate stability of each pose based on an effective approximate error metric related to the elastic energy Hessian of the bars in the mechanism. As demonstrated by our results, we obtain functional multistable mechanisms of manageable complexity that can be fabricated using 3D printing. Further, we evaluated the effectiveness of our method on a large number of examples in the simulation and fabricated several physical prototypes.",article presents method designing planar multistable compliant structures given sequence desired stable states corresponding poses structure identify topology geometric realization mechanism consisting bars joints able physically reproduce desired multistable behavior order solve problem efficiently build insights minimally rigid graph theory identify simple effective topologies mechanism optimize geometric parameters joint positions bar lengths obtain correct transitions given poses simultaneously ensure adequate stability pose based effective approximate error metric related elastic energy hessian bars mechanism demonstrated results obtain functional multistable mechanisms manageable complexity fabricated using printing evaluated effectiveness method large number examples simulation fabricated several physical prototypes
"Spiking Neural Networks (SNNs) represent a biologically inspired computation model capable of emulating neural computation in human brain and brain-like structures. The main promise is very low energy consumption. Classic Von Neumann architecture based SNN accelerators in hardware, however, often fall short of addressing demanding computation and data transfer requirements efficiently at scale. In this article, we propose a promising alternative to overcome scalability limitations, based on a network of in-memory SNN accelerators, which can reduce the energy consumption by up to 150.25= when compared to a representative ASIC solution. The significant reduction in energy comes from two key aspects of the hardware design to minimize data communication overheads: (1) each node represents an in-memory SNN accelerator based on a spintronic Computational RAM array, and (2) a novel, De Bruijn graph based architecture establishes the SNN array connectivity.",spiking neural networks snns represent biologically inspired computation model capable emulating neural computation human brain brain like structures main promise low energy consumption classic von neumann architecture based snn accelerators hardware however often fall short addressing demanding computation data transfer requirements efficiently scale article propose promising alternative overcome scalability limitations based network memory snn accelerators reduce energy consumption compared representative asic solution significant reduction energy comes two key aspects hardware design minimize data communication overheads node represents memory snn accelerator based spintronic computational ram array novel bruijn graph based architecture establishes snn array connectivity
"The traditional workflow in continuum mechanics simulations is that a geometry description —for example obtained using Constructive Solid Geometry (CSG) or Computer Aided Design (CAD) tools—forms the input for a mesh generator. The mesh is then used as the sole input for the finite element, finite volume, and finite difference solver, which at this point no longer has access to the original, “underlying” geometry. However, many modern techniques—for example, adaptive mesh refinement and the use of higher order geometry approximation methods—really do need information about the underlying geometry to realize their full potential. We have undertaken an exhaustive study of where typical finite element codes use geometry information, with the goal of determining what information geometry tools would have to provide. Our study shows that nearly all geometry-related needs inside the simulators can be satisfied by just two “primitives”: elementary queries posed by the simulation software to the geometry description. We then show that it is possible to provide these primitives in all of the frequently used ways in which geometries are described in common industrial workflows, and illustrate our solutions using a number of examples.",traditional workflow continuum mechanics simulations geometry description example obtained using constructive solid geometry csg computer aided design cad tools forms input mesh generator mesh used sole input finite element finite volume finite difference solver point longer access original underlying geometry however many modern techniques example adaptive mesh refinement use higher order geometry approximation methods really need information underlying geometry realize full potential undertaken exhaustive study typical finite element codes use geometry information goal determining information geometry tools would provide study shows nearly geometry related needs inside simulators satisfied two primitives elementary queries posed simulation software geometry description show possible provide primitives frequently used ways geometries described common industrial workflows illustrate solutions using number examples
"Wrapping objects using ropes is a common practice in our daily life. However, it is difficult to design and tie ropes on a 3D object with complex topology and geometry features while ensuring wrapping security and easy operation. In this article, we propose to compute a rope net that can tightly wrap around various 3D shapes. Our computed rope net not only immobilizes the object but also maintains the load balance during lifting. Based on the key observation that if every knot of the net has four adjacent curve edges, then only a single rope is needed to construct the entire net. We reformulate the rope net computation problem into a constrained curve network optimization. We propose a discrete-continuous optimization approach, where the topological constraints are satisfied in the discrete phase and the geometrical goals are achieved in the continuous stage. We also develop a hoist planning to pick anchor points so that the rope net equally distributes the load during hoisting. Furthermore, we simulate the wrapping process and use it to guide the physical rope net construction process. We demonstrate the effectiveness of our method on 3D objects with varying geometric and topological complexity. In addition, we conduct physical experiments to demonstrate the practicability of our method.",wrapping objects using ropes common practice daily life however difficult design tie ropes object complex topology geometry features ensuring wrapping security easy operation article propose compute rope net tightly wrap around various shapes computed rope net immobilizes object also maintains load balance lifting based key observation every knot net four adjacent curve edges single rope needed construct entire net reformulate rope net computation problem constrained curve network optimization propose discrete continuous optimization approach topological constraints satisfied discrete phase geometrical goals achieved continuous stage also develop hoist planning pick anchor points rope net equally distributes load hoisting furthermore simulate wrapping process use guide physical rope net construction process demonstrate effectiveness method objects varying geometric topological complexity addition conduct physical experiments demonstrate practicability method
"It is known that the satisfiability problems of the product logics K4 × S5 and S4 × S5 are NEXPTIME-hard and that the satisfiability problem of the logic SSL of subset spaces is PSPACE-hard. Furthermore, it is known that the satisfiability problems of these logics are in N2EXPTIME. We improve the lower and the upper bounds for the complexity of these problems by showing that all three problems are in ESPACE and are EXPSPACE-complete under logspace reduction.",known satisfiability problems product logics nexptime hard satisfiability problem logic ssl subset spaces pspace hard furthermore known satisfiability problems logics exptime improve lower upper bounds complexity problems showing three problems espace expspace complete logspace reduction
"In April, I decided that I wanted to give my writing more attention by committing to a daily writing practice. Although I really like to express myself using pen and paper, I find that it is much easier if I just digitize my writings from the beginning. I decided that the new version of Microsoft OneNote might be a good way to keep these daily scraps in an organized way. I am a mac user so I haven't used this software much. I opened my version of OneNote, which came with my office 365 subscription for mac. Note: this subscription must be renewed each year in order to keep using the service and to have access to OneNote.",april decided wanted give writing attention committing daily writing practice although really like express using pen paper find much easier digitize writings beginning decided new version microsoft onenote might good way keep daily scraps organized way mac user used software much opened version onenote came office subscription mac note subscription must renewed year order keep using service access onenote
"What you need to consider before collecting, processing, and analyzing mobile data for health applications.",need consider collecting processing analyzing mobile data health applications
"Rephotography is the process of recapturing the photograph of a location from the same perspective in which it was captured earlier. A rephotographed image is the best presentation to visualize and study the social changes of a location over time. Traditionally, only expert artists and photographers are capable of generating the rephotograph of any specific location. Manual editing or human eye judgment that is considered for generating rephotographs normally requires a lot of precision, effort and is not always accurate. In the era of computer science and deep learning, computer vision techniques make it easier and faster to perform precise operations to an image. Until now many research methodologies have been proposed for rephotography but none of them is fully automatic. Some of these techniques require manual input by the user or need multiple images of the same location with 3D point cloud data while others are only suggestions to the user to perform rephotography. In historical records/archives most of the time we can find only one 2D image of a certain location. Computational rephotography is a challenge in the case of using only one image of a location captured at different timestamps because it is difficult to find the accurate perspective of a single 2D historical image. Moreover, in the case of building rephotography, it is required to maintain the alignments and regular shape. The features of a building may change over time and in most of the cases, it is not possible to use a features detection algorithm to detect the key features. In this research paper, we propose a methodology to rephotograph house images by combining deep learning and traditional computer vision techniques. The purpose of this research is to rephotograph an image of the past based on a single image. This research will be helpful not only for computer scientists but also for history and cultural heritage research scholars to study the social changes of a location during a specific time period, and it will allow users to go back in time to see how a specific place looked in the past. We have achieved good, fully automatic rephotographed results based on façade segmentation using only a single image.",rephotography process recapturing photograph location perspective captured earlier rephotographed image best presentation visualize study social changes location time traditionally expert artists photographers capable generating rephotograph specific location manual editing human eye judgment considered generating rephotographs normally requires lot precision effort always accurate computer science deep learning computer vision techniques make easier faster perform precise operations image many research methodologies proposed rephotography none fully automatic techniques require manual input user need multiple images location point cloud data others suggestions user perform rephotography historical records archives time find one image certain location computational rephotography challenge case using one image location captured different timestamps difficult find accurate perspective single historical image moreover case building rephotography required maintain alignments regular shape features building may change time cases possible use features detection algorithm detect key features research paper propose methodology rephotograph house images combining deep learning traditional computer vision techniques purpose research rephotograph image past based single image research helpful computer scientists also history cultural heritage research scholars study social changes location specific time period allow users back time see specific place looked past achieved good fully automatic rephotographed results based ade segmentation using single image
"Annie Murphy Paul's new book, ""The Extended Mind: The Power of Thinking Outside the Brain,"" covers emerging research that extends our understanding of thinking beyond the typical view of ""mind in the brain."" Illustrated with stories, this book unpacks new recognitions, and provides the implications for the design of learning and instruction.",annie murphy paul new book extended mind power thinking outside brain covers emerging research extends understanding thinking beyond typical view mind brain illustrated stories book unpacks new recognitions provides implications design learning instruction
"The hypervolume indicator is one of the most used set-quality indicators for the assessment of stochastic multiobjective optimizers, as well as for selection in evolutionary multiobjective optimization algorithms. Its theoretical properties justify its wide acceptance, particularly the strict monotonicity with respect to set dominance, which is still unique of hypervolume-based indicators. This article discusses the computation of hypervolume-related problems, highlighting the relations between them, providing an overview of the paradigms and techniques used, a description of the main algorithms for each problem, and a rundown of the fastest algorithms regarding asymptotic complexity and runtime. By providing a complete overview of the computational problems associated to the hypervolume indicator, this article serves as the starting point for the development of new algorithms and supports users in the identification of the most appropriate implementations available for each problem.",hypervolume indicator one used set quality indicators assessment stochastic multiobjective optimizers well selection evolutionary multiobjective optimization algorithms theoretical properties justify wide acceptance particularly strict monotonicity respect set dominance still unique hypervolume based indicators article discusses computation hypervolume related problems highlighting relations providing overview paradigms techniques used description main algorithms problem rundown fastest algorithms regarding asymptotic complexity runtime providing complete overview computational problems associated hypervolume indicator article serves starting point development new algorithms supports users identification appropriate implementations available problem
"In “A Practical Approach to Subset Selection for Multi-Objective Optimization via Simulation,” Currie and Monks propose an algorithm for multi-objective simulation-based optimization. In contrast to sequential ranking and selection schemes, their algorithm follows a two-stage scheme. The approach is evaluated by comparing the results to those obtained using the existing OCBA-m algorithm for synthetic problems and for a hospital ward configuration problem. The authors provide the Python code used in the experiments in the form of Jupyter notebooks. The code successfully reproduced the results shown in the article.",practical approach subset selection multi objective optimization via simulation currie monks propose algorithm multi objective simulation based optimization contrast sequential ranking selection schemes algorithm follows two stage scheme approach evaluated comparing results obtained using existing ocba algorithm synthetic problems hospital ward configuration problem authors provide python code used experiments form jupyter notebooks code successfully reproduced results shown article
"We propose a notion of the Kripke-style model for intersection logic. Using a game interpretation, we prove soundness and completeness of the proposed semantics. In other words, a formula is provable (a type is inhabited) if and only if it is forced in every model. As a by-product, we obtain another proof of normalization for the Barendregt–Coppo–Dezani intersection type assignment system.",propose notion kripke style model intersection logic using game interpretation prove soundness completeness proposed semantics words formula provable type inhabited forced every model product obtain another proof normalization barendregt coppo dezani intersection type assignment system
"We solve the task of representing free forms by an arrangement of panels that are manufacturable by precise isometric bending of surfaces made from a small number of molds. In fact we manage to solve the paneling task with surfaces of constant Gaussian curvature alone. This includes the case of developable surfaces which exhibit zero curvature. Our computations are based on an existing discrete model of isometric mappings between surfaces which for this occasion has been refined to obtain higher numerical accuracy. Further topics are interesting connections of the paneling problem with the geometry of Killing vector fields, designing and actuating isometries, curved folding in the double-curved case, and quad meshes with rigid faces that are nevertheless flexible.",solve task representing free forms arrangement panels manufacturable precise isometric bending surfaces made small number molds fact manage solve paneling task surfaces constant gaussian curvature alone includes case developable surfaces exhibit zero curvature computations based existing discrete model isometric mappings surfaces occasion refined obtain higher numerical accuracy topics interesting connections paneling problem geometry killing vector fields designing actuating isometries curved folding double curved case quad meshes rigid faces nevertheless flexible
"We present a computational inverse design method for a new class of surface-based inflatable structure. Our deployable structures are fabricated by fusing together two layers of inextensible sheet material along carefully selected curves. The fusing curves form a network of tubular channels that can be inflated with air or other fluids. When fully inflated, the initially flat surface assumes a programmed double-curved shape and becomes stiff and load-bearing. We present a method that solves for the layout of air channels that, when inflated, best approximate a given input design. For this purpose, we integrate a forward simulation method for inflation with a gradient-based optimization algorithm that continuously adapts the geometry of the air channels to improve the design objectives. To initialize this non-linear optimization, we propose a novel surface flattening algorithm. When a channel is inflated, it approximately maintains its length, but contracts transversally to its main direction. Our algorithm approximates this deformation behavior by computing a mapping from the 3D design surface to the plane that allows for anisotropic metric scaling within the bounds realizable by the physical system. We show a wide variety of inflatable designs and fabricate several prototypes to validate our approach and highlight potential applications.",present computational inverse design method new class surface based inflatable structure deployable structures fabricated fusing together two layers inextensible sheet material along carefully selected curves fusing curves form network tubular channels inflated air fluids fully inflated initially flat surface assumes programmed double curved shape becomes stiff load bearing present method solves layout air channels inflated best approximate given input design purpose integrate forward simulation method inflation gradient based optimization algorithm continuously adapts geometry air channels improve design objectives initialize non linear optimization propose novel surface flattening algorithm channel inflated approximately maintains length contracts transversally main direction algorithm approximates deformation behavior computing mapping design surface plane allows anisotropic metric scaling within bounds realizable physical system show wide variety inflatable designs fabricate several prototypes validate approach highlight potential applications
"The edges of an image contains rich visual cognitive cues. However, the edge information of a natural scene usually is only a set of disorganized unorganized pixels for a computer. In psychology, the phenomenon of quickly perceiving global information from a complex pattern is called the global precedence effect (GPE). For example, when one observes the edge map of an image, some contours seem to automatically “pop out” from the complex background. This is a manifestation of GPE on edge information and is called global contour precedence (GCP). The primary visual cortex (V1) is closely related to the processing of edges. In this article, a neural computational model to simulate GCP based on the mechanisms of V1 is presented. There are three layers in the proposed model: the representation of line segments, organization of edges, and perception of global contours. In experiments, the ability to group edges is tested on the public dataset BSDS500. The results show that the grouping performance, robustness, and time cost of the proposed model are superior to those of other methods. In addition, the outputs of the proposed model can also be applied to the generation of object proposals, which indicates that the proposed model can contribute significantly to high-level visual tasks.",edges image contains rich visual cognitive cues however edge information natural scene usually set disorganized unorganized pixels computer psychology phenomenon quickly perceiving global information complex pattern called global precedence effect gpe example one observes edge map image contours seem automatically pop complex background manifestation gpe edge information called global contour precedence gcp primary visual cortex closely related processing edges article neural computational model simulate gcp based mechanisms presented three layers proposed model representation line segments organization edges perception global contours experiments ability group edges tested public dataset bsds results show grouping performance robustness time cost proposed model superior methods addition outputs proposed model also applied generation object proposals indicates proposed model contribute significantly high level visual tasks
"This paper examines volitionality of Facebook usage, that is, which individuals feel they have a choice about whether or not to use the site. It analyzes data from two large surveys, conducted three years apart. Across the two surveys, a variety of factors impacted whether or not respondents saw their Facebook usage as a matter of their own choice, such as engaging in non-use behaviors, measures of Facebook addiction, a sense of their own agency, and, across both studies, level of education. These results expand on prior literature around technology use and non-use, especially in terms of which populations may feel obligated to use, or be unwillingly prevented from using, social media such as Facebook. Furthermore, they provide potential implications both for future work and for technology policy.",paper examines volitionality facebook usage individuals feel choice whether use site analyzes data two large surveys conducted three years apart across two surveys variety factors impacted whether respondents saw facebook usage matter choice engaging non use behaviors measures facebook addiction sense agency across studies level education results expand prior literature around technology use non use especially terms populations may feel obligated use unwillingly prevented using social media facebook furthermore provide potential implications future work technology policy
"Our system generates abstract images from music that serve as inspiration for the creative process. We developed one of many possible approaches for a cross-domain association between the musical and visual domains, by extracting features from MIDI music files and associating them to visual characteristics. The associations were led by the authors' aesthetic preferences and some experimentation. Three different approaches were pursued, two with direct or random associations and a third using a genetic algorithm that considers music and color theory while searching for better results. The resulting images were evaluated through online surveys, which confirmed that not only they were abstract, but also that there was a relationship with the music that served as the basis for the association process. Moreover, the majority of the participants ranked highest the images improved with the genetic algorithm. This newsletter contribution summarizes the full version of the article, which was presented at EvoMUSART 2021 (the 10th International Conference on Artificial Intelligence in Music, Sound, Art and Design).",system generates abstract images music serve inspiration creative process developed one many possible approaches cross domain association musical visual domains extracting features midi music files associating visual characteristics associations led authors aesthetic preferences experimentation three different approaches pursued two direct random associations third using genetic algorithm considers music color theory searching better results resulting images evaluated online surveys confirmed abstract also relationship music served basis association process moreover majority participants ranked highest images improved genetic algorithm newsletter contribution summarizes full version article presented evomusart international conference artificial intelligence music sound art design
The article proposes a trimodal logical system that can express the strategic ability of coalitions to learn from their experience. The main technical result is the completeness of the proposed system.,article proposes trimodal logical system express strategic ability coalitions learn experience main technical result completeness proposed system
"We introduce translations between display calculus proofs and labeled calculus proofs in the context of tense logics. First, we show that every derivation in the display calculus for the minimal tense logic Kt extended with general path axioms can be effectively transformed into a derivation in the corresponding labeled calculus. Concerning the converse translation, we show that for Kt extended with path axioms, every derivation in the corresponding labeled calculus can be put into a special form that is translatable to a derivation in the associated display calculus. A key insight in this converse translation is a canonical representation of display sequents as labeled polytrees. Labeled polytrees, which represent equivalence classes of display sequents modulo display postulates, also shed light on related correspondence results for tense logics.",introduce translations display calculus proofs labeled calculus proofs context tense logics first show every derivation display calculus minimal tense logic extended general path axioms effectively transformed derivation corresponding labeled calculus concerning converse translation show extended path axioms every derivation corresponding labeled calculus put special form translatable derivation associated display calculus key insight converse translation canonical representation display sequents labeled polytrees labeled polytrees represent equivalence classes display sequents modulo display postulates also shed light related correspondence results tense logics
"This article studies a large class of two-player perfect-information turn-based parity games on infinite graphs, namely, those generated by collapsible pushdown automata. The main motivation for studying these games comes from the connections from collapsible pushdown automata and higher-order recursion schemes, both models being equi-expressive for generating infinite trees. Our main result is to establish the decidability of such games and to provide an effective representation of the winning region as well as of a winning strategy. Thus, the results obtained here provide all necessary tools for an in-depth study of logical properties of trees generated by collapsible pushdown automata/recursion schemes.",article studies large class two player perfect information turn based parity games infinite graphs namely generated collapsible pushdown automata main motivation studying games comes connections collapsible pushdown automata higher order recursion schemes models equi expressive generating infinite trees main result establish decidability games provide effective representation winning region well winning strategy thus results obtained provide necessary tools depth study logical properties trees generated collapsible pushdown automata recursion schemes
"We extend to natural deduction the approach of Linear Nested Sequents and of 2-Sequents. Formulas are decorated with a spatial coordinate, which allows a formulation of formal systems in the original spirit of natural deduction: only one introduction and one elimination rule per connective, no additional (structural) rule, no explicit reference to the accessibility relation of the intended Kripke models. We give systems for the normal modal logics from K to S4. For the intuitionistic versions of the systems, we define proof reduction, and prove proof normalization, thus obtaining a syntactical proof of consistency. For logics K and K4 we use existence predicates (à la Scott) for formulating sound deduction rules.",extend natural deduction approach linear nested sequents sequents formulas decorated spatial coordinate allows formulation formal systems original spirit natural deduction one introduction one elimination rule per connective additional structural rule explicit reference accessibility relation intended kripke models give systems normal modal logics intuitionistic versions systems define proof reduction prove proof normalization thus obtaining syntactical proof consistency logics use existence predicates scott formulating sound deduction rules
"As rocket launch cadences increase, access to space rises dramatically - setting the stage for the next space industry surge. New, smaller, and less expensive satellites - now ""nanosatellites"" - can be deployed en masse to form constellations of hundreds, thousands, or even tens of thousands of devices [27, 40, 41, 16, 17, 18, 43]. A constellation of nanosatellites equipped with sensors (e.g., visual or hyperspectral cameras, particle detectors, or magnetometers) and radios provides a first-time opportunity for orbital swarm sensing to synthesize data from the unique vantage point of low-Earth orbit (LEO).",rocket launch cadences increase access space rises dramatically setting stage next space industry surge new smaller less expensive satellites nanosatellites deployed masse form constellations hundreds thousands even tens thousands devices constellation nanosatellites equipped sensors visual hyperspectral cameras particle detectors magnetometers radios provides first time opportunity orbital swarm sensing synthesize data unique vantage point low earth orbit leo
Climate change is one symptom reflecting a larger problem of how we humans view ourselves as separate from the environment. How can computation and design help us expand our perception so we can better attend to the natural world?,climate change one symptom reflecting larger problem humans view separate environment computation design help expand perception better attend natural world
"Petri Nets are a widely used formalism to deal with concurrent systems. Dynamic Logics (DLs) are a family of modal logics where each modality corresponds to a program. Petri-PDL is a logical language that combines these two approaches: it is a dynamic logic where programs are replaced by Petri Nets. In this work we present a clausal resolution-based calculus for Petri-PDL. Given a Petri-PDL formula, we show how to obtain its translation into a normal form to which a set of resolution-based inference rules are applied. We show that the resulting calculus is sound, complete, and terminating. Some examples of the application of the method are also given.",petri nets widely used formalism deal concurrent systems dynamic logics dls family modal logics modality corresponds program petri pdl logical language combines two approaches dynamic logic programs replaced petri nets work present clausal resolution based calculus petri pdl given petri pdl formula show obtain translation normal form set resolution based inference rules applied show resulting calculus sound complete terminating examples application method also given
"The list segment predicate ls used in separation logic for verifying programs with pointers is well suited to express properties on singly-linked lists. We study the effects of adding ls to the full quantifier-free separation logic with the separating conjunction and implication, which is motivated by the recent design of new fragments in which all these ingredients are used indifferently and verification tools start to handle the magic wand connective. This is a very natural extension that has not been studied so far. We show that the restriction without the separating implication can be solved in polynomial space by using an appropriate abstraction for memory states, whereas the full extension is shown undecidable by reduction from first-order separation logic. Many variants of the logic and fragments are also investigated from the computational point of view when ls is added, providing numerous results about adding reachability predicates to quantifier-free separation logic.",list segment predicate used separation logic verifying programs pointers well suited express properties singly linked lists study effects adding full quantifier free separation logic separating conjunction implication motivated recent design new fragments ingredients used indifferently verification tools start handle magic wand connective natural extension studied far show restriction without separating implication solved polynomial space using appropriate abstraction memory states whereas full extension shown undecidable reduction first order separation logic many variants logic fragments also investigated computational point view added providing numerous results adding reachability predicates quantifier free separation logic
"As a new era in computing emerges, so too must our fundamental thinking patterns.",new computing emerges must fundamental thinking patterns
"CSCW researchers have long inquired into the ways that identity informs, and is informed by, the design of technological systems. Gender is a regularly considered aspect of identity, with extensive work documenting and exploring gendered experiences and designs with the aim of addressing inequalities in, or through, design. Recent work has questioned the way that we conceptualise and ""measure"" gender, advocating more nuanced classificatory schemes to avoid silencing or obscuring trans and/or non-binary experiences. Building on and extending this research, our work examines how gender is conceptualised more broadly. Drawing from a range of theoretical perspectives in gender studies, feminist and postcolonial theory, we argue for the treatment of gender as ""multiplicitous"" when we conceptualise and interpret research, in order to avoid unintentionally perpetuating silencing and inequality even as we work to tackle it. Illustrating our argument with examples from both within and without CSCW, we suggest both new research directions for CSCW scholars inquiring into gender, and sensitising questions that scholars can use when constructing and evaluating studies.",cscw researchers long inquired ways identity informs informed design technological systems gender regularly considered aspect identity extensive work documenting exploring gendered experiences designs aim addressing inequalities design recent work questioned way conceptualise measure gender advocating nuanced classificatory schemes avoid silencing obscuring trans non binary experiences building extending research work examines gender conceptualised broadly drawing range theoretical perspectives gender studies feminist postcolonial theory argue treatment gender multiplicitous conceptualise interpret research order avoid unintentionally perpetuating silencing inequality even work tackle illustrating argument examples within without cscw suggest new research directions cscw scholars inquiring gender sensitising questions scholars use constructing evaluating studies
"A growing number of people are using catch-up TV services rather than watching simultaneously with other audience members at the time of broadcast. However, computational support for such catching-up users has not been well explored. In particular, we are observing an emerging phenomenon in online media consumption experiences in which speculation plays a vital role. As the phenomenon of speculation implicitly assumes simultaneity in media consumption, there is a gap for catching-up users, who cannot directly appreciate the consumption experiences. This conversely suggests that there is potential for computational support to enhance the consumption experiences of catching-up users. Accordingly, we conducted a series of studies to pave the way for developing computational support for catching-up users. First, we conducted semi-structured interviews to understand how people are engaging with speculation during media consumption. As a result, we discovered the distinctive aspects of speculation-based consumption experiences in contrast to social viewing experiences sharing immediate reactions that have been discussed in previous studies. We then designed two prototypes for supporting catching-up users based on our quantitative analysis of Twitter data in regard to reaction- and speculation-based media consumption. Lastly, we evaluated the prototypes in a user experiment and, based on its results, discussed ways to empower catching-up users with computational supports in response to recent transformations in media consumption.",growing number people using catch services rather watching simultaneously audience members time broadcast however computational support catching users well explored particular observing emerging phenomenon online media consumption experiences speculation plays vital role phenomenon speculation implicitly assumes simultaneity media consumption gap catching users directly appreciate consumption experiences conversely suggests potential computational support enhance consumption experiences catching users accordingly conducted series studies pave way developing computational support catching users first conducted semi structured interviews understand people engaging speculation media consumption result discovered distinctive aspects speculation based consumption experiences contrast social viewing experiences sharing immediate reactions discussed previous studies designed two prototypes supporting catching users based quantitative analysis twitter data regard reaction speculation based media consumption lastly evaluated prototypes user experiment based results discussed ways empower catching users computational supports response recent transformations media consumption
"Mental state assessment by analysing user-generated content is a field that has recently attracted considerable attention. Today, many people are increasingly utilising online social media platforms to share their feelings and moods. This provides a unique opportunity for researchers and health practitioners to proactively identify linguistic markers or patterns that correlate with mental disorders such as depression, schizophrenia or suicide behaviour. This survey describes and reviews the approaches that have been proposed for mental state assessment and identification of disorders using online digital records. The presented studies are organised according to the assessment technology and the feature extraction process conducted. We also present a series of studies which explore different aspects of the language and behaviour of individuals suffering from mental disorders, and discuss various aspects related to the development of experimental frameworks. Furthermore, ethical considerations regarding the treatment of individuals’ data are outlined. The main contributions of this survey are a comprehensive analysis of the proposed approaches for online mental state assessment on social media, a structured categorisation of the methods according to their design principles, lessons learnt over the years and a discussion on possible avenues for future research.",mental state assessment analysing user generated content field recently attracted considerable attention today many people increasingly utilising online social media platforms share feelings moods provides unique opportunity researchers health practitioners proactively identify linguistic markers patterns correlate mental disorders depression schizophrenia suicide behaviour survey describes reviews approaches proposed mental state assessment identification disorders using online digital records presented studies organised according assessment technology feature extraction process conducted also present series studies explore different aspects language behaviour individuals suffering mental disorders discuss various aspects related development experimental frameworks furthermore ethical considerations regarding treatment individuals data outlined main contributions survey comprehensive analysis proposed approaches online mental state assessment social media structured categorisation methods according design principles lessons learnt years discussion possible avenues future research
"The momentum around computational thinking (CT) has kindled a rising wave of research initiatives and scholarly contributions seeking to capitalize on the opportunities that CT could bring. A number of literature reviews have showed a vibrant community of practitioners and a growing number of publications. However, the history and evolution of the emerging research topic, the milestone publications that have shaped its directions, and the timeline of the important developments may be better told through a quantitative, scientometric narrative. This article presents a bibliometric analysis of the drivers of the CT topic, as well as its main themes of research, international collaborations, influential authors, and seminal publications, and how authors and publications have influenced one another. The metadata of 1,874 documents were retrieved from the Scopus database using the keyword “computational thinking.” The results show that CT research has been US-centric from the start, and continues to be dominated by US researchers both in volume and impact. International collaboration is relatively low, but clusters of joint research are found between, for example, a number of Nordic countries, lusophone- and hispanophone countries, and central European countries. The results show that CT features the computing’s traditional tripartite disciplinary structure (design, modeling, and theory), a distinct emphasis on programming, and a strong pedagogical and educational backdrop including constructionism, self-efficacy, motivation, and teacher training.",momentum around computational thinking kindled rising wave research initiatives scholarly contributions seeking capitalize opportunities could bring number literature reviews showed vibrant community practitioners growing number publications however history evolution emerging research topic milestone publications shaped directions timeline important developments may better told quantitative scientometric narrative article presents bibliometric analysis drivers topic well main themes research international collaborations influential authors seminal publications authors publications influenced one another metadata documents retrieved scopus database using keyword computational thinking results show research centric start continues dominated researchers volume impact international collaboration relatively low clusters joint research found example number nordic countries lusophone hispanophone countries central european countries results show features computing traditional tripartite disciplinary structure design modeling theory distinct emphasis programming strong pedagogical educational backdrop including constructionism self efficacy motivation teacher training
"Emerging memcapacitive nanoscale devices have the potential to perform computations in new ways. In this article, we systematically study, to the best of our knowledge for the first time, the computational capacity of complex memcapacitive networks, which function as reservoirs inreservoir computing,one of the brain-inspired computing architectures. Memcapacitive networks are composed of memcapacitive devices randomly connected through nanowires. Previous studies have shown that both regular and random reservoirs provide sufficient dynamics to perform simple tasks. How do complex memcapacitive networks illustrate their computational capability, and what are the topological structures of memcapacitive networks that solve complex tasks with efficiency? Studies show that small-world power-law (SWPL) networks offer an ideal trade-off between the communication properties and the wiring cost of networks. In this study, we illustrate the computing nature of SWPL memcapacitive reservoirs by exploring the two essential properties: fading memory and linear separation through measurements of kernel quality. Compared to ideal reservoirs, nanowire memcapacitive reservoirs had a better dynamic response and improved their performance by 4.67% on three tasks: MNIST, Isolated Spoken Digits, and CIFAR-10. On the same three tasks, compared to memristive reservoirs, nanowire memcapacitive reservoirs achieved comparable performance with much less power, on average, about 99× , 17×, and 277×, respectively. Simulation results of the topological transformation of memcapacitive networks reveal that that topological structures of the memcapacitive SWPL reservoirs did not affect their performance but significantly contributed to the wiring cost and the power consumption of the systems. The minimum trade-off between the wiring cost and the power consumption occurred at different network settings ofαandβ: 4.5 and 0.61 forBiolekreservoirs, 2.7 and 1.0 forMohamedreservoirs, and 3.0 and 1.0 forNajemreservoirs. The results of our research illustrate the computational capacity of complex memcapacitive networks as reservoirs in reservoir computing. Such memcapacitive networks with an SWPL topology are energy-efficient systems that are suitable for low-power applications such as mobile devices and the Internet of Things.",emerging memcapacitive nanoscale devices potential perform computations new ways article systematically study best knowledge first time computational capacity complex memcapacitive networks function reservoirs inreservoir computing one brain inspired computing architectures memcapacitive networks composed memcapacitive devices randomly connected nanowires previous studies shown regular random reservoirs provide sufficient dynamics perform simple tasks complex memcapacitive networks illustrate computational capability topological structures memcapacitive networks solve complex tasks efficiency studies show small world power law swpl networks offer ideal trade communication properties wiring cost networks study illustrate computing nature swpl memcapacitive reservoirs exploring two essential properties fading memory linear separation measurements kernel quality compared ideal reservoirs nanowire memcapacitive reservoirs better dynamic response improved performance three tasks mnist isolated spoken digits cifar three tasks compared memristive reservoirs nanowire memcapacitive reservoirs achieved comparable performance much less power average respectively simulation results topological transformation memcapacitive networks reveal topological structures memcapacitive swpl reservoirs affect performance significantly contributed wiring cost power consumption systems minimum trade wiring cost power consumption occurred different network settings forbiolekreservoirs formohamedreservoirs fornajemreservoirs results research illustrate computational capacity complex memcapacitive networks reservoirs reservoir computing memcapacitive networks swpl topology energy efficient systems suitable low power applications mobile devices internet things
A new French keyboard standard is the first designed with the help of computational methods.,new french keyboard standard first designed help computational methods
"Have you ever heard the phrase, thinking like a lawyer? Perhaps the most commonly echoed phrase about law school is that students learn how to ""think like a lawyer,"" but what does that mean? As a lawyer, who went through the initiating ritual of law school, sitting for the bar, and litigating cases, I have allegedly gained this skill called ""thinking like a lawyer."" As I reflect upon how the experience of becoming a lawyer and practicing law changed my thinking, a few things come to mind. First, I am subconsciously scanning for risks and liability everywhere, fabricating fictitious lawsuits in my head that might occur. Second, I care a lot about words, what exactly was said, and the meaning and truthfulness of the assertion when a strict vs. lenient interpretation of those words is applied. Third, I have the ability to navigate complex systems and to guide others less familiar with those systems through the maze. Finally, I have acquired a sobering view of what the legal system does and does not do, its promise and its shortcomings.",ever heard phrase thinking like lawyer perhaps commonly echoed phrase law school students learn think like lawyer mean lawyer went initiating ritual law school sitting bar litigating cases allegedly gained skill called thinking like lawyer reflect upon experience becoming lawyer practicing law changed thinking things come mind first subconsciously scanning risks liability everywhere fabricating fictitious lawsuits head might occur second care lot words exactly said meaning truthfulness assertion strict lenient interpretation words applied third ability navigate complex systems guide others less familiar systems maze finally acquired sobering view legal system promise shortcomings
"Earlier this year, I volunteered to help maintain the website content for an organization with which I am associated. When I got into the administrative portion of the WordPress website, I realized that no one had made any updates to the plugins for the past a year. Knowing that many such updates patch security exploits and keep the site working properly, I was a bit unnerved.",earlier year volunteered help maintain website content organization associated got administrative portion wordpress website realized one made updates plugins past year knowing many updates patch security exploits keep site working properly bit unnerved
"The partial string avoidability problem is stated as follows: given a finite set of strings with possible “holes” (wildcard symbols), determine whether there exists a two-sided infinite string containing no substrings from this set, assuming that a hole matches every symbol. The problem is known to be NP-hard and in PSPACE, and this article establishes its PSPACE-completeness. Next, string avoidability over the binary alphabet is interpreted as a version of conjunctive normal form satisfiability problem, where each clause has infinitely many shifted variants. Non-satisfiability of these formulas can be proved using variants of classical propositional proof systems, augmented with derivation rules for shifting proof lines (such as clauses, inequalities, polynomials, etc.). First, it is proved that there is a particular formula that has a short refutation in Resolution with a shift rule but requires classical proofs of exponential size. At the same time, it is shown that exponential lower bounds for classical proof systems can be translated for their shifted versions. Finally, it is shown that superpolynomial lower bounds on the size of shifted proofs would separate NP from PSPACE; a connection to lower bounds on circuit complexity is also established.",partial string avoidability problem stated follows given finite set strings possible holes wildcard symbols determine whether exists two sided infinite string containing substrings set assuming hole matches every symbol problem known hard pspace article establishes pspace completeness next string avoidability binary alphabet interpreted version conjunctive normal form satisfiability problem clause infinitely many shifted variants non satisfiability formulas proved using variants classical propositional proof systems augmented derivation rules shifting proof lines clauses inequalities polynomials etc first proved particular formula short refutation resolution shift rule requires classical proofs exponential size time shown exponential lower bounds classical proof systems translated shifted versions finally shown superpolynomial lower bounds size shifted proofs would separate pspace connection lower bounds circuit complexity also established
"Building on the concept “prototypes that filter the design space,” we establish how other kinds of design artifacts and activities (e.g., sketching, tests, concept posters, metaphors, design tools) are equally critical in filtering the design space. We also suggest a parallel term, “informing the design space,” to define how design artifacts and activities expand the design space. We focus on a 16-month, full-scale media architecture design project and zero in on seven of its component events, and use design-space schemas to shed light on the dynamics of the design space with respect to informing and filtering the design space. Our concluding contribution is to propose design-space thinking as a sub-discipline of design research. We argue that this research perspective serves to address the creative aspects of the design process, the generative potential of design-space thinking, and the tools that support design-space thinking and research.",building concept prototypes filter design space establish kinds design artifacts activities sketching tests concept posters metaphors design tools equally critical filtering design space also suggest parallel term informing design space define design artifacts activities expand design space focus month full scale media architecture design project zero seven component events use design space schemas shed light dynamics design space respect informing filtering design space concluding contribution propose design space thinking sub discipline design research argue research perspective serves address creative aspects design process generative potential design space thinking tools support design space thinking research
"As Information and Communication Technology (ICT) literacy education has recently shifted to fostering computing thinking ability as well as ICT use, many countries are conducting research on national curriculum and evaluation. In this study, we measured Korean students’ ICT literacy levels by using the national measurement tool that assesses abilities of the IT (Information Technology) area and the CT (Computational Thinking) area. A research team revised an existing ICT literacy assessment tool for the IT test and developed a new CT test environment in which students could perform actual coding through a web-based programming tool such as Scratch. Additionally, after assessing ICT literacy levels, differences in ICT literacy levels by gender and grade were analyzed to provide evidence for national education policies. Approximately 23,000 elementary and middle school students participated in the 2018 national assessment of ICT literacy, accounting for 1% of the national population of students. The findings demonstrated that female students had higher literacy levels in most sub-factors of IT and CT areas. Additionally, in the areas of strengths and weaknesses, the ratio of below-basic achievement among male students was at least two times greater than that of female students. Nonetheless, male students scored higher on CT automation, a coding item that involved problem solving using Scratch. Looking at the difference according to grade level, the level improved as the school year increased in elementary school, but there was no difference in middle school. When analyzing the detailed elements of middle school students, the automation factor of seventh grade students was found to be higher than eighth and ninth grade students. Based on these results, this study discussed some implications for ICT and computing education in elementary and middle schools.",information communication technology ict literacy education recently shifted fostering computing thinking ability well ict use many countries conducting research national curriculum evaluation study measured korean students ict literacy levels using national measurement tool assesses abilities information technology area computational thinking area research team revised existing ict literacy assessment tool test developed new test environment students could perform actual coding web based programming tool scratch additionally assessing ict literacy levels differences ict literacy levels gender grade analyzed provide evidence national education policies approximately elementary middle school students participated national assessment ict literacy accounting national population students findings demonstrated female students higher literacy levels sub factors areas additionally areas strengths weaknesses ratio basic achievement among male students least two times greater female students nonetheless male students scored higher automation coding item involved problem solving using scratch looking difference according grade level level improved school year increased elementary school difference middle school analyzing detailed elements middle school students automation factor seventh grade students found higher eighth ninth grade students based results study discussed implications ict computing education elementary middle schools
"In this work we provide an alternative, and equivalent, formulation of the concept of λ-theory without introducing the notion of substitution and the sets of all, free and bound variables occurring in a term. We call α β-relations our alternative versions of λ-theories. We also clarify the actual role of α-renaming in the lambda calculus: it expresses a property of extensionality for a certain class of terms. To motivate the necessity of α-renaming, we construct an unusual denotational model of the lambda calculus that validates all structural and beta conditions but not α-renaming. The article also has a survey character.",work provide alternative equivalent formulation concept theory without introducing notion substitution sets free bound variables occurring term call relations alternative versions theories also clarify actual role renaming lambda calculus expresses property extensionality certain class terms motivate necessity renaming construct unusual denotational model lambda calculus validates structural beta conditions renaming article also survey character
"Understanding team viability --- a team's capacity for sustained and future success --- is essential for building effective teams. In this study, we aggregate features drawn from the organizational behavior literature to train a viability classification model over a dataset of 669 10-minute text conversations of online teams. We train classifiers to identify teams at the top decile (most viable teams), 50th percentile (above a median split), and bottom decile (least viable teams), then characterize the attributes of teams at each of these viability levels. We find that a lasso regression model achieves an accuracy of .74--.92 AUC ROC under different thresholds of classifying viability scores. From these models, we identify the use of exclusive language such as 'but' and 'except', and the use of second person pronouns, as the most predictive features for detecting the most viable teams, suggesting that active engagement with others' ideas is a crucial signal of a viable team. Only a small fraction of the 10-minute discussion, as little as 70 seconds, is required for predicting the viability of team interaction. This work suggests opportunities for teams to assess, track, and visualize their own viability in real time as they collaborate.",understanding team viability team capacity sustained future success essential building effective teams study aggregate features drawn organizational behavior literature train viability classification model dataset minute text conversations online teams train classifiers identify teams top decile viable teams percentile median split bottom decile least viable teams characterize attributes teams viability levels find lasso regression model achieves accuracy auc roc different thresholds classifying viability scores models identify use exclusive language except use second person pronouns predictive features detecting viable teams suggesting active engagement others ideas crucial signal viable team small fraction minute discussion little seconds required predicting viability team interaction work suggests opportunities teams assess track visualize viability real time collaborate
"Recent CSCW research on the collaborative design and development of research infrastructures for the natural sciences has increasingly focused on the challenges of open data sharing. This qualitative study describes and analyzes how multidisciplinary, geographically distributed ocean scientists are integrating highly diverse data as part of an effort to develop a new research infrastructure to advance science. This paper identifies different kinds of coordination that are necessary to align processes of data collection, production, and analysis. Some of the hard work to integrate data is undertaken before data integration can even become a technical problem. After data integration becomes a technical problem, social and organizational means continue to be critical for resolving differences in assumptions, methods, practices, and priorities. This work calls attention to the diversity of coordinative, social, and organizational practices and concerns that are needed to integrate data and also how, in highly innovative work, the process of integrating data also helps to define scientific problem spaces themselves.",recent cscw research collaborative design development research infrastructures natural sciences increasingly focused challenges open data sharing qualitative study describes analyzes multidisciplinary geographically distributed ocean scientists integrating highly diverse data part effort develop new research infrastructure advance science paper identifies different kinds coordination necessary align processes data collection production analysis hard work integrate data undertaken data integration even become technical problem data integration becomes technical problem social organizational means continue critical resolving differences assumptions methods practices priorities work calls attention diversity coordinative social organizational practices concerns needed integrate data also highly innovative work process integrating data also helps define scientific problem spaces
"Dependently typed programming languages and proof assistants such as Agda and Coq rely on computation to automatically simplify expressions during type checking. To overcome the lack of certain programming primitives or logical principles in those systems, it is common to appeal to axioms to postulate their existence. However, one can only postulate the bare existence of an axiom, not its computational behaviour. Instead, users are forced to postulate equality proofs and appeal to them explicitly to simplify expressions, making axioms dramatically more complicated to work with than built-in primitives. On the other hand, the equality reflection rule from extensional type theory solves these problems by collapsing computation and equality, at the cost of having no practical type checking algorithm. This paper introduces Rewriting Type Theory (RTT), a type theory where it is possible to add computational assumptions in the form of rewrite rules. Rewrite rules go beyond the computational capabilities of intensional type theory, but in contrast to extensional type theory, they are applied automatically so type checking does not require input from the user. To ensure type soundness of RTT—as well as effective type checking—we provide a framework where confluence of user-defined rewrite rules can be checked modularly and automatically, and where adding new rewrite rules is guaranteed to preserve subject reduction. The properties of RTT have been formally verified using the MetaCoq framework and an implementation of rewrite rules is already available in the Agda proof assistant.",dependently typed programming languages proof assistants agda coq rely computation automatically simplify expressions type checking overcome lack certain programming primitives logical principles systems common appeal axioms postulate existence however one postulate bare existence axiom computational behaviour instead users forced postulate equality proofs appeal explicitly simplify expressions making axioms dramatically complicated work built primitives hand equality reflection rule extensional type theory solves problems collapsing computation equality cost practical type checking algorithm paper introduces rewriting type theory rtt type theory possible add computational assumptions form rewrite rules rewrite rules beyond computational capabilities intensional type theory contrast extensional type theory applied automatically type checking require input user ensure type soundness rtt well effective type checking provide framework confluence user defined rewrite rules checked modularly automatically adding new rewrite rules guaranteed preserve subject reduction properties rtt formally verified using metacoq framework implementation rewrite rules already available agda proof assistant
"Compact closed categories include objects representing higher-order functions and are well-established as models of linear logic, concurrency, and quantum computing. We show that it is possible to construct such compact closed categories for conventional sum and product types by defining a dual to sum types, a negative type, and a dual to product types, a fractional type. Inspired by the categorical semantics, we define a sound operational semantics for negative and fractional types in which a negative type represents a computational effect that ``reverses execution flow'' and a fractional type represents a computational effect that ``garbage collects'' particular values or throws exceptions.Specifically, we extend a first-order reversible language of type isomorphisms with negative and fractional types, specify an operational semantics for each extension, and prove that each extension forms a compact closed category. We furthermore show that both operational semantics can be merged using the standard combination of backtracking and exceptions resulting in a smooth interoperability of negative and fractional types. We illustrate the expressiveness of this combination by writing a reversible SAT solver that uses backtracking search along freshly allocated and de-allocated locations. The operational semantics, most of its meta-theoretic properties, and all examples are formalized in a supplementary Agda package.",compact closed categories include objects representing higher order functions well established models linear logic concurrency quantum computing show possible construct compact closed categories conventional sum product types defining dual sum types negative type dual product types fractional type inspired categorical semantics define sound operational semantics negative fractional types negative type represents computational effect reverses execution flow fractional type represents computational effect garbage collects particular values throws exceptions specifically extend first order reversible language type isomorphisms negative fractional types specify operational semantics extension prove extension forms compact closed category furthermore show operational semantics merged using standard combination backtracking exceptions resulting smooth interoperability negative fractional types illustrate expressiveness combination writing reversible sat solver uses backtracking search along freshly allocated allocated locations operational semantics meta theoretic properties examples formalized supplementary agda package
"War video games recreate war situations with great realism, favoring the immersion of users and making them participants of the sufferings derived from war. While some of them help the players to understand the war, appealing to the hyper-realistic simulation, others are entertainment artifacts that banalize it. This research seeks to determine the edu-communicative potential of war video games, understood as the ability to drive critical thinking towards war. To do this, we adopt a qualitative methodology based on the study of 10 cases, using the WarVG-A (War video game evaluation) instrument to perform the content analysis, based on six dimensions (cognitive, personal-attitudinal, ethical, logical, argumentative, and expressive-communicative) with different indicators and categories. The results of the analysis indicate that these primary games terrorist tactics and historical adaptation, addressing war from a critical approach. Most evidence ethical dilemmas such as child soldiers, the economics of war (arms sales and conflict perpetuation). Economic and geo-political interests are the engines that drive wars, considered inevitable to defend themselves. The stereotypes that abound are gender and ethnic warmongers represented by photorealistic and cinematographic aesthetics. Only four of the selected playful artifacts can identify tools capable of driving critical thinking, inviting players to reflect on war from a more realistic position and emotionally involving them. However, the explicit representation of violence limits its use in school contexts.",war video games recreate war situations great realism favoring immersion users making participants sufferings derived war help players understand war appealing hyper realistic simulation others entertainment artifacts banalize research seeks determine edu communicative potential war video games understood ability drive critical thinking towards war adopt qualitative methodology based study cases using warvg war video game evaluation instrument perform content analysis based six dimensions cognitive personal attitudinal ethical logical argumentative expressive communicative different indicators categories results analysis indicate primary games terrorist tactics historical adaptation addressing war critical approach evidence ethical dilemmas child soldiers economics war arms sales conflict perpetuation economic geo political interests engines drive wars considered inevitable defend stereotypes abound gender ethnic warmongers represented photorealistic cinematographic aesthetics four selected playful artifacts identify tools capable driving critical thinking inviting players reflect war realistic position emotionally involving however explicit representation violence limits use school contexts
"We are excited to announce the twelve recipients of the ACM SIGHPC Computational and Data Science Fellowships for 2020. The program is intended to increase the diversity of students pursuing graduate degrees in data science and computational science, including women as well as students from racial and ethnic backgrounds that have been historically underrepresented in the computing field. The fellowship provides $15,000 annually for study anywhere in the world.",excited announce twelve recipients acm sighpc computational data science fellowships program intended increase diversity students pursuing graduate degrees data science computational science including women well students racial ethnic backgrounds historically underrepresented computing field fellowship provides annually study anywhere world
"This year's SIGHPC Doctoral Dissertation Award goes to Dr. Patrick Flick. This award is given each year for the best doctoral dissertation completed in high performance computing (HPC) in the previous year, and includes a $2,000 cash prize, a plaque, and recognition at SC20 in November. Nominations were evaluated on technical merit, the significance of the research contribution, the potential impact on theory and practice, and overall quality of work.",year sighpc doctoral dissertation award goes patrick flick award given year best doctoral dissertation completed high performance computing hpc previous year includes cash prize plaque recognition november nominations evaluated technical merit significance research contribution potential impact theory practice overall quality work
"Cold bent glass is a promising and cost-efficient method for realizing doubly curved glass façades. They are produced by attaching planar glass sheets to curved frames and must keep the occurring stress within safe limits. However, it is very challenging to navigate the design space of cold bent glass panels because of the fragility of the material, which impedes the form finding for practically feasible and aesthetically pleasing cold bent glass façades. We propose an interactive, data-driven approach for designing cold bent glass façades that can be seamlessly integrated into a typical architectural design pipeline. Our method allows non-expert users to interactively edit a parametric surface while providing real-time feedback on the deformed shape and maximum stress of cold bent glass panels. The designs are automatically refined to minimize several fairness criteria, while maximal stresses are kept within glass limits. We achieve interactive frame rates by using a differentiable Mixture Density Network trained from more than a million simulations. Given a curved boundary, our regression model is capable of handling multistable configurations and accurately predicting the equilibrium shape of the panel and its corresponding maximal stress. We show that the predictions are highly accurate and validate our results with a physical realization of a cold bent glass surface.",cold bent glass promising cost efficient method realizing doubly curved glass ades produced attaching planar glass sheets curved frames must keep occurring stress within safe limits however challenging navigate design space cold bent glass panels fragility material impedes form finding practically feasible aesthetically pleasing cold bent glass ades propose interactive data driven approach designing cold bent glass ades seamlessly integrated typical architectural design pipeline method allows non expert users interactively edit parametric surface providing real time feedback deformed shape maximum stress cold bent glass panels designs automatically refined minimize several fairness criteria maximal stresses kept within glass limits achieve interactive frame rates using differentiable mixture density network trained million simulations given curved boundary regression model capable handling multistable configurations accurately predicting equilibrium shape panel corresponding maximal stress show predictions highly accurate validate results physical realization cold bent glass surface
"The past 25 years have seen many attempts to introduce defeasible-reasoning capabilities into a description logic setting. Many, if not most, of these attempts are based on preferential extensions of description logics, with a significant number of these, in turn, following the so-called KLM approach to defeasible reasoning initially advocated for propositional logic by Kraus, Lehmann, and Magidor. Each of these attempts has its own aim of investigating particular constructions and variants of the (KLM-style) preferential approach. Here our aim is to provide a comprehensive study of the formal foundations of preferential defeasible reasoning for description logics in the KLM tradition.We start by investigating a notion ofdefeasible subsumptionin the spirit of defeasible conditionals as studied by Kraus, Lehmann, and Magidor in the propositional case. In particular, we consider a natural and intuitive semantics for defeasible subsumption, and we investigate KLM-style syntactic properties for bothpreferentialandrationalsubsumption. Our contribution includes two representation results linking our semantic constructions to the set of preferential and rational properties considered. Besides showing that our semantics is appropriate, these results pave the way for more effective decision procedures for defeasible reasoning in description logics. Indeed, we also analyse the problem of non-monotonic reasoning in description logics at the level ofentailmentand present an algorithm for the computation ofrational closureof a defeasible knowledge base. Importantly, our algorithm relies completely on classical entailment and shows that the computational complexity of reasoning over defeasible knowledge bases is no worse than that of reasoning in the underlying classical DLALC.",past years seen many attempts introduce defeasible reasoning capabilities description logic setting many attempts based preferential extensions description logics significant number turn following called klm approach defeasible reasoning initially advocated propositional logic kraus lehmann magidor attempts aim investigating particular constructions variants klm style preferential approach aim provide comprehensive study formal foundations preferential defeasible reasoning description logics klm tradition start investigating notion ofdefeasible subsumptionin spirit defeasible conditionals studied kraus lehmann magidor propositional case particular consider natural intuitive semantics defeasible subsumption investigate klm style syntactic properties bothpreferentialandrationalsubsumption contribution includes two representation results linking semantic constructions set preferential rational properties considered besides showing semantics appropriate results pave way effective decision procedures defeasible reasoning description logics indeed also analyse problem non monotonic reasoning description logics level ofentailmentand present algorithm computation ofrational closureof defeasible knowledge base importantly algorithm relies completely classical entailment shows computational complexity reasoning defeasible knowledge bases worse reasoning underlying classical dlalc
"We study the model-checking problem for a logic for true concurrency, whose formulae predicate about events in computations and their causal dependencies. The logic, which represents the logical counterpart of history-preserving bisimilarity, is naturally interpreted over event structures or any formalism that can be given a causal semantics, like Petri nets. It includes least and greatest fixpoint operators and thus it can express properties of infinite computations. Since the event structure associated with a system is typically infinite (even if the system is finite state), already the decidability of model-checking is non-trivial. We first develop a local model-checking technique based on a tableau system, for which, over a class of event structures satisfying a suitable regularity condition, referred to as strong regularity, we prove termination, soundness, and completeness. The tableau system allows for a clean and intuitive proof of decidability, but a direct implementation of the procedure can be extremely inefficient. For easing the development of a more efficient model-checking technique, we move to an automata-theoretic framework. Given a formula and a strongly regular event structure, we show how to construct a parity tree automaton whose language is non-empty if and only if the event structure satisfies the formula. The automaton is usually infinite. We discuss how it can be quotiented to an equivalent finite automaton, where emptiness can be checked effectively. To show the applicability of the approach, we discuss how it instantiates to finite safe Petri nets, providing also a corresponding proof-of-concept model-checking tool.",study model checking problem logic true concurrency whose formulae predicate events computations causal dependencies logic represents logical counterpart history preserving bisimilarity naturally interpreted event structures formalism given causal semantics like petri nets includes least greatest fixpoint operators thus express properties infinite computations since event structure associated system typically infinite even system finite state already decidability model checking non trivial first develop local model checking technique based tableau system class event structures satisfying suitable regularity condition referred strong regularity prove termination soundness completeness tableau system allows clean intuitive proof decidability direct implementation procedure extremely inefficient easing development efficient model checking technique move automata theoretic framework given formula strongly regular event structure show construct parity tree automaton whose language non empty event structure satisfies formula automaton usually infinite discuss quotiented equivalent finite automaton emptiness checked effectively show applicability approach discuss instantiates finite safe petri nets providing also corresponding proof concept model checking tool
"Strong equivalence is one of the basic notions of equivalence that have been proposed for logic programs subject to the answer-set semantics. In this article, we propose a new generalization of strong equivalence (SE) that takes the visibility of atoms into account and we characterize it in terms of appropriately revised SE-models. Our design resembles (relativized) strong equivalence but is substantially different due to adopting a strict one-to-one correspondence of models from the notion of visible equivalence. We additionally tailor the characterization for more convenient use with positive programs and provide formal tools to exploit the tailored version also in the case of some programs that use negation. We illustrate the use of visible strong equivalence and the characterizations in showing the correctness of program transformations that make use of atom visibility. Moreover, we present a translation that enables us to automate the task of verifying visible strong equivalence for particular fragments of answer-set programs. We experimentally study the efficiency of verification when the goal is to check whether an extended rule is visibly strongly equivalent to its normalization, i.e., a subprogram expressing the original rule in terms of normal rules only. In the process, we verify the outputs of several real implementations of normalization schemes on a considerable number of input rules.",strong equivalence one basic notions equivalence proposed logic programs subject answer set semantics article propose new generalization strong equivalence takes visibility atoms account characterize terms appropriately revised models design resembles relativized strong equivalence substantially different due adopting strict one one correspondence models notion visible equivalence additionally tailor characterization convenient use positive programs provide formal tools exploit tailored version also case programs use negation illustrate use visible strong equivalence characterizations showing correctness program transformations make use atom visibility moreover present translation enables automate task verifying visible strong equivalence particular fragments answer set programs experimentally study efficiency verification goal check whether extended rule visibly strongly equivalent normalization subprogram expressing original rule terms normal rules process verify outputs several real implementations normalization schemes considerable number input rules
"We investigate the decidability of model checking logics of time, knowledge, and probability, with respect to two epistemic semantics: the clock and synchronous perfect recall semantics in partially observable discrete-time Markov chains. Decidability results are known for certain restricted logics with respect to these semantics, subject to a variety of restrictions that are either unexplained or involve a longstanding unsolved mathematical problem. We show that mild generalizations of the known decidable cases suffice to render the model checking problem definitively undecidable. In particular, for the synchronous perfect recall semantics, a generalization from temporal operators with finite reach to operators with infinite reach renders model checking undecidable. The case of the clock semantics is closely related to a monadic second-order logic of time and probability that is known to be decidable, except on a set of measure zero. We show that two distinct extensions of this logic make model checking undecidable. One of these involves polynomial combinations of probability terms, the other involves monadic second-order quantification into the scope of probability operators. These results explain some of the restrictions in previous work.",investigate decidability model checking logics time knowledge probability respect two epistemic semantics clock synchronous perfect recall semantics partially observable discrete time markov chains decidability results known certain restricted logics respect semantics subject variety restrictions either unexplained involve longstanding unsolved mathematical problem show mild generalizations known decidable cases suffice render model checking problem definitively undecidable particular synchronous perfect recall semantics generalization temporal operators finite reach operators infinite reach renders model checking undecidable case clock semantics closely related monadic second order logic time probability known decidable except set measure zero show two distinct extensions logic make model checking undecidable one involves polynomial combinations probability terms involves monadic second order quantification scope probability operators results explain restrictions previous work
"This is a consolidated look at computational techniques for sustainability, and their limits and possibilities. Sustainability is already well established as a concern and a topic of study and practice, given the alarming increase of environmental degradation, pollution, and other adverse effects of industrialization and urbanization. Computational sustainability, which focuses on the use of effective computational models and computational approaches to help achieve the goal of sustainability, has attracted interest from computer science researchers worldwide. We review recent work on computational techniques applied to a range of domains related to sustainability, from bio-surveillance to poverty mapping, from renewable energy production forecasting to crop disease monitoring, and from agent-based modeling to stochastic network design. In sustainable computing, we discuss some directions that have recently been explored. Finally, we analyze research directions that could be explored in the future to achieve the goal of long-term environmental sustainability.",consolidated look computational techniques sustainability limits possibilities sustainability already well established concern topic study practice given alarming increase environmental degradation pollution adverse effects industrialization urbanization computational sustainability focuses use effective computational models computational approaches help achieve goal sustainability attracted interest computer science researchers worldwide review recent work computational techniques applied range domains related sustainability bio surveillance poverty mapping renewable energy production forecasting crop disease monitoring agent based modeling stochastic network design sustainable computing discuss directions recently explored finally analyze research directions could explored future achieve goal long term environmental sustainability
"Convex variational problems arise in many fields ranging from image processing to fluid and solid mechanics communities. Interesting applications usually involve non-smooth terms, which require well-designed optimization algorithms for their resolution. The present manuscript presents the Python package called fenics_optim built on top of the FEniCS finite element software, which enables one to automate the formulation and resolution of various convex variational problems. Formulating such a problem relies on FEniCS domain-specific language and the representation of convex functions, in particular, non-smooth ones, in the conic programming framework. The discrete formulation of the corresponding optimization problems hinges on the finite element discretization capabilities offered by FEniCS, while their numerical resolution is carried out by the interior-point solver Mosek. Through various illustrative examples, we show that convex optimization problems can be formulated using only a few lines of code, discretized in a very simple manner, and solved extremely efficiently.",convex variational problems arise many fields ranging image processing fluid solid mechanics communities interesting applications usually involve non smooth terms require well designed optimization algorithms resolution present manuscript presents python package called fenics optim built top fenics finite element software enables one automate formulation resolution various convex variational problems formulating problem relies fenics domain specific language representation convex functions particular non smooth ones conic programming framework discrete formulation corresponding optimization problems hinges finite element discretization capabilities offered fenics numerical resolution carried interior point solver mosek various illustrative examples show convex optimization problems formulated using lines code discretized simple manner solved extremely efficiently
"Chronic pain is a significant source of suffering, disability and societal cost in the US. However, while the ability to detect a person's risk for developing persistent pain is desirable for timely assessment, management, treatment, and reduced health care costs---no objective measure to detect clinical pain intensity exist. Recent Artificial Intelligence (AI) methods have deployed clinical decision- making and assessment tools to enhance pain risk detection across core social and clinical domains. Yet, risk assessment models are only as ""good"" as the data they are based on. Thus, ensuring fairness is also a critical component of equitable care in both the short and long term. This paper takes an intersectional and public health approach to AI fairness in the context of pain and invisible disability, suggesting that computational ethnography is a multimodal and participatory real-world data (RWD) methodology that can be used to enhance the curation of intersectional knowledge bases, thereby expanding existing boundaries of AI fairness in terms of inclusiveness and transparency for pain and invisible disability use cases.",chronic pain significant source suffering disability societal cost however ability detect person risk developing persistent pain desirable timely assessment management treatment reduced health care costs objective measure detect clinical pain intensity exist recent artificial intelligence methods deployed clinical decision making assessment tools enhance pain risk detection across core social clinical domains yet risk assessment models good data based thus ensuring fairness also critical component equitable care short long term paper takes intersectional public health approach fairness context pain invisible disability suggesting computational ethnography multimodal participatory real world data rwd methodology used enhance curation intersectional knowledge bases thereby expanding existing boundaries fairness terms inclusiveness transparency pain invisible disability use cases
"We study the first-order (FO) model checking problem of dense graph classes, namely, those that have FO interpretations in (or are FO transductions of) some sparse graph classes. We give a structural characterization of the graph classes that are FO interpretable in graphs of bounded degree. This characterization allows us to efficiently compute such an FO interpretation for an input graph. As a consequence, we obtain an FPT algorithm for successor-invariant FO model checking on any graph class that is FO interpretable in (or an FO transduction of) a graph class of bounded degree. The approach we use to obtain these results may also be of independent interest.",study first order model checking problem dense graph classes namely interpretations transductions sparse graph classes give structural characterization graph classes interpretable graphs bounded degree characterization allows efficiently compute interpretation input graph consequence obtain fpt algorithm successor invariant model checking graph class interpretable transduction graph class bounded degree approach use obtain results may also independent interest
"This study compared the results of a usability inspection conducted under two separate conditions: An explicit concurrent think-aloud that required explanations and silent working. 12 student analysts inspected two travel websites thinking-aloud and working in silence to produce a set of problem predictions. Overall, the silent working condition produced more initial predictions, but the think-aloud condition yielded a greater proportion of accurate predictions as revealed by falsification testing. The analysts used a range of problem discovery methods with system searching being favoured by the silent working condition and the more active, goal playing discovery method in the think-aloud condition. Thinking-aloud was also associated with a broader spread of knowledge resources.",study compared results usability inspection conducted two separate conditions explicit concurrent think aloud required explanations silent working student analysts inspected two travel websites thinking aloud working silence produce set problem predictions overall silent working condition produced initial predictions think aloud condition yielded greater proportion accurate predictions revealed falsification testing analysts used range problem discovery methods system searching favoured silent working condition active goal playing discovery method think aloud condition thinking aloud also associated broader spread knowledge resources
"Modern computing systems are highly concurrent. Threads run concurrently in shared-memory multi-core systems, and programs run in different servers communicating by sending messages to each other. Concurrent programming is hard because it requires to cope with many possible, unpredictable behaviors of the processes, and the communication media. The article argues that right from the start in 1960's, the main way of dealing with concurrency has been by reduction to sequential reasoning. It traces this history, and illustrates it through several examples, from early ideas based on mutual exclusion (which was initially introduced to access shared physical resources), passing through consensus and concurrent objects (which are immaterial data), until today distributed ledgers. A discussion is also presented, which addresses the limits that this approach encounters, related to fault-tolerance, performance, and inherently concurrent problems.",modern computing systems highly concurrent threads run concurrently shared memory multi core systems programs run different servers communicating sending messages concurrent programming hard requires cope many possible unpredictable behaviors processes communication media article argues right start main way dealing concurrency reduction sequential reasoning traces history illustrates several examples early ideas based mutual exclusion initially introduced access shared physical resources passing consensus concurrent objects immaterial data today distributed ledgers discussion also presented addresses limits approach encounters related fault tolerance performance inherently concurrent problems
"Overview. In this edition of the column, we have a fascinating contribution by Sergio Rajsbaum and Michel Raynal, who provide an in-depth survey of how our understanding of concurrency has evolved over the past sixty years.",overview edition column fascinating contribution sergio rajsbaum michel raynal provide depth survey understanding concurrency evolved past sixty years
"Very few studies have explored linkages between physiological, such as electroencephalograph (EEG), and behavioral patterns, such as wrist movements. These linkages provide us a unique mechanism to predict one set of patterns from other related patterns. Unlike conventional biometrics, EEG biometrics are hard to spoof using standard presentation attack methods, given the intrinsic liveness resulting from the bounded randomness of EEG signals specific to an individual. In this article, we propose a novel attack on the EEG-based authentication systems by investigating and leveraging the strong correlation between hand movements and brain signals captured through the motion sensors on a smartwatch and the wearable EEG headset, respectively. Based on this technique, we can successfully estimate the user’s EEG signals from the stolen hand movement data while the user was typing on the keyboard. Our attack results on the EEG biometric authentication system show an increase in the mean equal error rates of the classifiers by between 180% and 360% based on a dataset of 59 users. In summary, our pilot study calls for a rethinking of EEG-based authentication mechanisms from the perspective of unique vulnerabilities, particularly for multimodal biometric systems involving a variety of wearable or mobile devices.",studies explored linkages physiological electroencephalograph eeg behavioral patterns wrist movements linkages provide unique mechanism predict one set patterns related patterns unlike conventional biometrics eeg biometrics hard spoof using standard presentation attack methods given intrinsic liveness resulting bounded randomness eeg signals specific individual article propose novel attack eeg based authentication systems investigating leveraging strong correlation hand movements brain signals captured motion sensors smartwatch wearable eeg headset respectively based technique successfully estimate user eeg signals stolen hand movement data user typing keyboard attack results eeg biometric authentication system show increase mean equal error rates classifiers based dataset users summary pilot study calls rethinking eeg based authentication mechanisms perspective unique vulnerabilities particularly multimodal biometric systems involving variety wearable mobile devices
"A team's early interactions are influential: small behaviors cascade, driving the team either toward successful collaboration or toward fracture. Would a team be more viable if it could undo initial interactional missteps and try again? We introduce a technique that supports online and remote teams in creating multiple parallel worlds: the same team meets many times, led to believe that each convening is with a new team due to pseudonym masking while actual membership remains static. Afterward, the team moves forward with the parallel world with the highest viability by using the same pseudonyms and conversation history from that instance. In two experiments, we find that this technique improves team viability: teams that are reconvened from the highest-viability parallel world are significantly more viable than the same group meeting in a new parallel world. Our work suggests parallel worlds can help teams start off on the right foot - and stay there.",team early interactions influential small behaviors cascade driving team either toward successful collaboration toward fracture would team viable could undo initial interactional missteps try introduce technique supports online remote teams creating multiple parallel worlds team meets many times led believe convening new team due pseudonym masking actual membership remains static afterward team moves forward parallel world highest viability using pseudonyms conversation history instance two experiments find technique improves team viability teams reconvened highest viability parallel world significantly viable group meeting new parallel world work suggests parallel worlds help teams start right foot stay
"Prior studies of technology non-use demonstrate the need for approaches that go beyond a simple binary distinction between users and non-users. This paper proposes a set of two different methods by which researchers can identify types of non/use relevant to the particular sociotechnical settings they are studying. These methods are demonstrated by applying them to survey data about Facebook non/use. The results demonstrate that the different methods proposed here identify fairly comparable types of non/use. They also illustrate how the two methods make different trade offs between the granularity of the resulting typology and the total sample size. The paper also demonstrates how the different typologies resulting from these methods can be used in predictive modeling, allowing for the two methods to corroborate or disconfirm results from one another. The discussion considers implications and applications of these methods, both for research on technology non/use and for studying social computing more broadly.",prior studies technology non use demonstrate need approaches beyond simple binary distinction users non users paper proposes set two different methods researchers identify types non use relevant particular sociotechnical settings studying methods demonstrated applying survey data facebook non use results demonstrate different methods proposed identify fairly comparable types non use also illustrate two methods make different trade offs granularity resulting typology total sample size paper also demonstrates different typologies resulting methods used predictive modeling allowing two methods corroborate disconfirm results one another discussion considers implications applications methods research technology non use studying social computing broadly
"The use of computers in design is substantially different today from what it was only 30 years ago, and light-years ahead of how things were designed before computers entered the scene 60 years ago. This article discusses the use of computers, more specifically computational design, as a useful tool for designers. Herein, computational design refers to the application of computational tools to design practice.",use computers design substantially different today years ago light years ahead things designed computers entered scene years ago article discusses use computers specifically computational design useful tool designers herein computational design refers application computational tools design practice
"This article addresses refinement and testing based on CSP models, when we distinguish input and output events. In a testing experiment, the tester (or the environment) controls the inputs, and the system under test controls the outputs. The standard models and refinement relations of CSP, however, do not differentiate inputs and outputs and are not, therefore, entirely suitable for testing. Here, we consider an alphabet of events partitioned into inputs and outputs, and we present a novel refusal-testing model for CSP with a notion of input-output refusal-traces refinement. We compare that with the ioco relation often used in testing, and we find that it is more widely applicable and stronger. This means that mistakes found using traditional ioco testing do indicate mistakes in the development. Finally, we provide a CSP testing theory that takes into account inputs and outputs. With our theory, it becomes feasible to develop techniques and tools for automatic generation of realistic and sound tests from CSP models. Our work reconciles the normally disparate areas of refinement and (formal) testing by identifying how ioco testing can be used to inform refinement-based results and vice-versa.",article addresses refinement testing based csp models distinguish input output events testing experiment tester environment controls inputs system test controls outputs standard models refinement relations csp however differentiate inputs outputs therefore entirely suitable testing consider alphabet events partitioned inputs outputs present novel refusal testing model csp notion input output refusal traces refinement compare ioco relation often used testing find widely applicable stronger means mistakes found using traditional ioco testing indicate mistakes development finally provide csp testing theory takes account inputs outputs theory becomes feasible develop techniques tools automatic generation realistic sound tests csp models work reconciles normally disparate areas refinement formal testing identifying ioco testing used inform refinement based results vice versa
"We present a method for designing smooth cross fields on surfaces that automatically align to sharp features of an underlying geometry. Our approach introduces a novel class of energies based on a representation of cross fields in the spherical harmonic basis. We provide theoretical analysis of these energies in the smooth setting, showing that they penalize deviations from surface creases while otherwise promoting intrinsically smooth fields. We demonstrate the applicability of our method to quad meshing and include an extensive benchmark comparing our fields to other automatic approaches for generating feature-aligned cross fields on triangle meshes.",present method designing smooth cross fields surfaces automatically align sharp features underlying geometry approach introduces novel class energies based representation cross fields spherical harmonic basis provide theoretical analysis energies smooth setting showing penalize deviations surface creases otherwise promoting intrinsically smooth fields demonstrate applicability method quad meshing include extensive benchmark comparing fields automatic approaches generating feature aligned cross fields triangle meshes
"The usual homogeneous form of equality type in Martin-Löf Type Theory contains identifications between elements of the same type. By contrast, the heterogeneous form of equality contains identifications between elements of possibly different types. This short note introduces a simple set of axioms for such types. The axioms are shown to be equivalent to the combination of systematic elimination rules for both forms of equality, albeit with typal (also known as “propositional”) computation properties, together with Streicher’s Axiom K, or equivalently, the principle of uniqueness of identity proofs.",usual homogeneous form equality type martin type theory contains identifications elements type contrast heterogeneous form equality contains identifications elements possibly different types short note introduces simple set axioms types axioms shown equivalent combination systematic elimination rules forms equality albeit typal also known propositional computation properties together streicher axiom equivalently principle uniqueness identity proofs
"Novel uses of graphical processing units for accelerated computation revolutionized the field of high-performance scientific computing by providing specialized workflows tailored to algorithmic requirements. As the era of Moore’s law draws to a close, many new non–von Neumann processors are emerging as potential computational accelerators, including those based on the principles of neuromorphic computing, tensor algebra, and quantum information. While development of these new processors is continuing to mature, the potential impact on accelerated computing is anticipated to be profound. We discuss how different processing models can advance computing in key scientific paradigms: machine learning and constraint satisfaction. Significantly, each of these new processor types utilizes a fundamentally different model of computation, and this raises questions about how to best use such processors in the design and implementation of applications. While many processors are being developed with a specific domain target, the ubiquity of spin-glass models and neural networks provides an avenue for multi-functional applications. This also hints at the infrastructure needed to integrate next-generation processing units into future high-performance computing systems.",novel uses graphical processing units accelerated computation revolutionized field high performance scientific computing providing specialized workflows tailored algorithmic requirements moore law draws close many new non von neumann processors emerging potential computational accelerators including based principles neuromorphic computing tensor algebra quantum information development new processors continuing mature potential impact accelerated computing anticipated profound discuss different processing models advance computing key scientific paradigms machine learning constraint satisfaction significantly new processor types utilizes fundamentally different model computation raises questions best use processors design implementation applications many processors developed specific domain target ubiquity spin glass models neural networks provides avenue multi functional applications also hints infrastructure needed integrate next generation processing units future high performance computing systems
"In the last decade, the problem of computational metaphor processing has garnered immense attention from the domains of computational linguistics and cognition. A wide panorama of approaches, ranging from a hand-coded rule system to deep learning techniques, have been proposed to automate different aspects of metaphor processing. In this article, we systematically examine the major theoretical views on metaphor and present their classification. We discuss the existing literature to provide a concise yet representative picture of computational metaphor processing. We conclude the article with possible research directions.",last decade problem computational metaphor processing garnered immense attention domains computational linguistics cognition wide panorama approaches ranging hand coded rule system deep learning techniques proposed automate different aspects metaphor processing article systematically examine major theoretical views metaphor present classification discuss existing literature provide concise yet representative picture computational metaphor processing conclude article possible research directions
"In this column, we review the most recent results on processing persistence diagrams as purely geometric objects. (Yes, this is not a typo! While persistence diagrams originally come from computational topology, this column will mainly focus on the geometric parts and the minimal amount of knowledge on computational topology is needed to read this column.) We mainly focus on computing a center persistence diagram under the bottleneck and Wasserstein distances, and also the approximate nearest neighbor query under the bottleneck distance. At the end, we will go over a few open problems as well as some direction for future research, e.g., (approximate) farthest neighbor query under the bottleneck distance.",column review recent results processing persistence diagrams purely geometric objects yes typo persistence diagrams originally come computational topology column mainly focus geometric parts minimal amount knowledge computational topology needed read column mainly focus computing center persistence diagram bottleneck wasserstein distances also approximate nearest neighbor query bottleneck distance end open problems well direction future research approximate farthest neighbor query bottleneck distance
"The inverse method is a saturation-based theorem-proving technique; it relies on a forward proof-search strategy and can be applied to cut-free calculi enjoying the subformula property. Here, we apply this method to derive the unprovability of a goal formulaGin Intuitionistic Propositional Logic. To this aim we design a forward calculusFRJ(G) for Intuitionistic unprovability, which is appropriate for constructively ascertaining the unprovability of a formulaGby providing a concise countermodel for it; in particular, we prove that the generated countermodels have minimal height. Moreover, we clarify the role of the saturated database obtained as result of a failed proof-search inFRJ(G) by showing how to extract from such a database a derivation witnessing the Intuitionistic validity of the goal.",inverse method saturation based theorem proving technique relies forward proof search strategy applied cut free calculi enjoying subformula property apply method derive unprovability goal formulagin intuitionistic propositional logic aim design forward calculusfrj intuitionistic unprovability appropriate constructively ascertaining unprovability formulagby providing concise countermodel particular prove generated countermodels minimal height moreover clarify role saturated database obtained result failed proof search infrj showing extract database derivation witnessing intuitionistic validity goal
"My dissertation research explores assessing urban accessibility and building interactive tools that support decision making processes (e.g., to influence city policy). Using crowdsourcing, visualization, and data science techniques, I will develop a suite of tools that enables stakeholders to understand and communicate about urban accessibility, with a specific focus on sidewalk accessibility. These interactive tools would allow stakeholders to learn about city (in)accessibility, surface underlying causes behind the current state, and help present that information effectively for political advocacy.",dissertation research explores assessing urban accessibility building interactive tools support decision making processes influence city policy using crowdsourcing visualization data science techniques develop suite tools enables stakeholders understand communicate urban accessibility specific focus sidewalk accessibility interactive tools would allow stakeholders learn city accessibility surface underlying causes behind current state help present information effectively political advocacy
"We study the model-checking problem for first- and monadic second-order logic on finite relational structures. The problem of verifying whether a formula of these logics is true on a given structure is considered intractable in general, but it does become tractable on interesting classes of structures, such as on classes whose Gaifman graphs have bounded treewidth. In this article, we continue this line of research and study model-checking for first- and monadic second-order logic in the presence of an ordering on the input structure. We do so in two settings: the general ordered case, where the input structures are equipped with a fixed order or successor relation, and the order-invariant case, where the formulas may resort to an ordering, but their truth must be independent of the particular choice of order. In the first setting we show very strong intractability results for most interesting classes of structures. In contrast, in the order-invariant case we obtain tractability results for order-invariant monadic second-order formulas on the same classes of graphs as in the unordered case. For first-order logic, we obtain tractability of successor-invariant formulas on classes whose Gaifman graphs have bounded expansion. Furthermore, we show that model-checking for order-invariant first-order formulas is tractable on coloured posets of bounded width.",study model checking problem first monadic second order logic finite relational structures problem verifying whether formula logics true given structure considered intractable general become tractable interesting classes structures classes whose gaifman graphs bounded treewidth article continue line research study model checking first monadic second order logic presence ordering input structure two settings general ordered case input structures equipped fixed order successor relation order invariant case formulas may resort ordering truth must independent particular choice order first setting show strong intractability results interesting classes structures contrast order invariant case obtain tractability results order invariant monadic second order formulas classes graphs unordered case first order logic obtain tractability successor invariant formulas classes whose gaifman graphs bounded expansion furthermore show model checking order invariant first order formulas tractable coloured posets bounded width
"We consider games with two antagonistic players—Éloïse (modelling a program) and Abélard (modelling a Byzantine environment)—and a third, unpredictable and uncontrollable player, which we call Nature. Motivated by the fact that the usual probabilistic semantics very quickly leads to undecidability when considering either infinite game graphs or imperfect-information, we propose two alternative semantics that lead to decidability where the probabilistic one fails: one based on counting and one based on topology.",consider games two antagonistic players élo modelling program abélard modelling byzantine environment third unpredictable uncontrollable player call nature motivated fact usual probabilistic semantics quickly leads undecidability considering either infinite game graphs imperfect information propose two alternative semantics lead decidability probabilistic one fails one based counting one based topology
"The 1st International Workshop on Geo-computational Thinking in Education (GeoEd 2019) was held in conjunction with the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL 2019). The workshop is intended to bring together experts from both geography (or related) and computer science disciplines who have primary interest in geospatial data and technologies, either from academia or industry, to discuss the grand challenges towards improving existing learning pathways through integration of geo-computational thinking in higher education. This could impact a variety of disciplines that increasingly deal with geospatial data beyond geography, such as social sciences, environmental sciences, public policy, climatology, and other geo-related disciplines. The workshop speakers and attendants have discussed their vision on challenges and opportunities of various topics within the workshop scope.",international workshop geo computational thinking education geoed held conjunction acm sigspatial international conference advances geographic information systems acm sigspatial workshop intended bring together experts geography related computer science disciplines primary interest geospatial data technologies either academia industry discuss grand challenges towards improving existing learning pathways integration geo computational thinking higher education could impact variety disciplines increasingly deal geospatial data beyond geography social sciences environmental sciences public policy climatology geo related disciplines workshop speakers attendants discussed vision challenges opportunities various topics within workshop scope
"The frequent usage of figurative language on online social networks, especially on Twitter, has the potential to mislead traditional sentiment analysis and recommender systems. Due to the extensive use of slangs, bashes, flames, and non-literal texts, tweets are a great source of figurative language, such as sarcasm, irony, metaphor, simile, hyperbole, humor, and satire. Starting with a brief introduction of figurative language and its various categories, this article presents an in-depth survey of the state-of-the-art techniques for computational detection of seven different figurative language categories, mainly on Twitter. For each figurative language category, we present details about the characterizing features, datasets, and state-of-the-art computational detection approaches. Finally, we discuss open challenges and future directions of research for each figurative language category.",frequent usage figurative language online social networks especially twitter potential mislead traditional sentiment analysis recommender systems due extensive use slangs bashes flames non literal texts tweets great source figurative language sarcasm irony metaphor simile hyperbole humor satire starting brief introduction figurative language various categories article presents depth survey state art techniques computational detection seven different figurative language categories mainly twitter figurative language category present details characterizing features datasets state art computational detection approaches finally discuss open challenges future directions research figurative language category
"Parquetry is the art and craft of decorating a surface with a pattern of differently colored veneers of wood, stone, or other materials. Traditionally, the process of designing and making parquetry has been driven by color, using the texture found in real wood only for stylization or as a decorative effect. Here, we introduce a computational pipeline that draws from the rich natural structure of strongly textured real-world veneers as a source of detail to approximate a target image as faithfully as possible using a manageable number of parts. This challenge is closely related to the established problems of patch-based image synthesis and stylization in some ways, but fundamentally different in others. Most importantly, the limited availability of resources (any piece of wood can only be used once) turns the relatively simple problem of finding the right piece for the target location into the combinatorial problem of finding optimal parts while avoiding resource collisions. We introduce an algorithm that efficiently solves an approximation to the problem. It further addresses challenges like gamut mapping, feature characterization, and the search for fabricable cuts. We demonstrate the effectiveness of the system by fabricating a selection of pieces of parquetry from different kinds of unstained wood veneer.",parquetry art craft decorating surface pattern differently colored veneers wood stone materials traditionally process designing making parquetry driven color using texture found real wood stylization decorative effect introduce computational pipeline draws rich natural structure strongly textured real world veneers source detail approximate target image faithfully possible using manageable number parts challenge closely related established problems patch based image synthesis stylization ways fundamentally different others importantly limited availability resources piece wood used turns relatively simple problem finding right piece target location combinatorial problem finding optimal parts avoiding resource collisions introduce algorithm efficiently solves approximation problem addresses challenges like gamut mapping feature characterization search fabricable cuts demonstrate effectiveness system fabricating selection pieces parquetry different kinds unstained wood veneer
"Metric Temporal Logic (MTL) and Timed Propositional Temporal Logic (TPTL) are quantitative extensions of Linear Temporal Logic (LTL) that are prominent and widely used in the verification of real-timed systems. We study MTL and TPTL as specification languages for one-counter machines. It is known that model checking one-counter machines against formulas of Freeze LTL (FLTL), a strict fragment of TPTL, is undecidable. We prove that in our setting, MTL is strictly less expressive than TPTL, and incomparable in expressiveness to FLTL, so undecidability for MTL is not implied by the result for FLTL. We show, however, that the model-checking problem for MTL is undecidable. We further prove that the satisfiability problem for the unary fragments of TPTL and MTL are undecidable; for TPTL, this even holds for the fragment in which only one register and the finally modality is used. This is opposed to a known decidability result for the satisfiability problem for the same fragment of FLTL.",metric temporal logic mtl timed propositional temporal logic tptl quantitative extensions linear temporal logic ltl prominent widely used verification real timed systems study mtl tptl specification languages one counter machines known model checking one counter machines formulas freeze ltl fltl strict fragment tptl undecidable prove setting mtl strictly less expressive tptl incomparable expressiveness fltl undecidability mtl implied result fltl show however model checking problem mtl undecidable prove satisfiability problem unary fragments tptl mtl undecidable tptl even holds fragment one register finally modality used opposed known decidability result satisfiability problem fragment fltl
A 50-year history of concurrency.,year history concurrency
"“Code Generation for Generally Mapped Finite Elements” includes performance results for the finite element methods discussed in that manuscript. The authors provided a Zenodo archive with the Firedrake components and dependencies used, as well as the scripts that generated the results. The software was installed on two similar platforms; then, new results were gathered and compared to the original results. After completing this process, the results have been deemed replicable by the reviewer.",code generation generally mapped finite elements includes performance results finite element methods discussed manuscript authors provided zenodo archive firedrake components dependencies used well scripts generated results software installed two similar platforms new results gathered compared original results completing process results deemed replicable reviewer
"We provide the first proof complexity results for QBF dependency calculi. By showing that the reflexive resolution path dependency scheme admits exponentially shorter Q-resolution proofs on a known family of instances, we answer a question first posed by Slivovsky and Szeider in 2014 [37]. Further, we conceive a method of QBF solving in which dependency recomputation is utilised as a form of inprocessing. Formalising this notion, we introduce a new version of Q-resolution in which a dependency scheme is applied dynamically. We demonstrate the further potential of this approach beyond that of the existing static system with an exponential separation. Last, we show that the same picture emerges in an analogous approach to the universal expansion paradigm.",provide first proof complexity results qbf dependency calculi showing reflexive resolution path dependency scheme admits exponentially shorter resolution proofs known family instances answer question first posed slivovsky szeider conceive method qbf solving dependency recomputation utilised form inprocessing formalising notion introduce new version resolution dependency scheme applied dynamically demonstrate potential approach beyond existing static system exponential separation last show picture emerges analogous approach universal expansion paradigm
"Effective collaboration in data science can leverage domain expertise from each team member and thus improve the quality and efficiency of the work. Computational notebooks give data scientists a convenient interactive solution for sharing and keeping track of the data exploration process through a combination of code, narrative text, visualizations, and other rich media. In this paper, we report how synchronous editing in computational notebooks changes the way data scientists work together compared to working on individual notebooks. We first conducted a formative survey with 195 data scientists to understand their past experience with collaboration in the context of data science. Next, we carried out an observational study of 24 data scientists working in pairs remotely to solve a typical data science predictive modeling problem, working on either notebooks supported by synchronous groupware or individual notebooks in a collaborative setting. The study showed that working on the synchronous notebooks improves collaboration by creating a shared context, encouraging more exploration, and reducing communication costs. However, the current synchronous editing features may lead to unbalanced participation and activity interference without strategic coordination. The synchronous notebooks may also amplify the tension between quick exploration and clear explanations. Building on these findings, we propose several design implications aimed at better supporting collaborative editing in computational notebooks, and thus improving efficiency in teamwork among data scientists.",effective collaboration data science leverage domain expertise team member thus improve quality efficiency work computational notebooks give data scientists convenient interactive solution sharing keeping track data exploration process combination code narrative text visualizations rich media paper report synchronous editing computational notebooks changes way data scientists work together compared working individual notebooks first conducted formative survey data scientists understand past experience collaboration context data science next carried observational study data scientists working pairs remotely solve typical data science predictive modeling problem working either notebooks supported synchronous groupware individual notebooks collaborative setting study showed working synchronous notebooks improves collaboration creating shared context encouraging exploration reducing communication costs however current synchronous editing features may lead unbalanced participation activity interference without strategic coordination synchronous notebooks may also amplify tension quick exploration clear explanations building findings propose several design implications aimed better supporting collaborative editing computational notebooks thus improving efficiency teamwork among data scientists
"Was a problematic team always doomed to frustration, or could it have ended another way? In this paper, we study the consistency of team fracture: a loss of team viability so severe that the team no longer wants to work together. Understanding whether team fracture is driven by the membership of the team, or by how their collaboration unfolded, motivates the design of interventions that either identify compatible teammates or ensure effective early interactions. We introduce an online experiment that reconvenes the same team without members realizing that they have worked together before, enabling us to temporarily erase previous team dynamics. Participants in our study completed a series of tasks across multiple teams, including one reconvened team, and privately blacklisted any teams that they would not want to work with again. We identify fractured teams as those blacklisted by half the members. We find that reconvened teams are strikingly polarized by task in the consistency of their fracture outcomes. On a creative task, teams might as well have been a completely different set of people: the same teams changed their fracture outcomes at a random chance rate. On a cognitive conflict and on an intellective task, the team instead replayed the same dynamics without realizing it, rarely changing their fracture outcomes. These results indicate that, for some tasks, team fracture can be strongly influenced by interactions in the first moments of a team's collaboration, and that interventions targeting these initial moments may be critical to scaffolding long-lasting teams.",problematic team always doomed frustration could ended another way paper study consistency team fracture loss team viability severe team longer wants work together understanding whether team fracture driven membership team collaboration unfolded motivates design interventions either identify compatible teammates ensure effective early interactions introduce online experiment reconvenes team without members realizing worked together enabling temporarily erase previous team dynamics participants study completed series tasks across multiple teams including one reconvened team privately blacklisted teams would want work identify fractured teams blacklisted half members find reconvened teams strikingly polarized task consistency fracture outcomes creative task teams might well completely different set people teams changed fracture outcomes random chance rate cognitive conflict intellective task team instead replayed dynamics without realizing rarely changing fracture outcomes results indicate tasks team fracture strongly influenced interactions first moments team collaboration interventions targeting initial moments may critical scaffolding long lasting teams
"In this article, we address two problems related to idempotent anti-unification. First, we show that there exists an anti-unification problem with a single idempotent symbol that has an infinite minimal complete set of generalizations. It means that anti-unification with a single idempotent symbol has infinitary or nullary generalization type, similar to anti-unification with two idempotent symbols, shown earlier by Loïc Pottier. Next, we develop an algorithm that takes an arbitrary idempotent anti-unification problem and computes a representation of its solution set in the form of a regular tree grammar. The algorithm does not depend on the number of idempotent function symbols in the input terms. The language generated by the grammar is the minimal complete set of generalizations of the given anti-unification problem, which implies that idempotent anti-unification is infinitary.",article address two problems related idempotent anti unification first show exists anti unification problem single idempotent symbol infinite minimal complete set generalizations means anti unification single idempotent symbol infinitary nullary generalization type similar anti unification two idempotent symbols shown earlier pottier next develop algorithm takes arbitrary idempotent anti unification problem computes representation solution set form regular tree grammar algorithm depend number idempotent function symbols input terms language generated grammar minimal complete set generalizations given anti unification problem implies idempotent anti unification infinitary
Seeking to change computing teaching to improve computer science.,seeking change computing teaching improve computer science
"Epidemic intelligence deals with the detection of outbreaks using formal (such as hospital records) and informal sources (such as user-generated text on the web) of information. In this survey, we discuss approaches for epidemic intelligence that use textual datasets, referring to it as “text-based epidemic intelligence.” We view past work in terms of two broad categories: health mention classification (selecting relevant text from a large volume) and health event detection (predicting epidemic events from a collection of relevant text). The focus of our discussion is the underlying computational linguistic techniques in the two categories. The survey also provides details of the state of the art in annotation techniques, resources, and evaluation strategies for epidemic intelligence.",epidemic intelligence deals detection outbreaks using formal hospital records informal sources user generated text web information survey discuss approaches epidemic intelligence use textual datasets referring text based epidemic intelligence view past work terms two broad categories health mention classification selecting relevant text large volume health event detection predicting epidemic events collection relevant text focus discussion underlying computational linguistic techniques two categories survey also provides details state art annotation techniques resources evaluation strategies epidemic intelligence
"In recent years, a new approach has been developed for verifying security protocols with the aim of combining the benefits of symbolic attackers and the benefits of unconditional soundness: the technique of the computationally complete symbolic attacker of Bana and Comon (BC) [8]. In this article, we argue that the real breakthrough of this technique is the recent introduction of its version for indistinguishability [9], because, with the extensions we introduce here, for the first time, there is a computationally sound symbolic technique that is syntactically strikingly simple, to which translating standard computational security notions is a straightforward matter, and that can be effectively used for verification of not only equivalence properties but trace properties of protocols as well. We first fully develop the core elements of this newer version by introducing several new axioms. We illustrate the power and the diverse use of the introduced axioms on simple examples first. We introduce an axiom expressing the Decisional Diffie-Hellman property. We analyze the Diffie-Hellman key exchange, both in its simplest form and an authenticated version as well. We provide computationally sound verification of real-or-random secrecy of the Diffie-Hellman key exchange protocol for multiple sessions, without any restrictions on the computational implementation other than the DDH assumption. We also show authentication for a simplified version of the station-to-station protocol using UF-CMA assumption for digital signatures. Finally, we axiomatize IND-CPA, IND-CCA1, and IND-CCA2 security properties and illustrate their usage. We have formalized the axiomatic system in an interactive theorem prover, Coq, and have machine-checked the proofs of various auxiliary theorems and security properties of Diffie-Hellman and station-to-station protocol.",recent years new approach developed verifying security protocols aim combining benefits symbolic attackers benefits unconditional soundness technique computationally complete symbolic attacker bana comon article argue real breakthrough technique recent introduction version indistinguishability extensions introduce first time computationally sound symbolic technique syntactically strikingly simple translating standard computational security notions straightforward matter effectively used verification equivalence properties trace properties protocols well first fully develop core elements newer version introducing several new axioms illustrate power diverse use introduced axioms simple examples first introduce axiom expressing decisional diffie hellman property analyze diffie hellman key exchange simplest form authenticated version well provide computationally sound verification real random secrecy diffie hellman key exchange protocol multiple sessions without restrictions computational implementation ddh assumption also show authentication simplified version station station protocol using cma assumption digital signatures finally axiomatize ind cpa ind cca ind cca security properties illustrate usage formalized axiomatic system interactive theorem prover coq machine checked proofs various auxiliary theorems security properties diffie hellman station station protocol
"We present an approach for verifying systems at runtime. Our approach targets distributed systems whose components communicate with monitors over unreliable channels, where messages can be delayed, reordered, or even lost. Furthermore, our approach handles an expressive specification language that extends the real-time logic MTL with freeze quantifiers for reasoning about data values. The logic’s main novelty is a new three-valued semantics that is well suited for runtime verification as it accounts for partial knowledge about a system’s behavior. Based on this semantics, we present online algorithms that reason soundly and completely about streams where events can occur out of order. We also evaluate our algorithms experimentally. Depending on the specification, our prototype implementation scales to out-of-order streams with hundreds to thousands of events per second.",present approach verifying systems runtime approach targets distributed systems whose components communicate monitors unreliable channels messages delayed reordered even lost furthermore approach handles expressive specification language extends real time logic mtl freeze quantifiers reasoning data values logic main novelty new three valued semantics well suited runtime verification accounts partial knowledge system behavior based semantics present online algorithms reason soundly completely streams events occur order also evaluate algorithms experimentally depending specification prototype implementation scales order streams hundreds thousands events per second
"We reconsider the problem of containment of monadic datalog (MDL) queries in unions of conjunctive queries (UCQs). Prior work has dealt with special cases of the problem but has left the precise complexity characterization open. In addition, the complexity of one important special case, that of containment under access patterns, was not known before. We start by revisiting the connection between MDL/UCQ containment and containment problems involving regular tree languages. We then present a general approach for getting tighter bounds on the complexity of query containment, based on analysis of the number of mappings of queries into tree-like instances. We give two applications of the machinery. We first give an important special case of the MDL/UCQ containment problem that is in EXPTIME, and we use this bound to show an EXPTIME bound on containment under access patterns. Second, we show that the same technique can be used to get a new tight upper bound for containment of tree automata in UCQs. We finally show that the new MDL/UCQ upper bounds are tight. We establish a 2EXPTIME lower bound on the MDL/UCQ containment problem, resolving an open problem from the early 1990s. This bound holds for the MDL/CQ containment problem as well. We also show that changes to the conditions given in our special cases can not be eliminated, and that in particular slight variations of the problem of containment under access patterns become 2EXPTIME-complete.",reconsider problem containment monadic datalog mdl queries unions conjunctive queries ucqs prior work dealt special cases problem left precise complexity characterization open addition complexity one important special case containment access patterns known start revisiting connection mdl ucq containment containment problems involving regular tree languages present general approach getting tighter bounds complexity query containment based analysis number mappings queries tree like instances give two applications machinery first give important special case mdl ucq containment problem exptime use bound show exptime bound containment access patterns second show technique used get new tight upper bound containment tree automata ucqs finally show new mdl ucq upper bounds tight establish exptime lower bound mdl ucq containment problem resolving open problem early bound holds mdl containment problem well also show changes conditions given special cases eliminated particular slight variations problem containment access patterns become exptime complete
"We investigate the computational complexity of the satisfiability problem of modal inclusion logic. We distinguish two variants of the problem: one for the strict and another one for the lax semantics. Both problems turn out to be EXPTIME-complete on general structures. Finally, we show how for a specific class of structures NEXPTIME-completeness for these problems under strict semantics can be achieved.",investigate computational complexity satisfiability problem modal inclusion logic distinguish two variants problem one strict another one lax semantics problems turn exptime complete general structures finally show specific class structures nexptime completeness problems strict semantics achieved
"We revisit the following problem called Maximum Empty Box: Given a set S of n points inside an axis-parallel box U in Rd, nd a maximum-volume axis-parallel box that is contained in U but contains no points of S in its interior.",revisit following problem called maximum empty box given set points inside axis parallel box maximum volume axis parallel box contained contains points interior
"In this article, we propose a logic for reasoning about belief based on fusion of uncertain information. The resultant reason-maintenance possibilistic belief logic can represent both implicit and explicit uncertain beliefs of an agent. While implicit beliefs stipulate what are believable, explicit beliefs can trace the process of belief formation by information fusion. To set up the formal framework, we start with developing a basic reason-maintenance belief logic, present its syntax and semantics, and investigate its axiomatization and properties. Then, we extend the basic logic to accommodate the possibilistic uncertainty of information and beliefs, provide a complete axiomatization of the extended logic, and show that it can address the reason-maintenance issue of partially inconsistent beliefs. We also demonstrate the applicability of our formalisms by using several examples in realistic scenarios.",article propose logic reasoning belief based fusion uncertain information resultant reason maintenance possibilistic belief logic represent implicit explicit uncertain beliefs agent implicit beliefs stipulate believable explicit beliefs trace process belief formation information fusion set formal framework start developing basic reason maintenance belief logic present syntax semantics investigate axiomatization properties extend basic logic accommodate possibilistic uncertainty information beliefs provide complete axiomatization extended logic show address reason maintenance issue partially inconsistent beliefs also demonstrate applicability formalisms using several examples realistic scenarios
"The present article contributes to the development of the mathematical theory of epistemic updates using the tools of duality theory. Here, we focus on Probabilistic Dynamic Epistemic Logic (PDEL). We dually characterize the product update construction of PDEL-models as a certain construction transforming the complex algebras associated with the given model into the complex algebra associated with the updated model. Thanks to this construction, an interpretation of the language of PDEL can be defined on algebraic models based on Heyting algebras. This justifies our proposal for the axiomatization of the intuitionistic counterpart of PDEL.",present article contributes development mathematical theory epistemic updates using tools duality theory focus probabilistic dynamic epistemic logic pdel dually characterize product update construction pdel models certain construction transforming complex algebras associated given model complex algebra associated updated model thanks construction interpretation language pdel defined algebraic models based heyting algebras justifies proposal axiomatization intuitionistic counterpart pdel
"Computer and information scientists join forces with other fields to help solve societal and environmental challenges facing humanity, in pursuit of a sustainable future.",computer information scientists join forces fields help solve societal environmental challenges facing humanity pursuit sustainable future
"Resolution-based provers for multimodal normal logics require pruning of the search space for a proof to ameliorate the inherent intractability of the satisfiability problem for such logics. We present a clausal modal-layered hyper-resolution calculus for the basic multimodal logic, which divides the clause set according to the modal level at which clauses occur to reduce the number of possible inferences. We show that the calculus is complete for the logics being considered. We also show that the calculus can be combined with other strategies. In particular, we discuss the completeness of combining modal layering with negative and ordered resolution and provide experimental results comparing the different refinements.",resolution based provers multimodal normal logics require pruning search space proof ameliorate inherent intractability satisfiability problem logics present clausal modal layered hyper resolution calculus basic multimodal logic divides clause set according modal level clauses occur reduce number possible inferences show calculus complete logics considered also show calculus combined strategies particular discuss completeness combining modal layering negative ordered resolution provide experimental results comparing different refinements
"The expanding use of information systems in industrial and commercial settings has increased the need for interoperation between software systems. In particular, many social, industrial, and business information systems require a common basis for a seamless exchange of complex process information. This is, however, inhibited, because different systems may use distinct terminologies or assume different meanings for the same terms. A common solution to this problem is to develop logical theories that act as an intermediate language between different parties. In this article, we characterize a class of activities that can act as intermediate languages between different parties in those cases. We show that for each domain with finite number of elements there exists a class of activities, we called canonical activities, such that all possible changes within the domain can be represented as a sequence of occurrences of those activities. We use an algebraic structure for representing change and characterizing canonical activities, which enables us to abstract away domain-dependent properties of processes and activities, and demonstrate general properties of formalisms required for semantic integration of dynamic information systems.",expanding use information systems industrial commercial settings increased need interoperation software systems particular many social industrial business information systems require common basis seamless exchange complex process information however inhibited different systems may use distinct terminologies assume different meanings terms common solution problem develop logical theories act intermediate language different parties article characterize class activities act intermediate languages different parties cases show domain finite number elements exists class activities called canonical activities possible changes within domain represented sequence occurrences activities use algebraic structure representing change characterizing canonical activities enables abstract away domain dependent properties processes activities demonstrate general properties formalisms required semantic integration dynamic information systems
"We consider the setting of stochastic multiagent systems modelled as stochastic multiplayer games and formulate an automated verification framework for quantifying and reasoning about agents’ trust. To capture human trust, we work with a cognitive notion of trust defined as a subjective evaluation that agentAmakes about agentB’s ability to complete a task, which in turn may lead to a decision byAto rely onB. We propose a probabilistic rational temporal logic PRTL*, which extends the probabilistic computation tree logic PCTL* with reasoning about mental attitudes (beliefs, goals, and intentions) and includes novel operators that can express concepts of social trust such as competence, disposition, and dependence. The logic can express, for example, that “agentAwill eventually trust agentBwith probability at leastpthat B will behave in a way that ensures the successful completion of a given task.” We study the complexity of the automated verification problem and, while the general problem is undecidable, we identify restrictions on the logic and the system that result in decidable, or even tractable, subproblems.",consider setting stochastic multiagent systems modelled stochastic multiplayer games formulate automated verification framework quantifying reasoning agents trust capture human trust work cognitive notion trust defined subjective evaluation agentamakes agentb ability complete task turn may lead decision byato rely onb propose probabilistic rational temporal logic prtl extends probabilistic computation tree logic pctl reasoning mental attitudes beliefs goals intentions includes novel operators express concepts social trust competence disposition dependence logic express example agentawill eventually trust agentbwith probability leastpthat behave way ensures successful completion given task study complexity automated verification problem general problem undecidable identify restrictions logic system result decidable even tractable subproblems
"We consider probabilistic model checking for continuous-time Markov chains (CTMCs) induced from Stochastic Reaction Networks against a fragment of Continuous Stochastic Logic (CSL) extended with reward operators. Classical numerical algorithms for CSL model checking based on uniformisation are limited to finite CTMCs and suffer from exponential growth of the state space with respect to the number of species. However, approximate techniques such as mean-field approximations and simulations combined with statistical inference are more scalable but can be time-consuming and do not support the full expressiveness of CSL. In this article, we employ a continuous-space approximation of the CTMC in terms of a Gaussian process based on the Central Limit Approximation, also known as the Linear Noise Approximation, whose solution requires solving a number of differential equations that is quadratic in the number of species and independent of the population size. We then develop efficient and scalable approximate model checking algorithms on the resulting Gaussian process, where we restrict the target regions for probabilistic reachability to convex polytopes. This allows us to derive an abstraction in terms of a time-inhomogeneous discrete-time Markov chain (DTMC), whose dimension is independent of the number of species, on which model checking is performed. Using results from probability theory, we prove the convergence in distribution of our algorithms to the corresponding measures on the original CTMC. We implement the techniques and, on a set of examples, demonstrate that they allow us to overcome the state space explosion problem, while still correctly characterizing the stochastic behaviour of the system. Our methods can be used for formal analysis of a wide range of distributed stochastic systems, including biochemical systems, sensor networks, and population protocols.",consider probabilistic model checking continuous time markov chains ctmcs induced stochastic reaction networks fragment continuous stochastic logic csl extended reward operators classical numerical algorithms csl model checking based uniformisation limited finite ctmcs suffer exponential growth state space respect number species however approximate techniques mean field approximations simulations combined statistical inference scalable time consuming support full expressiveness csl article employ continuous space approximation ctmc terms gaussian process based central limit approximation also known linear noise approximation whose solution requires solving number differential equations quadratic number species independent population size develop efficient scalable approximate model checking algorithms resulting gaussian process restrict target regions probabilistic reachability convex polytopes allows derive abstraction terms time inhomogeneous discrete time markov chain dtmc whose dimension independent number species model checking performed using results probability theory prove convergence distribution algorithms corresponding measures original ctmc implement techniques set examples demonstrate allow overcome state space explosion problem still correctly characterizing stochastic behaviour system methods used formal analysis wide range distributed stochastic systems including biochemical systems sensor networks population protocols
"We present an inverse design tool for fabric formwork - a process where flat panels are sewn together to form a fabric container for casting a plaster sculpture. Compared to 3D printing techniques, the benefit of fabric formwork is its properties of low-cost and easy transport. The process of fabric formwork is akin to molding and casting but having a soft boundary. Deformation of the fabric container is governed by force equilibrium between the pressure forces from liquid fill and tension in the stretched fabric. The final result of fabrication depends on the shapes of the flat panels, the fabrication orientation and the placement of external supports. Our computational framework generates optimized flat panels and fabrication orientation with reference to a target shape, and determines effective locations for external supports. We demonstrate the function of this design tool on a variety of models with different shapes and topology. Physical fabrication is also demonstrated to validate our approach.",present inverse design tool fabric formwork process flat panels sewn together form fabric container casting plaster sculpture compared printing techniques benefit fabric formwork properties low cost easy transport process fabric formwork akin molding casting soft boundary deformation fabric container governed force equilibrium pressure forces liquid fill tension stretched fabric final result fabrication depends shapes flat panels fabrication orientation placement external supports computational framework generates optimized flat panels fabrication orientation reference target shape determines effective locations external supports demonstrate function design tool variety models different shapes topology physical fabrication also demonstrated validate approach
"Hybrid unmanned aerial vehicles (UAV) combine advantages of multicopters and fixed-wing planes: vertical take-off, landing, and low energy use. However, hybrid UAVs are rarely used because controller design is challenging due to its complex, mixed dynamics. In this paper, we propose a method to automate this design process by training a mode-free, model-agnostic neural network controller for hybrid UAVs. We present a neural network controller design with a novel error convolution input trained by reinforcement learning. Our controller exhibits two key features: First, it does not distinguish among flying modes, and the same controller structure can be used for copters with various dynamics. Second, our controller works for real models without any additional parameter tuning process, closing the gap between virtual simulation and real fabrication. We demonstrate the efficacy of the proposed controller both in simulation and in our custom-built hybrid UAVs (Figure 1, 8). The experiments show that the controller is robust to exploit the complex dynamics when both rotors and wings are active in flight tests.",hybrid unmanned aerial vehicles uav combine advantages multicopters fixed wing planes vertical take landing low energy use however hybrid uavs rarely used controller design challenging due complex mixed dynamics paper propose method automate design process training mode free model agnostic neural network controller hybrid uavs present neural network controller design novel error convolution input trained reinforcement learning controller exhibits two key features first distinguish among flying modes controller structure used copters various dynamics second controller works real models without additional parameter tuning process closing gap virtual simulation real fabrication demonstrate efficacy proposed controller simulation custom built hybrid uavs figure experiments show controller robust exploit complex dynamics rotors wings active flight tests
Smartphones and consumer cameras increasingly give professional photographers a run for their money.,smartphones consumer cameras increasingly give professional photographers run money
"An innovative, entry-level informatics course enables students to ponder CS problems in different ways, from different perspectives.",innovative entry level informatics course enables students ponder problems different ways different perspectives
"Single-round multiway join algorithms first reshuffle data over many servers and then evaluate the query at hand in a parallel and communication-free way. A key question is whether a given distribution policy for the reshuffle is adequate for computing a given query, also referred to as parallel-correctness. This article extends the study of the complexity of parallel-correctness and its constituents, parallel-soundness and parallel-completeness, to unions of conjunctive queries with negation. As a by-product, it is shown that the containment problem for conjunctive queries with negation is coNEXPTIME-complete.",single round multiway join algorithms first reshuffle data many servers evaluate query hand parallel communication free way key question whether given distribution policy reshuffle adequate computing given query also referred parallel correctness article extends study complexity parallel correctness constituents parallel soundness parallel completeness unions conjunctive queries negation product shown containment problem conjunctive queries negation conexptime complete
"Branch decomposition is a prominent method for structurally decomposing a graph, a hypergraph, or a propositional formula in conjunctive normal form. The width of a branch decomposition provides a measure of how well the object is decomposed. For many applications, it is crucial to computing a branch decomposition whose width is as small as possible. We propose an approach based on Boolean Satisfiability (SAT) to finding branch decompositions of small width. The core of our approach is an efficient SAT encoding that determines with a single SAT-call whether a given hypergraph admits a branch decomposition of a certain width. For our encoding, we propose a natural partition-based characterization of branch decompositions. The encoding size imposes a limit on the size of the given hypergraph. To break through this barrier and to scale the SAT approach to larger instances, we develop a new heuristic approach where the SAT encoding is used to locally improve a given candidate decomposition until a fixed-point is reached. This new SAT-based local improvement method scales now to instances with several thousands of vertices and edges.",branch decomposition prominent method structurally decomposing graph hypergraph propositional formula conjunctive normal form width branch decomposition provides measure well object decomposed many applications crucial computing branch decomposition whose width small possible propose approach based boolean satisfiability sat finding branch decompositions small width core approach efficient sat encoding determines single sat call whether given hypergraph admits branch decomposition certain width encoding propose natural partition based characterization branch decompositions encoding size imposes limit size given hypergraph break barrier scale sat approach larger instances develop new heuristic approach sat encoding used locally improve given candidate decomposition fixed point reached new sat based local improvement method scales instances several thousands vertices edges
"We examine sequential equilibrium in the context ofcomputational games(Halpern and Pass 2015), where agents are charged for computation. In such games, an agent can rationally choose to forget, so issues of imperfect recall arise. In this setting, we consider two notions of sequential equilibrium. One is anex antenotion, where a player chooses his strategy before the game starts and is committed to it, but chooses it in such a way that it remains optimal even off the equilibrium path. The second is aninterimnotion, where a player can reconsider at each information set whether he is doing the “right” thing, and if not, can change his strategy. The two notions agree in games of perfect recall, but not in games of imperfect recall. Although the interim notion seems more appealing, in a companion article (Halpern and Pass 2016), we argue that there are some deep conceptual problems with it in standard games of imperfect recall. We show that the conceptual problems largely disappear in the computational setting. Moreover, in this setting, under natural assumptions, the two notions coincide.",examine sequential equilibrium context ofcomputational games halpern pass agents charged computation games agent rationally choose forget issues imperfect recall arise setting consider two notions sequential equilibrium one anex antenotion player chooses strategy game starts committed chooses way remains optimal even equilibrium path second aninterimnotion player reconsider information set whether right thing change strategy two notions agree games perfect recall games imperfect recall although interim notion seems appealing companion article halpern pass argue deep conceptual problems standard games imperfect recall show conceptual problems largely disappear computational setting moreover setting natural assumptions two notions coincide
"Timed-register pushdown automata constitute a very expressive class of automata, whose transitions may involve state, input, and top-of-stack timed registers with unbounded differences. They strictly subsume pushdown timed automata of Bouajjani et al., dense-timed pushdown automata of Abdulla et al., and orbit-finite timed-register pushdown automata of Clemente and Lasota. We give an effective logical characterisation of the reachability relation of timed-register pushdown automata. As a corollary, we obtain a doubly exponential time procedure for the non-emptiness problem. We show that the complexity reduces to singly exponential under the assumption of monotonic time. The proofs involve a novel model of one-dimensional integer branching vector addition systems with states. As a result interesting on its own, we show that reachability sets of the latter model are semilinear and computable in exponential time.",timed register pushdown automata constitute expressive class automata whose transitions may involve state input top stack timed registers unbounded differences strictly subsume pushdown timed automata bouajjani dense timed pushdown automata abdulla orbit finite timed register pushdown automata clemente lasota give effective logical characterisation reachability relation timed register pushdown automata corollary obtain doubly exponential time procedure non emptiness problem show complexity reduces singly exponential assumption monotonic time proofs involve novel model one dimensional integer branching vector addition systems states result interesting show reachability sets latter model semilinear computable exponential time
"The ideal value of an AI Cosmology would be to help the general public, researchers, educators, and practitioners to devise the truth of the definition, meaning, applications, and implications of Artificial Intelligence. The pursuit of that truth even if through an arbitrary contrivance would be a noteworthy goal. The fact of the matter is whether any cosmological structure we have hinted at so far tracks the underlying reality, we cannot escape that there is an underlying reality. At some point in time, we (humans) began the endeavor of trying to replicate the human mind with machines. There was first an effort to understand the human mind, describe its inner workings, and then build machines that could essentially duplicate the thinking process. Surely this point in time must mark or at least point to the Cosmological ""Big Bang"" for AI. Right?",ideal value cosmology would help general public researchers educators practitioners devise truth definition meaning applications implications artificial intelligence pursuit truth even arbitrary contrivance would noteworthy goal fact matter whether cosmological structure hinted far tracks underlying reality escape underlying reality point time humans began endeavor trying replicate human mind machines first effort understand human mind describe inner workings build machines could essentially duplicate thinking process surely point time must mark least point cosmological big bang right
"Accurate and continuous monitoring of joint rotational motion is crucial for a wide range of applications such as physical rehabilitation [6, 85] and motion training [22, 54, 68]. Existing motion capture systems, however, either need instrumentation of the environment, or fail to track arbitrary joint motion, or impose wearing discomfort by requiring rigid electrical sensors right around the joint area. This work studies the use of everyday fabrics as a flexible and soft sensing medium to monitor joint angular motion accurately and reliably. Specifically we focus on the primary use of conductive stretchable fabrics to sense the skin deformation during joint motion and infer the joint rotational angle. We tackle challenges of fabric sensing originated by the inherent properties of elastic materials by leveraging two types of sensing fabric and characterizing their properties based on models in material science. We apply models from bio-mechanics to infer joint angles and propose the use of dual strain sensing to enhance sensing robustness against user diversity and fabric position offsets. We fabricate prototypes using off-the-shelf fabrics and micro-controller. Experiments with ten participants show 9.69° median angular error in tracking joint angle and its sensing robustness across various users and activities.",accurate continuous monitoring joint rotational motion crucial wide range applications physical rehabilitation motion training existing motion capture systems however either need instrumentation environment fail track arbitrary joint motion impose wearing discomfort requiring rigid electrical sensors right around joint area work studies use everyday fabrics flexible soft sensing medium monitor joint angular motion accurately reliably specifically focus primary use conductive stretchable fabrics sense skin deformation joint motion infer joint rotational angle tackle challenges fabric sensing originated inherent properties elastic materials leveraging two types sensing fabric characterizing properties based models material science apply models bio mechanics infer joint angles propose use dual strain sensing enhance sensing robustness user diversity fabric position offsets fabricate prototypes using shelf fabrics micro controller experiments ten participants show median angular error tracking joint angle sensing robustness across various users activities
"Understanding visual interestingness is a challenging task addressed by researchers in various disciplines ranging from humanities and psychology to, more recently, computer vision and multimedia. The rise of infographics and the visual information overload that we are facing today have given this task a crucial importance. Automatic systems are increasingly needed to help users navigate through the growing amount of visual information available, either on the web or our personal devices, for instance by selecting relevant and interesting content. Previous studies indicate that visual interest is highly related to concepts like arousal, unusualness, or complexity, where these connections are found based on psychological theories, user studies, or computational approaches. However, the link between visual interestingness and other related concepts has been only partially explored so far, for example, by considering only a limited subset of covariates at a time. In this article, we present a comprehensive survey on visual interestingness and related concepts, aiming to bring together works based on different approaches, highlighting controversies, and identifying links that have not been fully investigated yet. Finally, we present some open questions that may be addressed in future works. Our work aims to support researchers interested in visual interestingness and related subjective or abstract concepts, providing an in-depth overlook at state-of-the-art theories in humanities and methods in computational approaches, as well as providing an extended list of datasets.",understanding visual interestingness challenging task addressed researchers various disciplines ranging humanities psychology recently computer vision multimedia rise infographics visual information overload facing today given task crucial importance automatic systems increasingly needed help users navigate growing amount visual information available either web personal devices instance selecting relevant interesting content previous studies indicate visual interest highly related concepts like arousal unusualness complexity connections found based psychological theories user studies computational approaches however link visual interestingness related concepts partially explored far example considering limited subset covariates time article present comprehensive survey visual interestingness related concepts aiming bring together works based different approaches highlighting controversies identifying links fully investigated yet finally present open questions may addressed future works work aims support researchers interested visual interestingness related subjective abstract concepts providing depth overlook state art theories humanities methods computational approaches well providing extended list datasets
"Given the increasing popularity of customer service dialogue on Twitter, analysis of conversation data is essential to understanding trends in customer and agent behavior for the purpose of automating customer service interactions. In this work, we develop a novel taxonomy of fine-grained “dialogue acts” frequently observed in customer service, showcasing acts that are more suited to the domain than the more generic existing taxonomies. Using a sequential SVM-HMM model, we model conversation flow, predicting the dialogue act of a given turn in real time, and showcase this using our “PredDial” portal. We characterize differences between customer and agent behavior in Twitter customer service conversations and investigate the effect of testing our system on different customer service industries. Finally, we use a data-driven approach to predict important conversation outcomes: customer satisfaction, customer frustration, and overall problem resolution. We show that the type and location of certain dialogue acts in a conversation have a significant effect on the probability of desirable and undesirable outcomes and present actionable rules based on our findings. We explore the correlations between different dialogue acts and the outcome of the conversations in detail using an actionable-rule discovery task by leveraging a state-of-the-art sequential rule mining algorithm while modeling a set of conversations as a set of sequences. The patterns and rules we derive can be used as guidelines for outcome-driven automated customer service platforms.",given increasing popularity customer service dialogue twitter analysis conversation data essential understanding trends customer agent behavior purpose automating customer service interactions work develop novel taxonomy fine grained dialogue acts frequently observed customer service showcasing acts suited domain generic existing taxonomies using sequential svm hmm model model conversation flow predicting dialogue act given turn real time showcase using preddial portal characterize differences customer agent behavior twitter customer service conversations investigate effect testing system different customer service industries finally use data driven approach predict important conversation outcomes customer satisfaction customer frustration overall problem resolution show type location certain dialogue acts conversation significant effect probability desirable undesirable outcomes present actionable rules based findings explore correlations different dialogue acts outcome conversations detail using actionable rule discovery task leveraging state art sequential rule mining algorithm modeling set conversations set sequences patterns rules derive used guidelines outcome driven automated customer service platforms
"In alternating-time temporal logic ATL*, agents with perfect recall assign choices to sequences of states, i.e., to possible finite histories of the game. However, when a nested strategic modality is interpreted, the new strategy does not take into account the previous sequence of events. It is as if agents collect their observations in the nested game again from scratch, thus, effectively forgetting what they observed before. Intuitively, it does not fit the assumption of agents having perfect recall of the past. In this article, we investigate the alternative semantics for ATL*where the past is not forgotten in nested games. We show that the standard semantics of ATL*coincides with the “truly perfect recall” semantics for agents with perfect information and in case of so-called “objective” abilities under uncertainty. On the other hand, the two semantics differ significantly for the most popular (“subjective”) notion of ability under imperfect information. The same applies to the standard vs. “truly perfect recall” semantics of ATL*with persistent strategies. We compare the relevant variants of ATL*by looking at their expressive power, sets of validities, and tractability of model checking.",alternating time temporal logic atl agents perfect recall assign choices sequences states possible finite histories game however nested strategic modality interpreted new strategy take account previous sequence events agents collect observations nested game scratch thus effectively forgetting observed intuitively fit assumption agents perfect recall past article investigate alternative semantics atl past forgotten nested games show standard semantics atl coincides truly perfect recall semantics agents perfect information case called objective abilities uncertainty hand two semantics differ significantly popular subjective notion ability imperfect information applies standard truly perfect recall semantics atl persistent strategies compare relevant variants atl looking expressive power sets validities tractability model checking
"Computational creativity seeks to understand computational mechanisms that can be characterized as creative. The creation of new concepts is a central challenge for any creative system. In this article, we outline different approaches to computational concept creation and then review conceptual representations relevant to concept creation, and therefore to computational creativity. The conceptual representations are organized in accordance with two important perspectives on the distinctions between them. One distinction is between symbolic, spatial and connectionist representations. The other is between descriptive and procedural representations. Additionally, conceptual representations used in particular creative domains, such as language, music, image and emotion, are reviewed separately. For every representation reviewed, we cover the inference it affords, the computational means of building it, and its application in concept creation.",computational creativity seeks understand computational mechanisms characterized creative creation new concepts central challenge creative system article outline different approaches computational concept creation review conceptual representations relevant concept creation therefore computational creativity conceptual representations organized accordance two important perspectives distinctions one distinction symbolic spatial connectionist representations descriptive procedural representations additionally conceptual representations used particular creative domains language music image emotion reviewed separately every representation reviewed cover inference affords computational means building application concept creation
"In this work, we explore the connections between (linear) nested sequent calculi and ordinary sequent calculi for normal and non-normal modal logics. By proposing local versions to ordinary sequent rules, we obtain linear nested sequent calculi for a number of logics, including, to our knowledge, the first nested sequent calculi for a large class of simply dependent multimodal logics and for many standard non-normal modal logics. The resulting systems are modular and have separate left and right introduction rules for the modalities, which makes them amenable to specification as bipole clauses. While this granulation of the sequent rules introduces more choices for proof search, we show how linear nested sequent calculi can be restricted to blocked derivations, which directly correspond to ordinary sequent derivations.",work explore connections linear nested sequent calculi ordinary sequent calculi normal non normal modal logics proposing local versions ordinary sequent rules obtain linear nested sequent calculi number logics including knowledge first nested sequent calculi large class simply dependent multimodal logics many standard non normal modal logics resulting systems modular separate left right introduction rules modalities makes amenable specification bipole clauses granulation sequent rules introduces choices proof search show linear nested sequent calculi restricted blocked derivations directly correspond ordinary sequent derivations
Envisioning computing education that both teaches and empowers.,envisioning computing education teaches empowers
"Erick Elejalde obtained his Ph.D. in Computer Science from the University of Concepcion, Chile in 2018. His thesis focuses on analyzing the behavior of the mass media on-line and testing socioeconomic theories using computational methods. His research interests include computational social science, online media-behavior modeling, and social networks. He works now as a research assistant at the Data Science Institute, Universidad del Desarrollo, Santiago, Chile.",erick elejalde obtained computer science university concepcion chile thesis focuses analyzing behavior mass media line testing socioeconomic theories using computational methods research interests include computational social science online media behavior modeling social networks works research assistant data science institute universidad desarrollo santiago chile
"A brain-computer interface (BCI) provides a way to develop interaction between a brain and a computer. The communication is developed as a result of neural responses generated in the brain because of motor movements or cognitive activities. The means of communication here includes muscular and non-muscular actions. These actions generate brain activities or brain waves that are directed to a hardware device to perform a specific task. BCI initially was developed as the communication device for patients suffering from neuromuscular disorders. Owing to recent advancements in BCI devices—such as passive electrodes, wireless headsets, adaptive software, and decreased costs—it is also being used for developing communication between the general public. The BCI device records brain responses using various invasive and non-invasive acquisition techniques such as electrocorticography (ECoG), electroencephalography (EEG), magnetoencephalography (MEG), and magnetic resonance imaging (MRI). In this article, a survey on these techniques has been provided. The brain response needs to be translated using machine learning and pattern recognition methods to control any application. A brief review of various existing feature extraction techniques and classification algorithms applied on data recorded from the brain has been included in this article. A significant comparative analysis of popular existing BCI techniques is presented and possible future directives are provided.",brain computer interface bci provides way develop interaction brain computer communication developed result neural responses generated brain motor movements cognitive activities means communication includes muscular non muscular actions actions generate brain activities brain waves directed hardware device perform specific task bci initially developed communication device patients suffering neuromuscular disorders owing recent advancements bci devices passive electrodes wireless headsets adaptive software decreased costs also used developing communication general public bci device records brain responses using various invasive non invasive acquisition techniques electrocorticography ecog electroencephalography eeg magnetoencephalography meg magnetic resonance imaging mri article survey techniques provided brain response needs translated using machine learning pattern recognition methods control application brief review various existing feature extraction techniques classification algorithms applied data recorded brain included article significant comparative analysis popular existing bci techniques presented possible future directives provided
"Not at all did I expect the huge amount of positive and constructive feedback that I received on my short paper on Paul Feyerabend's philosophy which was published in issue 49(2) of The DATA BASE for Advances in Information Systems. More than two decades after his death, his provocative ideas are apparently still capable of triggering an inspiring academic debate. In this paper I will comment on the thoughtful rejoinders from Burton-Jones, Gregor, and Myers. I will also outline why, in spite of their wellfounded criticism, I still believe that Feyerabendian thinking is most appropriate for IS research-much more so than Popperian or Kuhnian. I will illustrate my reasoning by referring to several experiences with the academic double-blind peer review process. I will not only criticize academia's existing deficiencies, but also suggest a potential cure. My universal remedy is based on Feyerabend's philosophy of relativism, tolerance, and pluralism, which he not so cleverly disguised as anarchism. Recently, Chua et al. (2018) suggested changes in the rules of our academic review system. I do not aim at the system, but rather at the underlying mindset. If we allow him, Feyerabend can help us with that.",expect huge amount positive constructive feedback received short paper paul feyerabend philosophy published issue data base advances information systems two decades death provocative ideas apparently still capable triggering inspiring academic debate paper comment thoughtful rejoinders burton jones gregor myers also outline spite wellfounded criticism still believe feyerabendian thinking appropriate research much popperian kuhnian illustrate reasoning referring several experiences academic double blind peer review process criticize academia existing deficiencies also suggest potential cure universal remedy based feyerabend philosophy relativism tolerance pluralism cleverly disguised anarchism recently chua suggested changes rules academic review system aim system rather underlying mindset allow feyerabend help
"Computational sprinting is a class of mechanisms that boost performance but dissipate additional power. We describe a sprinting architecture in which many, independent chip multiprocessors share a power supply and sprints are constrained by the chips' thermal limits and the rack's power limits. Moreover, we present the computational sprinting game, a multi-agent perspective on managing sprints. Strategic agents decide whether to sprint based on application phases and system conditions. The game produces an equilibrium that improves task throughput for data analytics workloads by 4--6x over prior greedy heuristics and performs within 90% of an upper bound on throughput from a globally optimized policy.",computational sprinting class mechanisms boost performance dissipate additional power describe sprinting architecture many independent chip multiprocessors share power supply sprints constrained chips thermal limits rack power limits moreover present computational sprinting game multi agent perspective managing sprints strategic agents decide whether sprint based application phases system conditions game produces equilibrium improves task throughput data analytics workloads prior greedy heuristics performs within upper bound throughput globally optimized policy
"Considering the expression ""computational thinking"" as an entry point to understand why the fundamental contribution of computing to science is the shift from solving problems to having problems solved.",considering expression computational thinking entry point understand fundamental contribution computing science shift solving problems problems solved
"Binary Decision Diagrams (BDDs) and in particular ROBDDs (Reduced Ordered BDDs) are a common data structure for manipulating Boolean expressions, integrated circuit design, type inferencers, model checkers, and many other applications. Although the ROBDD is a lightweight data structure to implement, the behavior, in terms of memory allocation, may not be obvious to the program architect. We explore experimentally, numerically, and theoretically the typical and worst-case ROBDD sizes in terms of number of nodes and residual compression ratios, as compared to unreduced BDDs. While our theoretical results are not surprising, as they are in keeping with previously known results, we believe our method contributes to the current body of research by our experimental and statistical treatment of ROBDD sizes. In addition, we provide an algorithm to calculate the worst-case size. Finally, we present an algorithm for constructing a worst-case ROBDD of a given number of variables. Our approach may be useful to projects deciding whether the ROBDD is the appropriate data structure to use, and in building worst-case examples to test their code.",binary decision diagrams bdds particular robdds reduced ordered bdds common data structure manipulating boolean expressions integrated circuit design type inferencers model checkers many applications although robdd lightweight data structure implement behavior terms memory allocation may obvious program architect explore experimentally numerically theoretically typical worst case robdd sizes terms number nodes residual compression ratios compared unreduced bdds theoretical results surprising keeping previously known results believe method contributes current body research experimental statistical treatment robdd sizes addition provide algorithm calculate worst case size finally present algorithm constructing worst case robdd given number variables approach may useful projects deciding whether robdd appropriate data structure use building worst case examples test code
"This article proposes and explores the kinds of computational thinking, creative practices, design activities, and inclusive learning opportunities provided to diverse high school youth when designing integrated systems through simultaneously physically and digitally responsive wearable games and systems. Previous work in this area, conducted by Richard, coined the term “bidirectionally responsive design” (BRD) to describe the design of dual-feedback systems using multiple digital and physical interfaces. BRD also emphasizes using simplified fabrication tools, media and coding platforms, and microcontrollers common in youth content creation communities and makerspaces. This study provides a framework to analyze computational concepts, practices, and perspectives that leverage an integrated systems and multimodal learning approach, such as BRD, adding to, building on, and integrating previous analytic approaches to looking at Scratch coding, media design, physical computing and e-textiles. Using a detailed case study of one team during one of the early workshop iterations, we conduct a multimodal analysis of bidirectionally responsive making activities and discuss the ways that they present novel understanding of integrating diverse interests and encouraging collaborative and distributed computational thinking. We further examine how BRD operationalizes and extends multimodal learning theory by adding tangible and integrative dimensions as additional modalities learners can leverage to facilitate meaning making, metacognition, and agency. We also discuss how designing integrated systems, as facilitated through BRD, provides an opportunity to engage in authentic practices around the design of complex systems.",article proposes explores kinds computational thinking creative practices design activities inclusive learning opportunities provided diverse high school youth designing integrated systems simultaneously physically digitally responsive wearable games systems previous work area conducted richard coined term bidirectionally responsive design brd describe design dual feedback systems using multiple digital physical interfaces brd also emphasizes using simplified fabrication tools media coding platforms microcontrollers common youth content creation communities makerspaces study provides framework analyze computational concepts practices perspectives leverage integrated systems multimodal learning approach brd adding building integrating previous analytic approaches looking scratch coding media design physical computing textiles using detailed case study one team one early workshop iterations conduct multimodal analysis bidirectionally responsive making activities discuss ways present novel understanding integrating diverse interests encouraging collaborative distributed computational thinking examine brd operationalizes extends multimodal learning theory adding tangible integrative dimensions additional modalities learners leverage facilitate meaning making metacognition agency also discuss designing integrated systems facilitated brd provides opportunity engage authentic practices around design complex systems
"The complexity of large-scale distributed systems, particularly when deployed in physical space, calls for new mechanisms to address composability and reusability of collective adaptive behaviour. Computational fields have been proposed as an effective abstraction to fill the gap between the macro-level of such systems (specifying a system’s collective behaviour) and the micro-level (individual devices’ actions of computation and interaction to implement that collective specification), thereby providing a basis to better facilitate the engineering of collective APIs and complex systems at higher levels of abstraction. This article proposes a full formal foundation for field computations, in terms of a core (higher-order) calculus of computational fields containing a few key syntactic constructs, and equipped with typing, denotational and operational semantics. Critically, this allows formal establishment of a link between the micro- and macro-levels of collective adaptive systems by a result of computational adequacy and abstraction for the (aggregate) denotational semantics with respect to the (per-device) operational semantics.",complexity large scale distributed systems particularly deployed physical space calls new mechanisms address composability reusability collective adaptive behaviour computational fields proposed effective abstraction fill gap macro level systems specifying system collective behaviour micro level individual devices actions computation interaction implement collective specification thereby providing basis better facilitate engineering collective apis complex systems higher levels abstraction article proposes full formal foundation field computations terms core higher order calculus computational fields containing key syntactic constructs equipped typing denotational operational semantics critically allows formal establishment link micro macro levels collective adaptive systems result computational adequacy abstraction aggregate denotational semantics respect per device operational semantics
"Training a deep neural network (DNN) involves selecting a set of hyperparameters that define the network topology and influence the accuracy of the resulting network. Often, the goal is to maximize prediction accuracy on a given dataset. However, non-functional requirements of the trained network -- such as inference speed, size, and energy consumption -- can be very important as well. In this article, we aim to automate the process of selecting an appropriate DNN topology that fulfills both functional and non-functional requirements of the application. Specifically, we focus on tuning two important hyperparameters, depth and width, which together define the shape of the resulting network and directly affect its accuracy, speed, size, and energy consumption. To reduce the time needed to search the design space, we train a fraction of DNNs and build a model to predict the performances of the remaining ones. We are able to produce tuned ResNets, which are up to 4.22 times faster than original depth-scaled ResNets on a batch of 128 images while matching their accuracy.",training deep neural network dnn involves selecting set hyperparameters define network topology influence accuracy resulting network often goal maximize prediction accuracy given dataset however non functional requirements trained network inference speed size energy consumption important well article aim automate process selecting appropriate dnn topology fulfills functional non functional requirements application specifically focus tuning two important hyperparameters depth width together define shape resulting network directly affect accuracy speed size energy consumption reduce time needed search design space train fraction dnns build model predict performances remaining ones able produce tuned resnets times faster original depth scaled resnets batch images matching accuracy
"We analyze how the standard reductions between constraint satisfaction problems affect their proof complexity. We show that, for the most studied propositional, algebraic, and semialgebraic proof systems, the classical constructions of pp-interpretability, homomorphic equivalence, and addition of constants to a core preserve the proof complexity of the CSP. As a result, for those proof systems, the classes of constraint languages for which small unsatisfiability certificates exist can be characterized algebraically. We illustrate our results by a gap theorem saying that a constraint language either has resolution refutations of constant width or does not have bounded-depth Frege refutations of subexponential size. The former holds exactly for the widely studied class of constraint languages of bounded width. This class is also known to coincide with the class of languages with refutations of sublinear degree in Sums of Squares and Polynomial Calculus over the real field, for which we provide alternative proofs. We then ask for the existence of a natural proof system with good behavior with respect to reductions and simultaneously small-size refutations beyond bounded width. We give an example of such a proof system by showing that bounded-degree Lovász-Schrijver satisfies both requirements. Finally, building on the known lower bounds, we demonstrate the applicability of the method of reducibilities and construct new explicit hard instances of the graph three-coloring problem for all studied proof systems.",analyze standard reductions constraint satisfaction problems affect proof complexity show studied propositional algebraic semialgebraic proof systems classical constructions interpretability homomorphic equivalence addition constants core preserve proof complexity csp result proof systems classes constraint languages small unsatisfiability certificates exist characterized algebraically illustrate results gap theorem saying constraint language either resolution refutations constant width bounded depth frege refutations subexponential size former holds exactly widely studied class constraint languages bounded width class also known coincide class languages refutations sublinear degree sums squares polynomial calculus real field provide alternative proofs ask existence natural proof system good behavior respect reductions simultaneously small size refutations beyond bounded width give example proof system showing bounded degree lovász schrijver satisfies requirements finally building known lower bounds demonstrate applicability method reducibilities construct new explicit hard instances graph three coloring problem studied proof systems
"This article presents a new method for obtaining small algebras to check the admissibility—equivalently, validity in free algebras—of quasi-identities in a finitely generated quasivariety. Unlike a previous algebraic approach of Metcalfe and Röthlisberger, which is feasible only when the relevant free algebra is not too large, this method exploits natural dualities for quasivarieties to work with structures of smaller cardinality and surjective rather than injective morphisms. A number of case studies are described here that could not be be solved using the algebraic approach, including (quasi)varieties of MS-algebras, double Stone algebras, and involutive Stone algebras.",article presents new method obtaining small algebras check admissibility equivalently validity free algebras quasi identities finitely generated quasivariety unlike previous algebraic approach metcalfe thlisberger feasible relevant free algebra large method exploits natural dualities quasivarieties work structures smaller cardinality surjective rather injective morphisms number case studies described could solved using algebraic approach including quasi varieties algebras double stone algebras involutive stone algebras
"This column is devoted to geometric clustering and covering problems in Rd. As exact solutions for these problems are usually out of reach (unless d = 1), one is forced to deal with approximations. Here we mostly consider online algorithms, as the online setting introduces additional difficulty due to uncertainty about the future. One representative problem is the following (so-called Unit Covering): given a set of n points in Rd, cover the points by balls of unit diameter, so as to minimize the number of balls used.",column devoted geometric clustering covering problems exact solutions problems usually reach unless one forced deal approximations mostly consider online algorithms online setting introduces additional difficulty due uncertainty future one representative problem following called unit covering given set points cover points balls unit diameter minimize number balls used
"Over the recent years, computational trust and reputation models have become an invaluable method to improve computer-computer and human-computer interaction. As a result, a considerable amount of research has been published trying to solve open problems and improving existing models. This survey will bring additional structure into the already conducted research on both topics. After recapitulating the major underlying concepts, a new integrated review and analysis scheme for reputation and trust models is put forward. Using highly recognized review papers in this domain as a basis, this article will also introduce additional evaluation metrics to account for characteristics so far unstudied. A subsequent application of the new review schema on 40 top recent publications in this scientific field revealed interesting insights. While the area of computational trust and reputation models is still a very active research branch, the analysis carried out here was able to show that some aspects have already started to converge, whereas others are still subject to vivid discussions.",recent years computational trust reputation models become invaluable method improve computer computer human computer interaction result considerable amount research published trying solve open problems improving existing models survey bring additional structure already conducted research topics recapitulating major underlying concepts new integrated review analysis scheme reputation trust models put forward using highly recognized review papers domain basis article also introduce additional evaluation metrics account characteristics far unstudied subsequent application new review schema top recent publications scientific field revealed interesting insights area computational trust reputation models still active research branch analysis carried able show aspects already started converge whereas others still subject vivid discussions
"Computational notebooks aim to support collaborative data analysis by combining code, visualizations, and text in a single easily shared document. Yet, as notebooks evolve and grow they often become difficult to navigate or understand, discouraging sharing and reuse. We present the design and evaluation of a Jupyter Notebook extension providing facilities for annotated cell folding. Through a lab study and multi-week deployment we find cell folding aids notebook navigation and comprehension, not only by the original author, but also by collaborators viewing the notebook in a meeting or revising it on their own. However, in some cases cell folding encouraged collaborators to overlook folded sections or spend longer reviewing a notebook before editing it. These findings extend our understanding of code folding's trade-offs to a new medium and demonstrate its benefits for everyday collaboration. We conclude by discussing how dynamic reorganization can support sharing and reuse of computational notebooks.",computational notebooks aim support collaborative data analysis combining code visualizations text single easily shared document yet notebooks evolve grow often become difficult navigate understand discouraging sharing reuse present design evaluation jupyter notebook extension providing facilities annotated cell folding lab study multi week deployment find cell folding aids notebook navigation comprehension original author also collaborators viewing notebook meeting revising however cases cell folding encouraged collaborators overlook folded sections spend longer reviewing notebook editing findings extend understanding code folding trade offs new medium demonstrate benefits everyday collaboration conclude discussing dynamic reorganization support sharing reuse computational notebooks
"Collectives gather online around challenges they face, but frequently fail to envision shared outcomes to act on together. Prior work has developed systems for improving collective ideation and design by exposing people to each others' ideas and encouraging them to intermix those ideas. However, organizational behavior research has demonstrated that intermixing ideas does not result in meaningful engagement with those ideas. In this paper, we introduce a new class of collective design system that intermixes people instead of ideas: instead of receiving mere exposure to others' ideas, participants engage deeply with other members of the collective who represent those ideas, increasing engagement and influence. We thus present Hive: a system that organizes a collective into small teams, then intermixes people by rotating team membership over time. At a technical level, Hive must balance two competing forces: (1) networks are better at connecting diverse perspectives when network efficiency is high, but (2) moving people diminishes tie strength within teams. Hive balances these two needs through network rotation: an optimization algorithm that computes who should move where, and when. A controlled study compared network rotation to alternative rotation systems which maximize only tie strength or network efficiency, finding that network rotation produced higher-rated proposals. Hive has been deployed by Mozilla for a real-world open design drive to improve Firefox accessibility.",collectives gather online around challenges face frequently fail envision shared outcomes act together prior work developed systems improving collective ideation design exposing people others ideas encouraging intermix ideas however organizational behavior research demonstrated intermixing ideas result meaningful engagement ideas paper introduce new class collective design system intermixes people instead ideas instead receiving mere exposure others ideas participants engage deeply members collective represent ideas increasing engagement influence thus present hive system organizes collective small teams intermixes people rotating team membership time technical level hive must balance two competing forces networks better connecting diverse perspectives network efficiency high moving people diminishes tie strength within teams hive balances two needs network rotation optimization algorithm computes move controlled study compared network rotation alternative rotation systems maximize tie strength network efficiency finding network rotation produced higher rated proposals hive deployed mozilla real world open design drive improve firefox accessibility
"We study the applicability of spiking neural networks and neuromorphic hardware for solving general opti- mization problems without the use of adaptive training or learning algorithms. We leverage the dynamics of Hopfield networks and spin-glass systems to construct a fully connected spiking neural system to generate synchronous spike responses indicative of the underlying community structure in an undirected, unweighted graph. Mapping this fully connected system to current generation neuromorphic hardware is done by embedding sparse tree graphs to generate only the leading-order spiking dynamics. We demonstrate that for a chosen set of benchmark graphs, the spike responses generated on a current generation neuromorphic processor can improve the stability of graph partitions and non-overlapping communities can be identified even with the loss of higher-order spiking behavior if the graphs are sufficiently dense. For sparse graphs, the loss of higher-order spiking behavior improves the stability of certain graph partitions but does not retrieve the known community memberships.",study applicability spiking neural networks neuromorphic hardware solving general opti mization problems without use adaptive training learning algorithms leverage dynamics hopfield networks spin glass systems construct fully connected spiking neural system generate synchronous spike responses indicative underlying community structure undirected unweighted graph mapping fully connected system current generation neuromorphic hardware done embedding sparse tree graphs generate leading order spiking dynamics demonstrate chosen set benchmark graphs spike responses generated current generation neuromorphic processor improve stability graph partitions non overlapping communities identified even loss higher order spiking behavior graphs sufficiently dense sparse graphs loss higher order spiking behavior improves stability certain graph partitions retrieve known community memberships
"This article presents, for the first time, a Geometry of Interaction (GoI) interpretation inspired from Hughes--Van Glabbeek (HvG) proof-nets for multiplicative additive linear logic (MALL). Our GoI dynamically captures HvG’s geometric correctness criterion—the toggling cycle condition—in terms of algebraic operators. Our new ingredient is a scalar extension of the *-algebra in Girard’s *-ring of partial isometries over a Boolean polynomial ring with literals of eigenweights as indeterminates. To capture feedback arising from cuts, we construct a finer-grained execution formula. The expansion of this execution formula is longer than that for collections of slices for multiplicative GoI, hence it is harder to prove termination. Our GoI gives a dynamical, semantical account of Boolean valuations (in particular, pruning sub-proofs), conversion of weights (in particular, α-conversion), and additive (co)contraction, peculiar to additive proof-theory. Termination of our execution formula is shown to correspond to HvG’s toggling criterion. The slice-wise restriction of our execution formula (by collapsing the Boolean structure) yields the well-known correspondence, explicit or implicit in previous works on multiplicative GoI, between the convergence of execution formulas and acyclicity of proof-nets. Feedback arising from the execution formula by restricting to the Boolean polynomial structure yields autonomous definability of eigenweights among cuts from the rest of the eigenweights.",article presents first time geometry interaction goi interpretation inspired hughes van glabbeek hvg proof nets multiplicative additive linear logic mall goi dynamically captures hvg geometric correctness criterion toggling cycle condition terms algebraic operators new ingredient scalar extension algebra girard ring partial isometries boolean polynomial ring literals eigenweights indeterminates capture feedback arising cuts construct finer grained execution formula expansion execution formula longer collections slices multiplicative goi hence harder prove termination goi gives dynamical semantical account boolean valuations particular pruning sub proofs conversion weights particular conversion additive contraction peculiar additive proof theory termination execution formula shown correspond hvg toggling criterion slice wise restriction execution formula collapsing boolean structure yields well known correspondence explicit implicit previous works multiplicative goi convergence execution formulas acyclicity proof nets feedback arising execution formula restricting boolean polynomial structure yields autonomous definability eigenweights among cuts rest eigenweights
"The notions of strong/weak approximations have been studied extensively in recent years. These approximations are based on a structure of the form (W,{Ri}i∈ N), called the multiple-source approximation system, whereRiis an equivalence relation onW, andNis an initial segment of the set N of natural numbers. We propose and explore a simple modal language and semantics that can be used to reason about the strong/weak approximations of concepts. Moreover, our study is not confined to collections of equivalence relations only, but other types of relations are also considered. This study is important, keeping in view the notions of generalized approximation spaces with relations other than equivalence.",notions strong weak approximations studied extensively recent years approximations based structure form called multiple source approximation system whereriis equivalence relation onw andnis initial segment set natural numbers propose explore simple modal language semantics used reason strong weak approximations concepts moreover study confined collections equivalence relations types relations also considered study important keeping view notions generalized approximation spaces relations equivalence
"Formal verification of parameterized protocols such as cache coherence protocols is a significant challenge. In this article, we propose an automatic proving approach and its prototype paraVerifier to handle this challenge within a unified framework as follows: (1) To prove the correctness of a parameterized protocol, our approach automatically discovers auxiliary invariants and the corresponding dependency relations among the discovered invariants and protocol rules from a small instance of the to-be-verified protocol, and (2) the discovered invariants and dependency graph are then automatically generalized into a parameterized form and sent to the theorem prover, Isabelle. As a side product, the final verification result of a protocol is provided by a formal and human-readable proof. Our approach has been successfully applied to a number of benchmarks, including snoopying-based and directory-based cache coherence protocols.",formal verification parameterized protocols cache coherence protocols significant challenge article propose automatic proving approach prototype paraverifier handle challenge within unified framework follows prove correctness parameterized protocol approach automatically discovers auxiliary invariants corresponding dependency relations among discovered invariants protocol rules small instance verified protocol discovered invariants dependency graph automatically generalized parameterized form sent theorem prover isabelle side product final verification result protocol provided formal human readable proof approach successfully applied number benchmarks including snoopying based directory based cache coherence protocols
"This article presents an extension of temporal epistemic logic with operators that can express quantification over agent strategies. Unlike previous work on alternating temporal epistemic logic, the semantics works with systems whose states explicitly encode the strategy being used by each of the agents. This provides a natural way to express what agents would know were they to be aware of some of the strategies being used by other agents. A number of examples that rely on the ability to express an agent’s knowledge about the strategies being used by other agents are presented to motivate the framework, including reasoning about game-theoretic equilibria, knowledge-based programs, and information-theoretic computer security policies. Relationships to several variants of alternating temporal epistemic logic are discussed. The computational complexity of model checking the logic and several of its fragments are also characterized.",article presents extension temporal epistemic logic operators express quantification agent strategies unlike previous work alternating temporal epistemic logic semantics works systems whose states explicitly encode strategy used agents provides natural way express agents would know aware strategies used agents number examples rely ability express agent knowledge strategies used agents presented motivate framework including reasoning game theoretic equilibria knowledge based programs information theoretic computer security policies relationships several variants alternating temporal epistemic logic discussed computational complexity model checking logic several fragments also characterized
"This article explains why we need to break out of this repetitive dysfunctional behavior, and it introduces Essence, a new way of thinking that promises to free the practices from their method prisons and thus enable true learning organizations.",article explains need break repetitive dysfunctional behavior introduces essence new way thinking promises free practices method prisons thus enable true learning organizations
"We revisit Janin and Walukiewicz’s classic result on the expressive completeness of the modal mu-calculus with respect to Monadic Second Order Logic (MSO), which is where the mu-calculus corresponds precisely to the fragment of MSO that is invariant under bisimulation. We show that adding binary relations over finite paths in the picture may alter the situation. We consider a general setting where finite paths of transition systems are linked by means of a fixed binary relation. This setting gives rise to natural extensions of MSO and the mu-calculus, that we call the MSOwith paths relationand thejumping mu-calculus, the expressivities of which we aim at comparing. We first show that “bounded-memory” binary relations bring about no additional expressivity to either of the two logics, and thus preserve expressive completeness. In contrast, we show that for a natural, classic “infinite-memory” binary relation stemming from games with imperfect information, the existence of a winning strategy in such games, though expressible in the bisimulation-invariant fragment of MSO with paths relation, cannot be expressed in the jumping mu-calculus. Expressive completeness thus fails for this relation. These results crucially rely on our observation that the jumping mu-calculus has a tree automata counterpart: thejumping tree automata, hence the name of the jumping mu-calculus. We also prove that forobservablewinning conditions, the existence of winning strategies in games with imperfect information is expressible in the jumping mu-calculus. Finally, we derive from our main theorem that jumping automata cannot be projected, and ATL with imperfect information does not admit expansion laws.",revisit janin walukiewicz classic result expressive completeness modal calculus respect monadic second order logic mso calculus corresponds precisely fragment mso invariant bisimulation show adding binary relations finite paths picture may alter situation consider general setting finite paths transition systems linked means fixed binary relation setting gives rise natural extensions mso calculus call msowith paths relationand thejumping calculus expressivities aim comparing first show bounded memory binary relations bring additional expressivity either two logics thus preserve expressive completeness contrast show natural classic infinite memory binary relation stemming games imperfect information existence winning strategy games though expressible bisimulation invariant fragment mso paths relation expressed jumping calculus expressive completeness thus fails relation results crucially rely observation jumping calculus tree automata counterpart thejumping tree automata hence name jumping calculus also prove forobservablewinning conditions existence winning strategies games imperfect information expressible jumping calculus finally derive main theorem jumping automata projected atl imperfect information admit expansion laws
"We study the expressive power of fragments of inclusion logic under the so-called lax team semantics. The fragments are defined either by restricting the number of universal quantifiers, the number of inclusion atoms, or the arity of inclusion atoms. We show that the whole expressive power of inclusion logic can be captured using only five inclusion atoms in finite ordered models or, alternatively, only one universal quantifier in general. The arity hierarchy is shown to be strict by relating the question to the study of arity hierarchies in fixed point logics.",study expressive power fragments inclusion logic called lax team semantics fragments defined either restricting number universal quantifiers number inclusion atoms arity inclusion atoms show whole expressive power inclusion logic captured using five inclusion atoms finite ordered models alternatively one universal quantifier general arity hierarchy shown strict relating question study arity hierarchies fixed point logics
"For configurations of point-sets that are pairwise constrained by distance intervals, the EASAL software implements a suite of algorithms that characterize the structure and geometric properties of the configuration space. The algorithms generate, describe, and explore these configuration spaces using generic rigidity properties, classical results for stratification of semi-algebraic sets, and new results for efficient sampling by convex parametrization. The article reviews the key theoretical underpinnings, major algorithms, and their implementation. The article outlines the main applications such as the computation of free energy and kinetics of assembly of supramolecular structures or of clusters in colloidal and soft materials. In addition, the article surveys select experimental results and comparisons.",configurations point sets pairwise constrained distance intervals easal software implements suite algorithms characterize structure geometric properties configuration space algorithms generate describe explore configuration spaces using generic rigidity properties classical results stratification semi algebraic sets new results efficient sampling convex parametrization article reviews key theoretical underpinnings major algorithms implementation article outlines main applications computation free energy kinetics assembly supramolecular structures clusters colloidal soft materials addition article surveys select experimental results comparisons
"Further consideration of Feyerabend's ideas has potential value for information systems research. He continues in a long and commendable tradition of the scientist as a subversive and rebel - a tradition including Galileo, Franklin, Darwin, and Einstein - and a tradition that emphasizes science as a creative human activity. Treiblmaier advances three propositions regarding Feyerabend's ideas and their potential for IS research. The first, if taken to mean that in Feyerabend's spirit of anarchy there should be ongoing questioning of our epistemological foundations, is worth supporting for a number of reasons, including the apparent hegemony of hypothetico-deductive type perspectives. The second, if meaning that in Feyerabend's spirit a variety of research approaches should be supported, is accepted in many traditional views of science. The third proposition is benign if ""anything goes"" as a principle means more openness and tolerance of differing and new viewpoints. However, there are issues if this stance includes strong relativism. Information systems research generates knowledge that can be applied to practical problems. We should be able to retain Feyerabend's questioning spirit, yet develop actionable knowledge that has sufficient credibility to be useful in the world.",consideration feyerabend ideas potential value information systems research continues long commendable tradition scientist subversive rebel tradition including galileo franklin darwin einstein tradition emphasizes science creative human activity treiblmaier advances three propositions regarding feyerabend ideas potential research first taken mean feyerabend spirit anarchy ongoing questioning epistemological foundations worth supporting number reasons including apparent hegemony hypothetico deductive type perspectives second meaning feyerabend spirit variety research approaches supported accepted many traditional views science third proposition benign anything goes principle means openness tolerance differing new viewpoints however issues stance includes strong relativism information systems research generates knowledge applied practical problems able retain feyerabend questioning spirit yet develop actionable knowledge sufficient credibility useful world
"Addressing its cognitive essence, universal value, and curricular practices.",addressing cognitive essence universal value curricular practices
"Systems neuroscience studies involving in-vivo models often require realtime data processing. In these studies, many events must be monitored and processed quickly, including behavior of the subject (e.g., movement of a limb) or features of neural data (e.g., a neuron transmitting an action potential). Unfortunately, most realtime platforms are proprietary, require specific architectures, or are limited to low-level programming languages. Here we present a hardware-independent, open-source realtime computation platform that supports high-level programming. The resulting platform, LiCoRICE, can process on order 10e10 bits/sec of network data at 1 ms ticks with 18.2 µs jitter. It connects to various inputs and outputs (e.g., DIO, Ethernet, database logging, and analog line in/out) and minimizes reliance on custom device drivers by leveraging peripheral support via the Linux kernel. Its modular architecture supports model-based design for rapid prototyping with C and Python/Cython and can perform numerical operations via BLAS/LAPACK-optimized NumPy that is statically compiled via Numba’s pycc. LiCoRICE is not only suitable for systems neuroscience research, but also for applications requiring closed-loop realtime data processing from robotics and control systems to interactive applications and quantitative financial trading.",systems neuroscience studies involving vivo models often require realtime data processing studies many events must monitored processed quickly including behavior subject movement limb features neural data neuron transmitting action potential unfortunately realtime platforms proprietary require specific architectures limited low level programming languages present hardware independent open source realtime computation platform supports high level programming resulting platform licorice process order bits sec network data ticks jitter connects various inputs outputs dio ethernet database logging analog line minimizes reliance custom device drivers leveraging peripheral support via linux kernel modular architecture supports model based design rapid prototyping python cython perform numerical operations via blas lapack optimized numpy statically compiled via numba pycc licorice suitable systems neuroscience research also applications requiring closed loop realtime data processing robotics control systems interactive applications quantitative financial trading
"Social choice theory is an area of economics that studies collective decision making. Examples of collective decision making include sharing a cake or a resource among a group of people or friends, tallying votes in an election, and aggregating opinions of various experts. Computational social choice is a discipline which may be considered to be at the intersection of economics and computer science. This book deals with recent trends related to computational aspects of collective decision making. It contains contributions of experts in computational social choice. The book is divided into three parts that focus on scenarios, techniques, and applications, respectively. It has been published as a sequel to the Handbook of Computational Social Choice, Cambridge University Press, 2016, recently reviewed by me in this column (SIGACT News 48(4), December 2017, pp. 13-17). Ulle Endriss, the editor of this book, was also an editor of the Handbook. This book has been published by AI Access, a not-for-profit publisher. It is available for free online at the URL http://research.illc.uva.nl/COST-IC1205/Book/, and the hard copy is very nominally priced. The book's publication",social choice theory area economics studies collective decision making examples collective decision making include sharing cake resource among group people friends tallying votes election aggregating opinions various experts computational social choice discipline may considered intersection economics computer science book deals recent trends related computational aspects collective decision making contains contributions experts computational social choice book divided three parts focus scenarios techniques applications respectively published sequel handbook computational social choice cambridge university press recently reviewed column sigact news december ulle endriss editor book also editor handbook book published access profit publisher available free online url http research illc uva cost book hard copy nominally priced book publication
"In the limited workspace model, we consider algorithms whose input resides in read-only memory and that use only a constant or sublinear amount of writable memory to accomplish their task. We survey recent results in computational geometry that fall into this model and that strive to achieve the lowest possible running time. In addition to discussing the state of the art, we give some illustrative examples and mention open problems for further research.",limited workspace model consider algorithms whose input resides read memory use constant sublinear amount writable memory accomplish task survey recent results computational geometry fall model strive achieve lowest possible running time addition discussing state art give illustrative examples mention open problems research
"Cloud computing services often replicate data and may require ways to coordinate distributed actions. Here we present Derecho, a library for such tasks. The API provides interfaces for structuring applications into patterns of subgroups and shards, supports state machine replication within them, and includes mechanisms that assist in restart after failures. Running over 100Gbps RDMA, Derecho can send millions of events per second in each subgroup or shard and throughput peaks at 16GB/s, substantially outperforming prior solutions. Configured to run purely on TCP, Derecho is still substantially faster than comparable widely used, highly-tuned, standard tools. The key insight is that on modern hardware (including non-RDMA networks), data-intensive protocols should be built from non-blocking data-flow components.",cloud computing services often replicate data may require ways coordinate distributed actions present derecho library tasks api provides interfaces structuring applications patterns subgroups shards supports state machine replication within includes mechanisms assist restart failures running gbps rdma derecho send millions events per second subgroup shard throughput peaks substantially outperforming prior solutions configured run purely tcp derecho still substantially faster comparable widely used highly tuned standard tools key insight modern hardware including non rdma networks data intensive protocols built non blocking data flow components
"Until not long ago, manually capturing and storing provenance from scientific experiments were constant concerns for scientists. With the advent of computational experiments (modeled as scientific workflows) and Scientific Workflow Management Systems, produced and consumed data, as well as the provenance of a given experiment, are automatically managed, so provenance capturing and storing in such a context is no longer a major concern. Similarly to several existing big data problems, the bottom line is now on how to analyze the large amounts of provenance data generated by workflow executions and how to be able to extract useful knowledge of this data. In this context, this article surveys the current state of the art on provenance analytics by presenting the key initiatives that have been taken to support provenance data analysis. We also contribute by proposing a taxonomy to classify elements related to provenance analytics.",long ago manually capturing storing provenance scientific experiments constant concerns scientists advent computational experiments modeled scientific workflows scientific workflow management systems produced consumed data well provenance given experiment automatically managed provenance capturing storing context longer major concern similarly several existing big data problems bottom line analyze large amounts provenance data generated workflow executions able extract useful knowledge data context article surveys current state art provenance analytics presenting key initiatives taken support provenance data analysis also contribute proposing taxonomy classify elements related provenance analytics
"Connections between homotopy theory and type theory have recently attracted a lot of attention, with Voevodsky’s univalent foundations and the interpretation of Martin-Löf’s identity types in Quillen model categories as some of the highlights. In this article, we establish a connection between a natural weakening of Martin-Löf’s rules for the identity types that has been considered by Cohen, Coquand, Huber and Mörtberg in their work on a constructive interpretation of the univalence axiom on the one hand and the notion of a path category, a slight variation on the classic notion of a category of fibrant objects due to Brown, on the other. This involves showing that the syntactic category associated to a type theory with weak identity types carries the structure of a path category, strengthening earlier results by Avigad, Lumsdaine, and Kapulkin. In this way, we not only relate a well-known concept in homotopy theory with a natural concept in logic but also provide a framework for further developments.",connections homotopy theory type theory recently attracted lot attention voevodsky univalent foundations interpretation martin identity types quillen model categories highlights article establish connection natural weakening martin rules identity types considered cohen coquand huber rtberg work constructive interpretation univalence axiom one hand notion path category slight variation classic notion category fibrant objects due brown involves showing syntactic category associated type theory weak identity types carries structure path category strengthening earlier results avigad lumsdaine kapulkin way relate well known concept homotopy theory natural concept logic also provide framework developments
"We present a novel general technique that uses classifier learning to synthesize piece-wise functions (functions that split the domain into regions and apply simpler functions to each region) against logical synthesis specifications. Our framework works by combining a synthesizer of functions for fixed concrete inputs and a synthesizer of predicates that can be used to define regions. We develop a theory of single-point refutable specifications that facilitate generating concrete counterexamples using constraint solvers. We implement the framework for synthesizing piece-wise functions in linear integer arithmetic, combining leaf expression synthesis using constraint-solving with predicate synthesis using enumeration, and tie them together using a decision tree classifier. We demonstrate that this compositional approach is competitive compared to existing synthesis engines on a set of synthesis specifications.",present novel general technique uses classifier learning synthesize piece wise functions functions split domain regions apply simpler functions region logical synthesis specifications framework works combining synthesizer functions fixed concrete inputs synthesizer predicates used define regions develop theory single point refutable specifications facilitate generating concrete counterexamples using constraint solvers implement framework synthesizing piece wise functions linear integer arithmetic combining leaf expression synthesis using constraint solving predicate synthesis using enumeration tie together using decision tree classifier demonstrate compositional approach competitive compared existing synthesis engines set synthesis specifications
"We introduce three formal models of distributed systems for query evaluation on massive databases: Distributed Streaming with Register Automata (DSAs), Distributed Streaming with Register Transducers (DSTs), and Distributed Streaming with Register Transducers and Joins (DSTJs). These models are based on the map-reduce paradigm where the input is transformed into a dataset of key-value pairs, and on each key a local computation is performed on the values associated with that key resulting in another set of key-value pairs. Computation proceeds in a constant number of rounds, where the result of the last round is the input to the next round, and transformation of key-value pairs is required to be generic. The difference between the three models is in the local computation part. In DSAs it is limited to making one pass over its input using a register automaton, while in DSTs it can make two passes: in the first pass it uses a finite state automaton and in the second it uses a register transducer. The third model DSTJs is an extension of DSTs, where local computations are capable of constructing the Cartesian product of two sets. We obtain the following results: (1) DSAs can evaluate first-order queries over bounded degree databases; (2) DSTs can evaluate semijoin algebra queries over arbitrary databases; (3) DSTJs can evaluate the whole relational algebra over arbitrary databases; (4) DSTJs are strictly stronger than DSTs, which in turn are strictly stronger than DSAs; (5) within DSAs, DSTs, and DSTJs, there is a strict hierarchy w.r.t. the number of rounds.",introduce three formal models distributed systems query evaluation massive databases distributed streaming register automata dsas distributed streaming register transducers dsts distributed streaming register transducers joins dstjs models based map reduce paradigm input transformed dataset key value pairs key local computation performed values associated key resulting another set key value pairs computation proceeds constant number rounds result last round input next round transformation key value pairs required generic difference three models local computation part dsas limited making one pass input using register automaton dsts make two passes first pass uses finite state automaton second uses register transducer third model dstjs extension dsts local computations capable constructing cartesian product two sets obtain following results dsas evaluate first order queries bounded degree databases dsts evaluate semijoin algebra queries arbitrary databases dstjs evaluate whole relational algebra arbitrary databases dstjs strictly stronger dsts turn strictly stronger dsas within dsas dsts dstjs strict hierarchy number rounds
The Swedish government has recently introduced digital competence including programming in the Swedish K-9 curriculum starting no later than fall 2018. This means that 100 000 teachers need to learn programming and digital competence in less than a year. In this paper we report on our experience working with professional teacher training in Sweden's fifth largest city. The city has about 150 000 inhabitants and about 50 schools with about 14 000 students in primary education. The project has been carried out in close cooperation with the municipality.,swedish government recently introduced digital competence including programming swedish curriculum starting later fall means teachers need learn programming digital competence less year paper report experience working professional teacher training sweden fifth largest city city inhabitants schools students primary education project carried close cooperation municipality
"Most current information systems security theories assume a rational actor making deliberate decisions, yet recent research in psychology suggests that such deliberate thinking is not as common as we would expect. Much of human behavior is controlled by nonconscious automatic cognition (called System 1 cognition). The deliberate rational cognition of System 2 is triggered when System 1 detects something that is not normal; otherwise we often operate on autopilot. When we do engage System 2 cognition, it is influenced by the System 1 cognition that preceded it. In this paper we present an alternative theoretical approach to information security that is based on the nonconscious automatic cognition of System 1. In a System 1 world, cognition is a sub-second process of pattern-matching a stimulus to an existing person-context heuristic. These person-context heuristics are influenced by personality characteristics and a lifetime of experiences in the context. Thus System 1 theories are closely tied to individuals and the specific security context of interest. Methods to improve security compliance take on a very new form; the traditional approaches to security education and training that provide guidelines and ways to think about security have no effect when behavior is controlled by System 1, because System 1 cognition is instant pattern matching not deliberative. Thus in a System 1 world, we improve security by changing the heuristics used by System 1's pattern matching and/or by changing what System 1 sees as ""normal"" so that it triggers the deliberate cognition of System 2. In this article, we examine System 1 and System 2 cognition, while calling for increased research to develop theories of System 1 cognition in the cybersecurity literature.",current information systems security theories assume rational actor making deliberate decisions yet recent research psychology suggests deliberate thinking common would expect much human behavior controlled nonconscious automatic cognition called system cognition deliberate rational cognition system triggered system detects something normal otherwise often operate autopilot engage system cognition influenced system cognition preceded paper present alternative theoretical approach information security based nonconscious automatic cognition system system world cognition sub second process pattern matching stimulus existing person context heuristic person context heuristics influenced personality characteristics lifetime experiences context thus system theories closely tied individuals specific security context interest methods improve security compliance take new form traditional approaches security education training provide guidelines ways think security effect behavior controlled system system cognition instant pattern matching deliberative thus system world improve security changing heuristics used system pattern matching changing system sees normal triggers deliberate cognition system article examine system system cognition calling increased research develop theories system cognition cybersecurity literature
"We define a bi-directional embedding between hypersequent calculi and a subclass of systems of rules (2-systems). In addition to showing that the two proof frameworks have the same expressive power, the embedding allows for the recovery of the benefits of locality for 2-systems, analyticity results for a large class of such systems, and a rewriting of hypersequent rules as natural deduction rules.",define directional embedding hypersequent calculi subclass systems rules systems addition showing two proof frameworks expressive power embedding allows recovery benefits locality systems analyticity results large class systems rewriting hypersequent rules natural deduction rules
"The SIGIR 2017 workshop on Axiomatic Thinking for Information Retrieval and Related Tasks took place on August 11, 2017 in Tokyo, Japan. The workshop aimed to help foster collaboration of researchers working on different perspectives of axiomatic thinking and encourage discussion and research on general methodological issues related to applying axiomatic thinking to information retrieval and related tasks. The program consisted of one keynote talk, four research presentations and a final panel discussion. This report outlines the events of the workshop and summarizes the major outcomes. More information about the workshop is available at https://www.eecis.udel.edu/~hfang/ATIR.html.",sigir workshop axiomatic thinking information retrieval related tasks took place august tokyo japan workshop aimed help foster collaboration researchers working different perspectives axiomatic thinking encourage discussion research general methodological issues related applying axiomatic thinking information retrieval related tasks program consisted one keynote talk four research presentations final panel discussion report outlines events workshop summarizes major outcomes information workshop available https www eecis udel edu hfang atir html
The important intersection of computer science and social science.,important intersection computer science social science
"This article is a tutorial for researchers who are designing software to perform a creative task and want to evaluate their system using interdisciplinary theories of creativity. Researchers who study human creativity have a great deal to offer computational creativity. We summarize perspectives from psychology, philosophy, cognitive science, and computer science as to how creativity can be measured both in humans and in computers. We survey how these perspectives have been used in computational creativity research and make recommendations for how they should be used.",article tutorial researchers designing software perform creative task want evaluate system using interdisciplinary theories creativity researchers study human creativity great deal offer computational creativity summarize perspectives psychology philosophy cognitive science computer science creativity measured humans computers survey perspectives used computational creativity research make recommendations used
"Nominal terms extend first-order terms with nominal features and as such constitute a meta-language for reasoning about the named variables of an object language in the presence of meta-level variables. This article introduces a number of type systems for nominal terms of increasing sophistication and demonstrates their application in the areas of rewriting and equational reasoning. Two simple type systems inspired by Church’s simply typed lambda calculus are presented where only well-typed terms are considered to exist, over which α-equivalence is then axiomatised. The first requires atoms to be strictly annotated whilst the second explores the consequences of a more relaxed de Bruijn-style approach in the presence of atom-capturing substitution. A final type system of richer ML-like polymorphic types is then given in the style of Curry, in which elements of the term language are deemed typeable or not only subsequent to the definition of alpha-equivalence. Principal types are shown to exist and an inference algorithm given to compute them. This system is then used to define two presentations of typed nominal rewriting, one more expressive and one more efficient, the latter also giving rise to a notion of typed nominal equational reasoning.",nominal terms extend first order terms nominal features constitute meta language reasoning named variables object language presence meta level variables article introduces number type systems nominal terms increasing sophistication demonstrates application areas rewriting equational reasoning two simple type systems inspired church simply typed lambda calculus presented well typed terms considered exist equivalence axiomatised first requires atoms strictly annotated whilst second explores consequences relaxed bruijn style approach presence atom capturing substitution final type system richer like polymorphic types given style curry elements term language deemed typeable subsequent definition alpha equivalence principal types shown exist inference algorithm given compute system used define two presentations typed nominal rewriting one expressive one efficient latter also giving rise notion typed nominal equational reasoning
"The 10th International Workshop on Computational Transportation Science (IWCTS 2017) was held in conjunction with the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (SIGSPATIAL 2017) on November 7, Redondo Beach, California, USA. This workshop has built upon the success of previous workshops with a focus on data, computation, and knowledge discovery aspects of transportation systems. There were 17 registrations to the workshop.",international workshop computational transportation science iwcts held conjunction acm sigspatial international conference advances geographic information systems sigspatial november redondo beach california usa workshop built upon success previous workshops focus data computation knowledge discovery aspects transportation systems registrations workshop
"We classify the computational complexity of the satisfiability, validity, and model-checking problems for propositional independence, inclusion, and team logic. Our main result shows that the satisfiability and validity problems for propositional team logic are complete for alternating exponential-time with polynomially many alternations.",classify computational complexity satisfiability validity model checking problems propositional independence inclusion team logic main result shows satisfiability validity problems propositional team logic complete alternating exponential time polynomially many alternations
"This research presents a novel approach for translating legal texts into machine-executable computational logic to support the automation of public sector processes. Recognizing the high-stakes implications of artificial intelligence (AI) in legal domains, the proposed method emphasizes explainability by integrating explainable AI (XAI) techniques with natural language processing (NLP), employing scope-restricted pattern matching and grammatical parsing. The methodology involves several key steps: document structure inference from raw legal text, semantically neutral pre-processing, identification and resolution of internal and external references, contextualization of legal paragraphs, and rule extraction. The extracted rules are formalized as Prolog predicates and visualized as structured textual lists and graphical decision trees to enhance interpretability. To demonstrate the automatic extraction of explainable rules from legal text, we develop a Law-as-Code prototype and validate it through a real-world case study at the Austrian Ministry of Finance. The system successfully extracts executable rules from the Austrian Study Funding Act, confirming the feasibility and effectiveness of the proposed approach. This validation not only underscores the practical applicability of our method, but also highlights promising avenues for future research, particularly the integration of Generative AI and Large Language Models (LLMs) into the rule extraction pipeline, while preserving traceability and explainability.",research presents novel approach translating legal texts machine executable computational logic support automation public sector processes recognizing high stakes implications artificial intelligence legal domains proposed method emphasizes explainability integrating explainable xai techniques natural language processing nlp employing scope restricted pattern matching grammatical parsing methodology involves several key steps document structure inference raw legal text semantically neutral pre processing identification resolution internal external references contextualization legal paragraphs rule extraction extracted rules formalized prolog predicates visualized structured textual lists graphical decision trees enhance interpretability demonstrate automatic extraction explainable rules legal text develop law code prototype validate real world case study austrian ministry finance system successfully extracts executable rules austrian study funding act confirming feasibility effectiveness proposed approach validation underscores practical applicability method also highlights promising avenues future research particularly integration generative large language models llms rule extraction pipeline preserving traceability explainability
"In this paper, we study the multi-patch discontinuous Galerkin isogeometric (DG-IGA) approximations for full-potential electronic structure calculations. We decompose the physical domain into several subdomains, represent each part of the wavefunction separately using B-spline basis functions, possibly with different degrees, on varying mesh sizes, and then combine them by DG methods. We also provide a rigorous a priori error analysis of the DG-IGA approximations for linear eigenvalue problems. Furthermore, this work offers a unified analysis framework for the DG-IGA method applied to a class of elliptic eigenvalue problems. Finally, we present several numerical experiments to verify our theoretical results.",paper study multi patch discontinuous galerkin isogeometric iga approximations full potential electronic structure calculations decompose physical domain several subdomains represent part wavefunction separately using spline basis functions possibly different degrees varying mesh sizes combine methods also provide rigorous priori error analysis iga approximations linear eigenvalue problems furthermore work offers unified analysis framework iga method applied class elliptic eigenvalue problems finally present several numerical experiments verify theoretical results
"This paper focuses on continuous data assimilation (CDA) for the Navier–Stokes equations with nonlinear slip boundary conditions. CDA methods are typically employed to recover the original system when initial data or viscosity coefficients are unknown, by incorporating a feedback control term generated by observational data over a time period. In this study, based on a regularized form derived from the variational inequalities of the Navier–Stokes equations with nonlinear slip boundary conditions, we first investigate the classical CDA problem when initial data is absent. After establishing the existence, uniqueness and regularity of the solution, we prove its exponential convergence with respect to the time. Additionally, we extend the CDA to address the problem with missing viscosity coefficients and analyze its convergence order, too. Then, the finite element approximation for the CDA is considered. Furthermore, utilizing the predictive capabilities of partial evolutionary tensor neural networks (pETNNs) for time-dependent problems, we propose a novel CDA by replacing observational data with predictions got by pETNNs. Compared with the classical CDA, the new one can achieve similar approximation accuracy but need much less computational cost. Some numerical experiments are presented, which not only validate the theoretical results, but also demonstrate the efficiency of the CDA.",paper focuses continuous data assimilation cda navier stokes equations nonlinear slip boundary conditions cda methods typically employed recover original system initial data viscosity coefficients unknown incorporating feedback control term generated observational data time period study based regularized form derived variational inequalities navier stokes equations nonlinear slip boundary conditions first investigate classical cda problem initial data absent establishing existence uniqueness regularity solution prove exponential convergence respect time additionally extend cda address problem missing viscosity coefficients analyze convergence order finite element approximation cda considered furthermore utilizing predictive capabilities partial evolutionary tensor neural networks petnns time dependent problems propose novel cda replacing observational data predictions got petnns compared classical cda new one achieve similar approximation accuracy need much less computational cost numerical experiments presented validate theoretical results also demonstrate efficiency cda
"In the present paper, we present some counterexamples of some results related to the Hermite–Hadamard type inequalities for convex functions proved recently in the following paper Mehrez and Agarwal (2019).",present paper present counterexamples results related hermite hadamard type inequalities convex functions proved recently following paper mehrez agarwal
"Stochastic Gradient methods are widely used in the field of supervised learning associated with big data. In this context, importance sampling-based algorithms have been proposed to minimize the variance of the stochastic gradient by introducing practical strategies to approximate the optimal sampling distribution, which is otherwise only theoretically accessible. In this paper, we propose a scheme that combines stochastic gradient descent with adaptive importance sampling with automatic step-size selection based on a stochastic Armijo-type line-search. This approach makes the method robust to the choice of the initial step-size, which would otherwise require a tuning phase that is computationally expensive or even impractical in certain big data scenarios. Moreover, we introduce different mini-batch variants to foster the practical acceleration of the original scheme. Finally, numerical experiments are presented on real datasets to validate the proposed method in the context of supervised classification problems.",stochastic gradient methods widely used field supervised learning associated big data context importance sampling based algorithms proposed minimize variance stochastic gradient introducing practical strategies approximate optimal sampling distribution otherwise theoretically accessible paper propose scheme combines stochastic gradient descent adaptive importance sampling automatic step size selection based stochastic armijo type line search approach makes method robust choice initial step size would otherwise require tuning phase computationally expensive even impractical certain big data scenarios moreover introduce different mini batch variants foster practical acceleration original scheme finally numerical experiments presented real datasets validate proposed method context supervised classification problems
"The integration of advanced signal processing techniques into machine learning models has gained increasing attention due to its potential to improve model performance, particularly for classification tasks. Support Vector Machine (SVM) is widely recognized as a powerful tool for signal classification due to its robust mathematical foundation and effectiveness in handling high-dimensional data. Subdivision schemes, originally developed in computer graphics for geometric modeling, offer a novel and parametric approach to feature preprocessing by iteratively refining input data through an efficient computational procedure. This paper studies the impact of subdivision schemes on SVM performance in terms of class separability and provides insights into the relationship between feature transformation and SVM response. Specifically, it investigates the theoretical and empirical implications of applying subdivision schemes to input features in SVM-based classification. The conditions under which these schemes preserve or enhance class separability are analyzed, focusing on the tension parameter which governs both the smoothness properties of the limit curve and the subdivision rule at each iteration. An estimation method for the tension parameter from the training data is also provided. Experimental results, performed in the context of signal classification based on the wavelet scattering transform, demonstrate that the appropriate selection of the tension parameter of the scheme can significantly enhance class separability, highlighting that subdivision schemes are a promising tool for improving classification accuracy in machine learning workflows.",integration advanced signal processing techniques machine learning models gained increasing attention due potential improve model performance particularly classification tasks support vector machine svm widely recognized powerful tool signal classification due robust mathematical foundation effectiveness handling high dimensional data subdivision schemes originally developed computer graphics geometric modeling offer novel parametric approach feature preprocessing iteratively refining input data efficient computational procedure paper studies impact subdivision schemes svm performance terms class separability provides insights relationship feature transformation svm response specifically investigates theoretical empirical implications applying subdivision schemes input features svm based classification conditions schemes preserve enhance class separability analyzed focusing tension parameter governs smoothness properties limit curve subdivision rule iteration estimation method tension parameter training data also provided experimental results performed context signal classification based wavelet scattering transform demonstrate appropriate selection tension parameter scheme significantly enhance class separability highlighting subdivision schemes promising tool improving classification accuracy machine learning workflows
"In this article, we consider the problems of finding in d + 1 dimensions a minimum-volume axis-parallel box, a minimum-volume arbitrarily-oriented box and a minimum-volume convex body into which a given set of d-dimensional unit-radius balls can be packed under translations. The computational problem is neither known to be NP-hard nor to be in NP. We give a constant-factor approximation algorithm for each of these containers based on a reduction to finding a shortest Hamiltonian path in a weighted graph, which in turn models the problem of stabbing the centers of the input balls while keeping them disjoint. We also show that for n such balls, a container of volume O ( n d − 1 d ) is always sufficient and sometimes necessary. As a byproduct, this implies that for d ⩾ 2 there is no finite size ( d + 1 ) -dimensional convex body into which all d-dimensional unit-radius balls can be packed simultaneously.",article consider problems finding dimensions minimum volume axis parallel box minimum volume arbitrarily oriented box minimum volume convex body given set dimensional unit radius balls packed translations computational problem neither known hard give constant factor approximation algorithm containers based reduction finding shortest hamiltonian path weighted graph turn models problem stabbing centers input balls keeping disjoint also show balls container volume always sufficient sometimes necessary byproduct implies finite size dimensional convex body dimensional unit radius balls packed simultaneously
"In homogenization theory, mathematical models at the macro level are constructed based on the solution of auxiliary cell problems at the micro level within a single periodicity cell. These problems are formulated using asymptotic expansions of the solution with respect to a small parameter, which represents the characteristic size of spatial heterogeneity. When studying diffusion equations with contrasting coefficients, special attention is given to nonlocal models with weakly conducting inclusions. In this case, macro-level processes are described by integro-differential equations, where the difference kernel is determined by the solution of a nonstationary cell problem. The main contribution of this work is the development of a computational framework for the homogenization of nonstationary processes, accounting for memory effects. The effective diffusion tensor is computed using a standard numerical procedure based on finite element discretization in space. The memory kernel is approximated by a sum of exponentials obtained from solving a partial spectral problem on the periodicity cell. The nonlocal macro-level problem is transformed into a local one, where memory effects are incorporated through the solution of auxiliary nonstationary problems. Standard two-level time discretization schemes are employed, and unconditional stability of the discrete solutions is proved in appropriate norms. Key aspects of the proposed computational homogenization technique are illustrated by solving a two-dimensional model problem.",homogenization theory mathematical models macro level constructed based solution auxiliary cell problems micro level within single periodicity cell problems formulated using asymptotic expansions solution respect small parameter represents characteristic size spatial heterogeneity studying diffusion equations contrasting coefficients special attention given nonlocal models weakly conducting inclusions case macro level processes described integro differential equations difference kernel determined solution nonstationary cell problem main contribution work development computational framework homogenization nonstationary processes accounting memory effects effective diffusion tensor computed using standard numerical procedure based finite element discretization space memory kernel approximated sum exponentials obtained solving partial spectral problem periodicity cell nonlocal macro level problem transformed local one memory effects incorporated solution auxiliary nonstationary problems standard two level time discretization schemes employed unconditional stability discrete solutions proved appropriate norms key aspects proposed computational homogenization technique illustrated solving two dimensional model problem
"In scientific computing, the burgeoning usage of physics-informed neural networks (PINNs) for solving partial differential equations(PDEs) has spurred the need for, and ongoing research into, more accurate and efficient PINNs. One bottleneck of current PINNs is the computation of the high-order derivatives via automatic differentiation which often necessitates substantial computing resources especially when dealing with complex PDEs and high dimensional problems. To tackle this, we propose a spectral-based neural network that substitutes the differential operator with a multiplication. Compared to PINNs, our framework requires less GPU memory and a shorter training time. Furthermore, the exponential convergence of the spectral basis makes our approach more accurate. Moreover, to handle the different situations between the physics domain and the spectral domain, we provide a strategy to efficiently train networks in spectral domain. Through a series of comprehensive experiments, we validate the aforementioned merits of our proposed network.",scientific computing burgeoning usage physics informed neural networks pinns solving partial differential equations pdes spurred need ongoing research accurate efficient pinns one bottleneck current pinns computation high order derivatives via automatic differentiation often necessitates substantial computing resources especially dealing complex pdes high dimensional problems tackle propose spectral based neural network substitutes differential operator multiplication compared pinns framework requires less gpu memory shorter training time furthermore exponential convergence spectral basis makes approach accurate moreover handle different situations physics domain spectral domain provide strategy efficiently train networks spectral domain series comprehensive experiments validate aforementioned merits proposed network
"This study presents the Hybrid Butter-Flower Algorithm (HBFA), an innovative metaheuristic optimization approach that combines the strengths of the Sunflower Optimization Algorithm (SOA) and the Butterfly Optimization Algorithm (BOA) to improve convergence speed, accuracy, and robustness. While proficient in exploration, SOA may lack the aggressive exploitation required for swift convergence, potentially slowing down solution refinement. However, BOA might experience premature convergence in challenging milieu that results in demurrer in local optima. HBFA handles these challenges by amalgamating SOA’s unique exploration nuances with BOA’s effectual exploitation techniques, assuring an optimum exchange between orbicular and localized search. The algorithm is assessed on 23 unimodal and multimodal standard functions and six constrained mechanical design optimization problems with real-world applications. The performance of HBFA is benchmarked against nine state-of-the-art optimization methods, including PSO, SSA, and HSA, based on metrics such as best solution, average solution, and convergence rate. The results demonstrate that HBFA attains the highest performance efficiency (96.69 %), accomplishing all competitive algorithms by epochal margins, with amelioration ranging from 21 % to 63 % over conventional approaches. Notably, the proposed HBFA is 83.25 % faster in finding the optimal solution than other algorithms without falling for premature convergence and local optima. The superiority of HBFA is further validated through Wilcoxon signed-rank and Friedman statistical tests, with an average p-value of 3.14E-10, confirming its statistically significant advantage. Given to its adaptive nature and rapid convergence, HBFA emerges as a powerful tool for addressing complex optimization challenges in engineering, artificial intelligence, and industrial applications.",study presents hybrid butter flower algorithm hbfa innovative metaheuristic optimization approach combines strengths sunflower optimization algorithm soa butterfly optimization algorithm boa improve convergence speed accuracy robustness proficient exploration soa may lack aggressive exploitation required swift convergence potentially slowing solution refinement however boa might experience premature convergence challenging milieu results demurrer local optima hbfa handles challenges amalgamating soa unique exploration nuances boa effectual exploitation techniques assuring optimum exchange orbicular localized search algorithm assessed unimodal multimodal standard functions six constrained mechanical design optimization problems real world applications performance hbfa benchmarked nine state art optimization methods including pso ssa hsa based metrics best solution average solution convergence rate results demonstrate hbfa attains highest performance efficiency accomplishing competitive algorithms epochal margins amelioration ranging conventional approaches notably proposed hbfa faster finding optimal solution algorithms without falling premature convergence local optima superiority hbfa validated wilcoxon signed rank friedman statistical tests average value confirming statistically significant advantage given adaptive nature rapid convergence hbfa emerges powerful tool addressing complex optimization challenges engineering artificial intelligence industrial applications
"Due to their wide appearance in environmental settings as well as industrial and medical applications, the Stokes–Darcy problems with different sets of interface conditions establish an active research area in the community of mathematical modelers and computational scientists. For numerical simulation of such coupled problems in applications, robust and efficient computational algorithms are needed. In this work, we consider a generalization of the Beavers–Joseph interface condition recently developed using homogenization and boundary layer theory. This extension is applicable not only for the parallel flows to the fluid–porous interface as its predecessor, but also for arbitrary flow directions. To solve the Stokes–Darcy problem with these generalized interface conditions efficiently, we develop and analyze a Robin–Robin domain decomposition method using Fourier analysis to identify optimal weights in the Robin interface conditions. We study efficiency and robustness of the proposed method and provide numerical simulations which confirm the obtained theoretical results.",due wide appearance environmental settings well industrial medical applications stokes darcy problems different sets interface conditions establish active research area community mathematical modelers computational scientists numerical simulation coupled problems applications robust efficient computational algorithms needed work consider generalization beavers joseph interface condition recently developed using homogenization boundary layer theory extension applicable parallel flows fluid porous interface predecessor also arbitrary flow directions solve stokes darcy problem generalized interface conditions efficiently develop analyze robin robin domain decomposition method using fourier analysis identify optimal weights robin interface conditions study efficiency robustness proposed method provide numerical simulations confirm obtained theoretical results
"Splitting methods constitute a well-established class of numerical schemes for solving convection–diffusion–reaction problems. They have been shown to be effective in solving problems with periodic boundary conditions. However, in the case of Dirichlet boundary conditions, order reduction has been observed even with homogeneous boundary conditions. In this paper, we propose a novel splitting approach, the so-called initial-corrected splitting method, which succeeds in overcoming order reduction. A convergence analysis is performed to demonstrate, up to a logarithmic factor, second-order convergence of this modified Strang splitting method. Furthermore, we conduct numerical experiments to illustrate the performance of the newly developed splitting approach.",splitting methods constitute well established class numerical schemes solving convection diffusion reaction problems shown effective solving problems periodic boundary conditions however case dirichlet boundary conditions order reduction observed even homogeneous boundary conditions paper propose novel splitting approach called initial corrected splitting method succeeds overcoming order reduction convergence analysis performed demonstrate logarithmic factor second order convergence modified strang splitting method furthermore conduct numerical experiments illustrate performance newly developed splitting approach
"Time-evolving perforated domains arise in many engineering and geoscientific applications, including reactive transport, particle deposition, and structural degradation in porous media. Accurately capturing the macroscopic behavior of such systems poses significant computational challenges due to the dynamic fine-scale geometries. In this paper, we develop a robust and generalizable multiscale modeling framework based on multicontinuum homogenization to derive effective macroscopic equations in shrinking domains. The method distinguishes multiple continua according to the physical characteristics (e.g., channel widths), and couples them via space–time local cell problems formulated on representative volume elements. These local problems incorporate temporal derivatives and domain evolution, ensuring consistency with underlying fine-scale dynamics. The resulting upscaled system yields computable macroscopic coefficients and is suitable for large-scale simulations. Several numerical experiments are presented to validate the accuracy, efficiency, and potential applicability of the method to complex time-dependent engineering problems.",time evolving perforated domains arise many engineering geoscientific applications including reactive transport particle deposition structural degradation porous media accurately capturing macroscopic behavior systems poses significant computational challenges due dynamic fine scale geometries paper develop robust generalizable multiscale modeling framework based multicontinuum homogenization derive effective macroscopic equations shrinking domains method distinguishes multiple continua according physical characteristics channel widths couples via space time local cell problems formulated representative volume elements local problems incorporate temporal derivatives domain evolution ensuring consistency underlying fine scale dynamics resulting upscaled system yields computable macroscopic coefficients suitable large scale simulations several numerical experiments presented validate accuracy efficiency potential applicability method complex time dependent engineering problems
"From a first order Hermite interpolant on a triangle written in terms of barycentric coordinates, a Shepard-like interpolation is defined. It achieves fourth order of approximation. Its performance is checked by considering well-known test functions as well different Delaunay triangulations. The novel operator is compared to a Shepard-type operator and to a Hermite cubic interpolant over a Powell–Sabin partition.",first order hermite interpolant triangle written terms barycentric coordinates shepard like interpolation defined achieves fourth order approximation performance checked considering well known test functions well different delaunay triangulations novel operator compared shepard type operator hermite cubic interpolant powell sabin partition
"The approximation of data is a fundamental challenge encountered in various fields, including computer-aided geometric design, the numerical solution of partial differential equations, or the design of curves and surfaces. Numerous methods have been developed to address this issue, providing good results when the data is continuous. Among these, the Moving Least Squares (MLS) method has proven to be an effective strategy for fitting data, finding applications in both statistics and applied mathematics. However, the presence of isolated discontinuities in the data can lead to undesirable artifacts, such as the Gibbs phenomenon, which adversely affects the quality of the approximation. In this paper, we propose a novel approach that integrates the Moving Least Squares method with the well-established non-linear Weighted Essentially Non-Oscillatory (WENO) method. This combination aims to construct a non-linear operator that enhances the accuracy of approximations near discontinuities while maintaining the order of accuracy in smooth regions. We investigate the properties of this operator in one dimension, demonstrating its effectiveness through a series of numerical experiments that validate our theoretical findings.",approximation data fundamental challenge encountered various fields including computer aided geometric design numerical solution partial differential equations design curves surfaces numerous methods developed address issue providing good results data continuous among moving least squares mls method proven effective strategy fitting data finding applications statistics applied mathematics however presence isolated discontinuities data lead undesirable artifacts gibbs phenomenon adversely affects quality approximation paper propose novel approach integrates moving least squares method well established non linear weighted essentially non oscillatory weno method combination aims construct non linear operator enhances accuracy approximations near discontinuities maintaining order accuracy smooth regions investigate properties operator one dimension demonstrating effectiveness series numerical experiments validate theoretical findings
"The construction of B-spline wavelet bases on nonequispaced knots is extended to wavelets that are piecewise segments from any combination of smooth functions. The extended wavelet family thus provides multiresolution basis functions with support as compact as possible and belonging to a user controlled smoothness class. The construction proceeds in two phases. In the first phase of segmentation, a set of smooth functions is used in the welding of compact supported, piecewise smooth basis functions. These piecewise smooth basis functions are refinable, meaning that they can be written as linear combinations of similar basis functions constructed on a finer grid of knots. The expression of the linear combination between the bases at two scales is known as a refinement or two-scale equation. In the second phase, the refinability enables the construction of a wavelet transform. To this end, the refinement equation of the piecewise smooth scaling functions is factored into a lifting scheme, to which the desired properties of the subsequent wavelet basis can then be added. Special attention is paid to proper boundary treatment. Next to the details of the construction, the paper discusses the conditions for it to fit into the classical framework of multiresolution analyses.",construction spline wavelet bases nonequispaced knots extended wavelets piecewise segments combination smooth functions extended wavelet family thus provides multiresolution basis functions support compact possible belonging user controlled smoothness class construction proceeds two phases first phase segmentation set smooth functions used welding compact supported piecewise smooth basis functions piecewise smooth basis functions refinable meaning written linear combinations similar basis functions constructed finer grid knots expression linear combination bases two scales known refinement two scale equation second phase refinability enables construction wavelet transform end refinement equation piecewise smooth scaling functions factored lifting scheme desired properties subsequent wavelet basis added special attention paid proper boundary treatment next details construction paper discusses conditions fit classical framework multiresolution analyses
"We propose a unified framework for delay differential equations (DDEs) based on deep neural networks (DNNs) - the physics-informed neural network for delay differential equations (PINN-DDEs), aimed at solving the forward and inverse problems of delay differential equations. This framework could embed delay differential equations into neural networks to accommodate the diverse requirements of DDEs in terms of initial conditions, control equations, and known data. PINN-DDEs adjust the network parameters through automatic differentiation and optimization algorithms to minimize the loss function, thereby obtaining numerical solutions to the delay differential equations without the grid dependence and polynomial interpolation typical of traditional numerical methods. In addressing inverse problems, the PINN-DDEs framework can utilize observational data to perform precise estimation of single or multiple delay parameters, which is very important in practical mathematical modeling. The results of multiple numerical experiments have shown that PINN-DDEs demonstrate high precision in both forward and inverse problems, proving their effectiveness and promising potential in dealing with delayed differential equation issues.",propose unified framework delay differential equations ddes based deep neural networks dnns physics informed neural network delay differential equations pinn ddes aimed solving forward inverse problems delay differential equations framework could embed delay differential equations neural networks accommodate diverse requirements ddes terms initial conditions control equations known data pinn ddes adjust network parameters automatic differentiation optimization algorithms minimize loss function thereby obtaining numerical solutions delay differential equations without grid dependence polynomial interpolation typical traditional numerical methods addressing inverse problems pinn ddes framework utilize observational data perform precise estimation single multiple delay parameters important practical mathematical modeling results multiple numerical experiments shown pinn ddes demonstrate high precision forward inverse problems proving effectiveness promising potential dealing delayed differential equation issues
"This paper focuses on studying a class of generalized coupled Sylvester-type matrix equations within quaternion algebra. We develop a conjugate linear operator in quaternion algebra based on the real inner product. Following this, we introduce a quaternion version of biconjugate residual method to solve these quaternion matrix equations and analyze its theoretical properties. We further propose a novel framework based on quaternion matrices for simultaneous color video encryption and decryption. Numerical examples are provided to validate the theoretical results.",paper focuses studying class generalized coupled sylvester type matrix equations within quaternion algebra develop conjugate linear operator quaternion algebra based real inner product following introduce quaternion version biconjugate residual method solve quaternion matrix equations analyze theoretical properties propose novel framework based quaternion matrices simultaneous color video encryption decryption numerical examples provided validate theoretical results
"This paper is devoted to studies of doubly-history dependent hemivariational inequalities in contact mechanics. Existence and uniqueness of a solution to the problem is proved by applying a basic well-posedness result combined with a Banach fixed-point argument. A fully discrete scheme is used to solve the problem, with temporal integrals approximated by rectangular rules and the spatial discretization done by the linear element method. Under suitable solution regularity assumptions, an optimal order error bound is proved for the numerical solutions. Finally, simulation results on a numerical example are reported to illustrate numerical convergence orders.",paper devoted studies doubly history dependent hemivariational inequalities contact mechanics existence uniqueness solution problem proved applying basic well posedness result combined banach fixed point argument fully discrete scheme used solve problem temporal integrals approximated rectangular rules spatial discretization done linear element method suitable solution regularity assumptions optimal order error bound proved numerical solutions finally simulation results numerical example reported illustrate numerical convergence orders
The smoothing spline estimator minimizes a penalized least squares criterion over functions with square integrable m th-order partial derivatives. We prove it converges almost surely and uniformly over the entire domain as the sample size increases.,smoothing spline estimator minimizes penalized least squares criterion functions square integrable order partial derivatives prove converges almost surely uniformly entire domain sample size increases
"For an ordered point set in a Euclidean space or, more generally, in an abstract metric space, the ordered Nearest Neighbor Graph is obtained by connecting each of the points to its closest predecessor by a directed edge. We show that for every set of n points in R d , there exists an order such that the corresponding ordered Nearest Neighbor Graph has maximum degree at least log ⁡ n / ( 4 d ) . Apart from the 1 / ( 4 d ) factor, this bound is the best possible. As for the abstract setting, we show that for every n-element metric space, there exists an order such that the corresponding ordered Nearest Neighbor Graph has maximum degree Ω ( log ⁡ n / log ⁡ log ⁡ n ) .",ordered point set euclidean space generally abstract metric space ordered nearest neighbor graph obtained connecting points closest predecessor directed edge show every set points exists order corresponding ordered nearest neighbor graph maximum degree least log apart factor bound best possible abstract setting show every element metric space exists order corresponding ordered nearest neighbor graph maximum degree log log log
"In this study, mathematical modeling of diabetes disease is handled with the Caputo fractional derivative, and the resulting fractional differential equation system is examined in detail. The existence of the solution of this system was mathematically proven under appropriate conditions. Then, for the management and treatment of diabetes, three control parameters are added, and the number of people with diabetes is tried to be kept in the desired range with the optimal control theory. In addition to this, our model gains from parameter estimation and fitting with more accurately generated parameters. The numerical results of the fractional model and the effect of the control parameters are determined with the optimal control approach on glucose levels, examined with various simulations, and presented graphically.",study mathematical modeling diabetes disease handled caputo fractional derivative resulting fractional differential equation system examined detail existence solution system mathematically proven appropriate conditions management treatment diabetes three control parameters added number people diabetes tried kept desired range optimal control theory addition model gains parameter estimation fitting accurately generated parameters numerical results fractional model effect control parameters determined optimal control approach glucose levels examined various simulations presented graphically
"Trajectory planning for port handling equipment is crucial for efficient and safe industrial operations. However, this task is significantly challenged by parameter uncertainties. To address this issue, this paper proposes a novel robust trajectory planning strategy. The method formulates the problem as a stochastic optimal control problem using polynomial chaos expansion and solves it through an adaptive sequential convex optimization algorithm. The strategy is applied to two common port handling equipment systems, validating its effectiveness in handling parameter uncertainties. Experimental results show that the proposed method enhances computational efficiency by at least 45% and achieves greater consistency and robustness compared to existing advanced methods, with its outcomes closely aligning Monte Carlo simulations. The key contributions include a new robust trajectory planning approach, improved prediction accuracy via simultaneous state and control variable expansion, and enhanced computational efficiency for solving high-dimensional nonlinear optimal control problems.",trajectory planning port handling equipment crucial efficient safe industrial operations however task significantly challenged parameter uncertainties address issue paper proposes novel robust trajectory planning strategy method formulates problem stochastic optimal control problem using polynomial chaos expansion solves adaptive sequential convex optimization algorithm strategy applied two common port handling equipment systems validating effectiveness handling parameter uncertainties experimental results show proposed method enhances computational efficiency least achieves greater consistency robustness compared existing advanced methods outcomes closely aligning monte carlo simulations key contributions include new robust trajectory planning approach improved prediction accuracy via simultaneous state control variable expansion enhanced computational efficiency solving high dimensional nonlinear optimal control problems
"In this paper, we investigate a fractional collocation method for linear neutral Volterra integro-differential equations with weakly singular kernels. The proposed method utilizes piecewise polynomials of fractional order, which match the typical singularity of this kind of problems well. A rigorous convergence analysis proves that the optimal orders of convergence can be attained under appropriate choices of the fractional exponent and graded mesh. Superconvergence results are also achieved. Finally, numerical experiments confirm the theoretical results.",paper investigate fractional collocation method linear neutral volterra integro differential equations weakly singular kernels proposed method utilizes piecewise polynomials fractional order match typical singularity kind problems well rigorous convergence analysis proves optimal orders convergence attained appropriate choices fractional exponent graded mesh superconvergence results also achieved finally numerical experiments confirm theoretical results
"Metaheuristic algorithms are commonly used optimization techniques for solving high-dimensional complex problems. To achieve more successful results, improvements are made on these metaheuristic algorithms. This study aims to enhance the performance of the Migrating Birds Optimization (MBO) Algorithm, a neighborhood-based metaheuristic algorithm. To this end, MBO is implemented on a high-performance computing (HPC) architecture using Open MPI, and the Multi-Island Distributed Migrating Birds Optimization (MIDMBO) Algorithm is developed. MIDMBO is a distributed metaheuristic optimization algorithm that utilizes multiple islands, where successful solutions migrate between islands. The migration process between islands, facilitated by MPI, enables MIDMBO to generate more successful outcomes. Therefore, studies focused on the migration process are of great importance. In this study, the relationship between algorithm performance and variations in fundamental migration parameters such as migration rate, migration interval, and the number of islands is examined. Regression analyses are performed on the results obtained from MIDMBO, and the model that best explains performance variation is identified. Additionally, artificial neural network (ANN) models are constructed, and the degree to which changes in parameter values explain the results is evaluated.",metaheuristic algorithms commonly used optimization techniques solving high dimensional complex problems achieve successful results improvements made metaheuristic algorithms study aims enhance performance migrating birds optimization mbo algorithm neighborhood based metaheuristic algorithm end mbo implemented high performance computing hpc architecture using open mpi multi island distributed migrating birds optimization midmbo algorithm developed midmbo distributed metaheuristic optimization algorithm utilizes multiple islands successful solutions migrate islands migration process islands facilitated mpi enables midmbo generate successful outcomes therefore studies focused migration process great importance study relationship algorithm performance variations fundamental migration parameters migration rate migration interval number islands examined regression analyses performed results obtained midmbo model best explains performance variation identified additionally artificial neural network ann models constructed degree changes parameter values explain results evaluated
"Many practical applications of Navier–Stokes problems require solving sequences of saddle-point systems. These systems often arise from the mixed-finite element discretization of the Navier–Stokes equations. A popular strategy to solve them is by combining preconditioning techniques with Krylov subspace methods, which has proven effective in achieving computational efficiency. Since global Krylov subspace methods are specifically designed for solving saddle-point problems with multiple right-hand sides, they cannot be utilized to solve sequences of these problems. We propose an alternative approach applicable to matrices with a saddle-point structure. In this new approach, projection into the null-space of the block-(2,1) of the saddle-point matrix leads to a linear system with several right-hand sides. Due to the large scale and ill-conditioned nature of these linear systems, effective solvers must be implemented. Instead of applying a standard iterative method to solve each linear system independently, it is often more effective to apply a global method. Here, we develop and assess various methods for solving linear systems with several right-hand sides, with a special focus on the preconditioned global conjugate gradient (PGCG) method. We show that our suggested approach is highly effective for sequences of saddle-point systems with varying matrices. The efficiency of our new approach is evaluated by measuring computational time.",many practical applications navier stokes problems require solving sequences saddle point systems systems often arise mixed finite element discretization navier stokes equations popular strategy solve combining preconditioning techniques krylov subspace methods proven effective achieving computational efficiency since global krylov subspace methods specifically designed solving saddle point problems multiple right hand sides utilized solve sequences problems propose alternative approach applicable matrices saddle point structure new approach projection null space block saddle point matrix leads linear system several right hand sides due large scale ill conditioned nature linear systems effective solvers must implemented instead applying standard iterative method solve linear system independently often effective apply global method develop assess various methods solving linear systems several right hand sides special focus preconditioned global conjugate gradient pgcg method show suggested approach highly effective sequences saddle point systems varying matrices efficiency new approach evaluated measuring computational time
"Differential equations posed on quadratic matrix Lie groups arise in the context of classical mechanics and quantum dynamical systems. Lie group numerical integrators preserve the constants of motions defining the Lie group. Thus, they respect important physical laws of the dynamical system, such as unitarity and energy conservation in the context of quantum dynamical systems, for instance. In this article we develop a high-order commutator free Lie group integrator for non-autonomous differential equations evolving on quadratic Lie groups. Instead of matrix exponentials, which are expensive to evaluate and need to be approximated by appropriate rational functions in order to preserve the Lie group structure, the proposed method is obtained as a composition of Cayley transforms which naturally respect the structure of quadratic Lie groups while being computationally efficient to evaluate. Unlike Cayley–Magnus methods the method is also free from nested matrix commutators.",differential equations posed quadratic matrix lie groups arise context classical mechanics quantum dynamical systems lie group numerical integrators preserve constants motions defining lie group thus respect important physical laws dynamical system unitarity energy conservation context quantum dynamical systems instance article develop high order commutator free lie group integrator non autonomous differential equations evolving quadratic lie groups instead matrix exponentials expensive evaluate need approximated appropriate rational functions order preserve lie group structure proposed method obtained composition cayley transforms naturally respect structure quadratic lie groups computationally efficient evaluate unlike cayley magnus methods method also free nested matrix commutators
"This research introduces a novel numerical technique based on shifted Gegenbauer wavelets for solving Fredholm–Volterra fractional integro-differential equations (FVFIDEs), a class characterized by the presence of both Fredholm and Volterra integral parts. By assuming properties of the fractional derivative and applying the wavelet solution directly to the equation, the problem is transferred to finding the family of solutions of the system of algebraic equations, whose solutions are the coefficients of the series of wavelet solutions. The accuracy and efficiency of the Gegenbauer wavelet approach are primarily evaluated through a direct comparison against solutions generated using the Genocchi polynomials method for established test problems. The study demonstrates that the shifted Gegenbauer wavelet method provides precise and effective solutions, which were analyzed under varying resolution parameters and degrees of Gegenbauer polynomials.",research introduces novel numerical technique based shifted gegenbauer wavelets solving fredholm volterra fractional integro differential equations fvfides class characterized presence fredholm volterra integral parts assuming properties fractional derivative applying wavelet solution directly equation problem transferred finding family solutions system algebraic equations whose solutions coefficients series wavelet solutions accuracy efficiency gegenbauer wavelet approach primarily evaluated direct comparison solutions generated using genocchi polynomials method established test problems study demonstrates shifted gegenbauer wavelet method provides precise effective solutions analyzed varying resolution parameters degrees gegenbauer polynomials
"The consistent Riccati expansion (CRE) method is extended and applied to derive interaction solutions of the Drinfel’d-Sokolov-Wilson (DSW) system. It has been shown that the DSW system is solvable using the extended CRE (ECRE) method. By utilizing the ECRE method, interaction solutions between solitons and Jacobi sine periodic waves are explicitly obtained for the DSW system. Furthermore, the dynamic behavior of these interactions is analyzed in detail.",consistent riccati expansion cre method extended applied derive interaction solutions drinfel sokolov wilson dsw system shown dsw system solvable using extended cre ecre method utilizing ecre method interaction solutions solitons jacobi sine periodic waves explicitly obtained dsw system furthermore dynamic behavior interactions analyzed detail
"We investigate multiple orthogonal polynomials associated with the system of measures obtained by applying a Christoffel transform to each of the orthogonality measures. We present an algorithm for computing the transformed recurrence coefficients and determinantal formulas for the transformed multiple orthogonal polynomials of type I and type II. We apply these results to show that zeros of multiple orthogonal polynomials of an Angelesco or an AT system interlace with the zeros of the polynomials corresponding to its one-step Christoffel transform. This allows us to prove a number of interlacing properties satisfied by the multiple orthogonality analogues of classical orthogonal polynomials. For the discrete polynomials, this also produces an estimate on the smallest distance between consecutive zeros. We also identify a connection between the Christoffel transform of orthogonal polynomials and multiple orthogonality systems containing a finitely supported measure. In consequence, the compatibility relations for the nearest neighbour recurrence coefficients provide a new algorithm for the computation of the Jacobi coefficients of the one-step or multi-step Christoffel transforms.",investigate multiple orthogonal polynomials associated system measures obtained applying christoffel transform orthogonality measures present algorithm computing transformed recurrence coefficients determinantal formulas transformed multiple orthogonal polynomials type type apply results show zeros multiple orthogonal polynomials angelesco system interlace zeros polynomials corresponding one step christoffel transform allows prove number interlacing properties satisfied multiple orthogonality analogues classical orthogonal polynomials discrete polynomials also produces estimate smallest distance consecutive zeros also identify connection christoffel transform orthogonal polynomials multiple orthogonality systems containing finitely supported measure consequence compatibility relations nearest neighbour recurrence coefficients provide new algorithm computation jacobi coefficients one step multi step christoffel transforms
"This paper investigates the inverses and determinants of generalized anti-tridiagonal Hankel matrices. We first present an explicit formula for the inverses of these matrices, based on tensor products and the inverses of general anti-tridiagonal Hankel matrices. An efficient algorithm for matrix inversion is then proposed, reducing memory requirements for storing these matrices and simplifying analysis by leveraging the properties of tensor products. Additionally, we introduce a cost-effective algorithm for computing the determinants of generalized anti-tridiagonal Hankel matrices, transforming the problem into that of lower-order anti-tridiagonal Hankel matrix determinants. We also derive an explicit formula for determinant evaluation. Numerical results from MATLAB simulations demonstrate the accuracy, efficiency, and performance of the proposed algorithms, which are compared to MATLAB built-in functions, showing clear advantages in computational accuracy and speed.",paper investigates inverses determinants generalized anti tridiagonal hankel matrices first present explicit formula inverses matrices based tensor products inverses general anti tridiagonal hankel matrices efficient algorithm matrix inversion proposed reducing memory requirements storing matrices simplifying analysis leveraging properties tensor products additionally introduce cost effective algorithm computing determinants generalized anti tridiagonal hankel matrices transforming problem lower order anti tridiagonal hankel matrix determinants also derive explicit formula determinant evaluation numerical results matlab simulations demonstrate accuracy efficiency performance proposed algorithms compared matlab built functions showing clear advantages computational accuracy speed
"This paper presents a numerical analysis of an Implicit–Explicit scheme for a non-linear parabolic PDE model of tumor angiogenesis. The model describes the evolution of endothelial cells, proteases, inhibitors, and extracellular matrix through coupled equations incorporating diffusion, chemotaxis, haptotaxis, and reaction kinetics. We design a numerical approach that manages stiff linear terms implicitly while handling non-stiff nonlinear terms explicitly. Theoretical analysis establishes main features of the scheme such as stability properties, second-order convergence, and preservation of conservation laws. Moreover, the computational complexity is analyzed, demonstrating an efficiency gains compared to fully explicit methods. Numerical experiments validate these findings and show the ability of the method to accurately capture complex biological phenomena.",paper presents numerical analysis implicit explicit scheme non linear parabolic pde model tumor angiogenesis model describes evolution endothelial cells proteases inhibitors extracellular matrix coupled equations incorporating diffusion chemotaxis haptotaxis reaction kinetics design numerical approach manages stiff linear terms implicitly handling non stiff nonlinear terms explicitly theoretical analysis establishes main features scheme stability properties second order convergence preservation conservation laws moreover computational complexity analyzed demonstrating efficiency gains compared fully explicit methods numerical experiments validate findings show ability method accurately capture complex biological phenomena
"This study presents an efficient numerical framework for solving an inverse heat equation problem involving a time-dependent heat source and temperature distribution. The problem incorporates non-classical boundary conditions and integral over-determination conditions, where the latter represents error-contaminated measurements. A time-stepping scheme is employed to discretize the time derivative, while a meshless local collocation method using radial basis functions (RBFs) is applied for spatial discretization. The stability and convergence of the time-discretized approach are rigorously established using the energy method. To address noise in input data, Tikhonov regularization of three orders is implemented, along with a novel approach for determining the optimal regularization parameter. This method outperforms traditional techniques such as the quasi-optimality criterion, L-curve, and discrepancy principle. Numerical experiments validate the approach by demonstrating its accuracy for exact data and stability under noisy conditions, thereby establishing it as a reliable tool for inverse problem-solving.",study presents efficient numerical framework solving inverse heat equation problem involving time dependent heat source temperature distribution problem incorporates non classical boundary conditions integral determination conditions latter represents error contaminated measurements time stepping scheme employed discretize time derivative meshless local collocation method using radial basis functions rbfs applied spatial discretization stability convergence time discretized approach rigorously established using energy method address noise input data tikhonov regularization three orders implemented along novel approach determining optimal regularization parameter method outperforms traditional techniques quasi optimality criterion curve discrepancy principle numerical experiments validate approach demonstrating accuracy exact data stability noisy conditions thereby establishing reliable tool inverse problem solving
"In this paper, we design a second-order unconditional energy stable numerical scheme for the modified Cahn–Hilliard phase field model simulating wetting phenomenon. We use the scalar auxiliary variable (SAV) method to transform the Cahn–Hilliard equation into an equivalent form, use second-order backward difference formula (BDF2) for time discretization and finite element method (FEM) for space discretization. A relaxation technique is employed to correct the numerical errors of the scalar auxiliary variable. The scheme is proved to be unconditionally energy stable. We perform several numerical experiments for wetting phenomena on flat, curved and rough substrates, demonstrating the capability of our proposed numerical scheme. At the same time, we also combine our numerical scheme with an adaptive time stepping strategy for acceleration.",paper design second order unconditional energy stable numerical scheme modified cahn hilliard phase field model simulating wetting phenomenon use scalar auxiliary variable sav method transform cahn hilliard equation equivalent form use second order backward difference formula bdf time discretization finite element method fem space discretization relaxation technique employed correct numerical errors scalar auxiliary variable scheme proved unconditionally energy stable perform several numerical experiments wetting phenomena flat curved rough substrates demonstrating capability proposed numerical scheme time also combine numerical scheme adaptive time stepping strategy acceleration
"In this paper, the stability of IMEX-BDF methods for delay differential equations (DDEs) is studied based on the test equation y ′ ( t ) = − A y ( t ) + B y ( t − τ ) , where τ is a constant delay, A diagonalizes in R + , but B might be any matrix. First, it is analyzed the case where both matrices diagonalize simultaneously, and sufficient conditions to obtain linear stability are proved. However, the paper focuses on the case where the matrices A and B are not simultaneously diagonalizable. The concept of field of values, denoted by F ( ⋅ ) , is used to prove a sufficient condition for unconditional stability of these methods: if A is Hermitian, IMEX-BDF2 is unconditionally stable whenever F ( A − 1 B ) is contained in the disk centered at 0 and radius 1 3 , D ( 0 , 1 3 ) , while IMEX-BDF3 is unconditionally stable whenever F ( A − 1 B ) ⊆ D ( 0 , 1 7 ) . Furthermore, sufficient conditions are derived to ensure stability, but according to the step size as a function of the radius of the disk containing F ( A − 1 B ) . The approach developed herein is illustrated through several examples in which the discussed theory is applied not only to DDEs, but also to parabolic problems given by partial delay differential equations (PDDEs) with a diffusion term.",paper stability imex bdf methods delay differential equations ddes studied based test equation constant delay diagonalizes might matrix first analyzed case matrices diagonalize simultaneously sufficient conditions obtain linear stability proved however paper focuses case matrices simultaneously diagonalizable concept field values denoted used prove sufficient condition unconditional stability methods hermitian imex bdf unconditionally stable whenever contained disk centered radius imex bdf unconditionally stable whenever furthermore sufficient conditions derived ensure stability according step size function radius disk containing approach developed herein illustrated several examples discussed theory applied ddes also parabolic problems given partial delay differential equations pddes diffusion term
"Examining how everyday and tourism contexts are subjectively perceived to elicit distinct mental states offers valuable insights into the intrinsic value of tourism. This research examines the impact of contact with nature on creative thinking in tourism compared to everyday contexts through a mixed-methods approach involving an online survey (Study 1: N = 438), a scenario-based experiment (Study 2: N = 300), and a field survey conducted at Beijing Olympic Forest Park (Study 3: N = 164). The results demonstrate that contact with nature during tourism significantly enhances creative thinking compared to everyday contexts. Mental bandwidth partially mediates this dynamic, with openness to experience further moderating the impact of nature contact contexts on mental bandwidth. These findings advance the literature by highlighting the distinctive social and psychological benefits of tourism in promoting creativity beyond what is typically experienced in everyday life.",examining everyday tourism contexts subjectively perceived elicit distinct mental states offers valuable insights intrinsic value tourism research examines impact contact nature creative thinking tourism compared everyday contexts mixed methods approach involving online survey study scenario based experiment study field survey conducted beijing olympic forest park study results demonstrate contact nature tourism significantly enhances creative thinking compared everyday contexts mental bandwidth partially mediates dynamic openness experience moderating impact nature contact contexts mental bandwidth findings advance literature highlighting distinctive social psychological benefits tourism promoting creativity beyond typically experienced everyday life
"In this article, we propose a novel class of fractal cubic multiquadric functions that generalize the classical cubic multiquadric functions. By employing these fractal multiquadric functions, we develop a fractal quasi-interpolation operator, denoted by L d α . We examine various properties of these fractal cubic multiquadric approximants, such as shape-preserving characteristics and the ability to reproduce quadratic polynomials. Error estimates for these approximants are also derived, and estimates for the box-dimension of the graphs of fractal cubic multiquadric approximants are given. Numerical examples are presented to validate these theoretical findings and highlight the benefits of the fractal quasi-interpolant L d α f . Additionally, we apply the proposed fractal quasi-interpolants to solve an integral equation with a non-smooth degenerate kernel. This approach shows a high rate of convergence to the exact solution. Box-dimension results for the numerical solution of this integral equation are also established.",article propose novel class fractal cubic multiquadric functions generalize classical cubic multiquadric functions employing fractal multiquadric functions develop fractal quasi interpolation operator denoted examine various properties fractal cubic multiquadric approximants shape preserving characteristics ability reproduce quadratic polynomials error estimates approximants also derived estimates box dimension graphs fractal cubic multiquadric approximants given numerical examples presented validate theoretical findings highlight benefits fractal quasi interpolant additionally apply proposed fractal quasi interpolants solve integral equation non smooth degenerate kernel approach shows high rate convergence exact solution box dimension results numerical solution integral equation also established
"This paper introduces a novel domain-based regularization approach for estimating landslide thickness from surface velocity data. Such quantity is crucial for accurately assessing landslides behavior, potential impact, and associated risks. Here, we formulate the problem as an ill-posed inverse problem and propose, for its solution, a multipenalty regularization approach based on the decomposition of the landslide domain in several regions with uniform magnitude of the horizontal velocity. We extend the Balancing Principle to accommodate non-constant balancing parameters across decomposed regions. Our Domain-based Majorization-Minimization algorithm converges to solutions that satisfy this extended principle, demonstrating superior performance compared to traditional methods. Through rigorous testing on both synthetic and real-world landslide data, we show that strategic domain decomposition based on velocity field homogeneity enhances estimation accuracy. Our findings reveal that while excessive subdivision is counterproductive, identifying appropriate velocity-based macro-regions yields optimal results. This methodology provides more reliable thickness estimates crucial for landslide risk assessment and monitoring.",paper introduces novel domain based regularization approach estimating landslide thickness surface velocity data quantity crucial accurately assessing landslides behavior potential impact associated risks formulate problem ill posed inverse problem propose solution multipenalty regularization approach based decomposition landslide domain several regions uniform magnitude horizontal velocity extend balancing principle accommodate non constant balancing parameters across decomposed regions domain based majorization minimization algorithm converges solutions satisfy extended principle demonstrating superior performance compared traditional methods rigorous testing synthetic real world landslide data show strategic domain decomposition based velocity field homogeneity enhances estimation accuracy findings reveal excessive subdivision counterproductive identifying appropriate velocity based macro regions yields optimal results methodology provides reliable thickness estimates crucial landslide risk assessment monitoring
"Numerical simulation of semiconductor device with heat and magnetic influences is a preliminary problem in information science. In this paper, mathematical model, numerical method and theoretical analysis are discussed. Four major physical unknowns (the potential, electron concentration, hole concentration, and the heat) are defined by several nonlinear PDEs, an elliptic equation, two convection–diffusion equations and a heat conductor equation. A block-centered numerical method with conservative nature is used to obtain the potential, and the computational accuracy is improved. An upwind block-centered difference method is adopted for solving other PDEs on changing meshes. The diffusion and convection operators are discretized by block-centered differences and upwind differences, respectively. Changing meshes are effective for simulating the status nearby P-N junction. The composite scheme avoids numerical dispersion and nonphysical oscillation, and the conservation is preserved. The unknowns and their adjoint vectors are computed at the same time. Some theoretical techniques such as energy norm estimates, the method of duality and mathematical induction are used to finish convergence analysis. Error results are concluded. Finally, numerical examples show the efficiency and possible application.",numerical simulation semiconductor device heat magnetic influences preliminary problem information science paper mathematical model numerical method theoretical analysis discussed four major physical unknowns potential electron concentration hole concentration heat defined several nonlinear pdes elliptic equation two convection diffusion equations heat conductor equation block centered numerical method conservative nature used obtain potential computational accuracy improved upwind block centered difference method adopted solving pdes changing meshes diffusion convection operators discretized block centered differences upwind differences respectively changing meshes effective simulating status nearby junction composite scheme avoids numerical dispersion nonphysical oscillation conservation preserved unknowns adjoint vectors computed time theoretical techniques energy norm estimates method duality mathematical induction used finish convergence analysis error results concluded finally numerical examples show efficiency possible application
"In the present study, a finite set of biorthogonal polynomials in two variables, produced from Konhauser polynomials, is introduced. Some properties like Laplace transform, integral and operational representation, fractional calculus operators of this family are investigated. Also, we compute Fourier transform for this new set and discover a new family of finite biorthogonal functions with the help of Parseval’s identity. Further, in order to have semigroup property, we modify this finite set by adding two new parameters and construct fractional calculus operators. Thus, integral equation and integral operator are obtained for the modified version.",present study finite set biorthogonal polynomials two variables produced konhauser polynomials introduced properties like laplace transform integral operational representation fractional calculus operators family investigated also compute fourier transform new set discover new family finite biorthogonal functions help parseval identity order semigroup property modify finite set adding two new parameters construct fractional calculus operators thus integral equation integral operator obtained modified version
"Inverse magnetisation problem consists in inferring information about a magnetic source from measurements of its magnetic field. Unlike a general magnetisation distribution, the total magnetisation (net moment) of the source is a quantity that theoretically can be uniquely determined from the field. At the same time, it is often the most useful quantity for practical applications (on large and small scales) such as detection of a magnetic anomaly in magnetic prospection problem or finding the overall strength and mean direction of the magnetisation distribution of a magnetised rock sample. It is known that the net moment components can be explicitly estimated using the so-called Helbig’s integrals which involve integration of the magnetic field data on the plane against simple polynomials. Evaluation of these integrals requires knowledge of the magnetic field data on a large region or the use of ad hoc methods to compensate for the lack thereof. In this paper, we derive higher-order analogs of Helbig’s integrals which permit estimation of total magnetisation components in terms of measurement data available on a smaller region. Motivated by a concrete experimental setup for analysing remanent magnetisation of rock samples with a scanning microscope, we also extend Helbig’s integrals to the situation when knowledge of only one field component is necessary. Moreover, apart from derivation of these novel formulas, we rigorously prove their accuracy. The presented approach, based on an appropriate splitting in the Fourier domain and estimates of oscillatory integrals (involving both small and large parameters), elucidates the derivation of asymptotic formulas for the net moment components to an arbitrary order, a possibility that was previously unclear. The obtained results are illustrated numerically and their robustness with respect to the noise is discussed.",inverse magnetisation problem consists inferring information magnetic source measurements magnetic field unlike general magnetisation distribution total magnetisation net moment source quantity theoretically uniquely determined field time often useful quantity practical applications large small scales detection magnetic anomaly magnetic prospection problem finding overall strength mean direction magnetisation distribution magnetised rock sample known net moment components explicitly estimated using called helbig integrals involve integration magnetic field data plane simple polynomials evaluation integrals requires knowledge magnetic field data large region use hoc methods compensate lack thereof paper derive higher order analogs helbig integrals permit estimation total magnetisation components terms measurement data available smaller region motivated concrete experimental setup analysing remanent magnetisation rock samples scanning microscope also extend helbig integrals situation knowledge one field component necessary moreover apart derivation novel formulas rigorously prove accuracy presented approach based appropriate splitting fourier domain estimates oscillatory integrals involving small large parameters elucidates derivation asymptotic formulas net moment components arbitrary order possibility previously unclear obtained results illustrated numerically robustness respect noise discussed
"Recently, Multiway Delay-embedding Transform (MDT)-based low-rank tensor completion has achieved a lot of attention for color image recovery. However, existing studies mostly focus on tensor decomposition to encode the low-rankness of the Hankel tensor derived from MDT, which are sensitive to the predefined rank and limit the recovery performance. Aiming at addressing this issue, in this paper, we use the High-order Tensor Nuclear Norm (HTNN) to approximate the Hankel tensor rank, thus a new model named MDT-HTNN is proposed for low-rank tensor completion. Efficient algorithm based on the alternating direction method of multipliers (ADMM) is developed to solve the proposed model and its convergence analysis is discussed in detail. Extensive experiments on a series of color images and MRI illustrate that our proposed algorithm significantly improve the recovery accuracy. Specifically, under multiple sampling rate settings for multiple color images, the average PSNR value increased by 14.8% and the CPU time decreased by 89.5% compared with the classical MDT-Tucker method.",recently multiway delay embedding transform mdt based low rank tensor completion achieved lot attention color image recovery however existing studies mostly focus tensor decomposition encode low rankness hankel tensor derived mdt sensitive predefined rank limit recovery performance aiming addressing issue paper use high order tensor nuclear norm htnn approximate hankel tensor rank thus new model named mdt htnn proposed low rank tensor completion efficient algorithm based alternating direction method multipliers admm developed solve proposed model convergence analysis discussed detail extensive experiments series color images mri illustrate proposed algorithm significantly improve recovery accuracy specifically multiple sampling rate settings multiple color images average psnr value increased cpu time decreased compared classical mdt tucker method
"This paper investigates the problem of recovering the spatial distribution of the source term in a parabolic system from local measurement data. We propose a method based on Physics-Informed Neural Networks (PINNs) to approximate the solution of this inverse problem. Within this framework, we design a novel loss function that incorporates the residuals of the partial differential equation (PDE), measurement data residuals, as well as residuals from initial–boundary conditions and boundary derivatives. These additional terms enhance the regularity of the solution to address the ill-posedness of the inverse problem. Based on the conditional stability of the inverse problem, we demonstrate that the neural network can effectively approximate its solution, and we further establish generalization error estimates for the source term, which depend on the training error and data noise level, while also verifying the convergence of the network. A series of numerical experiments further validate the effectiveness and robustness of the proposed method, showing that it achieves high accuracy and efficiency under noisy data and varying measurement conditions.",paper investigates problem recovering spatial distribution source term parabolic system local measurement data propose method based physics informed neural networks pinns approximate solution inverse problem within framework design novel loss function incorporates residuals partial differential equation pde measurement data residuals well residuals initial boundary conditions boundary derivatives additional terms enhance regularity solution address ill posedness inverse problem based conditional stability inverse problem demonstrate neural network effectively approximate solution establish generalization error estimates source term depend training error data noise level also verifying convergence network series numerical experiments validate effectiveness robustness proposed method showing achieves high accuracy efficiency noisy data varying measurement conditions
"The interaction between fluids in porous media and free flow regions is significant in various fields such as geology, environmental engineering, and petroleum engineering. This problem can be modeled using a coupled system of Stokes equations and Darcy’s law. Finite element discretization of the coupled Stokes–Darcy model results in a double saddle-point system. In this work, we propose a class of inexact block three-by-three triangular preconditioners for these discretized saddle-point systems by fully utilizing their special structure. When used to precondition Krylov subspace methods, each step requires solving only three simple symmetric positive definite linear subsystems. Additionally, we derive explicit and sharp spectral bounds for the preconditioned matrices and show that all eigenvalues have positive real parts. Numerical experiments demonstrate the effectiveness and robustness of our preconditioners in solving the coupled Stokes–Darcy model.",interaction fluids porous media free flow regions significant various fields geology environmental engineering petroleum engineering problem modeled using coupled system stokes equations darcy law finite element discretization coupled stokes darcy model results double saddle point system work propose class inexact block three three triangular preconditioners discretized saddle point systems fully utilizing special structure used precondition krylov subspace methods step requires solving three simple symmetric positive definite linear subsystems additionally derive explicit sharp spectral bounds preconditioned matrices show eigenvalues positive real parts numerical experiments demonstrate effectiveness robustness preconditioners solving coupled stokes darcy model
"This work presents a novel numerical and quantitative methodology grounded in Constraint Satisfaction Problem (CSP) theory, aimed at developing a specialized tool for the structural analysis of fully connected, feed-forward Neural Networks (NNs). The proposed approach enables a systematic exploration of neuron configurations within the hidden layers. A backtracking search algorithm was specifically designed to traverse the space of admissible architectural parameters, thereby implementing a constrained combinatorial strategy for neural network architecture exploration. This study introduces a practical tool for researchers aiming to identify diverse neuronal organizational patterns within hidden layers, subject to predefined hyperparameter constraints. The proposed algorithm was subsequently validated by exhaustively exploring all feasible architectural configurations for solving a two-dimensional Poisson equation using a Physics-Informed Neural Network (PINN).",work presents novel numerical quantitative methodology grounded constraint satisfaction problem csp theory aimed developing specialized tool structural analysis fully connected feed forward neural networks nns proposed approach enables systematic exploration neuron configurations within hidden layers backtracking search algorithm specifically designed traverse space admissible architectural parameters thereby implementing constrained combinatorial strategy neural network architecture exploration study introduces practical tool researchers aiming identify diverse neuronal organizational patterns within hidden layers subject predefined hyperparameter constraints proposed algorithm subsequently validated exhaustively exploring feasible architectural configurations solving two dimensional poisson equation using physics informed neural network pinn
"Reliability engineering addresses the challenge of analyzing how components contribute to system failures when designing large and complex systems. This analysis is commonly referred to as importance analysis. One specific type of importance measure is Joint Structural Importance (JSI), which assesses the significance of two or more components based on their positions within the system. This measure helps in effectively allocating redundancy and guiding design decisions to enhance overall reliability. This paper proposes a new method for computing the JSI of independent components within a coherent system. The key idea is to utilize Logical Differential Calculus, specifically through the Direct Partial Logical Derivatives (DPLD). Applying the DPLD for calculating the JSI based on the indicator function is a novel approach. Notably, this method allows for calculating the JSI from the structure function represented by a truth table without the need to transform the function into a Boolean expression. Numerical examples are provided to illustrate the application and advantages of the proposed method. Additionally, case studies on a substation automation system and an auxiliary feedwater system are conducted to demonstrate the technique's effectiveness in engineering applications.",reliability engineering addresses challenge analyzing components contribute system failures designing large complex systems analysis commonly referred importance analysis one specific type importance measure joint structural importance jsi assesses significance two components based positions within system measure helps effectively allocating redundancy guiding design decisions enhance overall reliability paper proposes new method computing jsi independent components within coherent system key idea utilize logical differential calculus specifically direct partial logical derivatives dpld applying dpld calculating jsi based indicator function novel approach notably method allows calculating jsi structure function represented truth table without need transform function boolean expression numerical examples provided illustrate application advantages proposed method additionally case studies substation automation system auxiliary feedwater system conducted demonstrate technique effectiveness engineering applications
"Sinc-collocation methods for Volterra-Fredholm integral equations of the second kind were proposed independently by multiple authors: by Shamloo et al. in 2012 and by Mesgarani and Mollapourasl in 2013. Their theoretical analyses and numerical experiments suggest that the presented methods can attain root-exponential convergence. However, their convergence has not been strictly proved. This study improves these methods to facilitate implementation, and provides a convergence theorem for the improved method. For the same equations, another Sinc-collocation method was proposed in 2016 by John and Ogbonna, which is regarded as an improvement to the variable transformation employed by Shamloo et al. It may attain a higher rate than the previous methods, but its convergence has not yet been proved. Therefore, this study improves it to facilitate implementation, and provides a convergence theorem for the improved method.",sinc collocation methods volterra fredholm integral equations second kind proposed independently multiple authors shamloo mesgarani mollapourasl theoretical analyses numerical experiments suggest presented methods attain root exponential convergence however convergence strictly proved study improves methods facilitate implementation provides convergence theorem improved method equations another sinc collocation method proposed john ogbonna regarded improvement variable transformation employed shamloo may attain higher rate previous methods convergence yet proved therefore study improves facilitate implementation provides convergence theorem improved method
"In this paper, we study the valuation of American put options based on the 4/2 stochastic volatility model that incorporates multiscale double mean-reverting (DMR) volatility. The option price problem is transformed into a partial differential equation (PDE) problem with free boundary, which in turn leads to PDE problems for a few terms of asymptotic expansions of the option price and free boundary. The approximate American put price is decomposed into the sum of the corresponding European put price and the early exercise premium. The chosen modification of the 4/2 stochastic volatility allows for a step-by-step approach to the option price starting from the Black–Scholes price of the corresponding European option, making it easier to approximate the price of an American put option. We check the accuracy of the resultant approximate option price and free boundary by using the least squares Monte Carlo simulation method and investigate the impact of the Heston and 3/2 factors of the volatility on the option price and free boundary. We calibrate our model to real market data and benchmark it against the widely used Heston model and the 3/2 model. We also conduct a sensitivity analysis to show how small changes in model parameters influence the American put option premium and early exercise boundary, and discuss limiting scenarios when the 3/2 term vanishes or volatility becomes deterministic. In addition, two specific results are provided. We derive a semi-analytic solution for the approximate option price and free boundary when an American put option is near expiration. We also study the pricing of an American put option without an expiration date and obtain a closed-form analytic formula for the approximate option price and free boundary.",paper study valuation american put options based stochastic volatility model incorporates multiscale double mean reverting dmr volatility option price problem transformed partial differential equation pde problem free boundary turn leads pde problems terms asymptotic expansions option price free boundary approximate american put price decomposed sum corresponding european put price early exercise premium chosen modification stochastic volatility allows step step approach option price starting black scholes price corresponding european option making easier approximate price american put option check accuracy resultant approximate option price free boundary using least squares monte carlo simulation method investigate impact heston factors volatility option price free boundary calibrate model real market data benchmark widely used heston model model also conduct sensitivity analysis show small changes model parameters influence american put option premium early exercise boundary discuss limiting scenarios term vanishes volatility becomes deterministic addition two specific results provided derive semi analytic solution approximate option price free boundary american put option near expiration also study pricing american put option without expiration date obtain closed form analytic formula approximate option price free boundary
"This research examines the relationships between fractional calculus, a field that has recently attracted considerable research interest, and multiplicative calculus. In the Introduction, the arithmetic structure of multiplicative calculus is examined, along with the fundamental operational operators and their systematics. Additionally, the construction of generator functions and their role in deriving new arithmetic frameworks are demonstrated, with examples of alternative arithmetic systems provided. The Preliminaries present the fundamental concepts, definitions, and theorems of multiplicative calculus, including its derivative and integral operators. Subsequently, we present the main results obtained in this study. We introduce, for the first time, multiplicative fractional integral and derivative operators for functions of two variables. Subsequently, we establish several important properties of the proposed multiplicative fractional operators. Finally, we present some examples and graphical illustrations to demonstrate applications of these operators.",research examines relationships fractional calculus field recently attracted considerable research interest multiplicative calculus introduction arithmetic structure multiplicative calculus examined along fundamental operational operators systematics additionally construction generator functions role deriving new arithmetic frameworks demonstrated examples alternative arithmetic systems provided preliminaries present fundamental concepts definitions theorems multiplicative calculus including derivative integral operators subsequently present main results obtained study introduce first time multiplicative fractional integral derivative operators functions two variables subsequently establish several important properties proposed multiplicative fractional operators finally present examples graphical illustrations demonstrate applications operators
"We propose a global method based on Lagrange interpolation at Jacobi nodes to approximate the solution of second-kind Volterra–Fredholm integral equations on the interval [ − 1 , 1 ] . The considered equations involve kernels with a multiple zero at the origin and Jacobi-type weight functions, allowing for algebraic endpoint singularities in the data. Accordingly, the problem is studied in suitable weighted spaces of continuous functions. We prove the stability and convergence of our numerical method and derive explicit a priori error estimates in the weighted uniform norm, showing that the approximation essentially achieves the convergence rate of the best weighted polynomial approximation, up to an additional logarithmic factor log m . Some numerical experiments are presented to illustrate the accuracy and efficiency of the proposed approach.",propose global method based lagrange interpolation jacobi nodes approximate solution second kind volterra fredholm integral equations interval considered equations involve kernels multiple zero origin jacobi type weight functions allowing algebraic endpoint singularities data accordingly problem studied suitable weighted spaces continuous functions prove stability convergence numerical method derive explicit priori error estimates weighted uniform norm showing approximation essentially achieves convergence rate best weighted polynomial approximation additional logarithmic factor log numerical experiments presented illustrate accuracy efficiency proposed approach
"In recent years, various fractional-order basis functions have been developed to tackle different types of fractional problems. This paper introduces a new class of fractional-order hybrid functions constructed from block-pulse functions and Chelyshkov polynomials. By applying the Riemann–Liouville integral operator, we derive explicit results using the closed form of Chelyshkov polynomials. These results are then employed to develop a numerical scheme for solving a general class of fractional integro-differential equations with weakly singular kernels. The approach, combined with the essential properties of the Caputo derivative and the Riemann–Liouville operator, leads to the definition of remainders associated with the main problem. By selecting suitable collocation points, the problem is transformed into a solvable system of equations. An error bound is established for the proposed approximation, and the effectiveness of the method is demonstrated through several illustrative examples.",recent years various fractional order basis functions developed tackle different types fractional problems paper introduces new class fractional order hybrid functions constructed block pulse functions chelyshkov polynomials applying riemann liouville integral operator derive explicit results using closed form chelyshkov polynomials results employed develop numerical scheme solving general class fractional integro differential equations weakly singular kernels approach combined essential properties caputo derivative riemann liouville operator leads definition remainders associated main problem selecting suitable collocation points problem transformed solvable system equations error bound established proposed approximation effectiveness method demonstrated several illustrative examples
"Currently, several efficient algorithms utilize randomization techniques to compute low-rank matrix approximations, such as Randomized QR with Column Pivoting (RQRCP) and Flip-Flop spectrum-revealing QR (FFSRQR). While these algorithms have achieved notable efficiency in dealing with large-scale low-rank matrix approximations, there is still scope for improvement. This paper introduces an improved version of the RQRCP algorithm, enhanced by incorporating a sparse embedding matrix (SEM) for sparse projection, referred to as RQRCP-SEM. Furthermore, building on RQRCP-SEM and employing pass-efficient techniques, this paper proposes two versions of the PEFFSRQR-SEM algorithm to further optimize the efficiency of the FFSRQR algorithm. The paper theoretically analyzes the approximation error and computational complexity of these new algorithms and validates these analyses through numerical experiments.",currently several efficient algorithms utilize randomization techniques compute low rank matrix approximations randomized column pivoting rqrcp flip flop spectrum revealing ffsrqr algorithms achieved notable efficiency dealing large scale low rank matrix approximations still scope improvement paper introduces improved version rqrcp algorithm enhanced incorporating sparse embedding matrix sem sparse projection referred rqrcp sem furthermore building rqrcp sem employing pass efficient techniques paper proposes two versions peffsrqr sem algorithm optimize efficiency ffsrqr algorithm paper theoretically analyzes approximation error computational complexity new algorithms validates analyses numerical experiments
"In this paper, a new scalar auxiliary variable (SAV) spectral Petrov–Galerkin approximation is proposed and studied for nonlinear Hamiltonian systems. The new algorithm is built upon the SAV approach and the spectral Petrov–Galerkin (SPG) method, which enjoys both the advantages of SAV and SPG methods such as the properties of energy preserving and high order accuracy. A rigorous theoretical analysis is provided to show that the proposed algorithm is well-posed and convergent. Furthermore, the conservation properties are investigated. In particular, we prove that SAV-SPG method preserves the modified energy exactly and maintains the symplectic structure up to a spectral accuracy. Numerical experiments have been conducted to verify the efficacy of our algorithm.",paper new scalar auxiliary variable sav spectral petrov galerkin approximation proposed studied nonlinear hamiltonian systems new algorithm built upon sav approach spectral petrov galerkin spg method enjoys advantages sav spg methods properties energy preserving high order accuracy rigorous theoretical analysis provided show proposed algorithm well posed convergent furthermore conservation properties investigated particular prove sav spg method preserves modified energy exactly maintains symplectic structure spectral accuracy numerical experiments conducted verify efficacy algorithm
"We consider nonlinear Volterra integral equations of the second kind with discontinuous kernels, which present significant analytical and numerical challenges due to the combined presence of nonlinearity and kernel discontinuity. To address these difficulties, we develop a new method based on shifted alternative Legendre polynomials and associated operational matrices. The proposed approach approximates the unknown solution via truncated polynomial expansions and systematically transforms the original integral equation into a system of nonlinear algebraic equations through matrix-based discretization. We establish several theoretical results concerning the convergence, stability, and error bounds of the method. Numerical experiments are conducted to validate the proposed approach, demonstrating its accuracy, efficiency, and capability in handling these equations .",consider nonlinear volterra integral equations second kind discontinuous kernels present significant analytical numerical challenges due combined presence nonlinearity kernel discontinuity address difficulties develop new method based shifted alternative legendre polynomials associated operational matrices proposed approach approximates unknown solution via truncated polynomial expansions systematically transforms original integral equation system nonlinear algebraic equations matrix based discretization establish several theoretical results concerning convergence stability error bounds method numerical experiments conducted validate proposed approach demonstrating accuracy efficiency capability handling equations
"This paper is concerned with computing generalized eigenpairs of weakly symmetric tensors. We first show that computing generalized eigenpairs of weakly symmetric tensors is equivalent to finding the nonzero solutions of a nonlinear system of equations, and then propose a derivative-free spectral residual method for it. The method utilizes the residual vector as the search direction and incorporates a derivative-free nonmonotone line search strategy, avoiding any explicit information associated with the Jacobian matrix of the considered nonlinear system of equations. Additionally, the global convergence of the method is established. Numerical results are presented to demonstrate superior efficiency of the proposed method through comparisons with the alternating least squares (ALS) method and other existing approaches. Furthermore, the results show that the proposed method can capture more, and in some cases all, generalized eigenvalues of weakly symmetric tensors.",paper concerned computing generalized eigenpairs weakly symmetric tensors first show computing generalized eigenpairs weakly symmetric tensors equivalent finding nonzero solutions nonlinear system equations propose derivative free spectral residual method method utilizes residual vector search direction incorporates derivative free nonmonotone line search strategy avoiding explicit information associated jacobian matrix considered nonlinear system equations additionally global convergence method established numerical results presented demonstrate superior efficiency proposed method comparisons alternating least squares als method existing approaches furthermore results show proposed method capture cases generalized eigenvalues weakly symmetric tensors
"ExSpliNet is a neural network model that combines ideas of Kolmogorov networks, ensembles of probabilistic trees, and multivariate B-spline representations. In this paper, we study the expressivity of the ExSpliNet model and present two constructive approximation results that mitigate the curse of dimensionality. More precisely, we prove new error bounds for the ExSpliNet approximation of a subset of multivariate continuous functions and also of multivariate generalized bandlimited functions. The main ingredients of the proofs are a constructive version of the Kolmogorov superposition theorem, Maurey’s theorem, and spline approximation results. The curse of dimensionality is lessened in the first case, while it is completely overcome in the second case. Since the considered ExSpliNet model can be regarded as a particular version of the recently introduced neural network architecture called Kolmogorov–Arnold network (KAN), our results also provide insights into the analysis of the expressivity of KANs.",exsplinet neural network model combines ideas kolmogorov networks ensembles probabilistic trees multivariate spline representations paper study expressivity exsplinet model present two constructive approximation results mitigate curse dimensionality precisely prove new error bounds exsplinet approximation subset multivariate continuous functions also multivariate generalized bandlimited functions main ingredients proofs constructive version kolmogorov superposition theorem maurey theorem spline approximation results curse dimensionality lessened first case completely overcome second case since considered exsplinet model regarded particular version recently introduced neural network architecture called kolmogorov arnold network kan results also provide insights analysis expressivity kans
"In this paper, we leverage a recent deep kernel representer theorem to connect kernel based learning and (deep) neural networks in order to understand their interplay. In particular, we show that the use of special types of kernels yields models reminiscent of neural networks that are founded in the same theoretical framework of classical kernel methods, while benefiting from the computational advantages of deep neural networks. Especially the introduced Structured Deep Kernel Networks (SDKNs) can be viewed as neural networks (NNs) with optimizable activation functions obeying a representer theorem. This link allows us to analyze also NNs within the framework of kernel networks. We prove analytic properties of the SDKNs which show their universal approximation properties in three different asymptotic regimes of unbounded number of centers, width and depth. Especially in the case of unbounded depth, more accurate constructions can be achieved using fewer layers compared to corresponding constructions for ReLU neural networks. This is made possible by leveraging properties of kernel approximation.",paper leverage recent deep kernel representer theorem connect kernel based learning deep neural networks order understand interplay particular show use special types kernels yields models reminiscent neural networks founded theoretical framework classical kernel methods benefiting computational advantages deep neural networks especially introduced structured deep kernel networks sdkns viewed neural networks nns optimizable activation functions obeying representer theorem link allows analyze also nns within framework kernel networks prove analytic properties sdkns show universal approximation properties three different asymptotic regimes unbounded number centers width depth especially case unbounded depth accurate constructions achieved using fewer layers compared corresponding constructions relu neural networks made possible leveraging properties kernel approximation
"Cusp-type catastrophe is the most commonly encountered and widely applicable type among elementary catastrophes. However, the canonical mathematical model of the cusp-type catastrophe, constructed using the classical regularization method in catastrophe theory, often lacks quantitative prediction capability. This issue arises from the regularization operations involved in the regularization method, particularly the Taylor expansion and truncation of the potential function, which can introduce unforeseen and potentially large regularization errors. To address this issue, a novel precise regularization method is proposed, which bypasses the Taylor expansion of the potential function and instead employs a global diffeomorphism to achieve precise regularization of the cusp-type catastrophe mathematical model. Using the proposed method, the precise regularization of the cusp-type catastrophe mathematical model for Zeeman catastrophe machine is achieved. Computational results show that the model obtained via the proposed method has significantly lower regularization errors than the classical method, thus meeting the requirements for constructing catastrophe models with quantitative predictive capability. Moreover, due to the implementation of the global diffeomorphism, the proposed precise regularization method can be easily modified into the experimental modeling approach (inverse regularization method) of catastrophe theory, enabling the development of experimental mathematical models of cusp-type catastrophe with quantitative predictive capability for specific catastrophe phenomena. Taking the chip flow angle catastrophe as an example, the detailed implementation steps of this experimental modeling approach are demonstrated. This work not only contributes to the refinement and advancement of catastrophe theory but also addresses practical challenges encountered in its application.",cusp type catastrophe commonly encountered widely applicable type among elementary catastrophes however canonical mathematical model cusp type catastrophe constructed using classical regularization method catastrophe theory often lacks quantitative prediction capability issue arises regularization operations involved regularization method particularly taylor expansion truncation potential function introduce unforeseen potentially large regularization errors address issue novel precise regularization method proposed bypasses taylor expansion potential function instead employs global diffeomorphism achieve precise regularization cusp type catastrophe mathematical model using proposed method precise regularization cusp type catastrophe mathematical model zeeman catastrophe machine achieved computational results show model obtained via proposed method significantly lower regularization errors classical method thus meeting requirements constructing catastrophe models quantitative predictive capability moreover due implementation global diffeomorphism proposed precise regularization method easily modified experimental modeling approach inverse regularization method catastrophe theory enabling development experimental mathematical models cusp type catastrophe quantitative predictive capability specific catastrophe phenomena taking chip flow angle catastrophe example detailed implementation steps experimental modeling approach demonstrated work contributes refinement advancement catastrophe theory also addresses practical challenges encountered application
"A new Chebyshev-type family of stabilized explicit methods for solving mildly stiff ODEs is presented. Besides conventional conditions of order and stability we impose an additional restriction on the methods: their stability function must be monotonically increasing and positive along the largest possible interval of negative real axis. Although stability intervals of the proposed methods are smaller than those of classic Chebyshev-type methods, their stability functions are more consistent with the exponent, they have more convex stability regions and smaller error constants. These properties allow the monotonic methods to be competitive with contemporary stabilized second-order methods, as the presented results of numerical experiments demonstrate.",new chebyshev type family stabilized explicit methods solving mildly stiff odes presented besides conventional conditions order stability impose additional restriction methods stability function must monotonically increasing positive along largest possible interval negative real axis although stability intervals proposed methods smaller classic chebyshev type methods stability functions consistent exponent convex stability regions smaller error constants properties allow monotonic methods competitive contemporary stabilized second order methods presented results numerical experiments demonstrate
"In this paper, we investigate sparse functional additive models with multivariate functional predictors and a scalar response variable. This model adopts a nonparametric additive framework to flexibly incorporate multivariate functional principal component analysis (FPCA) scores, effectively capturing complex nonlinear relationships while mitigating the curse of dimensionality. To enhance robustness against outliers and heavy-tailed errors, we propose a regularized Huber regression method incorporating the component selection and smoothing operator (COSSO) penalty. The proposed approach is formulated within a reproducing kernel Hilbert space (RKHS) framework, enabling simultaneous component selection and estimation in a robust manner. Furthermore, we extend the locally adaptive majorize-minimization (LAMM) principle to develop a general iterative optimization algorithm applicable to any loss function with continuous gradients. Under mild assumptions on the error distribution (without requiring sub-Gaussian tails) and standard regularity conditions, we establish theoretical guarantees for the proposed estimator. Extensive simulation studies and a real data application to fluorescence spectroscopy demonstrate the superior performance of our method compared to existing alternatives.",paper investigate sparse functional additive models multivariate functional predictors scalar response variable model adopts nonparametric additive framework flexibly incorporate multivariate functional principal component analysis fpca scores effectively capturing complex nonlinear relationships mitigating curse dimensionality enhance robustness outliers heavy tailed errors propose regularized huber regression method incorporating component selection smoothing operator cosso penalty proposed approach formulated within reproducing kernel hilbert space rkhs framework enabling simultaneous component selection estimation robust manner furthermore extend locally adaptive majorize minimization lamm principle develop general iterative optimization algorithm applicable loss function continuous gradients mild assumptions error distribution without requiring sub gaussian tails standard regularity conditions establish theoretical guarantees proposed estimator extensive simulation studies real data application fluorescence spectroscopy demonstrate superior performance method compared existing alternatives
"Pixel mapping is a computational approach that involves the analysis of individual pixels within digital images to extract clinically relevant information. This technique offers significant advantages in medical imaging applications, particularly in enhancing diagnostic accuracy and optimizing treatment planning. By reducing the need for complex manual programming, pixel mapping also contributes to workflow efficiency. The incorporation of machine learning strategies, such as transfer learning and other advanced algorithms, has further extended the capabilities of pixel mapping by enabling the segmentation of intricate anatomical structures and the detection of pathological anomalies in medical scans. Given the global prevalence of cardiovascular diseases (CVDs) which account for approximately 17.9 million deaths annually early diagnosis and effective clinical management remain paramount. This figure constitutes approximately 32% of all global deaths, with 85% of the deaths being because of heart attacks and strokes. In addition, 38% of deaths due to cardiovascular diseases occur at an early age in individuals under 70. Among these conditions, heart failure represents a major complication, underscoring the need for innovative diagnostic technologies like pixel-based analysis. This paper proposes an innovative hybrid model, which is Pixel Mapping Multi-Modal Medical Image -based Neural Networks (PMMI- NNs), by combining transfer and machine learning for diagnosing cardiovascular disease using pixel map images. Heart failure is also a common consequence of these diseases, which makes early diagnosis and management critical. This paper proposes an innovative hybrid model, which is Pixel Mapping Multi-Modal Medical Image -based Neural Networks (PMMI- NNs), by combining transfer and machine learning for diagnosing cardiovascular disease using pixel map images. Six different pre-trained convolutional neural network (CNN) models extract the significant features from the images: DenseNet121, VGG16, VGG19, InceptionV3, InceptionResNetV2 and MobileNetV2. The Improved Salp Swarm Algorithm is applied to improve classification accuracy and select the most discriminative features. The selected features are then classified by three machine learning algorithms, which are the k-nearest Neighbor, Random Forest and Support Vector Machine. The findings obtained by the analyses demonstrate that integration of transfer learning, feature selection, and machine learning proves to be effective in improving CVD diagnosis in medical image analysis, providing a scalable and applicable framework for future clinical applications. The primary objective of the study whose datasets include clinical and physiological signal elements is to compare different machine learning and deep learning approaches to identify the most effective methods for early diagnostic process of cardiovascular diseases.",pixel mapping computational approach involves analysis individual pixels within digital images extract clinically relevant information technique offers significant advantages medical imaging applications particularly enhancing diagnostic accuracy optimizing treatment planning reducing need complex manual programming pixel mapping also contributes workflow efficiency incorporation machine learning strategies transfer learning advanced algorithms extended capabilities pixel mapping enabling segmentation intricate anatomical structures detection pathological anomalies medical scans given global prevalence cardiovascular diseases cvds account approximately million deaths annually early diagnosis effective clinical management remain paramount figure constitutes approximately global deaths deaths heart attacks strokes addition deaths due cardiovascular diseases occur early age individuals among conditions heart failure represents major complication underscoring need innovative diagnostic technologies like pixel based analysis paper proposes innovative hybrid model pixel mapping multi modal medical image based neural networks pmmi nns combining transfer machine learning diagnosing cardiovascular disease using pixel map images heart failure also common consequence diseases makes early diagnosis management critical paper proposes innovative hybrid model pixel mapping multi modal medical image based neural networks pmmi nns combining transfer machine learning diagnosing cardiovascular disease using pixel map images six different pre trained convolutional neural network cnn models extract significant features images densenet vgg vgg inceptionv inceptionresnetv mobilenetv improved salp swarm algorithm applied improve classification accuracy select discriminative features selected features classified three machine learning algorithms nearest neighbor random forest support vector machine findings obtained analyses demonstrate integration transfer learning feature selection machine learning proves effective improving cvd diagnosis medical image analysis providing scalable applicable framework future clinical applications primary objective study whose datasets include clinical physiological signal elements compare different machine learning deep learning approaches identify effective methods early diagnostic process cardiovascular diseases
"This paper focuses on analyzing the error of the randomized Euler algorithm when only noisy information about the coefficients of the underlying stochastic differential equation (SDE) and the driving Wiener process is available. Two classes of disturbed Wiener process are considered, and the dependence of the algorithm’s error on the regularity of the disturbing functions is investigated. The paper also presents results from numerical experiments to support the theoretical findings.",paper focuses analyzing error randomized euler algorithm noisy information coefficients underlying stochastic differential equation sde driving wiener process available two classes disturbed wiener process considered dependence algorithm error regularity disturbing functions investigated paper also presents results numerical experiments support theoretical findings
"We present a novel class of methods to compute functions of matrices or their action on vectors that are suitable for parallel programming and can improve the performance of exponential integrators. Solving appropriate linear systems of equations in parallel (or computing the inverse of several matrices) and with a proper linear combination of the results, allows us to obtain new high order approximations to the desired functions of matrices. An error analysis to obtain forward and backward error bounds is presented. The coefficients of each method, which depends on the number of processors, can be adjusted to improve the accuracy, the stability or to reduce the round off errors of the methods. We illustrate this procedure by explicitly constructing some methods which are then tested on several numerical examples.",present novel class methods compute functions matrices action vectors suitable parallel programming improve performance exponential integrators solving appropriate linear systems equations parallel computing inverse several matrices proper linear combination results allows obtain new high order approximations desired functions matrices error analysis obtain forward backward error bounds presented coefficients method depends number processors adjusted improve accuracy stability reduce round errors methods illustrate procedure explicitly constructing methods tested several numerical examples
"Despite the wide applications of nonlinear functional mixed Volterra-Fredholm equations (VFIEs), not much attention has been paid to their numerical analysis. Further, a well-known challenge in solving nonlinear integral equations is the problem of simultaneously achieving high-order accuracy, computational efficiency and avoiding to solve nonlinear algebraic systems. For contraction maps in Banach spaces, fixed-point iterative methods can address the problem of solving systems. However, the issues of computational efficiency, high-order accuracy, and approximation of functional VFIEs remain largely unaddressed. In this article, a new cubature rule is proposed and used to develop a high (fourth) order method for nonlinear functional mixed VFIEs. To ensure computational efficiency and avoid solving systems, a Gauss–Seidel-type algorithm (GSTA) is formulated. In this case of GSTA, the convergence proof becomes quite challenging (no wonder the inefficient Jacobi-type idea is very popular in the literature). We use the Banach contraction principle and mathematical induction to rigorously prove the fourth-order convergence of the method. Several numerical examples are used to verify the theoretical convergence results. It is our belief that both the numerical scheme and convergence proof presented in this paper will serve researchers in devising and analyzing efficient, high-order schemes for other integral equations, even in higher dimensions.",despite wide applications nonlinear functional mixed volterra fredholm equations vfies much attention paid numerical analysis well known challenge solving nonlinear integral equations problem simultaneously achieving high order accuracy computational efficiency avoiding solve nonlinear algebraic systems contraction maps banach spaces fixed point iterative methods address problem solving systems however issues computational efficiency high order accuracy approximation functional vfies remain largely unaddressed article new cubature rule proposed used develop high fourth order method nonlinear functional mixed vfies ensure computational efficiency avoid solving systems gauss seidel type algorithm gsta formulated case gsta convergence proof becomes quite challenging wonder inefficient jacobi type idea popular literature use banach contraction principle mathematical induction rigorously prove fourth order convergence method several numerical examples used verify theoretical convergence results belief numerical scheme convergence proof presented paper serve researchers devising analyzing efficient high order schemes integral equations even higher dimensions
"The HLLEM scheme is a popular numerical method for computing the convective fluxes in the Euler equations and the Navier–Stokes equations due to its accuracy and positivity-preserving. However, it still has two defects. One is the numerical instability, such as the carbuncle phenomenon, in computations involving multidimensional strong shock waves; the other is the non-physical results caused by accuracy issues in computations of low Mach number flows. In the engineering field, high Mach number flows with strong shock waves and low Mach number flows often coexist. Therefore, it is necessary to develop a numerical scheme that is both accurate and robust across all Mach number flows. The shock instabilities of the HLLEM scheme are addressed by simply modifying the wave speeds to balance advection dissipation and acoustic dissipation. Additionally, the performance in computing low-speed flows is improved by controlling the excessive numerical dissipation corresponding to the velocity-difference terms in the momentum equations. Numerical results from high Mach number test cases and low Mach number test cases demonstrate the accuracy and robustness of the proposed scheme for simulating flows across all Mach number.",hllem scheme popular numerical method computing convective fluxes euler equations navier stokes equations due accuracy positivity preserving however still two defects one numerical instability carbuncle phenomenon computations involving multidimensional strong shock waves non physical results caused accuracy issues computations low mach number flows engineering field high mach number flows strong shock waves low mach number flows often coexist therefore necessary develop numerical scheme accurate robust across mach number flows shock instabilities hllem scheme addressed simply modifying wave speeds balance advection dissipation acoustic dissipation additionally performance computing low speed flows improved controlling excessive numerical dissipation corresponding velocity difference terms momentum equations numerical results high mach number test cases low mach number test cases demonstrate accuracy robustness proposed scheme simulating flows across mach number
"In this paper, minimizing the sum of an average of finite proper closed nonconvex functions and a proper lower semicontinuous convex function over a closed convex set, is considered. We propose the general inertial proximal stochastic mirror descent (IPSMD for short) algorithm framework, which not only introduces the more general inertial technique and the variance reduced gradient estimator, but also circumvents the restrictive condition of Lipschitz smoothness by using Legendre function. In theory, we establish that the sequence generated by IPSMD algorithm globally converges to the critical point, under the condition that the objective function is semialgebraic. Besides the theoretical improvement in the convergence analysis, there are also possible computational advantages which provide an interesting option for practical problems.",paper minimizing sum average finite proper closed nonconvex functions proper lower semicontinuous convex function closed convex set considered propose general inertial proximal stochastic mirror descent ipsmd short algorithm framework introduces general inertial technique variance reduced gradient estimator also circumvents restrictive condition lipschitz smoothness using legendre function theory establish sequence generated ipsmd algorithm globally converges critical point condition objective function semialgebraic besides theoretical improvement convergence analysis also possible computational advantages provide interesting option practical problems
"Based on the supply chain management rules, a new 4D conformable fractional-order supply chain finance system is discussed. The numerical solution of the proposed system is obtained by adopting the conformable Adomian decomposition method (CADM). Some basic dynamical characteristics are employed either numerically or analytically to demonstrate the chaotic dynamical behaviors of the new system, including equilibrium points and their stability, Lyapunov exponents spectrum, fractal dimension, bifurcation diagrams, and complexity analysis. The fractional calculus theory and computer simulations reveal that the system’s lowest order to yield chaos is 0.461. Bifurcation diagrams, phase plots, and a multiscale spectral entropy (MSE) complexity analysis validate the results. Additionally, chaos synchronization of the novel conformable fractional-order chaotic system is achieved by designing a suitable nonlinear controller, based on the stability theory of fractional-order dynamical systems. Furthermore, this paper constructs a new scheme for encrypting color images based on chaos synchronization to enhance practicality. Experiments and computer simulations are conducted to verify the performance and security of the proposed image cryptosystem.",based supply chain management rules new conformable fractional order supply chain finance system discussed numerical solution proposed system obtained adopting conformable adomian decomposition method cadm basic dynamical characteristics employed either numerically analytically demonstrate chaotic dynamical behaviors new system including equilibrium points stability lyapunov exponents spectrum fractal dimension bifurcation diagrams complexity analysis fractional calculus theory computer simulations reveal system lowest order yield chaos bifurcation diagrams phase plots multiscale spectral entropy mse complexity analysis validate results additionally chaos synchronization novel conformable fractional order chaotic system achieved designing suitable nonlinear controller based stability theory fractional order dynamical systems furthermore paper constructs new scheme encrypting color images based chaos synchronization enhance practicality experiments computer simulations conducted verify performance security proposed image cryptosystem
"This paper studies the problem of estimating unknown parameters involved in a system which is equipped with a protection block. The system has different failure rates depending on whether the protection block is present or not, as the protection block is modeled by its own lifetime distribution and contributes an additional failure component to the system. The model is analyzed under the assumption of exponentially distributed lifetimes, leading to the study of its distributional properties and the estimation problem for its unknown parameters. Closed-form expressions for the maximum likelihood estimators are obtained. Furthermore, theoretical expectations and variances of the estimators are derived. We also discuss the stress–strength reliability estimation problem and construct confidence intervals for the associated reliability measure. Numerical results are provided to demonstrate the implementation of the proposed methods.",paper studies problem estimating unknown parameters involved system equipped protection block system different failure rates depending whether protection block present protection block modeled lifetime distribution contributes additional failure component system model analyzed assumption exponentially distributed lifetimes leading study distributional properties estimation problem unknown parameters closed form expressions maximum likelihood estimators obtained furthermore theoretical expectations variances estimators derived also discuss stress strength reliability estimation problem construct confidence intervals associated reliability measure numerical results provided demonstrate implementation proposed methods
"This paper focuses on a perfectly matched layer (PML) model developed by Bécache et al. [6] for Drude metamaterials. Although this PML model performs well in practice — exhibiting stable behavior and effectively absorbing outgoing waves — its stability has not yet been rigorously proven for general damping coefficients. To address this gap, we derive an equivalent PML model from the original formulation and establish its stability under general damping functions. We then develop a finite element scheme to solve the equivalent PML model and provide proofs for both its discrete stability and optimal error estimates. Finally, we present numerical results to support our theoretical analysis and to demonstrate the effectiveness of the equivalent PML model in absorbing outgoing waves.",paper focuses perfectly matched layer pml model developed bécache drude metamaterials although pml model performs well practice exhibiting stable behavior effectively absorbing outgoing waves stability yet rigorously proven general damping coefficients address gap derive equivalent pml model original formulation establish stability general damping functions develop finite element scheme solve equivalent pml model provide proofs discrete stability optimal error estimates finally present numerical results support theoretical analysis demonstrate effectiveness equivalent pml model absorbing outgoing waves
"We propose a unified framework that effectively characterizes challenging phenomena such as anomalous transport in heterogeneous media and long-range memory effects and interactions. This framework transports agent densities from a prescribed initial distribution to a terminal distribution while minimizing the associated energy cost. Motivated by optimal transport theory, we introduce a nonlocal dispersive optimal transport (NDOT) model governed by a space–time fractional partial differential equation (PDE). We solve the NDOT formulation using the general-proximal primal–dual hybrid gradient (G-prox PDHG) algorithm, and then introduce a novel preconditioner derived from the discretization of the space–time fractional PDE to accelerate the convergence. Numerical experiments – especially those with target states represented by power functions typical of fractional differential equation solutions – show that our model substantially reduces kinetic energy costs compared with its integer-order counterparts, highlighting its effectiveness and applicability for complex phenomena such as anomalous transport in heterogeneous environments.",propose unified framework effectively characterizes challenging phenomena anomalous transport heterogeneous media long range memory effects interactions framework transports agent densities prescribed initial distribution terminal distribution minimizing associated energy cost motivated optimal transport theory introduce nonlocal dispersive optimal transport ndot model governed space time fractional partial differential equation pde solve ndot formulation using general proximal primal dual hybrid gradient prox pdhg algorithm introduce novel preconditioner derived discretization space time fractional pde accelerate convergence numerical experiments especially target states represented power functions typical fractional differential equation solutions show model substantially reduces kinetic energy costs compared integer order counterparts highlighting effectiveness applicability complex phenomena anomalous transport heterogeneous environments
"This research work seeks to construct a model for commodity spot prices by incorporating the concept of stochastic convenience yield within a Markov-switching framework. The model presented in this paper applies the Gibson-Schwartz commodity model under the risk-neutral measure, enabling regime-switching in the convenience yield and spot price dynamics. Using the WTI crude oil spot prices, the parameters involved in the proposed commodity regime-switching model are estimated by expectation–maximization algorithm. We then carry out a semi-analytical formula for the commodity futures contracts and European option price written on them. We calibrate the option pricing model parameters using the flower pollination optimization algorithm based on the European call option prices in WTI crude oil market. The results show that the provided Markov-switching model, whose parameters are calibrated by the flower pollination optimization algorithm is superior to the some common models in commodity literature.",research work seeks construct model commodity spot prices incorporating concept stochastic convenience yield within markov switching framework model presented paper applies gibson schwartz commodity model risk neutral measure enabling regime switching convenience yield spot price dynamics using wti crude oil spot prices parameters involved proposed commodity regime switching model estimated expectation maximization algorithm carry semi analytical formula commodity futures contracts european option price written calibrate option pricing model parameters using flower pollination optimization algorithm based european call option prices wti crude oil market results show provided markov switching model whose parameters calibrated flower pollination optimization algorithm superior common models commodity literature
"We investigate stochastic gradient methods and stochastic counterparts of the Barzilai–Borwein steplengths and their application to finite-sum minimization problems. Our proposal is based on the Trust-Region-ish (TRish) framework introduced in [Curtis et al. (2019)]. The new framework, named TRishBB, aims to enhance the performance of TRish and at reducing the computational cost of the second-order TRish variant. We propose three different methods belonging to the TRishBB framework and present the convergence analysis for possibly nonconvex objective functions, considering biased and unbiased gradient approximations. Our analysis requires neither diminishing step-sizes nor full gradient evaluation. The numerical experiments in machine learning applications demonstrate the effectiveness of applying the Barzilai–Borwein steplength with stochastic gradients and show improved testing accuracy compared to the TRish method.",investigate stochastic gradient methods stochastic counterparts barzilai borwein steplengths application finite sum minimization problems proposal based trust region ish trish framework introduced curtis new framework named trishbb aims enhance performance trish reducing computational cost second order trish variant propose three different methods belonging trishbb framework present convergence analysis possibly nonconvex objective functions considering biased unbiased gradient approximations analysis requires neither diminishing step sizes full gradient evaluation numerical experiments machine learning applications demonstrate effectiveness applying barzilai borwein steplength stochastic gradients show improved testing accuracy compared trish method
"Recently, Wang and Li (2019) studied a new extended shift-splitting (NESS) iteration method for solving nonsingular saddle point problems. In this paper we investigate the singular NESS (SNESS) preconditioner for solving singular saddle point problems and discuss three SNESS iterations. With the SNESS preconditioner, the splitting of the corresponding coefficient matrix is a proper splitting, which help the three SNESS iterations to converge to the generalized inverse solution. Numerical results demonstrate the effectiveness of the SNESS iterations for solving singular saddle point problems.",recently wang studied new extended shift splitting ness iteration method solving nonsingular saddle point problems paper investigate singular ness sness preconditioner solving singular saddle point problems discuss three sness iterations sness preconditioner splitting corresponding coefficient matrix proper splitting help three sness iterations converge generalized inverse solution numerical results demonstrate effectiveness sness iterations solving singular saddle point problems
"Recently, the stability of most explicit high-order methods has been established using the von Neumann method or eigenvalue analysis, rather than through energy equations or energy inequalities. In this paper, we propose two fourth-order schemes that combine Störmer–Verlet (S–V) method with Richardson extrapolation method and three-stage method for solving Maxwell’s equations, where the second-order S–V method serves as an equivalent formulation of the leapfrog method. We provide proofs for the energy equations and inequalities of our two fourth-order schemes. Additionally, we conduct a theoretical analysis and numerical experiments to compare the efficiency, stability, and accuracy of these three schemes. Furthermore, we discuss the necessity of the S–V method in the context of this research.",recently stability explicit high order methods established using von neumann method eigenvalue analysis rather energy equations energy inequalities paper propose two fourth order schemes combine rmer verlet method richardson extrapolation method three stage method solving maxwell equations second order method serves equivalent formulation leapfrog method provide proofs energy equations inequalities two fourth order schemes additionally conduct theoretical analysis numerical experiments compare efficiency stability accuracy three schemes furthermore discuss necessity method context research
This work is devoted to an approach for the quality improvement of numerical solving of initial-boundary problems for one-dimensional shallow water equations on an adaptive mesh. A Godunov type numerical scheme for the problem on moving mesh is applied. The grid motion law is based on the solution of the local Riemann discontinuity breakdown problem and forces the computational nodes to follow the wave of the greatest amplitude. The regularization mechanism prevents the degeneration of moving computational mesh and gives an estimate for the mesh compression. The proposed algorithm is tested on several typical Riemann problems with known exact solutions. The numerical experiments show that the proposed approach allows to improve the quality of the obtained numerical solution.,work devoted approach quality improvement numerical solving initial boundary problems one dimensional shallow water equations adaptive mesh godunov type numerical scheme problem moving mesh applied grid motion law based solution local riemann discontinuity breakdown problem forces computational nodes follow wave greatest amplitude regularization mechanism prevents degeneration moving computational mesh gives estimate mesh compression proposed algorithm tested several typical riemann problems known exact solutions numerical experiments show proposed approach allows improve quality obtained numerical solution
"This paper is concerned with a backward problem of a stochastic space-fractional diffusion equation. The source term is driven by fractional Brownian motion. The well-posedness of the forward problem is studied at first. The backward problem is ill-posed, i.e., the solution of this problem does not depend continuously on the data. The instability is discussed in the sense of expectation and variance. A truncated regularization method is used to solve the backward problem. Under the a priori and the a posteriori regularization parameter choice rules, the error estimates between the regularization solution and the exact solution are obtained, respectively. Different numerical examples are presented to illustrate the validity and effectiveness of our method.",paper concerned backward problem stochastic space fractional diffusion equation source term driven fractional brownian motion well posedness forward problem studied first backward problem ill posed solution problem depend continuously data instability discussed sense expectation variance truncated regularization method used solve backward problem priori posteriori regularization parameter choice rules error estimates regularization solution exact solution obtained respectively different numerical examples presented illustrate validity effectiveness method
"We describe conditions for preserving relations with given distributions in the transform orders: convex transform, star, and superadditive ones of the marginal distributions of identically distributed component lifetimes by semicoherent system lifetimes. If the distinguished extremal element of the family has a left bounded support, then the conditions are necessary and sufficient. In particular, we specify the conditions for preserving the monotone generalized failure rates and their reversed versions, generalized monotone failure rates on the average, generalized new better and worse than used, and decreasing log-odds rate properties. The conditions depend on the minimal cut sets of the system and the copula of dependence among the component lifetimes. An alternative conditions are based on the respective minimal path sets and survival copula.",describe conditions preserving relations given distributions transform orders convex transform star superadditive ones marginal distributions identically distributed component lifetimes semicoherent system lifetimes distinguished extremal element family left bounded support conditions necessary sufficient particular specify conditions preserving monotone generalized failure rates reversed versions generalized monotone failure rates average generalized new better worse used decreasing log odds rate properties conditions depend minimal cut sets system copula dependence among component lifetimes alternative conditions based respective minimal path sets survival copula
"In actuarial practice, the usual independence assumptions for the collective risk model are often violated, which implies a growing need for considering more general models that incorporate dependence. To this purpose, the present paper studies the mixed counterpart of the classical Panjer family of claim number distributions and their compound version, by allowing the parameters of the distributions to be viewed as random variables. Under the assumptions that the claim size process is conditionally i.i.d. and (conditionally) independent of the claim counts, we provide a recursive formula for the computation of the probability mass function of the aggregate claim sizes. The case of a compound Panjer distribution with exchangeable claim sizes is also studied. Numerical examples are also provided to highlight the applicability of this work.",actuarial practice usual independence assumptions collective risk model often violated implies growing need considering general models incorporate dependence purpose present paper studies mixed counterpart classical panjer family claim number distributions compound version allowing parameters distributions viewed random variables assumptions claim size process conditionally conditionally independent claim counts provide recursive formula computation probability mass function aggregate claim sizes case compound panjer distribution exchangeable claim sizes also studied numerical examples also provided highlight applicability work
"Inquiry-based learning (IBL) has gained popularity globally and has become a focus of curriculum reform initiatives in Confucian-heritage contexts. In Taiwan, the 2019 curriculum reform has positioned inquiry as an interdisciplinary core competency for all preservice teachers. However, previous studies have indicated that preservice teachers face significant challenges during the planning stages of inquiry and have emphasized the need for explicit instruction in teacher education programs. Given that empirical studies addressing these gaps remain limited, the present study employed a case study approach to examine the effectiveness of an inquiry thinking scaffold in a 16-week foundational course in a teacher education program in Taiwan. Analyses of 16 participants’ performance and written reflections revealed that developing a coherent inquiry plan involved a steep learning curve, with challenges emerging in the early stages and evolving as learning progressed. The major challenges included achieving logical coherence and identifying research gaps. The findings support the effectiveness of the thinking scaffold and highlight the importance of fostering preservice teachers’ awareness of logical coherence, as well as engaging them in the process of knowledge construction. Although the effects were limited, this study also found that IBL experience could enhance preservice teachers’ self-efficacy for teaching inquiry. Implications for instructional and curricular adaptations to better integrate inquiry into teacher education programs are discussed.",inquiry based learning ibl gained popularity globally become focus curriculum reform initiatives confucian heritage contexts taiwan curriculum reform positioned inquiry interdisciplinary core competency preservice teachers however previous studies indicated preservice teachers face significant challenges planning stages inquiry emphasized need explicit instruction teacher education programs given empirical studies addressing gaps remain limited present study employed case study approach examine effectiveness inquiry thinking scaffold week foundational course teacher education program taiwan analyses participants performance written reflections revealed developing coherent inquiry plan involved steep learning curve challenges emerging early stages evolving learning progressed major challenges included achieving logical coherence identifying research gaps findings support effectiveness thinking scaffold highlight importance fostering preservice teachers awareness logical coherence well engaging process knowledge construction although effects limited study also found ibl experience could enhance preservice teachers self efficacy teaching inquiry implications instructional curricular adaptations better integrate inquiry teacher education programs discussed
"Creativity is a multifaceted construct that manifests itself in a variety of modalities. While previous research suggests that creativity is a diversified skill in school-aged children and adults, little is known about preschoolers’ creativity. This study aims to identify the structure of creativity in a sample of preschoolers (N = 83, Mage 4.40 years). We examined associations between three performance-based creativity measures and one parent-report measure of creativity. Findings from bivariate analysis and confirmatory factor analysis (CFA) revealed substantive associations within and non-substantive associations between creativity tasks and parent-report items. Results provide preliminary evidence of diversity in performance-based assessments and unity in parent-report measures of preschool creativity. Results also showed inconsistent associations between performance-based and parent-report measures of creativity in preschool, suggesting measures are capturing different facets of preschoolers' creativity. These results shed light on the multidimensional structure of creativity, the validity of behavioural measures of creativity, and contribute to the conceptualization and assessment of creativity in early childhood.",creativity multifaceted construct manifests variety modalities previous research suggests creativity diversified skill school aged children adults little known preschoolers creativity study aims identify structure creativity sample preschoolers mage years examined associations three performance based creativity measures one parent report measure creativity findings bivariate analysis confirmatory factor analysis cfa revealed substantive associations within non substantive associations creativity tasks parent report items results provide preliminary evidence diversity performance based assessments unity parent report measures preschool creativity results also showed inconsistent associations performance based parent report measures creativity preschool suggesting measures capturing different facets preschoolers creativity results shed light multidimensional structure creativity validity behavioural measures creativity contribute conceptualization assessment creativity early childhood
"Discrete responses are frequently encountered in applications, particularly in classification problems. However, the high cost of collecting responses or labels often leads to a scarcity of samples, which significantly diminishes the accuracy of statistical inferences, particularly in high-dimensional settings. To address this limitation, transfer learning can be utilized for high-dimensional data with discrete responses by incorporating relevant source data into the target study of interest. Within the framework of generalized linear models, the case where responses are bounded are first considered, and an importance-weighted transfer learning method, referred to as IWTL-DR, is proposed. This method selects data at the individual level, thereby utilizing the source data more efficiently. Subsequently, this approach is extended to scenarios involving unbounded responses. Theoretical properties of the IWTL-DR method are established and compared with existing techniques. Extensive simulations and analyses of real data show the advantages of our approach.",discrete responses frequently encountered applications particularly classification problems however high cost collecting responses labels often leads scarcity samples significantly diminishes accuracy statistical inferences particularly high dimensional settings address limitation transfer learning utilized high dimensional data discrete responses incorporating relevant source data target study interest within framework generalized linear models case responses bounded first considered importance weighted transfer learning method referred iwtl proposed method selects data individual level thereby utilizing source data efficiently subsequently approach extended scenarios involving unbounded responses theoretical properties iwtl method established compared existing techniques extensive simulations analyses real data show advantages approach
"In this work, we consider ozone infiltration process into bone tissue. The mathematical model is described by a coupled system of flow and transport equations. Due to the multiscale nature of the bone porous structure, numerical simulation of the infiltration process presents computational challenges. Therefore, we propose a multicontinuum modeling approach based on the multicontinuum homogenization method. We formulate constraint cell problems in oversampled regions for flow and transport equations. By solving the cell problems, we obtain multicontinuum expansions and derive the corresponding multicontinuum equations. To check the obtained multicontinuum model, we conduct numerical experiments in different multiscale porous structures. The numerical results demonstrate the high accuracy of our multicontinuum approach.",work consider ozone infiltration process bone tissue mathematical model described coupled system flow transport equations due multiscale nature bone porous structure numerical simulation infiltration process presents computational challenges therefore propose multicontinuum modeling approach based multicontinuum homogenization method formulate constraint cell problems oversampled regions flow transport equations solving cell problems obtain multicontinuum expansions derive corresponding multicontinuum equations check obtained multicontinuum model conduct numerical experiments different multiscale porous structures numerical results demonstrate high accuracy multicontinuum approach
"An insurance premium is defined as the product of two elements: the a priori rate, which depends on the policyholder’s observable risk characteristics at the time of contract, and the a posteriori rate, which encompasses the residual component not explained by the a priori information. This paper explores the mathematical structure of the a posteriori rate and its corresponding statistical properties. As in the cases of the Bayes premium and the credibility premium, the a posteriori rate depends on both the a priori rate and the claim history in general. However, in certain cases, such as the bonus-malus premium in auto insurance, the a posteriori rate depends solely on the claim history. We refer to such insurance premiums, where the a posteriori rate is solely a function of the claim history, as commercial insurance premiums. Although the simplified structure of commercial insurance premiums enhances communication with policyholders, it can introduce a systematic bias known as the double counting problem. The insurance literature has empirically identified this bias in bonus-malus systems. Our study extends the existing empirical analysis of bonus-malus systems to a wider range of commercial insurance premiums and provides a rigorous mathematical framework demonstrating that any commercial insurance premium is susceptible to the double counting problem. Then, we propose a simple solution to mitigate this issue while retaining the structural simplicity of the commercial insurance premium. Numerical analysis, including real data analysis, demonstrates the extent of the double counting problem in commercial insurance premiums and the effectiveness of the proposed method in mitigating this issue.",insurance premium defined product two elements priori rate depends policyholder observable risk characteristics time contract posteriori rate encompasses residual component explained priori information paper explores mathematical structure posteriori rate corresponding statistical properties cases bayes premium credibility premium posteriori rate depends priori rate claim history general however certain cases bonus malus premium auto insurance posteriori rate depends solely claim history refer insurance premiums posteriori rate solely function claim history commercial insurance premiums although simplified structure commercial insurance premiums enhances communication policyholders introduce systematic bias known double counting problem insurance literature empirically identified bias bonus malus systems study extends existing empirical analysis bonus malus systems wider range commercial insurance premiums provides rigorous mathematical framework demonstrating commercial insurance premium susceptible double counting problem propose simple solution mitigate issue retaining structural simplicity commercial insurance premium numerical analysis including real data analysis demonstrates extent double counting problem commercial insurance premiums effectiveness proposed method mitigating issue
"This paper develops two discretization schemes for a class of two-dimensional magnetic diffusion equations with step-function resistivity. Comparative numerical experiments reveal that the linear finite element discretization scheme (based on triangular mesh discretization) fails to maintain the stability of numerical solutions, exhibiting a “comb-like” phenomenon. In contrast, the bilinear finite element discretization scheme (based on quadrilateral mesh discretization) demonstrates superior stability preservation. This study rigorously identifies the fundamental reasons behind the bilinear scheme’s stability through theoretical analysis and further investigates the root cause of instability in the linear scheme observed in experiments. Based on these insights, two improvement strategies are proposed: one completely eliminates the instability, while the other significantly enhances numerical stability.",paper develops two discretization schemes class two dimensional magnetic diffusion equations step function resistivity comparative numerical experiments reveal linear finite element discretization scheme based triangular mesh discretization fails maintain stability numerical solutions exhibiting comb like phenomenon contrast bilinear finite element discretization scheme based quadrilateral mesh discretization demonstrates superior stability preservation study rigorously identifies fundamental reasons behind bilinear scheme stability theoretical analysis investigates root cause instability linear scheme observed experiments based insights two improvement strategies proposed one completely eliminates instability significantly enhances numerical stability
N/D,
"Robust principal component analysis (RPCA), which aims to decompose observational data into low-rank and sparse matrices, has been widely used in various machine learning and low-vision tasks. The traditional methods use nuclear norms to approximate the rank function, which makes the obtained solution seriously deviate from the real one. Therefore, we propose a new RPCA model by employing a recently proposed truncated reweighting nuclear norm as a better approximation of low-rank constraint. The proposed model could not only preserve the main information of data as much as possible, but also greatly enhance the robustness of the model against noise or outliers. To solve this model, an efficient and simple optimization algorithm is designed by exploiting the framework of the alternating direction method of multipliers (ADMM). More importantly, the convergence of the proposed algorithm is discussed in detail and rigorously mathematically proved. Experimental results on two typical low-vision tasks demonstrate the effectiveness and superiority of the proposed algorithm, that is, the proposed algorithm outperforms state-of-the-art algorithms in terms of visual and quantitative effects.",robust principal component analysis rpca aims decompose observational data low rank sparse matrices widely used various machine learning low vision tasks traditional methods use nuclear norms approximate rank function makes obtained solution seriously deviate real one therefore propose new rpca model employing recently proposed truncated reweighting nuclear norm better approximation low rank constraint proposed model could preserve main information data much possible also greatly enhance robustness model noise outliers solve model efficient simple optimization algorithm designed exploiting framework alternating direction method multipliers admm importantly convergence proposed algorithm discussed detail rigorously mathematically proved experimental results two typical low vision tasks demonstrate effectiveness superiority proposed algorithm proposed algorithm outperforms state art algorithms terms visual quantitative effects
"This research explores neural network-based numerical approximation of two-dimensional convection-dominated singularly perturbed problems on square, circular, and elliptic domains. Singularly perturbed boundary value problems pose significant challenges due to sharp boundary layers in their solutions. Additionally, the characteristic points of these domains give rise to degenerate boundary layer problems. The stiffness of these problems, caused by sharp singular layers, can lead to substantial computational errors if not properly addressed. Conventional neural network-based approaches often fail to capture these sharp transitions accurately, highlighting a critical flaw in machine learning methods. To address these issues, we conduct a thorough boundary layer analysis to enhance our understanding of sharp transitions within the boundary layers, guiding the application of numerical methods. Specifically, we employ physics-informed neural networks (PINNs) to better handle these boundary layer problems. However, PINNs may struggle with rapidly varying singularly perturbed solutions in small domain regions, leading to inaccurate or unstable results. To overcome this limitation, we introduce a semi-analytic method that augments PINNs with singular layers or corrector functions. Our numerical experiments demonstrate significant improvements in both accuracy and stability, showcasing the effectiveness of our proposed approach.",research explores neural network based numerical approximation two dimensional convection dominated singularly perturbed problems square circular elliptic domains singularly perturbed boundary value problems pose significant challenges due sharp boundary layers solutions additionally characteristic points domains give rise degenerate boundary layer problems stiffness problems caused sharp singular layers lead substantial computational errors properly addressed conventional neural network based approaches often fail capture sharp transitions accurately highlighting critical flaw machine learning methods address issues conduct thorough boundary layer analysis enhance understanding sharp transitions within boundary layers guiding application numerical methods specifically employ physics informed neural networks pinns better handle boundary layer problems however pinns may struggle rapidly varying singularly perturbed solutions small domain regions leading inaccurate unstable results overcome limitation introduce semi analytic method augments pinns singular layers corrector functions numerical experiments demonstrate significant improvements accuracy stability showcasing effectiveness proposed approach
"Refractory medium entropy alloys (RMEAs) have attracted significant attention in recent years due to their exceptional mechanical properties and high-temperature stability, making them suitable for a number of advanced applications. While computational modelling such as density functional theory (DFT) and molecular dynamics (MD) are powerful for investigating RMEAs, these traditional methods are often constrained by high computational cost and limited accuracy. In this work, a deep neural network potential (DNNP) was developed to address the complex compositional nature of FeCr2V-based RMEAs with varying levels of tungsten doping. The DNNP demonstrated high accuracy, comparable to that of DFT calculations. Utilizing the DNNP, high-accuracy MD simulations were conducted to examine large-scale effects, including short-range ordering (SRO), twining, and dislocation behaviour, on the mechanical properties of the RMEAs. The results indicate that in the SRO structure, the covalency of V-V, Cr-Cr, and V-W ordered atomic pairs enhances local bonding strength and increases the elastic modulus of the RMEA. As the simulation temperature increases, dislocation mobility improves while dislocation density decreases, thereby enhancing the ductility of the material. Above 823 K, the SRO structure demonstrates superior mechanical performance, which is attributed to the increased length of 1/2 < 111>, dislocations facilitated by the formation of Cr-Fe and Cr-Cr ordered twins. This work underscores the potential of DNNP and MD simulations in predicting and analyzing the mechanical properties of RMEAs, advancing their development for various applications.",refractory medium entropy alloys rmeas attracted significant attention recent years due exceptional mechanical properties high temperature stability making suitable number advanced applications computational modelling density functional theory dft molecular dynamics powerful investigating rmeas traditional methods often constrained high computational cost limited accuracy work deep neural network potential dnnp developed address complex compositional nature fecr based rmeas varying levels tungsten doping dnnp demonstrated high accuracy comparable dft calculations utilizing dnnp high accuracy simulations conducted examine large scale effects including short range ordering sro twining dislocation behaviour mechanical properties rmeas results indicate sro structure covalency ordered atomic pairs enhances local bonding strength increases elastic modulus rmea simulation temperature increases dislocation mobility improves dislocation density decreases thereby enhancing ductility material sro structure demonstrates superior mechanical performance attributed increased length dislocations facilitated formation ordered twins work underscores potential dnnp simulations predicting analyzing mechanical properties rmeas advancing development various applications
"In this paper, a projected infeasible algorithm is proposed to compute the positive ground states of the nonlinear Schrödinger (NLS) equation, which can be regarded as an energy minimization problem with an orthogonal constraint. To further preserve the positivity of the ground states which is a necessary condition in some physical systems such as the non-rotating Bose–Einstein condensates, the saturable equation and the modified Gross–Pitaevskii equation, the projection is added to the infeasible method. The local Q-linearly convergence analysis of algorithm is established for the appropriate parameter values. Numerical experiments about computing the ground states of different types of the NLS equations illustrate that the proposed algorithm is efficient and faster than other feasible algorithms in dealing with large-scale problems.",paper projected infeasible algorithm proposed compute positive ground states nonlinear schr dinger nls equation regarded energy minimization problem orthogonal constraint preserve positivity ground states necessary condition physical systems non rotating bose einstein condensates saturable equation modified gross pitaevskii equation projection added infeasible method local linearly convergence analysis algorithm established appropriate parameter values numerical experiments computing ground states different types nls equations illustrate proposed algorithm efficient faster feasible algorithms dealing large scale problems
"Although some researchers have suggested that video games such as Minecraft have the potential to foster student creativity, little is known regarding how educational games should be designed to effectively promote creativity. To address this gap, this study aimed to develop an educational game by modifying Minecraft and evaluate the effects of two game design features, challenge and autonomy, on the in-game creative performance of middle school students. A creativity support game intervention was implemented with 88 sixth-grade students. The results revealed that games incorporating challenge and autonomy had a positive influence on students' in-game creative performance. This impact was observed to be mediated by two factors: creative self-efficacy and engagement. Findings of the current study suggest that designing educational games with specific features can enhance students' creative performance within the game environment.",although researchers suggested video games minecraft potential foster student creativity little known regarding educational games designed effectively promote creativity address gap study aimed develop educational game modifying minecraft evaluate effects two game design features challenge autonomy game creative performance middle school students creativity support game intervention implemented sixth grade students results revealed games incorporating challenge autonomy positive influence students game creative performance impact observed mediated two factors creative self efficacy engagement findings current study suggest designing educational games specific features enhance students creative performance within game environment
"The benefits of Augmented Reality (AR) in modernizing teaching are widely recognized. Nonetheless, doubts persist about aligning AR materials with learning goals and ensuring scientific accuracy. Current research trends highlight teachers as developers of mobile AR content (mAR) to address these gaps, yet few studies have explored biology-focused mAR. This study employed Diffusion of Innovations Theory and mixed-methods approach to examine pre-service biology teachers’ (PSBTs) perceptions of developing and integrating mAR in future teaching. Participants received training to develop their own mAR teaching content. Data are collected with using questionnaires and audio recorded discussions during training. Qualitative data were processed using content analysis, while quantitative data underwent descriptive statistics and Spearman's rank correlation. Results confirm that PSBTs are eager to produce and employ mAR materials in teaching. They believe such development stimulates creativity and fosters collaboration among teachers. Results indicate that PSBT are having opinions that teachers’ developed mAR resources can better address students’ needs, promote critical thinking, and enable hands-on experimentation that might otherwise be unfeasible. PSBTs also note that these materials can be blended with traditional tools and real-world natural surroundings, enhancing students’ connection with nature. However, technical complexity, accessibility of digital resources needs for mAR development, and lack of institutional support pose obstacles. Nonetheless, quantitative outcomes reveal a positive overall perception of mAR’s impact on teaching quality and student motivation, though PSBTs highlight the importance of ongoing training and stronger alliances with colleagues and educational managers.",benefits augmented reality modernizing teaching widely recognized nonetheless doubts persist aligning materials learning goals ensuring scientific accuracy current research trends highlight teachers developers mobile content mar address gaps yet studies explored biology focused mar study employed diffusion innovations theory mixed methods approach examine pre service biology teachers psbts perceptions developing integrating mar future teaching participants received training develop mar teaching content data collected using questionnaires audio recorded discussions training qualitative data processed using content analysis quantitative data underwent descriptive statistics spearman rank correlation results confirm psbts eager produce employ mar materials teaching believe development stimulates creativity fosters collaboration among teachers results indicate psbt opinions teachers developed mar resources better address students needs promote critical thinking enable hands experimentation might otherwise unfeasible psbts also note materials blended traditional tools real world natural surroundings enhancing students connection nature however technical complexity accessibility digital resources needs mar development lack institutional support pose obstacles nonetheless quantitative outcomes reveal positive overall perception mar impact teaching quality student motivation though psbts highlight importance ongoing training stronger alliances colleagues educational managers
"Scientific argumentation is a key competency in science education, yet creating effective online environments to support it remains a challenging task. Recent research in science education has focused on using online gamified learning as a means to cultivate scientific argumentation skills. This study introduces “hypotheticality” as a unidimensional construct in online learning contexts. Hypotheticality can be applied to both gamified learning and scientific topics. By categorizing three degrees of hypotheticality from low to high, learners acquire distinct learning experiences in direct, vicarious, and hypothetical contexts, progressing from reality to imagination. The study further explores how hypotheticality and gamified learning impact students’ scientific knowledge, motivation, and argumentation learnings. A quasi-experimental design was adopted, involving 254 eighth graders. An online learning platform was developed specifically for the study, using a baseball player’s career as the narrative setting and incorporating online scaffolding to facilitate collaborative scientific argumentation. The four components of scientific argumentation—claim, warrant, rebuttal, and qualifier—were analyzed in terms of their frequency and quality. Statistical analysis revealed an interaction between the two variables, demonstrating that gamified learning had the most positive impact in contexts with the highest degree of hypotheticality. Subsequent qualitative analysis indicated that students in the gamified condition and in contexts with a high degree of hypotheticality showed remarkable improvements in scientific knowledge, learning motivation, and argumentation performance. Notably, scientific arguments typically considered challenging to construct, such as evidence-based rebuttals and qualifiers, were effectively explored and addressed. Our findings suggest that designing gamified argumentation activities requires careful consideration of the degree of hypotheticality in learning contexts. In practice, attention should be given to arranging appropriate scaffolds and guidance across varying hypotheticality to foster higher-order scientific thinking.",scientific argumentation key competency science education yet creating effective online environments support remains challenging task recent research science education focused using online gamified learning means cultivate scientific argumentation skills study introduces hypotheticality unidimensional construct online learning contexts hypotheticality applied gamified learning scientific topics categorizing three degrees hypotheticality low high learners acquire distinct learning experiences direct vicarious hypothetical contexts progressing reality imagination study explores hypotheticality gamified learning impact students scientific knowledge motivation argumentation learnings quasi experimental design adopted involving eighth graders online learning platform developed specifically study using baseball player career narrative setting incorporating online scaffolding facilitate collaborative scientific argumentation four components scientific argumentation claim warrant rebuttal qualifier analyzed terms frequency quality statistical analysis revealed interaction two variables demonstrating gamified learning positive impact contexts highest degree hypotheticality subsequent qualitative analysis indicated students gamified condition contexts high degree hypotheticality showed remarkable improvements scientific knowledge learning motivation argumentation performance notably scientific arguments typically considered challenging construct evidence based rebuttals qualifiers effectively explored addressed findings suggest designing gamified argumentation activities requires careful consideration degree hypotheticality learning contexts practice attention given arranging appropriate scaffolds guidance across varying hypotheticality foster higher order scientific thinking
"Critical thinking (CT) is widely recognized to be among the most essential skills for university graduates to successfully compete in the 21st century global economy; yet, much obscurity remains regarding its practical role in higher education. In fact, there is a need for an accepted educational plan with pedagogical guidelines that university instructors can lean on for integrating CT into subject instruction. Accordingly, this study aimed to identify the instructional design components recommended by instructors and students for promoting CT in higher education academic courses. The study involved ten higher education instructors and 342 undergraduate students in science and engineering. In order to address the research goal, the study followed the grounded theory approach in which the data was analyzed from a qualitative perspective. The data was collected through two research tools: semi-structured interviews and an online survey. The current study identified eight instructional design components for promoting CT. These components include four pedagogical practices: explicit teaching of CT, integration of digital tools, collaborative learning and constructing questions, as well as four modes of engagement: critical engagement with information, comparing approaches to problem-solving, guided inquiry and experimentation and raising doubts and evaluating credibility. The study introduces an adaptive model incorporating CT-enhanced pedagogical practices, for policymakers and instructional designers, to foster CT in science and engineering courses.",critical thinking widely recognized among essential skills university graduates successfully compete century global economy yet much obscurity remains regarding practical role higher education fact need accepted educational plan pedagogical guidelines university instructors lean integrating subject instruction accordingly study aimed identify instructional design components recommended instructors students promoting higher education academic courses study involved ten higher education instructors undergraduate students science engineering order address research goal study followed grounded theory approach data analyzed qualitative perspective data collected two research tools semi structured interviews online survey current study identified eight instructional design components promoting components include four pedagogical practices explicit teaching integration digital tools collaborative learning constructing questions well four modes engagement critical engagement information comparing approaches problem solving guided inquiry experimentation raising doubts evaluating credibility study introduces adaptive model incorporating enhanced pedagogical practices policymakers instructional designers foster science engineering courses
"In this paper, a type of energy-preserving mixed finite element method (FEM) is proposed for a class of generalized nonlinear fourth-order wave equations by means of the Raviart–Thomas mixed element and Crank–Nicolson temporal discretization in its full discretization, where besides the primary variable, its numerical gradient and Laplacian are also obtained, simultaneously, with the energy conservation property and optimal convergence rates in their energy norms that are rigorously proved in the theoretical analyses. In addition, a particular projection technique is introduced and analyzed in a coupling and mixed finite element form to assist in proving the optimal convergence properties. Numerical experiments are carried out to validate all theoretical results. The developed numerical method provides a guide to solve physical vibration problems of beams and thin plates, which own the form of nonlinear fourth-order wave equations, stably and accurately for a long time in a way that ensures energy conservation.",paper type energy preserving mixed finite element method fem proposed class generalized nonlinear fourth order wave equations means raviart thomas mixed element crank nicolson temporal discretization full discretization besides primary variable numerical gradient laplacian also obtained simultaneously energy conservation property optimal convergence rates energy norms rigorously proved theoretical analyses addition particular projection technique introduced analyzed coupling mixed finite element form assist proving optimal convergence properties numerical experiments carried validate theoretical results developed numerical method provides guide solve physical vibration problems beams thin plates form nonlinear fourth order wave equations stably accurately long time way ensures energy conservation
"Creativity requires both originality and effectiveness. Originality without effectiveness indicates pseudo-creativity, often due to deficits in idea evaluation. Research on the processes underlying pseudo-creativity and authentic creativity is limited. This study operationally identified individuals with pseudo-creative tendencies and compared them with authentically creative individuals using associative thinking tasks related to idea generation, focusing on memory search and semantic memory representations. In Experiment 1, moderation analysis classified participants into three groups based on multiple creativity measurements: authentic creativity, pseudo-creativity, and low creativity (control). These groups completed a goal-directed association task involving animal concept searching. Results showed that pseudo-creativity was associated with reduced clustering and cluster switching, along with more frequent hard switching. Experiment 2 constructed semantic networks for the three groups using free association, revealing that pseudo-creativity group exhibited a de-structured and hyperconnective semantic network. These findings provide the first characterization of the cognitive processes underlying pseudo-creativity. They reveal that pseudo-creativity exhibits looser conceptual boundaries and demonstrates differences from authentic creativity as early as the idea-generation phase. This offers a direct investigation for the theory that creativity requires a balance between the flexibility and rigidity of lexicon structure.",creativity requires originality effectiveness originality without effectiveness indicates pseudo creativity often due deficits idea evaluation research processes underlying pseudo creativity authentic creativity limited study operationally identified individuals pseudo creative tendencies compared authentically creative individuals using associative thinking tasks related idea generation focusing memory search semantic memory representations experiment moderation analysis classified participants three groups based multiple creativity measurements authentic creativity pseudo creativity low creativity control groups completed goal directed association task involving animal concept searching results showed pseudo creativity associated reduced clustering cluster switching along frequent hard switching experiment constructed semantic networks three groups using free association revealing pseudo creativity group exhibited structured hyperconnective semantic network findings provide first characterization cognitive processes underlying pseudo creativity reveal pseudo creativity exhibits looser conceptual boundaries demonstrates differences authentic creativity early idea generation phase offers direct investigation theory creativity requires balance flexibility rigidity lexicon structure
"A regularized correntropy induced estimator is proposed for robust trace regression in high-dimensional setting. The proposed estimator, leverages the robustness of correntropy-induced loss, combined with the nuclear norm penalty, to yield robust low-rank estimation. The robustness is controlled by the scale parameter which behaves similarly to the squared loss for small residuals and caps the effect of large residuals. Despite the nonconvex nature of the loss, we prove that the used estimation algorithm still enjoys global convergence and the proposed estimator achieves the best breakdown point of 1/2, and the desired non-asymptotic statistical rate is further derived under some regularity conditions. The theoretical results also indicate that the estimated rank is of the same order as the unknown true rank. Furthermore, we develop the adaptive nuclear norm (ANN) regularized estimator to better estimate the true rank. The global convergence of its estimation algorithm is also maintained though the objective function is entirely nonconvex. We derive the statistical rate for the ANN counterpart, and the theoretical results show that the estimated rank by ANN is closer to the true rank. Superior performance of the method proposed is demonstrated on both simulations and real-data analysis.",regularized correntropy induced estimator proposed robust trace regression high dimensional setting proposed estimator leverages robustness correntropy induced loss combined nuclear norm penalty yield robust low rank estimation robustness controlled scale parameter behaves similarly squared loss small residuals caps effect large residuals despite nonconvex nature loss prove used estimation algorithm still enjoys global convergence proposed estimator achieves best breakdown point desired non asymptotic statistical rate derived regularity conditions theoretical results also indicate estimated rank order unknown true rank furthermore develop adaptive nuclear norm ann regularized estimator better estimate true rank global convergence estimation algorithm also maintained though objective function entirely nonconvex derive statistical rate ann counterpart theoretical results show estimated rank ann closer true rank superior performance method proposed demonstrated simulations real data analysis
"The present study investigates the role played by academic performance in the creative development and creative barriers of university students preparing for degrees in Education who participate in artistic activities, compared to other students in the same degree who do not. A total of 332 students from two public universities participated. The participants were distributed into two groups, according to whether they regularly participated in artistic activities (141 students) or did not (191 students), and were compared according to academic performance or execution: good and excellent. A personal socio-demographic data sheet, the Creative Imagination Test for Adults (PIC-A), and the Personal Creativity Barrier Inventory were used. We carried out non-parametric analysis, the results of which show that students with higher grades obtain better scores in the narrative and general creative test in both groups, while those who participate in artistic activities achieve better results. No creative barriers were detected in either group. In our conclusion, we underline the importance that participation in artistic activities and academic performance seem to have in these students’ creative development. Future research should consider including these measures when analyzing the creative process. Lastly, we explain some limitations of the study.",present study investigates role played academic performance creative development creative barriers university students preparing degrees education participate artistic activities compared students degree total students two public universities participated participants distributed two groups according whether regularly participated artistic activities students students compared according academic performance execution good excellent personal socio demographic data sheet creative imagination test adults pic personal creativity barrier inventory used carried non parametric analysis results show students higher grades obtain better scores narrative general creative test groups participate artistic activities achieve better results creative barriers detected either group conclusion underline importance participation artistic activities academic performance seem students creative development future research consider including measures analyzing creative process lastly explain limitations study
"This paper proposes a fractional-order seven-compartmental model including five human compartments and two vector compartments to describe the transmission dynamics of dengue disease. We take into account the saturated treatment function while forming the model, incorporating three controls: awareness control, treatment control, and insecticide control. The intricate dynamics of the system, encompassing the existence and uniqueness of solutions as well as their biological feasibility, are thoroughly examined. The threshold parameter of the system, known as the basic reproduction number, is derived, along with the conditions for the occurrence of backward bifurcation and transcritical bifurcation. We also acquired the equilibria and their stability in relation to variations in the basic reproduction number. Furthermore, we have demonstrated that both backward and transcritical bifurcations can be influenced simply by adjusting the cure rate efficiency and the potency of insecticide controls, which remain within our sphere of control. The analytical result is verified through some numerical work. Finally, we apply the genetic algorithm optimization technique, which is independent of the initial state of the population, to minimize the basic reproduction number by optimizing three control parameters and the cure rate. Optimizing the control parameters and cure rate, we have deduced that the disease will die out in the case of transcritical bifurcation, but it will remain in the population in an equilibrium state for the backward bifurcation.",paper proposes fractional order seven compartmental model including five human compartments two vector compartments describe transmission dynamics dengue disease take account saturated treatment function forming model incorporating three controls awareness control treatment control insecticide control intricate dynamics system encompassing existence uniqueness solutions well biological feasibility thoroughly examined threshold parameter system known basic reproduction number derived along conditions occurrence backward bifurcation transcritical bifurcation also acquired equilibria stability relation variations basic reproduction number furthermore demonstrated backward transcritical bifurcations influenced simply adjusting cure rate efficiency potency insecticide controls remain within sphere control analytical result verified numerical work finally apply genetic algorithm optimization technique independent initial state population minimize basic reproduction number optimizing three control parameters cure rate optimizing control parameters cure rate deduced disease die case transcritical bifurcation remain population equilibrium state backward bifurcation
"Fractal dimension is a nonlinear fractal concept, which is used for analyzing the complexity of stock indices. In this paper we execute fractal dimension (FD) concept for analyzing the complexity of stock indices. From different sectors, the closing price indices of stock indices BSESN, NYSE, NSE-Nifty and NASDAQ are chosen for the investigation and the outcome on research are exhibited on that wide-open convenient dataset associated with various kinds of stock indices data. Four notable assessment methods, namely Generalized fractal dimension (GFD), Katz’s Fractal dimension (KFD), Higuchi’s Fractal dimension (HFD) and Sevcik’s Fractal dimension (SFD) methods are examined in this investigation for the classification of complex stock data. The whole stock indices data is separated into subgroups with different sizing for the computation of fractal dimension which leads this research to the multifractal analysis. Anlaysis data set through subgroups with different size exhibits multifractal behavior. The whole data considered represents random walk for all the stock indices. The regarded exchange data of the indices are irregular and randomized in nature and authorize to employ fractal analysis. The results show that KFD method is more consistent than the results obtained by GFD, HFD and SFD methods.",fractal dimension nonlinear fractal concept used analyzing complexity stock indices paper execute fractal dimension concept analyzing complexity stock indices different sectors closing price indices stock indices bsesn nyse nse nifty nasdaq chosen investigation outcome research exhibited wide open convenient dataset associated various kinds stock indices data four notable assessment methods namely generalized fractal dimension gfd katz fractal dimension kfd higuchi fractal dimension hfd sevcik fractal dimension sfd methods examined investigation classification complex stock data whole stock indices data separated subgroups different sizing computation fractal dimension leads research multifractal analysis anlaysis data set subgroups different size exhibits multifractal behavior whole data considered represents random walk stock indices regarded exchange data indices irregular randomized nature authorize employ fractal analysis results show kfd method consistent results obtained gfd hfd sfd methods
Ulam–Hyers (UH) stability for systems of incommensurate weakly singular integral equations (WSIEs) is studied. The existence and uniqueness of the solutions for systems of incommensurate WSIEs are obtained in the space of discontinuous functions. The related analysis for Laplace transforms is developed and it is used for UH stability. It is shown that the UH stability is equivalent to the convergence of the perturbed system toward the original system when the error of the perturbed source vanishes. Numerical methods based on UH stability are proposed. The analysis is investigated through some examples.,ulam hyers stability systems incommensurate weakly singular integral equations wsies studied existence uniqueness solutions systems incommensurate wsies obtained space discontinuous functions related analysis laplace transforms developed used stability shown stability equivalent convergence perturbed system toward original system error perturbed source vanishes numerical methods based stability proposed analysis investigated examples
"Mixed-type orthogonal Laurent polynomials on the unit circle of CMV type are constructed utilizing a matrix of moments and its Gauss–Borel factorization and employing a multiple extension of the CMV ordering. A systematic analysis of the associated multiple orthogonality and biorthogonality relations, and an examination of the degrees of the Laurent polynomials is given. Recurrence relations, expressed in terms of banded matrices, are found. These recurrence relations lay the groundwork for corresponding Christoffel–Darboux kernels and relations, as well as for elucidating the ABC theorem. The paper also develops the theory of diagonal Christoffel and Geronimus perturbations of the matrix of measures. Christoffel formulas are found for both perturbations.",mixed type orthogonal laurent polynomials unit circle cmv type constructed utilizing matrix moments gauss borel factorization employing multiple extension cmv ordering systematic analysis associated multiple orthogonality biorthogonality relations examination degrees laurent polynomials given recurrence relations expressed terms banded matrices found recurrence relations lay groundwork corresponding christoffel darboux kernels relations well elucidating abc theorem paper also develops theory diagonal christoffel geronimus perturbations matrix measures christoffel formulas found perturbations
"In this paper, we propose a double inertial subgradient extragradient method for solving variational inequalities and fixed point problems in Hilbert spaces. We add a parameter in the second step projection to adjust the step size, thereby increasing the convergence rate of the proposed algorithm. In addition, we make appropriate improvements based on the common self adaptive rules. Finally, through numerical experiments, we not only verify the effectiveness of the added parameters, but also show that our algorithm has better convergence behavior than other known results.",paper propose double inertial subgradient extragradient method solving variational inequalities fixed point problems hilbert spaces add parameter second step projection adjust step size thereby increasing convergence rate proposed algorithm addition make appropriate improvements based common self adaptive rules finally numerical experiments verify effectiveness added parameters also show algorithm better convergence behavior known results
"We investigate feedback stabilization of linear high-order systems. Using a block matrix expression of the fundamental matrix, stability criteria of the systems are derived. We also present the Fourier transform formulas of the first row in the block matrix expression of the fundamental matrix. Based on the stability criteria and the Fourier transform formulas of the first row, a frequency-domain method is presented to design a stabilizing controller of the systems. We emphasize that all the computations in this paper involve only matrices with lower size. Numerical examples are given to illustrate the main results.",investigate feedback stabilization linear high order systems using block matrix expression fundamental matrix stability criteria systems derived also present fourier transform formulas first row block matrix expression fundamental matrix based stability criteria fourier transform formulas first row frequency domain method presented design stabilizing controller systems emphasize computations paper involve matrices lower size numerical examples given illustrate main results
"We initiate the study of computational complexity of graph coverings, aka locally bijective graph homomorphisms, for graphs with semi-edges. The notion of graph covering is a discretization of coverings between surfaces or topological spaces, a notion well known and deeply studied in classical topology. Graph covers have found applications in discrete mathematics for constructing highly symmetric graphs, and in computer science in the theory of local computations. In 1991, Abello, Fellows, and Stillwell asked for a classification of the computational complexity of deciding if an input graph covers a fixed target graph, in the ordinary setting (of graphs with only edges). Although many general results are known, the full classification is still open. In spite of that, we propose to study the more general case of covering graphs composed of normal edges (including multiedges and loops) and so-called semi-edges. Semi-edges are becoming increasingly popular in modern topological graph theory, as well as in mathematical physics. They also naturally occur in the local computation setting, since they are lifted to matchings in the covering graph. We show that the presence of semi-edges makes the covering problem considerably harder; e.g., it is no longer sufficient to specify the vertex mapping induced by the covering, but one necessarily has to deal with the edge mapping as well. We show some solvable cases and, in particular, completely characterize the complexity of the already very nontrivial problem of covering one- and two-vertex (multi)graphs with semi-edges. Our NP-hardness results are proven for simple input graphs, and in the case of regular two-vertex target graphs, even for bipartite ones. We remark that our new characterization results also strengthen previously known results for covering graphs without semi-edges, and they in turn apply to an infinite class of simple target graphs with at most two vertices of degree more than two. Some of the results are moreover proven in a more general setting (e.g., finding k-tuples of pairwise disjoint perfect matchings in regular graphs).",initiate study computational complexity graph coverings aka locally bijective graph homomorphisms graphs semi edges notion graph covering discretization coverings surfaces topological spaces notion well known deeply studied classical topology graph covers found applications discrete mathematics constructing highly symmetric graphs computer science theory local computations abello fellows stillwell asked classification computational complexity deciding input graph covers fixed target graph ordinary setting graphs edges although many general results known full classification still open spite propose study general case covering graphs composed normal edges including multiedges loops called semi edges semi edges becoming increasingly popular modern topological graph theory well mathematical physics also naturally occur local computation setting since lifted matchings covering graph show presence semi edges makes covering problem considerably harder longer sufficient specify vertex mapping induced covering one necessarily deal edge mapping well show solvable cases particular completely characterize complexity already nontrivial problem covering one two vertex multi graphs semi edges hardness results proven simple input graphs case regular two vertex target graphs even bipartite ones remark new characterization results also strengthen previously known results covering graphs without semi edges turn apply infinite class simple target graphs two vertices degree two results moreover proven general setting finding tuples pairwise disjoint perfect matchings regular graphs
"The integration of 21st-century skills, communication, collaboration, critical thinking, and creativity (the 4Cs) is pivotal in contemporary higher education to prepare students for a rapidly evolving global landscape. Despite widespread theoretical emphasis, empirical insights into how these competencies are implemented in Vietnamese universities remain limited. This study aims to explore the extent of 4Cs integration in Vietnamese higher education and to identify lecturers' opinions on the instructional strategies used and the barriers encountered. To address this aim, a cross-sectional quantitative study using an adapted, validated survey was distributed electronically across multiple institutions in four major Vietnamese cities, employing a multi-stage institutional sampling approach. A total of 140 valid lecturer responses met the inclusion criteria, reflecting the accessible population during the data collection window. Data were analyzed using descriptive statistics (frequencies, means, standard deviations) and internal-consistency estimates in SPSS 26. Descriptive analyses indicate widespread integration of the 4Cs, with lecturers most consistently employing structured collaboration and creativity strategies (e.g., group activities and granting student autonomy in resource selection). Conversely, project-based learning, collaborative online tasks, and the use of graphic organizers were reported as less consistently applied. Patterned differences across strategies highlight instructional and institutional barriers, including inadequate technological infrastructure, rigid curricula, and limited professional development opportunities. The results extend the existing literature by providing context-specific evidence on practice patterns and constraints, emphasizing the need for targeted institutional support to enable comprehensive implementation of the 4Cs. Implications include policy measures to enhance infrastructure, greater curriculum flexibility, and sustained professional learning to strengthen lecturers' capacity to embed 21st-century skills in teaching and learning.",integration century skills communication collaboration critical thinking creativity pivotal contemporary higher education prepare students rapidly evolving global landscape despite widespread theoretical emphasis empirical insights competencies implemented vietnamese universities remain limited study aims explore extent integration vietnamese higher education identify lecturers opinions instructional strategies used barriers encountered address aim cross sectional quantitative study using adapted validated survey distributed electronically across multiple institutions four major vietnamese cities employing multi stage institutional sampling approach total valid lecturer responses met inclusion criteria reflecting accessible population data collection window data analyzed using descriptive statistics frequencies means standard deviations internal consistency estimates spss descriptive analyses indicate widespread integration lecturers consistently employing structured collaboration creativity strategies group activities granting student autonomy resource selection conversely project based learning collaborative online tasks use graphic organizers reported less consistently applied patterned differences across strategies highlight instructional institutional barriers including inadequate technological infrastructure rigid curricula limited professional development opportunities results extend existing literature providing context specific evidence practice patterns constraints emphasizing need targeted institutional support enable comprehensive implementation implications include policy measures enhance infrastructure greater curriculum flexibility sustained professional learning strengthen lecturers capacity embed century skills teaching learning
"System dynamics provides a valuable framework for analyzing the complexities of a system over time. Using causal loop diagrams (CLDs), this study identifies two feedback mechanisms in the context of integrating computational thinking (CT) into education, focusing on leverage points such as coding-centric curricula and teacher competence development. The modeling shows how reinforcing and balancing feedback loops shape system behavior, demonstrating that interventions like coding-based approaches can deliver immediate benefits but risk hindering the long-term development of broader CT skills applicable to interdisciplinary problem-solving. The study enhances the analysis by uncovering patterns emerging from system dynamics, including the constraints limiting growth, reliance on symptomatic solutions, unintended consequences of quick fixes, and the prioritization of successful areas at the expense of others. These patterns highlight the importance of addressing foundational issues, such as teacher training and comprehensive curriculum design, to avoid overshadowing other critical goals of CT applications in interdisciplinary fields. By synthesizing CLDs, the study showcases the circular interactions between variables and the diverse dynamics influencing CT education, offering insights into potential scenarios and storylines for integrating CT into education systems.",system dynamics provides valuable framework analyzing complexities system time using causal loop diagrams clds study identifies two feedback mechanisms context integrating computational thinking education focusing leverage points coding centric curricula teacher competence development modeling shows reinforcing balancing feedback loops shape system behavior demonstrating interventions like coding based approaches deliver immediate benefits risk hindering long term development broader skills applicable interdisciplinary problem solving study enhances analysis uncovering patterns emerging system dynamics including constraints limiting growth reliance symptomatic solutions unintended consequences quick fixes prioritization successful areas expense others patterns highlight importance addressing foundational issues teacher training comprehensive curriculum design avoid overshadowing critical goals applications interdisciplinary fields synthesizing clds study showcases circular interactions variables diverse dynamics influencing education offering insights potential scenarios storylines integrating education systems
"This article proposes a closed-loop symplectic regularized algorithm for constrained time-varying optimal control problems. Due to the limitations of symplectic methods in constrained time-varying systems, the optimal control problem is transformed and discretized into a discretized symplectic Runge–Kutta form by using variational integrator. Moreover, we derive first-order necessary conditions for the discrete optimal control problem and obtain a set of Euler–Lagrange (EL) equations. To solve the EL equations, we provide a forward–backward sweep iteration algorithm and analyze its error estimation along with regularization terms. Based on this iteration algorithm, a closed-loop symplectic regularized algorithm is proposed consisting of symplectic update and regularized iteration. To be specific, a sequence of quadratic programmings are leveraged in the forward stage of symplectic update to provide good initial values for the regularized iteration. Furthermore, an interior-point barrier function is applied to handle the constraints in the regularized iteration. The convergence analysis of the proposed algorithm is provided, and simulations are conducted to verify its effectiveness.",article proposes closed loop symplectic regularized algorithm constrained time varying optimal control problems due limitations symplectic methods constrained time varying systems optimal control problem transformed discretized discretized symplectic runge kutta form using variational integrator moreover derive first order necessary conditions discrete optimal control problem obtain set euler lagrange equations solve equations provide forward backward sweep iteration algorithm analyze error estimation along regularization terms based iteration algorithm closed loop symplectic regularized algorithm proposed consisting symplectic update regularized iteration specific sequence quadratic programmings leveraged forward stage symplectic update provide good initial values regularized iteration furthermore interior point barrier function applied handle constraints regularized iteration convergence analysis proposed algorithm provided simulations conducted verify effectiveness
"In this work, we study the numerical performance of the parallel-in-time contour integral method for one-dimensional differential equations of convection–diffusion type. The considered method is based on the representation of a Dunford-Cauchy integral along a contour which encompasses the spectrum of an unbounded infinitesimal operator A corresponding to the equation. The discrete operator is constructed as a Radial Basis Function-generated Finite Differences (RBF-FD) discretization of diffusion and advective, resp. convective, terms. Its accuracy and performance as a part of the contour integration method is compared with those by finite difference approaches. A contour is chosen to be of elliptic shape with varying number of cubature points. Numerical performance of the contour integral method is compared with sequential-in-time realizations of the θ -scheme where discrete operators have similar discretizations by finite differences. The studied algorithm is 2 N time N cub parallelizable, where N time is the number of time points at which the numerical solution is computed and N cub is the number of cubature points on the contour. Its efficiency, however, strongly depends on the performance of the linear solver used for the resolvent part at each cubature point on the contour. In this work, we thoroughly examine the selection of a linear solver and analyze its performance with respect to various problem parameters. The proposed method is flexible in the choice of discretization techniques in space and can be readily extended to multiple dimensions.",work study numerical performance parallel time contour integral method one dimensional differential equations convection diffusion type considered method based representation dunford cauchy integral along contour encompasses spectrum unbounded infinitesimal operator corresponding equation discrete operator constructed radial basis function generated finite differences rbf discretization diffusion advective resp convective terms accuracy performance part contour integration method compared finite difference approaches contour chosen elliptic shape varying number cubature points numerical performance contour integral method compared sequential time realizations scheme discrete operators similar discretizations finite differences studied algorithm time cub parallelizable time number time points numerical solution computed cub number cubature points contour efficiency however strongly depends performance linear solver used resolvent part cubature point contour work thoroughly examine selection linear solver analyze performance respect various problem parameters proposed method flexible choice discretization techniques space readily extended multiple dimensions
"The synchronization research in time-varying networks serves as the foundation for further discussion of information transmission and node consensus. Moreover, changing topology and connection mode in a dynamic network, the time scale of opinion evolution, and an uncertain dynamic chaotic environment such as random noise and time delay are all important factors affecting individual opinion synchronization. Motivated by this observation, a multi-layer time-varying network model is built on the basis of complex multi-layer network, which can better capture the dynamic changes and evolutionary processes of real-world networks. At the same time, time delay of real system and multi-chaotic system are added in order to more realistically simulate the nonlinearity and complexity of social dynamic network systems. On the premise that the change rate of network topology is greater than the synchronization rate of nodes, network controllers and updating rules derived in this paper can effectively realize adaptive synchronization of multi-layer time-varying networks under a single chaotic system and a multi-chaotic system, respectively. These findings shed new light on the evolution of opinions on time-varying complex networks and help decision-makers predict the prevalence of hot topics and public opinions.",synchronization research time varying networks serves foundation discussion information transmission node consensus moreover changing topology connection mode dynamic network time scale opinion evolution uncertain dynamic chaotic environment random noise time delay important factors affecting individual opinion synchronization motivated observation multi layer time varying network model built basis complex multi layer network better capture dynamic changes evolutionary processes real world networks time time delay real system multi chaotic system added order realistically simulate nonlinearity complexity social dynamic network systems premise change rate network topology greater synchronization rate nodes network controllers updating rules derived paper effectively realize adaptive synchronization multi layer time varying networks single chaotic system multi chaotic system respectively findings shed new light evolution opinions time varying complex networks help decision makers predict prevalence hot topics public opinions
"The pervasive impact of artificial intelligence (AI) on society underscores the critical need for comprehensive AI education, particularly for young students. This study investigates how design fiction pedagogy (DFP) may enhance K-12 AI education by fostering understanding of AI and encouraging critical thinking about its social impact. Grounded in constructivist and constructionist theories, DFP integrates speculative design and narrative learning to provide an interdisciplinary pedagogical approach. The DFP model, which consists of seven pedagogical steps—researching a problem, designing a prototype, creating a future context, building a narrative, sharing with stakeholders, reflecting on ethical considerations, and evaluating and redesigning—was implemented through two separate week-long AI education camps in Ontario, Canada. Using a qualitative case study approach, we examined how DFP supports upper elementary students in grasping AI concepts and AI’s social impact. The findings indicate that, by integrating technical principles, ethical considerations, and futuristic thinking about human-AI interactions, DFP helps students develop a deeper understanding of AI while fostering creativity, critical thinking, and futuristic thinking—skills essential for the responsible development and use of AI. Consequently, DFP emerges as a promising approach for an AI education that integrates technical knowledge with ethical considerations.",pervasive impact artificial intelligence society underscores critical need comprehensive education particularly young students study investigates design fiction pedagogy dfp may enhance education fostering understanding encouraging critical thinking social impact grounded constructivist constructionist theories dfp integrates speculative design narrative learning provide interdisciplinary pedagogical approach dfp model consists seven pedagogical steps researching problem designing prototype creating future context building narrative sharing stakeholders reflecting ethical considerations evaluating redesigning implemented two separate week long education camps ontario canada using qualitative case study approach examined dfp supports upper elementary students grasping concepts social impact findings indicate integrating technical principles ethical considerations futuristic thinking human interactions dfp helps students develop deeper understanding fostering creativity critical thinking futuristic thinking skills essential responsible development use consequently dfp emerges promising approach education integrates technical knowledge ethical considerations
"The integration of Generative Artificial Intelligence (GenAI) into creative tasks offers strong potential to enhance team creativity and collaborative competence. However, the factors shaping attitudes and behavioral intentions toward GenAI in creative collaboration have received limited attention, particularly in cross-cultural contexts. To address these gaps, this study has developed and validated an integrated theoretical framework via a hybrid multistage approach. First, focus group interviews (N = 15) informed the extension of the TAM-TPB model by incorporating perceived risk, openness to experience, and AI literacy. Then, cross-cultural survey data from China (N = 529) and the USA (N = 544) were analyzed using structural equation modeling (SEM), multi-group analysis (MGA), and artificial neural networks (ANN). Results showed that subjective norm influenced user attitudes in both countries. In China, perceived usefulness and risk were key predictors, whereas in the USA, ease of use and openness to experience were more influential. Attitude, subjective norm, and perceived behavioral control were found to be critical determinants of behavioral intentions across both groups. By integrating perspectives from creativity research, this study provides theoretical and practical implications for the adoption of GenAI in creative domains, informing educators, industry practitioners, and technology developers.",integration generative artificial intelligence genai creative tasks offers strong potential enhance team creativity collaborative competence however factors shaping attitudes behavioral intentions toward genai creative collaboration received limited attention particularly cross cultural contexts address gaps study developed validated integrated theoretical framework via hybrid multistage approach first focus group interviews informed extension tam tpb model incorporating perceived risk openness experience literacy cross cultural survey data china usa analyzed using structural equation modeling sem multi group analysis mga artificial neural networks ann results showed subjective norm influenced user attitudes countries china perceived usefulness risk key predictors whereas usa ease use openness experience influential attitude subjective norm perceived behavioral control found critical determinants behavioral intentions across groups integrating perspectives creativity research study provides theoretical practical implications adoption genai creative domains informing educators industry practitioners technology developers
"Early engineering interest (EEI) plays a key role in promoting children’s engineering learning in later stages, and children with EEI are likely to later consider engineering as a career choice. Thus, EEI development can provide a crucial foundation for talent training in science, technology, engineering, and mathematics. The present study (1) explored the potential constructs of young children’s EEI and (2) developed an age-appropriate EEI scale. Using this instrument, early childhood engineering education researchers, curriculum developers, and educators can obtain a clearer understanding of children’s EEI. An EEI scale was developed; children aged between 5 and 6 years were selected as pretest participants. Exploratory factor analysis and confirmatory factor analysis were conducted using Mplus 8.3 to verify the reliability and validity of the EEI scale. This study provided evidence indicating the reliability and validity of the EEI scale for measuring kindergarteners’ EEI.",early engineering interest eei plays key role promoting children engineering learning later stages children eei likely later consider engineering career choice thus eei development provide crucial foundation talent training science technology engineering mathematics present study explored potential constructs young children eei developed age appropriate eei scale using instrument early childhood engineering education researchers curriculum developers educators obtain clearer understanding children eei eei scale developed children aged years selected pretest participants exploratory factor analysis confirmatory factor analysis conducted using mplus verify reliability validity eei scale study provided evidence indicating reliability validity eei scale measuring kindergarteners eei
"A consistent goodness-of-fit test for distributional regression is introduced. The test statistic is based on a process that traces the difference between a nonparametric and a semi-parametric estimate of the marginal distribution function of Y . As its asymptotic null distribution is not distribution-free, a parametric bootstrap method is used to determine critical values. Empirical results suggest that, in certain scenarios, the test outperforms existing specification tests by achieving a higher power and thereby offering greater sensitivity to deviations from the assumed parametric distribution family. Notably, the proposed test does not involve any hyperparameters and can easily be applied to individual datasets using the gofreg-package in R.",consistent goodness fit test distributional regression introduced test statistic based process traces difference nonparametric semi parametric estimate marginal distribution function asymptotic null distribution distribution free parametric bootstrap method used determine critical values empirical results suggest certain scenarios test outperforms existing specification tests achieving higher power thereby offering greater sensitivity deviations assumed parametric distribution family notably proposed test involve hyperparameters easily applied individual datasets using gofreg package
"Computing the infinity Wasserstein distance and retrieving projections of a probability measure onto a closed subset of probability measures are critical sub-problems in various applied fields. However, the practical applicability of these objects is limited by two factors: either the associated quantities are computationally prohibitive or there is a lack of available algorithms capable of calculating them. In this paper, we propose a novel class of Linear Programming problems and a routine that allows us to compute the infinity Wasserstein distance and to compute a projection of a probability measure over a generic subset of probability measures with respect to any p -Wasserstein distance with p ∈ [ 1 , ∞ ] .",computing infinity wasserstein distance retrieving projections probability measure onto closed subset probability measures critical sub problems various applied fields however practical applicability objects limited two factors either associated quantities computationally prohibitive lack available algorithms capable calculating paper propose novel class linear programming problems routine allows compute infinity wasserstein distance compute projection probability measure generic subset probability measures respect wasserstein distance
"One of the most significant challenges in many real-world applications is selecting a limited number of representative points (RPs) that retain as much valuable information as possible from large datasets or continuous functions. Space-filling RPs (SFRPs) are commonly used to ensure comprehensive coverage of the input domain. However, in practical settings, it is equally important to achieve a well-distributed representation in both the input and response (output) spaces. This paper explores the advantages of input-output space-filling RPs (IOSFRPs), which are designed to ensure coverage across both domains. IOSFRPs are employed in three primary contexts: (i) Clustering, where they serve as cluster centers that promote well-distributed clusters across the input–response space; (ii) Modeling, where they act as experimental designs that capture variability in both domains, thereby improving the accuracy of surrogate models; and (iii) Estimation, where they function as efficient samples or discrete approximations of continuous distributions to enhance statistical inference. Extensive simulations and comparative evaluations demonstrate that IOSFRPs outperform widely used existing methods across all three applications. These findings highlight the flexibility, robustness, and efficiency of the IOSFRPs. Importantly, the weighting parameter allows for tuning the balance between input and output space-filling properties, enabling the IOSFRPs to adapt to a wide range of data-driven applications.",one significant challenges many real world applications selecting limited number representative points rps retain much valuable information possible large datasets continuous functions space filling rps sfrps commonly used ensure comprehensive coverage input domain however practical settings equally important achieve well distributed representation input response output spaces paper explores advantages input output space filling rps iosfrps designed ensure coverage across domains iosfrps employed three primary contexts clustering serve cluster centers promote well distributed clusters across input response space modeling act experimental designs capture variability domains thereby improving accuracy surrogate models iii estimation function efficient samples discrete approximations continuous distributions enhance statistical inference extensive simulations comparative evaluations demonstrate iosfrps outperform widely used existing methods across three applications findings highlight flexibility robustness efficiency iosfrps importantly weighting parameter allows tuning balance input output space filling properties enabling iosfrps adapt wide range data driven applications
"As an innovative AI-driven chatbot, ChatGPT holds substantial potential to redefine human-computer interaction and enhance teaching and learning. However, current applications often prioritize functional utility over active learning facilitation, leaving its role in promoting collaborative knowledge construction and metacognitive regulation empirically underexplored. To address this gap, this study integrated ChatGPT into pre-service teachers' collaborative learning experiences, leveraging the Six Thinking Hats thinking strategy to scaffold instructional design tasks. Using a mixed-methods approach combining co-regulation coding and epistemic network analysis (ENA), this study compared the co-regulation processes and focus patterns of groups using ChatGPT with the Six Thinking Hats strategy versus those using Six Thinking Hats alone. Results indicate that ChatGPT-assisted groups exhibited intensified co-regulatory focus on Evaluation, along with significantly heightened attention to Task Understanding and Content Monitoring. These findings not only illuminate ChatGPT's transformative role in teacher education but also provide actionable insights for educators designing AI-enhanced collaborative learning environments.",innovative driven chatbot chatgpt holds substantial potential redefine human computer interaction enhance teaching learning however current applications often prioritize functional utility active learning facilitation leaving role promoting collaborative knowledge construction metacognitive regulation empirically underexplored address gap study integrated chatgpt pre service teachers collaborative learning experiences leveraging six thinking hats thinking strategy scaffold instructional design tasks using mixed methods approach combining regulation coding epistemic network analysis ena study compared regulation processes focus patterns groups using chatgpt six thinking hats strategy versus using six thinking hats alone results indicate chatgpt assisted groups exhibited intensified regulatory focus evaluation along significantly heightened attention task understanding content monitoring findings illuminate chatgpt transformative role teacher education also provide actionable insights educators designing enhanced collaborative learning environments
"In this paper, we propose two fast strongly convergent forward-reflected-anchored-backward algorithms with self-adaptive step sizes to solve variational inequalities in the framework of quasi-monotonicity and Hilbert spaces. At each iteration, the proposed algorithms require one projection onto the feasible set and one functional evaluation, a distinctive trait that gives a special attraction to our algorithms and makes our algorithms state-of-the-art algorithms among strongly convergent algorithms for variational inequalities in Hilbert spaces. The available strongly convergent algorithms in the literature on variational inequalities require more than one projection onto the feasible set and (or) more than one functional evaluation per iteration with on-line rule imposed on such algorithms when inertial versions are studied. Our proposed algorithms are computationally cheaper than other relevant strongly convergent algorithms in the literature, as evident in our numerical tests.",paper propose two fast strongly convergent forward reflected anchored backward algorithms self adaptive step sizes solve variational inequalities framework quasi monotonicity hilbert spaces iteration proposed algorithms require one projection onto feasible set one functional evaluation distinctive trait gives special attraction algorithms makes algorithms state art algorithms among strongly convergent algorithms variational inequalities hilbert spaces available strongly convergent algorithms literature variational inequalities require one projection onto feasible set one functional evaluation per iteration line rule imposed algorithms inertial versions studied proposed algorithms computationally cheaper relevant strongly convergent algorithms literature evident numerical tests
"Assume the Black–Scholes model. This paper proves linear convergence rates for (1) the default probability and (2) the implied barrier of a down-and-out digital barrier call when the strike price equals the barrier, both computed by the binomial-trinomial tree. Numerical results confirm the linear convergence rates and lend support to the same rates for the more general step barrier.",assume black scholes model paper proves linear convergence rates default probability implied barrier digital barrier call strike price equals barrier computed binomial trinomial tree numerical results confirm linear convergence rates lend support rates general step barrier
"The continuous-time renewal risk model is a significant research topic in risk theory and actuarial science. This paper investigates the second-order tail asymptotics of discounted aggregate claims in continuous-time renewal risk models with constant interest force. By constructing a weighted Kesten-type inequality of second-order subexponential distributions, the second-order tail asymptotics for two types of continuous-time renewal risk models without and with by-claims are separately built. Compared with first-order tail asymptotics, our results are superior and more precise, as demonstrated by two numerical examples.",continuous time renewal risk model significant research topic risk theory actuarial science paper investigates second order tail asymptotics discounted aggregate claims continuous time renewal risk models constant interest force constructing weighted kesten type inequality second order subexponential distributions second order tail asymptotics two types continuous time renewal risk models without claims separately built compared first order tail asymptotics results superior precise demonstrated two numerical examples
"This study constructs a three-layer theoretical framework for critical reading aiming to explore the visual attention and reading performance of design students in critical reading. Specifically, we compared the performance of expert (more than three years) and novice (less than four months) design students when reading different layout configurations of a product design plan. A total of 80 participants, 40 experts and 40 novices, were recruited from a Chinese university. Using a 2 (expertise: expert vs. novice) × 2 (layout: text-image proximity vs. text-image separation) experimental design, participants' recall, insight scores, visual attention distribution and visual attention transition were analyzed. Eye-tracking data revealed that experts allocated significantly more visual attention to images and exhibited more complex fixation transitions between images and texts, suggesting higher levels of critical engagement. In contrast, novices displayed a more linear reading pattern, particularly in layouts that separated images from texts. Correspondingly, experts outperformed novices in both recall and insight scores. Additionally, layout configurations had a more complex impact on visual attention and performance. In the second layout, fixation time on images showed a positive correlation with insight, and experts exhibited fixation transitions between images. The results provide valuable insights for design educators, emphasizing the need for tailored instructional strategies to foster critical thinking skills in students at different educational levels.",study constructs three layer theoretical framework critical reading aiming explore visual attention reading performance design students critical reading specifically compared performance expert three years novice less four months design students reading different layout configurations product design plan total participants experts novices recruited chinese university using expertise expert novice layout text image proximity text image separation experimental design participants recall insight scores visual attention distribution visual attention transition analyzed eye tracking data revealed experts allocated significantly visual attention images exhibited complex fixation transitions images texts suggesting higher levels critical engagement contrast novices displayed linear reading pattern particularly layouts separated images texts correspondingly experts outperformed novices recall insight scores additionally layout configurations complex impact visual attention performance second layout fixation time images showed positive correlation insight experts exhibited fixation transitions images results provide valuable insights design educators emphasizing need tailored instructional strategies foster critical thinking skills students different educational levels
"Food browning during cooking often results from Maillard reaction, though other processes, such as caramelisation and enzymatic browning, also contribute depending on the type of food and cooking conditions. This paper presents a browning model based on Maillard reaction kinetics that integrates into comprehensive CFD baking models, predicting melanoidin concentration and food browning during baking. Initial validation uses data from established studies on the Maillard reaction and melanoidin formation in simplified model systems (sugar–amino acid mixtures heated under controlled conditions). Further experimental validation involves baking muffins in a domestic oven, comparing observed surface browning and melanoidin absorption with the model’s simulated browning index and melanoidins over time. A method for calculating the browning index from experiments and CFD simulations is presented. Validation shows differences in melanoidin concentration and browning index between measurements and simulations below 8% and 7%, respectively, across all baking times. This research provides insight into the mechanisms of browning reactions in baked goods, enabling the integration of Maillard reaction kinetics into CFD models. These findings offer practical tools for predicting browning, allowing the food industry to optimise baking processes, improve product consistency, and enhance safety and quality.",food browning cooking often results maillard reaction though processes caramelisation enzymatic browning also contribute depending type food cooking conditions paper presents browning model based maillard reaction kinetics integrates comprehensive cfd baking models predicting melanoidin concentration food browning baking initial validation uses data established studies maillard reaction melanoidin formation simplified model systems sugar amino acid mixtures heated controlled conditions experimental validation involves baking muffins domestic oven comparing observed surface browning melanoidin absorption model simulated browning index melanoidins time method calculating browning index experiments cfd simulations presented validation shows differences melanoidin concentration browning index measurements simulations respectively across baking times research provides insight mechanisms browning reactions baked goods enabling integration maillard reaction kinetics cfd models findings offer practical tools predicting browning allowing food industry optimise baking processes improve product consistency enhance safety quality
"Collaborative Problem Solving (CPS) work in mathematics education are widely recognized for engaging students in cognitively demanding activities that foster Higher-Order Thinking Skills (HOTS), like critical thinking and reasoning. However, connections between design features, CPS processes, and learning outcomes remain complex and not fully understood. To address this, we applied a conjecture-based framework to systematically review 45 empirical studies published between 2010 and 2022, focusing on how specific task designs and CPS processes contribute to HOTS. We used a machine learning tool to prioritize relevant studies and streamline the selection process, ending after a threshold number of consecutive irrelevant articles. Guided by the conjecture-based framework, our analysis highlighted how cognitive processes in CPS function as essential mechanisms of learning and measurable outcomes. Specifically, design features, such as technology-supported exploratory tasks and open-ended problems, encourage reflective discourse and deeper cognitive engagement. We also found that structured group procedures, including clear roles and guided interaction protocols, improve collaboration. Nonetheless, challenges like miscommunication and uneven participation can limit CPS from fully realizing its potential to cultivate HOTS. Overall, these findings underscore the importance of aligning task design with CPS processes and using strategies to address collaboration barriers, particularly those related to communication. Without clear protocols and consistent dialogue, even well-designed CPS tasks can fail to cultivate HOTS. In conclusion, this review offers practical insights for educators and researchers implementing CPS effectively in mathematics education, highlighting that fostering open, structured communication is vital for optimizing both collaborative processes and the development of advanced cognitive skills.",collaborative problem solving cps work mathematics education widely recognized engaging students cognitively demanding activities foster higher order thinking skills hots like critical thinking reasoning however connections design features cps processes learning outcomes remain complex fully understood address applied conjecture based framework systematically review empirical studies published focusing specific task designs cps processes contribute hots used machine learning tool prioritize relevant studies streamline selection process ending threshold number consecutive irrelevant articles guided conjecture based framework analysis highlighted cognitive processes cps function essential mechanisms learning measurable outcomes specifically design features technology supported exploratory tasks open ended problems encourage reflective discourse deeper cognitive engagement also found structured group procedures including clear roles guided interaction protocols improve collaboration nonetheless challenges like miscommunication uneven participation limit cps fully realizing potential cultivate hots overall findings underscore importance aligning task design cps processes using strategies address collaboration barriers particularly related communication without clear protocols consistent dialogue even well designed cps tasks fail cultivate hots conclusion review offers practical insights educators researchers implementing cps effectively mathematics education highlighting fostering open structured communication vital optimizing collaborative processes development advanced cognitive skills
"Creative problem-solving has been highly acknowledged as an important skill for students in the 21st century. While risk-taking tendency has been identified as an important factor for creative problem-solving performance, empirical findings regarding their relationships remain inconsistent – partly due to variations in conceptualizing and measuring creativity, as well as a general lack of consideration for contextual moderators. Drawing on Trait Activation Theory (TAT), the present study takes a tentative and exploratory approach to examining how perceived teacher support for creativity moderates the relationships between risk-taking tendency and four subskills of creative problem-solving (problem identification, information processing, idea generation, idea evaluation) among Chinese fourth graders. Using a computer-based interactive assessment to objectively measure the subskills, data from 1544 students suggested that risk-taking tendency positively predicted overall creative problem-solving performance. Furthermore, perceived teacher support appeared to play a moderating role, though this was observed primarily in the dimensions of idea generation and idea evaluation. The findings offer preliminary insights into how contextual factors may shape trait expression in creative processes, with implications for educators and parents to better support student creativity development.",creative problem solving highly acknowledged important skill students century risk taking tendency identified important factor creative problem solving performance empirical findings regarding relationships remain inconsistent partly due variations conceptualizing measuring creativity well general lack consideration contextual moderators drawing trait activation theory tat present study takes tentative exploratory approach examining perceived teacher support creativity moderates relationships risk taking tendency four subskills creative problem solving problem identification information processing idea generation idea evaluation among chinese fourth graders using computer based interactive assessment objectively measure subskills data students suggested risk taking tendency positively predicted overall creative problem solving performance furthermore perceived teacher support appeared play moderating role though observed primarily dimensions idea generation idea evaluation findings offer preliminary insights contextual factors may shape trait expression creative processes implications educators parents better support student creativity development
"In this paper, we introduce a high order space–time approximation of generalized Korteweg de-Vries equations. More specifically, the method uses continuous H 1 -conforming finite elements for the spatial approximation and implicit–explicit methods for the temporal approximation. The discretization implies the method is asymptotic preserving. The low-order variant is provably stable and mass conservative; the high-order variant is at least second-order accurate in both space and time, well-defined, and mass-conservative. The scheme is formulated, its properties are proven, and numerical simulations are provided to illustrate the proposed methodology.",paper introduce high order space time approximation generalized korteweg vries equations specifically method uses continuous conforming finite elements spatial approximation implicit explicit methods temporal approximation discretization implies method asymptotic preserving low order variant provably stable mass conservative high order variant least second order accurate space time well defined mass conservative scheme formulated properties proven numerical simulations provided illustrate proposed methodology
"In this paper, we propose and numerically solve a Biot–Darcy model for describing the coupled geomechanics and fluid flow in fractured poroelastic media. The poroelastic media is governed by the quasi-static Biot model, and the fluid flow in the fracture is governed by the reduced dimensional Darcy model. The two models are fully coupled by four interface conditions. A spatial discretization is constructed by combining different methods, the polyhedral weighted discontinuous Galerkin method for the quasi-static Biot model, and the continuous Galerkin finite element method for the reduced dimensional fracture model. We prove the existence and uniqueness, stability, a priori error estimates for the solution of semidiscrete and fully discrete formulation with the backward Euler scheme. Through numerical experiments, we verify the convergence order of the numerical method, design a comparative case to validate the model, test the robustness of the numerical method in solving the Biot–Darcy model with different permeability coefficients, fracture morphologies, and curved fracture, simulate hydraulic fracturing. The results demonstrate that the model we have investigated is valid, and our numerical method is robust enough to accurately simulate the coupling of geomechanics and fluid flow in fracture poroelastic media.",paper propose numerically solve biot darcy model describing coupled geomechanics fluid flow fractured poroelastic media poroelastic media governed quasi static biot model fluid flow fracture governed reduced dimensional darcy model two models fully coupled four interface conditions spatial discretization constructed combining different methods polyhedral weighted discontinuous galerkin method quasi static biot model continuous galerkin finite element method reduced dimensional fracture model prove existence uniqueness stability priori error estimates solution semidiscrete fully discrete formulation backward euler scheme numerical experiments verify convergence order numerical method design comparative case validate model test robustness numerical method solving biot darcy model different permeability coefficients fracture morphologies curved fracture simulate hydraulic fracturing results demonstrate model investigated valid numerical method robust enough accurately simulate coupling geomechanics fluid flow fracture poroelastic media
"Equipping English-as-a-Foreign-Language (EFL) learners with critical thinking (CT) skills offers significant benefits for both their academic advancement and future careers. Due to its pivotal role in learners' lives, the pursuit of more effective interventions for developing CT skills among EFL learners remains ongoing. This study aims to examine how CT skills are fostered among EFL learners in higher education, as evidenced in published research. To achieve this, a secondary research method using the PRISMA systematic review framework was employed. The review systematically analyzed 47 published research articles selected from an initial search of 11,143 titles across eight online databases. The findings of the review are as follows. First, fostering CT skills in higher education has become a major concern among EFL educators and researchers in Asia. Second, the review highlights that both immersion and infusion approaches can effectively promote CT development, particularly when used in combination. Key strategies such as self-learning, discussion, and mentoring are central to these approaches, with a strong emphasis on collaborative learning and reflective practices. The duration of interventions plays a critical role: longer and more sustained programs tend to produce stronger outcomes. Additionally, the learning environment—whether traditional, web-based, or blended—impacts the effectiveness of CT interventions. Among these, blended learning consistently yields favorable results, suggesting that integrating in-class and online instruction is highly recommended. Finally, methodological factors generally enhance the outcomes of CT instruction. In contrast, personal factors (i.e., learner-related variables) can both support and hinder the development of CT skills, depending on context. Therefore, these factors may act as either enablers or inhibitors of CT skill acquisition under different conditions. This review proposes several educational implications to improve the cultivation of CT skills among EFL learners in higher education, focusing on instructional strategies, assessment methods, and learning resources.",equipping english foreign language efl learners critical thinking skills offers significant benefits academic advancement future careers due pivotal role learners lives pursuit effective interventions developing skills among efl learners remains ongoing study aims examine skills fostered among efl learners higher education evidenced published research achieve secondary research method using prisma systematic review framework employed review systematically analyzed published research articles selected initial search titles across eight online databases findings review follows first fostering skills higher education become major concern among efl educators researchers asia second review highlights immersion infusion approaches effectively promote development particularly used combination key strategies self learning discussion mentoring central approaches strong emphasis collaborative learning reflective practices duration interventions plays critical role longer sustained programs tend produce stronger outcomes additionally learning environment whether traditional web based blended impacts effectiveness interventions among blended learning consistently yields favorable results suggesting integrating class online instruction highly recommended finally methodological factors generally enhance outcomes instruction contrast personal factors learner related variables support hinder development skills depending context therefore factors may act either enablers inhibitors skill acquisition different conditions review proposes several educational implications improve cultivation skills among efl learners higher education focusing instructional strategies assessment methods learning resources
"In recent times, the artificial intelligence (AI) based deep neural networks (DNNs) have attracted much attention from researchers. The aforesaid tools provide best solutions in many real world applications. On the other hand, combining the aforementioned tools with the traditional and fractional calculus tools, significant results can be created for comprehensive analysis of many problems of real world phenomenon and engineering disciplines. Systems of differential equations play important roles in modeling various real world problems involving ordinary or fractional order derivatives. Keeping the mentioned importance in minds, this paper investigates a coupled system of fractional integro-differential equations for qualitative and computational analysis. Utilizing the well known conformable fractional derivative, sufficient results are developed to analyze the solution of the system for existence and uniqueness. The fixed point concepts are utilized to advances to the desired results. Stability results are deduced by using the concept of Ulam–Hyers. At the last section the numerical aspect are investigated of the problem by using RK-4 (Runge–Kutta of order four) method under the mentioned fractional derivative. Also, the mentioned DNNs techniques have used to analysis the considered problems from AI perspectives. To demonstrate our adopted results, some important real world examples are presented at the end. The Levenberg–Marquardt training algorithm is used for classifications of various results including root mean squared error (RMSE), mean square error (MSE), regression coefficient by taking a specified numbers of neuron and epoches. Various graphical illustrations for training, testing, validation and all data are presented.",recent times artificial intelligence based deep neural networks dnns attracted much attention researchers aforesaid tools provide best solutions many real world applications hand combining aforementioned tools traditional fractional calculus tools significant results created comprehensive analysis many problems real world phenomenon engineering disciplines systems differential equations play important roles modeling various real world problems involving ordinary fractional order derivatives keeping mentioned importance minds paper investigates coupled system fractional integro differential equations qualitative computational analysis utilizing well known conformable fractional derivative sufficient results developed analyze solution system existence uniqueness fixed point concepts utilized advances desired results stability results deduced using concept ulam hyers last section numerical aspect investigated problem using runge kutta order four method mentioned fractional derivative also mentioned dnns techniques used analysis considered problems perspectives demonstrate adopted results important real world examples presented end levenberg marquardt training algorithm used classifications various results including root mean squared error rmse mean square error mse regression coefficient taking specified numbers neuron epoches various graphical illustrations training testing validation data presented
"Creativity is a fundamental competency of the 21st century and its cultivation—particularly in science education—has attracted substantial interest. Indeed, enablers of creativity represent a dominant field in this regard. Based on “Confluence approaches”, the study chose 14,041 fourth graders from 309 primary schools from a city in the Central Region of China, all of which completed online questionnaires that looked into the individual enablers and social enablers of creativity in science. Results of the descriptive analysis of individual enablers showed that there was a decrease observed in students’ attitude toward science, interest in science, engagement in science learning, and career orientation toward science. While the LASSO and nomogram results indicated six common variables of social enablers including teachers’ concerns about students’ interests, individualized teaching, collaborative group teaching, the infrastructure of science, after-school service in science subjects, and parent engagement in daily life. These variables consistently emerged as pivotal when predicting the correlation between social and individual enablers of creativity in science. Teaching methods (individualized teaching, collaborative group teaching) were identified as the most influential social enablers on individual enablers of creativity in science. These findings provide valuable insight into what teachers, schools, and families should concentrate on to effectively support the expression and enhancement of student creativity in science.",creativity fundamental competency century cultivation particularly science education attracted substantial interest indeed enablers creativity represent dominant field regard based confluence approaches study chose fourth graders primary schools city central region china completed online questionnaires looked individual enablers social enablers creativity science results descriptive analysis individual enablers showed decrease observed students attitude toward science interest science engagement science learning career orientation toward science lasso nomogram results indicated six common variables social enablers including teachers concerns students interests individualized teaching collaborative group teaching infrastructure science school service science subjects parent engagement daily life variables consistently emerged pivotal predicting correlation social individual enablers creativity science teaching methods individualized teaching collaborative group teaching identified influential social enablers individual enablers creativity science findings provide valuable insight teachers schools families concentrate effectively support expression enhancement student creativity science
"A focus on process and not on product should be the emphasis of research into creative thinking. This is a basic notion characterizing the thinking of Heinz Werner, who already in 1937 discussed the preoccupation with product at the expense of process as a basic problem for research and theory. In this paper, the type of process that Werner is referring to is addressed. Two test cases are presented: that of metaphor comprehension, and that of metaphor production which, itself, is a prime example of creative thinking. The focus of the discussion of metaphor production is on the stages of incubation and illumination/insight. The use of a microgenetic procedure, via which metaphor production (creative thinking) is investigated in real time, focusing on ongoing experience while producing a metaphor, is promoted here.",focus process product emphasis research creative thinking basic notion characterizing thinking heinz werner already discussed preoccupation product expense process basic problem research theory paper type process werner referring addressed two test cases presented metaphor comprehension metaphor production prime example creative thinking focus discussion metaphor production stages incubation illumination insight use microgenetic procedure via metaphor production creative thinking investigated real time focusing ongoing experience producing metaphor promoted
"Research suggests that social exclusion enhances the retaliatory malevolent creativity of the excluded. However, its influence on the retaliatory malevolent creativity of bystanders, another key group in social exclusion, remains unknown. Two studies were conducted to investigate how social exclusion, particularly when coupled with attacks from excluders or the excluded, shapes bystanders’ retaliatory malevolent creativity. Study 1 and Study 2 recruited 69 and 74 participants, respectively. In both studies, participants were randomly assigned into the social exclusion group or control group. The social exclusion group underwent two blocks of social exclusion manipulation, while the control group experienced two blocks of sham exclusion manipulation. Following each manipulation, participants completed a retaliatory malevolent creative ideation task (MCT) targeting the excluded in Study 1 and excluder in Study 2. Study 1 showed that, only in the second block, the exclusion group exhibited higher MCT performance (fluency, originality, and malevolence) targeting the excluded than the control group. In study 2, the excluded group demonstrated significantly higher MCT performance targeting excluder exclusively in the first block than the control group. This exclusion effect on bystander’s retaliatory malevolent creativity was not mediated by the bystander’s sympathy or aggression. These findings indicate that simply witnessing social exclusion may increase bystanders’ retaliatory malevolent creativity toward both excluder and the excluded. Notably, the timing of this effect varies depending on the target of the MCT task, and its underlying mechanisms may involve factors other than aggression or empathy such as moral disengagement.",research suggests social exclusion enhances retaliatory malevolent creativity excluded however influence retaliatory malevolent creativity bystanders another key group social exclusion remains unknown two studies conducted investigate social exclusion particularly coupled attacks excluders excluded shapes bystanders retaliatory malevolent creativity study study recruited participants respectively studies participants randomly assigned social exclusion group control group social exclusion group underwent two blocks social exclusion manipulation control group experienced two blocks sham exclusion manipulation following manipulation participants completed retaliatory malevolent creative ideation task mct targeting excluded study excluder study study showed second block exclusion group exhibited higher mct performance fluency originality malevolence targeting excluded control group study excluded group demonstrated significantly higher mct performance targeting excluder exclusively first block control group exclusion effect bystander retaliatory malevolent creativity mediated bystander sympathy aggression findings indicate simply witnessing social exclusion may increase bystanders retaliatory malevolent creativity toward excluder excluded notably timing effect varies depending target mct task underlying mechanisms may involve factors aggression empathy moral disengagement
"This study explores the nonlinear hemodynamics of ternary hybrid nanoparticles (THNPs) in a diverging, ciliated microtube, accounting for interfacial nanolayer effects, electromagnetic forces, and cilia-driven propulsion. A fractional second-grade fluid model captures memory-dependent viscoelastic behavior. The governing nonlinear equations are analytically solved via the Homotopy Perturbation Method (HPM), yielding rapidly converging series solutions. Results show that the fractional parameter enhances axial velocity, while relaxation time impedes flow near the tube center. Lorentz forces boost flow rates, whereas Hall and ion-slip currents exert a dampening effect. Meanwhile, nanolayer interactions intensify wall shear stress but hinder thermal efficiency by reducing heat transfer. A comprehensive dataset derived from the HPM solution was used to train, test, and validate an artificial neural network using the Backpropagation Levenberg–Marquardt scheme (ANN-BPLMS), achieving high predictive accuracy (99.99% testing, 99.996% cross-validation). These insights have significant implications for magnetically guided drug delivery, artificial cilia systems, and nanoscale biomedical transport. By integrating fractional dynamics, electrokinetics, and interfacial physics, this work advances the modeling of complex physiological microflows.",study explores nonlinear hemodynamics ternary hybrid nanoparticles thnps diverging ciliated microtube accounting interfacial nanolayer effects electromagnetic forces cilia driven propulsion fractional second grade fluid model captures memory dependent viscoelastic behavior governing nonlinear equations analytically solved via homotopy perturbation method hpm yielding rapidly converging series solutions results show fractional parameter enhances axial velocity relaxation time impedes flow near tube center lorentz forces boost flow rates whereas hall ion slip currents exert dampening effect meanwhile nanolayer interactions intensify wall shear stress hinder thermal efficiency reducing heat transfer comprehensive dataset derived hpm solution used train test validate artificial neural network using backpropagation levenberg marquardt scheme ann bplms achieving high predictive accuracy testing cross validation insights significant implications magnetically guided drug delivery artificial cilia systems nanoscale biomedical transport integrating fractional dynamics electrokinetics interfacial physics work advances modeling complex physiological microflows
"With the development of the applied discipline of commutative quaternions, the commutative quaternion equality constrained least squares (CQLSE) problem is gaining more and more attention as an effective tool. However, the knowledge gap in numerous CQLSE problems is now unresolved. This paper, by means of the complex representation matrix of a commutative quaternion matrix, first studies the QR decomposition and generalized singular value decomposition (GSVD) of commutative quaternion matrices, and gives the corresponding theorems and algorithms. In addition, the algorithms for solving the CQLSE problem based on QR decomposition and GSVD of commutative quaternion matrices are given in this paper. Finally, numerical experiments show the effectiveness of the algorithms proposed in this paper.",development applied discipline commutative quaternions commutative quaternion equality constrained least squares cqlse problem gaining attention effective tool however knowledge gap numerous cqlse problems unresolved paper means complex representation matrix commutative quaternion matrix first studies decomposition generalized singular value decomposition gsvd commutative quaternion matrices gives corresponding theorems algorithms addition algorithms solving cqlse problem based decomposition gsvd commutative quaternion matrices given paper finally numerical experiments show effectiveness algorithms proposed paper
"In this paper, we propose an adaptive approach, based on mesh refinement or parametric enrichment with polynomial degree adaption, for numerical solution of convection dominated equations with random input data. A parametric system emerged from an application of stochastic Galerkin approach is discretized by using symmetric interior penalty Galerkin (SIPG) method with upwinding for the convection term in the spatial domain. We derive a residual-based error estimator contributed by the error due to the SIPG discretization, the (generalized) polynomial chaos discretization in the stochastic space, and data oscillations. Then, the reliability of the proposed error estimator, an upper bound for the energy error up to a multiplicative constant, is shown. Moreover, to balance the errors stemmed from spatial and stochastic spaces, the truncation error emerged from Karhunen–Loève expansion is considered in the numerical simulations. Last, several benchmark examples including a random diffusivity parameter, a random convectivity parameter, random diffusivity/convectivity parameters, and a random (jump) discontinuous diffusivity parameter, are tested to illustrate the performance of the proposed estimator.",paper propose adaptive approach based mesh refinement parametric enrichment polynomial degree adaption numerical solution convection dominated equations random input data parametric system emerged application stochastic galerkin approach discretized using symmetric interior penalty galerkin sipg method upwinding convection term spatial domain derive residual based error estimator contributed error due sipg discretization generalized polynomial chaos discretization stochastic space data oscillations reliability proposed error estimator upper bound energy error multiplicative constant shown moreover balance errors stemmed spatial stochastic spaces truncation error emerged karhunen expansion considered numerical simulations last several benchmark examples including random diffusivity parameter random convectivity parameter random diffusivity convectivity parameters random jump discontinuous diffusivity parameter tested illustrate performance proposed estimator
"Causal inference is a crucial framework in a variety of fields, such as economics, healthcare, and social science. In this context, data-driven machine learning models have become more popular for estimating the effects of treatments. One fundamental technique for causal inference is structural equation modeling (SEM), which makes it possible to estimate the relationships between variables. However, conventional SEM methods need help with the complexities of structural causal models (SCM) in real-world data, including latent confounders, nonlinear relationships, and challenges in accurately specifying model structures. These limitations could cause the interpretation or biased causal effects. To address these challenges, we introduce a new proposed method combining the piecewise structural equation modeling (PSEM) with the backdoor criterion, named PSEMBC. The main innovation of PSEMBC is the estimation of causal effects with the use of a linear SCM model, which captures complex relationships and interactions in the data. We demonstrate the value of PSEMBC for precisely and reliably identifying the average treatment effect (ATE) in simulated and real-world datasets utilizing a comparative study with current causal inference approaches.",causal inference crucial framework variety fields economics healthcare social science context data driven machine learning models become popular estimating effects treatments one fundamental technique causal inference structural equation modeling sem makes possible estimate relationships variables however conventional sem methods need help complexities structural causal models scm real world data including latent confounders nonlinear relationships challenges accurately specifying model structures limitations could cause interpretation biased causal effects address challenges introduce new proposed method combining piecewise structural equation modeling psem backdoor criterion named psembc main innovation psembc estimation causal effects use linear scm model captures complex relationships interactions data demonstrate value psembc precisely reliably identifying average treatment effect ate simulated real world datasets utilizing comparative study current causal inference approaches
"This article investigates the pricing problem of vulnerable options. We consider the default risk of these options, and assume that the dynamic evolution of the underlying asset price and its instantaneous variance can be described by an affine model that incorporates stochastic volatility with simultaneous jumps. The default intensity and risk-free interest rate are assumed to be random and satisfy the CIR model. An analytical formula for the option price is derived using Fourier transform techniques. Based on this pricing formula, some models for the price of the underlying asset of option and for the default intensity of the option seller are calibrated and the model parameters are estimated. The implied volatilities corresponding to the theoretical prices of the option derived from the models are calculated and compared with the implied volatilities calculated from the option market prices.",article investigates pricing problem vulnerable options consider default risk options assume dynamic evolution underlying asset price instantaneous variance described affine model incorporates stochastic volatility simultaneous jumps default intensity risk free interest rate assumed random satisfy cir model analytical formula option price derived using fourier transform techniques based pricing formula models price underlying asset option default intensity option seller calibrated model parameters estimated implied volatilities corresponding theoretical prices option derived models calculated compared implied volatilities calculated option market prices
"This exploratory study examined the moderator role of representational fluency in mental imagery's effect on scientific creativity. The study was conducted with the participation of 125 (60: girls, 65: boys) elementary school students aged 9–10 in Düzce, Türkiye, in the 2024–2025 academic year. The study's hypotheses were analyzed and interpreted using the Process Macro program (for SPSS) developed by Hayes. The moderator variable effect of representational fluency was tested with Model 1, the moderator effect of the sub-dimensions of representational fluency was tested with Model 2, and the moderating effect of gender on the moderator variable was tested with Model 3. The study results show that representational fluency has a moderating role in the effect of mental imagery on scientific creativity. In addition, it was determined that this effect was not significant at different levels of representational fluency. However, only high representational fluency positively regulated the effect of mental imagery on scientific creativity. It was concluded that the ""connection"" dimension came to the fore in the moderating effect of representational fluency. Gender was a moderator variable affecting the moderating effect of representational fluency on the effect of mental imagery on scientific creativity. Theoretical explanations and implications related to the research results were shared.",exploratory study examined moderator role representational fluency mental imagery effect scientific creativity study conducted participation girls boys elementary school students aged düzce türkiye academic year study hypotheses analyzed interpreted using process macro program spss developed hayes moderator variable effect representational fluency tested model moderator effect sub dimensions representational fluency tested model moderating effect gender moderator variable tested model study results show representational fluency moderating role effect mental imagery scientific creativity addition determined effect significant different levels representational fluency however high representational fluency positively regulated effect mental imagery scientific creativity concluded connection dimension came fore moderating effect representational fluency gender moderator variable affecting moderating effect representational fluency effect mental imagery scientific creativity theoretical explanations implications related research results shared
"In this paper, we show the increasing stability of the inverse source problem for the biharmonic operator. Especially, we develop a new, unified approach to study the increasing stability in any dimension. The estimate consists of the Lipschitz-type data discrepancy and the high-frequency tail of the source function, where the latter decreases as the upper bound of the frequency increases. The method is based on the Fourier transform and explicit bounds for the analytic continuation.",paper show increasing stability inverse source problem biharmonic operator especially develop new unified approach study increasing stability dimension estimate consists lipschitz type data discrepancy high frequency tail source function latter decreases upper bound frequency increases method based fourier transform explicit bounds analytic continuation
"This paper studies the pricing of European options using a sub-mixed fractional Geometric Brownian Motion (GBM) model and explores different hedging strategies. First, we analyze S&P market price data, using statistical tests to identify key properties, and show that the data have long memory and non-stationary features. Next, we introduce the sub-mixed fractional GBM model as a better tool for stock price prediction. We then propose two investment strategies to reduce unsystematic risk, based on the investor’s wealth. The first strategy uses a mixed hedging approach, allowing investors to hedge their full position with the contract funds, leading to the European option price. The second strategy lets investors hedge only part of their risk, depending on their risk tolerance.",paper studies pricing european options using sub mixed fractional geometric brownian motion gbm model explores different hedging strategies first analyze market price data using statistical tests identify key properties show data long memory non stationary features next introduce sub mixed fractional gbm model better tool stock price prediction propose two investment strategies reduce unsystematic risk based investor wealth first strategy uses mixed hedging approach allowing investors hedge full position contract funds leading european option price second strategy lets investors hedge part risk depending risk tolerance
"This paper introduces a new, low computational-cost method of retraction (projection) on the special orthogonal group used in the ICA problem. In ICA geodesic algorithms, the step of retraction onto the optimization manifold is usually performed via matrix exponentiation, which is a computationally expensive operation. In order to increase the speed of the algorithm, a new retraction method was introduced based on the real Schur decomposition. The algorithm was tested using the cost function, both parametric (mutual information approximation) and semi-parametric (kernel method). The simulations showed a significant increase in the speed of the ICA algorithm, which indicates the practical usage potential of this method in on-line applications.",paper introduces new low computational cost method retraction projection special orthogonal group used ica problem ica geodesic algorithms step retraction onto optimization manifold usually performed via matrix exponentiation computationally expensive operation order increase speed algorithm new retraction method introduced based real schur decomposition algorithm tested using cost function parametric mutual information approximation semi parametric kernel method simulations showed significant increase speed ica algorithm indicates practical usage potential method line applications
"Random number generators are extensively used in science. Generating pseudo-random numbers is the base for many data analysis techniques in computational statistics. This is the case, for instance, of most of the Bayesian methods, which are enabled by means of samplers such as the well-known Gibbs sampler and the Metropolis–Hastings. These classical Markov Chain Monte Carlo samplers are designed to generate a sequence of numbers that, under certain conditions, converge to a sequence that behaves as if sampled from a user-defined target distribution. In general, the number of iterations required to reach such convergence is not deterministic. There are several statistical tests for identifying that convergence has not yet been achieved, but not for actually signaling convergence. The present work introduces an exact non-parametric sequential test for signaling the convergence of random number generators in general. The solution is derived in the light of the type I error probability spending approach.",random number generators extensively used science generating pseudo random numbers base many data analysis techniques computational statistics case instance bayesian methods enabled means samplers well known gibbs sampler metropolis hastings classical markov chain monte carlo samplers designed generate sequence numbers certain conditions converge sequence behaves sampled user defined target distribution general number iterations required reach convergence deterministic several statistical tests identifying convergence yet achieved actually signaling convergence present work introduces exact non parametric sequential test signaling convergence random number generators general solution derived light type error probability spending approach
"The paper presents an in-depth exploration of the multinode Shepard interpolant on a regular rectangular grid, demonstrating its efficacy in reconstructing surfaces from DEM data. Additionally, we study the approximation order associated to this interpolant and present a detailed algorithm for reconstructing surfaces. Numerical tests showcase the effectiveness of the proposed algorithm.",paper presents depth exploration multinode shepard interpolant regular rectangular grid demonstrating efficacy reconstructing surfaces dem data additionally study approximation order associated interpolant present detailed algorithm reconstructing surfaces numerical tests showcase effectiveness proposed algorithm
"In this study, a machine learning radial basis deep neural network process is presented with the optimization of Bayesian regularization for solving the fractional order chaotic financial system, which is divided into four different categories. This novel designed machine learning procedure contains two hidden layers with 15 and 30 neurons in the feed-forward neural network, radial basis activation function, and optimization of Bayesian regularization. A fractional Caputo derivative is used to present more reliable solutions of the chaotic financial system as compared to integer order. The construction of the data is performed through the Adam solver to reduce the mean square error by splitting the statics into training 80 %, while 9 %, 9 % for both testing and authentication. Three different model’s cases are used to check the correctness of the deep neural network solver through the comparison and small absolute error. Moreover, the reliability of the novel designed solver is observed by using different measures based on regression coefficient, state transition, error histogram and best training values.",study machine learning radial basis deep neural network process presented optimization bayesian regularization solving fractional order chaotic financial system divided four different categories novel designed machine learning procedure contains two hidden layers neurons feed forward neural network radial basis activation function optimization bayesian regularization fractional caputo derivative used present reliable solutions chaotic financial system compared integer order construction data performed adam solver reduce mean square error splitting statics training testing authentication three different model cases used check correctness deep neural network solver comparison small absolute error moreover reliability novel designed solver observed using different measures based regression coefficient state transition error histogram best training values
"This study investigated the mediation mechanisms underlying the internal evolution of high-order thinking skills (HOTSs) among 132 seventh-grade students in China, focusing on the hierarchical relationships between specific HOTSs (S-HOTSs: evidence-based reasoning, model construction, systematic thinking and critical discussion skills) and complex HOTSs (C-HOTSs: critical-thinking and problem-solving skills). Using a longitudinal design with formative and summative assessments, the study employed single and chain mediation models to analyze HOTSs development during a 9-session biological science course on photosynthesis, respiration and the food chain. The results revealed that S-HOTS indirectly promote C–HOTS through sequential mediation pathways, with stronger effects from chain mediators compared to direct effects. Notably, model construction and systematic thinking skills emerged as pivotal mediators, bridging early-developing HOTSs and later-developing HOTSs. These findings highlighted the importance of stepwise instructional strategies aligned with hierarchical skill evolution, offering empirical insights for designing curricula that foster the development of HOTSs in science education.",study investigated mediation mechanisms underlying internal evolution high order thinking skills hotss among seventh grade students china focusing hierarchical relationships specific hotss hotss evidence based reasoning model construction systematic thinking critical discussion skills complex hotss hotss critical thinking problem solving skills using longitudinal design formative summative assessments study employed single chain mediation models analyze hotss development session biological science course photosynthesis respiration food chain results revealed hots indirectly promote hots sequential mediation pathways stronger effects chain mediators compared direct effects notably model construction systematic thinking skills emerged pivotal mediators bridging early developing hotss later developing hotss findings highlighted importance stepwise instructional strategies aligned hierarchical skill evolution offering empirical insights designing curricula foster development hotss science education
"Test anxiety is generally perceived to impede L2 achievement. One focus of test anxiety studies has been on predictors of test anxiety, aiming to detect factors that may reduce the harm of test anxiety to L2 achievement. Considerable efforts have been invested in psychological and contextual factors at the neglect of cognitive factors such as uncertainty tolerance. The current study examined the interplay between uncertainty tolerance and test anxiety in predicting L2 achievement, with a focus on the mediation of test anxiety between uncertainty tolerance and L2 achievement. Participants involved 400 Grade 10 students from a high school in East China. Structural equation modeling results showed that (1) test anxiety was negatively associated with L2 achievement, (2) uncertainty tolerance was positively associated with L2 achievement, and (3) test anxiety partly mediated the relationship between uncertainty tolerance and L2 achievement. The study provided theoretical and practical implications for L2 instruction.",test anxiety generally perceived impede achievement one focus test anxiety studies predictors test anxiety aiming detect factors may reduce harm test anxiety achievement considerable efforts invested psychological contextual factors neglect cognitive factors uncertainty tolerance current study examined interplay uncertainty tolerance test anxiety predicting achievement focus mediation test anxiety uncertainty tolerance achievement participants involved grade students high school east china structural equation modeling results showed test anxiety negatively associated achievement uncertainty tolerance positively associated achievement test anxiety partly mediated relationship uncertainty tolerance achievement study provided theoretical practical implications instruction
"To compare discourses related to different ways of thinking, a bibliometric analysis was performed on the Dimensions database of research documents. Terms related to 78 ways of thinking were ranked according to the number of documents containing them over the last five years (2020 to 2024). Twenty ways of thinking were subjected to further analyses, examining (i) those that are most prevalent (with the top five being critical thinking, design thinking, creative thinking, systems thinking then computational thinking); (ii) the different suffixes appended to each term (with critical thinking mostly referred to as a skill, systems thinking mostly referred to as an approach and design thinking almost equally referred to as a method, approach and process); (iii) the proportion of documents focusing on each of them, rather than simply referring to them (with computational thinking having the highest such proportion); (iv) their rise to prevalence since 1975 (with critical thinking and systems thinking slowly gaining prevalence compared to the more recent rise of design thinking and computational thinking); (v) the frequency with which they are referred to together (with critical thinking and creative thinking co-occurring most often); (vi) their distribution across various academic disciplines (with futures thinking being quite evenly distributed, but computational thinking being highly concentrated); (vii) their distribution across the Sustainable Development Goals (with critical thinking, systems thinking and design thinking being most prevalent in this respect). The findings and visualisations provide a useful basis for identifying, comparing, selecting and combining different ways of thinking.",compare discourses related different ways thinking bibliometric analysis performed dimensions database research documents terms related ways thinking ranked according number documents containing last five years twenty ways thinking subjected analyses examining prevalent top five critical thinking design thinking creative thinking systems thinking computational thinking different suffixes appended term critical thinking mostly referred skill systems thinking mostly referred approach design thinking almost equally referred method approach process iii proportion documents focusing rather simply referring computational thinking highest proportion rise prevalence since critical thinking systems thinking slowly gaining prevalence compared recent rise design thinking computational thinking frequency referred together critical thinking creative thinking occurring often distribution across various academic disciplines futures thinking quite evenly distributed computational thinking highly concentrated vii distribution across sustainable development goals critical thinking systems thinking design thinking prevalent respect findings visualisations provide useful basis identifying comparing selecting combining different ways thinking
"In this paper, we propose a Gauss–Seidel type inertial proximal alternating linearized minimization method with incremental aggregated gradient (IAG-GiPALM) for solving a class of nonconvex and nonsmooth composite optimization problems, whose objective function is the sum of a finite number of smooth nonconvex functions and nonsmooth weakly convex functions. This new algorithm inherits the advantages of the Gauss–Seidel type inertial proximal alternating linearized minimization method (GiPALM) and the incremental aggregated proximal method. Under some mild conditions, we prove that any limit point of the sequence generated by IAG-GiPALM is a critical point of the optimization problems. Moreover, we establish the global convergence and convergence rate of the algorithm under the Kurdyka-Łojasiewicz property. In addition, some numerical results are conducted to demonstrate the efficiency of the new method.",paper propose gauss seidel type inertial proximal alternating linearized minimization method incremental aggregated gradient iag gipalm solving class nonconvex nonsmooth composite optimization problems whose objective function sum finite number smooth nonconvex functions nonsmooth weakly convex functions new algorithm inherits advantages gauss seidel type inertial proximal alternating linearized minimization method gipalm incremental aggregated proximal method mild conditions prove limit point sequence generated iag gipalm critical point optimization problems moreover establish global convergence convergence rate algorithm kurdyka ojasiewicz property addition numerical results conducted demonstrate efficiency new method
"This paper presents a new least-squares support vector regression (LSSVR) approach, based on operational matrices for efficiently solving time-fractional integro-differential equations. By employing Chebyshev polynomials as the LSSVR kernel, we reformulate the problem in a fully matrix-based framework that significantly accelerates training compared to traditional symbolic LSSVR methods, while preserving the exponential convergence rate. The method’s versatility is validated across various Caputo fractional integro-differential problems, including nonlinear, variable-order fractional derivative and multi-dimensional cases. Numerical experiments confirm its superior performance and accuracy relative to existing numerical solution methods.",paper presents new least squares support vector regression lssvr approach based operational matrices efficiently solving time fractional integro differential equations employing chebyshev polynomials lssvr kernel reformulate problem fully matrix based framework significantly accelerates training compared traditional symbolic lssvr methods preserving exponential convergence rate method versatility validated across various caputo fractional integro differential problems including nonlinear variable order fractional derivative multi dimensional cases numerical experiments confirm superior performance accuracy relative existing numerical solution methods
"The primal–dual hybrid gradient (PDHG) algorithm has been extensively studied and utilized for solving bilinear saddle point problems due to its inexpensive computations. Most recently, a prediction–correction PDHG (PC-PDHG) algorithm has been proposed in literature for solving linearly constrained convex optimization problems, which improves the convergence condition of PDHG and accelerates its numerical performance. In this paper, we introduce a dual–primal version of the newly developed PC-PDHG algorithm and extend its applicability to general bilinear saddle point problems, resulting in the generalized new prediction–correction dual–primal hybrid gradient (GNPC-DPHG) algorithm. In many instances, the proposed algorithm significantly reduces the number of iterations compared to the original algorithm, while only adding a small amount of computational cost per iteration, thereby enhancing the convergence rate. Furthermore, we provide a rigorous proof of the global convergence of the proposed GNPC-DPHG algorithm. Finally, numerical experiments demonstrate that the proposed algorithm can achieve faster convergence compared to the original algorithm.",primal dual hybrid gradient pdhg algorithm extensively studied utilized solving bilinear saddle point problems due inexpensive computations recently prediction correction pdhg pdhg algorithm proposed literature solving linearly constrained convex optimization problems improves convergence condition pdhg accelerates numerical performance paper introduce dual primal version newly developed pdhg algorithm extend applicability general bilinear saddle point problems resulting generalized new prediction correction dual primal hybrid gradient gnpc dphg algorithm many instances proposed algorithm significantly reduces number iterations compared original algorithm adding small amount computational cost per iteration thereby enhancing convergence rate furthermore provide rigorous proof global convergence proposed gnpc dphg algorithm finally numerical experiments demonstrate proposed algorithm achieve faster convergence compared original algorithm
"This paper presents the upper bounds for eigenvalues of the symmetric substochastic matrices, which are related to the measure of irreducibility of matrices. Then three applications in different fields are presented to indicate further applications. In the first one, the bounds for eigenvalues are used to prove that a class of iteration systems could reach a consensus under certain conditions. In the second one, they are used to prove the flocking behaviors of the Cucker–Smale model under rooted leadership. In the last one, a new approach to estimate the upper bounds for eigenvalues of nonnegative matrices is provided.",paper presents upper bounds eigenvalues symmetric substochastic matrices related measure irreducibility matrices three applications different fields presented indicate applications first one bounds eigenvalues used prove class iteration systems could reach consensus certain conditions second one used prove flocking behaviors cucker smale model rooted leadership last one new approach estimate upper bounds eigenvalues nonnegative matrices provided
"This paper is concerned with problems of scattering of time-harmonic acoustic waves by a two-layered medium with a non-locally perturbed boundary (called a rough boundary in this paper) in two dimensions, where a Dirichlet or impedance boundary condition is imposed on the boundary. The two-layered medium is composed of two unbounded media with different physical properties and the interface between the two media is considered to be a planar surface. We formulate the scattering problems considered as boundary value problems and present the result of the well-posedness of each boundary value problem by utilizing the integral equation method associated with the two-layered Green function. Moreover, we develop a Nyström method for numerically solving the boundary value problems considered, based on the proposed integral equation formulations. We establish the convergence results of the Nyström method with the convergence rates depending on the smoothness of the rough boundary. It is worth noting that in establishing the convergence results of the Nyström method, an essential role is played by the investigation of the asymptotic properties of the two-layered Green function for small and large arguments. Finally, numerical experiments are carried out to show the effectiveness of the Nyström method.",paper concerned problems scattering time harmonic acoustic waves two layered medium non locally perturbed boundary called rough boundary paper two dimensions dirichlet impedance boundary condition imposed boundary two layered medium composed two unbounded media different physical properties interface two media considered planar surface formulate scattering problems considered boundary value problems present result well posedness boundary value problem utilizing integral equation method associated two layered green function moreover develop nystr method numerically solving boundary value problems considered based proposed integral equation formulations establish convergence results nystr method convergence rates depending smoothness rough boundary worth noting establishing convergence results nystr method essential role played investigation asymptotic properties two layered green function small large arguments finally numerical experiments carried show effectiveness nystr method
"Histopolation, or interpolation on segments, is a mathematical technique used to approximate a function f over a given interval I = [ a , b ] by exploiting integral information over a set of subintervals of I . Unlike classical polynomial interpolation, which is based on pointwise function evaluations, histopolation reconstructs a function using integral data. However, similar to classical polynomial interpolation, histopolation suffers from the well-known Runge phenomenon when integral data are based on a grid with many equispaced nodes, as well as the Gibbs phenomenon when approximating discontinuous functions. In contrast, quasi-histopolation is designed to relax the strict requirement of passing through all the given data points. This inherent flexibility can reduce the likelihood of oscillatory behavior using, for example, rational approximation operators. In this work, we introduce a C ∞ rational quasi-histopolation operator, for bounded (integrable) functions, which reconstruct a function by defeating both the Runge and Gibbs phenomena. A key element of our approach is to blend local histopolation polynomials on a few nodes using multinode Shepard functions as blending functions. Several numerical experiments demonstrate the accuracy of our method.",histopolation interpolation segments mathematical technique used approximate function given interval exploiting integral information set subintervals unlike classical polynomial interpolation based pointwise function evaluations histopolation reconstructs function using integral data however similar classical polynomial interpolation histopolation suffers well known runge phenomenon integral data based grid many equispaced nodes well gibbs phenomenon approximating discontinuous functions contrast quasi histopolation designed relax strict requirement passing given data points inherent flexibility reduce likelihood oscillatory behavior using example rational approximation operators work introduce rational quasi histopolation operator bounded integrable functions reconstruct function defeating runge gibbs phenomena key element approach blend local histopolation polynomials nodes using multinode shepard functions blending functions several numerical experiments demonstrate accuracy method
"In the current work, we propose numerical anti-reflective boundary conditions (BCs) in the context of nonlocal problems of fractional differential type: the numerical linear algebra goal is a O ( N log N ) complexity of the resulting direct and iterative algorithms, accompanied by a qualitative better approximation, with the mitigation of boundary artifacts. In fact, for showing the quality of the numerical anti-reflective BCs, we compare various types of numerical BCs, including the anti-symmetric ones considered in the case of fractional differential problems for modeling reasons. More in detail, given important similarities between anti-symmetric and anti-reflective BCs, we compare them from the perspective of computational efficiency, by considering nontruncated and truncated versions, and also other standard numerical BCs such as periodic BCs or reflective/Neumann BCs. A short theoretical analysis and several numerical tests, tables, and visualizations are provided and critically discussed. The conclusion is that the truncated numerical anti-reflective BCs perform better, both in terms of low computational cost and accuracy.",current work propose numerical anti reflective boundary conditions bcs context nonlocal problems fractional differential type numerical linear algebra goal log complexity resulting direct iterative algorithms accompanied qualitative better approximation mitigation boundary artifacts fact showing quality numerical anti reflective bcs compare various types numerical bcs including anti symmetric ones considered case fractional differential problems modeling reasons detail given important similarities anti symmetric anti reflective bcs compare perspective computational efficiency considering nontruncated truncated versions also standard numerical bcs periodic bcs reflective neumann bcs short theoretical analysis several numerical tests tables visualizations provided critically discussed conclusion truncated numerical anti reflective bcs perform better terms low computational cost accuracy
"Despite the importance of critical evaluation of online information, many teachers struggle to integrate evaluation instruction in their disciplinary teaching. This study examined the contribution of a professional development (PD) program designed to foster teachers’ capabilities to cultivate critical evaluation competencies in their disciplines. Grounded in the AIR model of epistemic thinking, the program focused on supporting teachers in promoting students’ evaluation Aims, evaluation Ideals (criteria), and Reliable evaluation processes (strategies) across the regular curriculum. Teachers collaboratively learned, designed, and taught evaluation activities that were intended to foster these elements. We report on the program’s initial implementation with seven junior high school teachers and their students (N = 118), who were assigned to intervention and control groups. Using a mixed-method design, we administered assessments of information source evaluation to teachers and students before and after the intervention. Qualitative data sources included teacher interviews, teacher reflections, classroom observations, and PD meeting transcripts. Following the PD, teachers’ critical evaluation competencies became more elaborate and explicit. All of the teachers integrated information source evaluation in their instruction and adapted or designed evaluation activities that were tailored to their disciplines. Teachers faced challenges related to the complexities of integrating evaluation instruction in disciplinary topics, metacognitive instruction, and curriculum structure. Following the PD, students increasingly used source evaluation criteria, but there were no changes in their evaluation strategies. These results demonstrate the potential of the PD model and identify challenges that may inform future PD initiatives.",despite importance critical evaluation online information many teachers struggle integrate evaluation instruction disciplinary teaching study examined contribution professional development program designed foster teachers capabilities cultivate critical evaluation competencies disciplines grounded air model epistemic thinking program focused supporting teachers promoting students evaluation aims evaluation ideals criteria reliable evaluation processes strategies across regular curriculum teachers collaboratively learned designed taught evaluation activities intended foster elements report program initial implementation seven junior high school teachers students assigned intervention control groups using mixed method design administered assessments information source evaluation teachers students intervention qualitative data sources included teacher interviews teacher reflections classroom observations meeting transcripts following teachers critical evaluation competencies became elaborate explicit teachers integrated information source evaluation instruction adapted designed evaluation activities tailored disciplines teachers faced challenges related complexities integrating evaluation instruction disciplinary topics metacognitive instruction curriculum structure following students increasingly used source evaluation criteria changes evaluation strategies results demonstrate potential model identify challenges may inform future initiatives
"Creative thinking often requires overcoming habitual responses and forming remote associations, which may be supported or hindered by cognitive control. This study investigated how two components of inhibitory control—conflict detection and conflict inhibition—contribute to remote associative thinking under different types of experience constraints. Participants completed a Chinese Compound Remote Associates Task (CCAT) under three conditions (Non-Constrained, Perceptual-Constrained, and Semantic-Constrained), along with Flanker and Stroop tasks to measure distractor and prepotent response inhibition. Event-related potentials components (N200 and N450) and response time differences were used to assess conflict detection and inhibition, respectively. Both correlation and multiple regression analyses were conducted to evaluate the effects of these cognitive control processes on creative performance. The results showed that conflict inhibition consistently facilitated creative performance, while the effect of conflict detection varied across experience conditions: greater sensitivity to semantic conflict shortened response latency, whereas greater sensitivity to perceptual conflict reduced accuracy. These findings suggest that the role of inhibitory control in creativity is context-dependent and highlight the differentiated contributions of detection and inhibition in experience-constrained creative problem-solving.",creative thinking often requires overcoming habitual responses forming remote associations may supported hindered cognitive control study investigated two components inhibitory control conflict detection conflict inhibition contribute remote associative thinking different types experience constraints participants completed chinese compound remote associates task ccat three conditions non constrained perceptual constrained semantic constrained along flanker stroop tasks measure distractor prepotent response inhibition event related potentials components response time differences used assess conflict detection inhibition respectively correlation multiple regression analyses conducted evaluate effects cognitive control processes creative performance results showed conflict inhibition consistently facilitated creative performance effect conflict detection varied across experience conditions greater sensitivity semantic conflict shortened response latency whereas greater sensitivity perceptual conflict reduced accuracy findings suggest role inhibitory control creativity context dependent highlight differentiated contributions detection inhibition experience constrained creative problem solving
"The landscape of signal processing has witnessed significant advancements over the years, driven by the need for efficient and accurate techniques to analyze and manipulate complex data. While traditional Fourier-based methods have been instrumental, they often struggle to capture the intricate nuances of signals that exhibit non-linearity, non-stationarity, or dual characteristics. To address these limitations, a novel approach namely the Discrete Biquaternion Linear Canonical Transform (DBiQLCT) is introduced in this paper. Because of the inherent non-commutativity of biquaternion algebra multiplication, the Discrete Biquaternion Linear Canonical Transform (DBiQLCT) has three unique forms: left-sided DBiQLCT, right-sided DBiQLCT, and two-sided DBiQLCT. We first introduce a notion of DBiQLCT and subsequently investigate the connections between these transformations. Following that, we investigate the two-sided discrete biquaternion linear canonical transform (TDBiQLCT), revealing important fundamental features such as linearity, time shift, conjugate, and modulation. We also establish the Plancherel and convolution theorems associated with the two-sided DBiQLCT. Furthermore, applications of the proposed transform are discussed at the end.",landscape signal processing witnessed significant advancements years driven need efficient accurate techniques analyze manipulate complex data traditional fourier based methods instrumental often struggle capture intricate nuances signals exhibit non linearity non stationarity dual characteristics address limitations novel approach namely discrete biquaternion linear canonical transform dbiqlct introduced paper inherent non commutativity biquaternion algebra multiplication discrete biquaternion linear canonical transform dbiqlct three unique forms left sided dbiqlct right sided dbiqlct two sided dbiqlct first introduce notion dbiqlct subsequently investigate connections transformations following investigate two sided discrete biquaternion linear canonical transform tdbiqlct revealing important fundamental features linearity time shift conjugate modulation also establish plancherel convolution theorems associated two sided dbiqlct furthermore applications proposed transform discussed end
"We implement a novel, effective approximation of the conformal mapping of a polygonal annulus, i.e., a doubly connected domain with two polygonal boundaries, onto a concentric round annulus. We also provide numerical estimates of the modulus and the period.",implement novel effective approximation conformal mapping polygonal annulus doubly connected domain two polygonal boundaries onto concentric round annulus also provide numerical estimates modulus period
"To comprehend complex systems with multiple states, it is imperative to reveal the identity of these states by system outputs. Nevertheless, the mathematical models describing these systems often exhibit nonlinearity, making the solution of the parameter inverse problem from observed spatiotemporal data a challenging task. Starting from the observed data obtained from such systems, we propose a novel framework that facilitates the investigation of parameter identification for multi-state systems governed by spatiotemporal varying parametric partial differential equations. Our framework consists of two integral components: a constrained self-adaptive physics-informed neural networks, encompassing a sub-network, and a finite mixture model with Gaussian components, as our methodology for parameter identification and change-point detection. Through our scheme, we can accurately estimate the unknown varying parameters of the complex multi-state system. Furthermore, we have showcased the efficacy of our framework on two numerical cases: the 1D Burgers’ equation with time-varying parameters and the 2D wave equation with a space-varying parameter.",comprehend complex systems multiple states imperative reveal identity states system outputs nevertheless mathematical models describing systems often exhibit nonlinearity making solution parameter inverse problem observed spatiotemporal data challenging task starting observed data obtained systems propose novel framework facilitates investigation parameter identification multi state systems governed spatiotemporal varying parametric partial differential equations framework consists two integral components constrained self adaptive physics informed neural networks encompassing sub network finite mixture model gaussian components methodology parameter identification change point detection scheme accurately estimate unknown varying parameters complex multi state system furthermore showcased efficacy framework two numerical cases burgers equation time varying parameters wave equation space varying parameter
"In this paper we study the utility indifference valuation of defaultable bonds and credit default swaps (CDS) within a robust framework that incorporates the constant elasticity of variance (CEV) model. Within a reduced-form framework, we study an optimal investment problem in the presence of default where the stochastic default intensity is linked to a non-traded state variable that exhibits slow mean-reversion. For an optimal investment objective governed by a constant absolute risk aversion (CARA) utility function, we derive semi-closed asymptotic solutions to the value function and the indifference prices for defaultable bonds and CDS. We demonstrate that the indifference prices can be interpreted as a perturbation around the prices derived under a constant default intensity framework. Finally, we conduct a sensitivity analysis to evaluate the impact of various risk parameters on the derived prices.",paper study utility indifference valuation defaultable bonds credit default swaps cds within robust framework incorporates constant elasticity variance cev model within reduced form framework study optimal investment problem presence default stochastic default intensity linked non traded state variable exhibits slow mean reversion optimal investment objective governed constant absolute risk aversion cara utility function derive semi closed asymptotic solutions value function indifference prices defaultable bonds cds demonstrate indifference prices interpreted perturbation around prices derived constant default intensity framework finally conduct sensitivity analysis evaluate impact various risk parameters derived prices
"In this paper we propose and analyze a new fully-mixed finite element method for the coupled model arising from the Navier–Stokes equations, with variable viscosity, in an incompressible fluid, and the Darcy equations in an adjacent porous medium, so that suitable transmission conditions are considered on the corresponding interface. The approach is based on the introduction of the further unknowns in the fluid given by the velocity gradient and the pseudostress tensor, where the latter includes the respective diffusive and convective terms. The above allows the elimination from the system of the fluid pressure, which can be calculated later on via a postprocessing formula. In addition, the traces of the fluid velocity and the Darcy pressure become the Lagrange multipliers enforcing weakly the interface conditions. In this way, the resulting variational formulation is given by a nonlinear perturbation of a threefold saddle point operator equation, where the saddle-point in the middle of them is, in turn, perturbed. A fixed-point strategy along with the generalized Babuška-Brezzi theory, a related abstract result for perturbed saddle-point problems, the Banach-Nečas-Babuška theorem, and the Banach fixed-point theorem, are employed to prove the well-posedness of the continuous and Galerkin schemes. In particular, Raviart–Thomas and piecewise polynomial subspaces of the lowest degree for the domain unknowns, as well as continuous piecewise linear polynomials for the Lagrange multipliers on the interface, constitute a feasible choice of the finite element subspaces. Optimal error estimates and associated rates of convergence are then established. Finally, several numerical results illustrating the good performance of the method in 2D and confirming the theoretical findings, are reported.",paper propose analyze new fully mixed finite element method coupled model arising navier stokes equations variable viscosity incompressible fluid darcy equations adjacent porous medium suitable transmission conditions considered corresponding interface approach based introduction unknowns fluid given velocity gradient pseudostress tensor latter includes respective diffusive convective terms allows elimination system fluid pressure calculated later via postprocessing formula addition traces fluid velocity darcy pressure become lagrange multipliers enforcing weakly interface conditions way resulting variational formulation given nonlinear perturbation threefold saddle point operator equation saddle point middle turn perturbed fixed point strategy along generalized babu brezzi theory related abstract result perturbed saddle point problems banach babu theorem banach fixed point theorem employed prove well posedness continuous galerkin schemes particular raviart thomas piecewise polynomial subspaces lowest degree domain unknowns well continuous piecewise linear polynomials lagrange multipliers interface constitute feasible choice finite element subspaces optimal error estimates associated rates convergence established finally several numerical results illustrating good performance method confirming theoretical findings reported
N/D,
"The integration of Generative AI (GenAI) in higher education represents a paradigmatic shift from automation to augmentation. It fundamentally transforms how students engage with technology. But how do students' interactions with GenAItools shape their learning, critical thinking, and ethical engagement? Our qualitative study examines how 101 postgraduate students at an Indian university interact with GenAI tools (ChatGPT, Bard, Bing Chat, and Perplexity.ai) through contextualized learning experiences in supply chain analytics and manufacturing logistics. Using grounded theory analysis of open-ended interviews, we identified six interconnected themes spanning technological, cognitive and ethical dimensions as a result of students navigating AI-mediated learning environments. Our findings showed that students were able to develop sophisticated “distributed creativity” and ""augmented metacognition,"" enabling them to negotiate complex relationships between AI assistance and intellectual ownership. Rather than passive consumption, students demonstrated emerging AI literacy, whereby they were able to critically evaluate AI output, recognize its limitations while also maintain authorial control. These insights illuminate evidence-based strategies for fostering AI literacy while preserving academic integrity, enabling educators to responsibly harness GenAI's transformative potential.",integration generative genai higher education represents paradigmatic shift automation augmentation fundamentally transforms students engage technology students interactions genaitools shape learning critical thinking ethical engagement qualitative study examines postgraduate students indian university interact genai tools chatgpt bard bing chat perplexity contextualized learning experiences supply chain analytics manufacturing logistics using grounded theory analysis open ended interviews identified six interconnected themes spanning technological cognitive ethical dimensions result students navigating mediated learning environments findings showed students able develop sophisticated distributed creativity augmented metacognition enabling negotiate complex relationships assistance intellectual ownership rather passive consumption students demonstrated emerging literacy whereby able critically evaluate output recognize limitations also maintain authorial control insights illuminate evidence based strategies fostering literacy preserving academic integrity enabling educators responsibly harness genai transformative potential
"To contribute to a deeper understanding of whether and how AI (artificial intelligence) can be strategically leveraged to enrich the educational landscape, particularly in enhancing creative capabilities in school settings, we employed a participatory action research (PAR) methodology. This research involved workshops, class observations, and one-on-one interventions with K-12 teachers. Using a framework adapted from O’Toole & Horvát (2024), our findings revealed several key aspects of AI’s role in fostering creativity: (a) its utility in freeing up time for creative endeavors, (b) its role as a catalyst for creativity, (c) its capacity to enable personalized learning experiences, and (d) its potential to enhance creative teaching. However, the study also highlighted significant challenges, including (a) the necessity for teacher readiness to utilize AI, (b) perceived biases against AI, and (c) the need to find a balance between human creativity and AI facilitation in co-creative processes. Rather than framing AI as either friend or foe, this research calls for a nuanced, bidirectional understanding of human–AI collaboration, contributing to the emerging discourse on how AI can be thoughtfully integrated into creative, ethical, and student-centered pedagogy.",contribute deeper understanding whether artificial intelligence strategically leveraged enrich educational landscape particularly enhancing creative capabilities school settings employed participatory action research par methodology research involved workshops class observations one one interventions teachers using framework adapted toole horvát findings revealed several key aspects role fostering creativity utility freeing time creative endeavors role catalyst creativity capacity enable personalized learning experiences potential enhance creative teaching however study also highlighted significant challenges including necessity teacher readiness utilize perceived biases need find balance human creativity facilitation creative processes rather framing either friend foe research calls nuanced bidirectional understanding human collaboration contributing emerging discourse thoughtfully integrated creative ethical student centered pedagogy
"This survey presents a focused and conceptually distinct framework for understanding recent advancements in reasoning large language models (LLMs) designed to emulate “slow thinking”, a deliberate, analytical mode of cognition analogous to System 2 in dual-process theory from cognitive psychology. While prior review works have surveyed reasoning LLMs through fragmented lenses, such as isolated technical paradigms (e.g., reinforcement learning or test-time scaling) or broad post-training taxonomies, this work uniquely integrates reinforcement learning and test-time scaling as synergistic mechanisms within a unified “slow thinking” paradigm. By synthesizing insights from over 200 studies, we identify three interdependent pillars that collectively enable advanced reasoning: (1) Test-time scaling, which dynamically allocates computational resources based on task complexity via search, adaptive computation, and verification; (2) Reinforcement learning, which refines reasoning trajectories through reward modeling, policy optimization, and self-improvement; and (3) Slow-thinking frameworks, which structure reasoning into stepwise, hierarchical, or hybrid processes such as long Chain-of-Thought and multi-agent deliberation. Unlike existing surveys, our framework is goal-oriented, centering on the cognitive objective of “slow thinking” as both a unifying principle and a design imperative. This perspective enables a systematic analysis of how diverse techniques converge toward human-like deep reasoning. The survey charts a trajectory toward next-generation LLMs that balance cognitive fidelity with computational efficiency, while also outlining key challenges and future directions. Advancing such reasoning capabilities is essential for deploying LLMs in high-stakes domains including scientific discovery, autonomous agents, and complex decision support systems.",survey presents focused conceptually distinct framework understanding recent advancements reasoning large language models llms designed emulate slow thinking deliberate analytical mode cognition analogous system dual process theory cognitive psychology prior review works surveyed reasoning llms fragmented lenses isolated technical paradigms reinforcement learning test time scaling broad post training taxonomies work uniquely integrates reinforcement learning test time scaling synergistic mechanisms within unified slow thinking paradigm synthesizing insights studies identify three interdependent pillars collectively enable advanced reasoning test time scaling dynamically allocates computational resources based task complexity via search adaptive computation verification reinforcement learning refines reasoning trajectories reward modeling policy optimization self improvement slow thinking frameworks structure reasoning stepwise hierarchical hybrid processes long chain thought multi agent deliberation unlike existing surveys framework goal oriented centering cognitive objective slow thinking unifying principle design imperative perspective enables systematic analysis diverse techniques converge toward human like deep reasoning survey charts trajectory toward next generation llms balance cognitive fidelity computational efficiency also outlining key challenges future directions advancing reasoning capabilities essential deploying llms high stakes domains including scientific discovery autonomous agents complex decision support systems
"As artificial intelligence (AI) becomes increasingly integrated into educational contexts, its impact on students’ creative thinking remains unclear. Given the critical role of creativity in engineering and design education, understanding how AI tools shape students’ ideation processes is essential for developing effective pedagogical practices. This study examines the impact of generative AI, specifically ChatGPT, on creative ideation among undergraduate design engineering students. The research was conducted through a randomised crossover experiment, with students alternating between AI-assisted and unaided ideation tasks. A mixed-methods approach combined quantitative analyses of the 728 ideas generated with a qualitative evaluation of students’ interactions with AI. Results show that the use of AI did not reduce fluency, flexibility, or originality, nor did it lead to thematic homogenisation. However, semantic divergence was significantly lower in the AI-assisted condition, suggesting convergence in the way ideas were formulated. Additionally, AI-assisted ideas more frequently resembled existing products and exhibited reduced textual elaboration. A mixed between-within subjects ANOVA revealed that students who began with AI support produced more original and diverse ideas across both tasks, pointing to a lasting effect of early AI use. Qualitative analysis of student-AI interactions revealed important patterns, including predominantly passive and directive use, with limited exploratory or collaborative engagement. These findings provide new insights into human-AI co-creation and highlight the importance of promoting intentional, critical, and pedagogically guided use of generative AI tools in education.",artificial intelligence becomes increasingly integrated educational contexts impact students creative thinking remains unclear given critical role creativity engineering design education understanding tools shape students ideation processes essential developing effective pedagogical practices study examines impact generative specifically chatgpt creative ideation among undergraduate design engineering students research conducted randomised crossover experiment students alternating assisted unaided ideation tasks mixed methods approach combined quantitative analyses ideas generated qualitative evaluation students interactions results show use reduce fluency flexibility originality lead thematic homogenisation however semantic divergence significantly lower assisted condition suggesting convergence way ideas formulated additionally assisted ideas frequently resembled existing products exhibited reduced textual elaboration mixed within subjects anova revealed students began support produced original diverse ideas across tasks pointing lasting effect early use qualitative analysis student interactions revealed important patterns including predominantly passive directive use limited exploratory collaborative engagement findings provide new insights human creation highlight importance promoting intentional critical pedagogically guided use generative tools education
"This paper explores youth entrepreneurship education, most suitable at the secondary school level, through the mobilization of embedded entrepreneurship pedagogy. The paper is informed by a two, recently completed research projects, involving classroom teachers, and practicing entrepreneurs. Those projects, which examined how secondary schools can sharpen their entrepreneurial offerings to students, and in addition to their primary findings, hinted at the possibility of infusing key entrepreneurial principles into all curricular learning experiences. To illustrate this approach, this paper will examine how embedded entrepreneurship education can be included as part of the curricular and pedagogical planning of any subject area. It introduces six key practices for embedded entrepreneurship and explores, with examples, their curricular and cross-curricular potential.",paper explores youth entrepreneurship education suitable secondary school level mobilization embedded entrepreneurship pedagogy paper informed two recently completed research projects involving classroom teachers practicing entrepreneurs projects examined secondary schools sharpen entrepreneurial offerings students addition primary findings hinted possibility infusing key entrepreneurial principles curricular learning experiences illustrate approach paper examine embedded entrepreneurship education included part curricular pedagogical planning subject area introduces six key practices embedded entrepreneurship explores examples curricular cross curricular potential
"Museum cultural and creative products (MCCPs) aim to promote heritage through design, yet they often face homogenization of creativity. This study revisits the formative structure of the MPCM model, which conceptualizes creativity as comprising Novelty, Usefulness, Affect, Aesthetics, and cultural values. Using expert-screened product samples (N = 9) and consumer survey data (N = 545), we employed PLS-SEM to test a second-order formative model. Results show that only Usefulness and Cultural Values significantly predict creativity, while Affect, Novelty, and Aesthetics, despite high internal reliability, exhibit multicollinearity and non-significance. These findings challenge the additive logic of existing formative models and suggest that consumers rely on holistic impressions in evaluating MCCPs. We propose a dual-path evaluative framework that distinguishes between perception-dominant (emotional and aesthetic) and cognition-dominant (functional and cultural) processes, offering both theoretical refinement and practical guidance for assessing MCCPs.",museum cultural creative products mccps aim promote heritage design yet often face homogenization creativity study revisits formative structure mpcm model conceptualizes creativity comprising novelty usefulness affect aesthetics cultural values using expert screened product samples consumer survey data employed pls sem test second order formative model results show usefulness cultural values significantly predict creativity affect novelty aesthetics despite high internal reliability exhibit multicollinearity non significance findings challenge additive logic existing formative models suggest consumers rely holistic impressions evaluating mccps propose dual path evaluative framework distinguishes perception dominant emotional aesthetic cognition dominant functional cultural processes offering theoretical refinement practical guidance assessing mccps
"Modern design education requires innovative approaches to inspire and enhance creativity. Prior research suggests that cognitive stimulation levels significantly influence creative performance, with an optimal level known as the “sweet spot” effect. However, studies specifically addressing inexperienced designers remain limited. While mood boards are commonly used as visual stimuli in design processes, few studies, particularly in educational contexts, have explored the optimal level of stimulation. This gap may stem from challenges such as novice users’ limited expertise and the absence of refined instructional strategies. This controlled experiment aimed to explore whether an optimal level of stimulation exists in mood board design activities. Participants were classified into low-, moderate-, and high-constraint groups based on the degree of image involvement. The study employed the sweet spot framework, treating creativity level as the dependent variable and constraint level as the independent variable. Creativity in design sketches was evaluated across four dimensions: novelty, variety, quality, and quantity. Mood board effectiveness was subjectively assessed using three criteria: innovation, consistency, and visual attractiveness. The results indicated that the moderate-constraint group proved most effective in stimulating creativity across these four metrics. They also required less creation time than the high-constraint group. Moreover, moderately constrained mood boards demonstrated the largest number of significant positive correlations with sketch creativity. Furthermore, the study identified the characteristics of mood boards with low and high constraint levels. Based on these findings, this study proposes practical instructional strategies to foster creative education using mood boards.",modern design education requires innovative approaches inspire enhance creativity prior research suggests cognitive stimulation levels significantly influence creative performance optimal level known sweet spot effect however studies specifically addressing inexperienced designers remain limited mood boards commonly used visual stimuli design processes studies particularly educational contexts explored optimal level stimulation gap may stem challenges novice users limited expertise absence refined instructional strategies controlled experiment aimed explore whether optimal level stimulation exists mood board design activities participants classified low moderate high constraint groups based degree image involvement study employed sweet spot framework treating creativity level dependent variable constraint level independent variable creativity design sketches evaluated across four dimensions novelty variety quality quantity mood board effectiveness subjectively assessed using three criteria innovation consistency visual attractiveness results indicated moderate constraint group proved effective stimulating creativity across four metrics also required less creation time high constraint group moreover moderately constrained mood boards demonstrated largest number significant positive correlations sketch creativity furthermore study identified characteristics mood boards low high constraint levels based findings study proposes practical instructional strategies foster creative education using mood boards
"The study surveyed 451 undergraduate students from a Chinese university and found that teacher autonomy support affects student engagement through the chain mediating effect of student’s motivational belief and SRL. This study not only explores the direct impact of teacher support on student engagement but also investigates the mediating mechanisms of this influence. This paper helps to fully reveal the relationship between teacher support and student engagement, providing theoretical support for colleges and universities to formulate effective educational policies to enhance the learning engagement of college students. The study cannot only deepen our understanding of college students' learning engagement behaviors, but also provide practical guidance for constructing a learning environment that promotes college students' learning engagement.",study surveyed undergraduate students chinese university found teacher autonomy support affects student engagement chain mediating effect student motivational belief srl study explores direct impact teacher support student engagement also investigates mediating mechanisms influence paper helps fully reveal relationship teacher support student engagement providing theoretical support colleges universities formulate effective educational policies enhance learning engagement college students study deepen understanding college students learning engagement behaviors also provide practical guidance constructing learning environment promotes college students learning engagement
"In statistical inference, errors often stem from insufficient or inaccurate information, highlighting the need for inaccuracy measures to mitigate these challenges. The analysis of system inactivity time is particularly important in reliability data analysis, providing valuable insights into system failures and overall performance. This study extends the cumulative past inaccuracy measure to a bivariate framework, addressing prediction errors in bivariate random processes while examining its properties in the absence of a known probability density function. Additionally, we introduce a conditionally weighted extension, the conditional weighted cumulative past inaccuracy measure, and explore its theoretical properties within conditionally specified models. When the probability density function is unknown, we adopt a nonparametric approach using kernel estimation techniques to approximate the measure effectively. Furthermore, we investigate the asymptotic properties of the proposed kernel estimator to ensure its reliability. To demonstrate the practical utility of the estimator, we provide a numerical illustration, showcasing its application in reliability and survival analysis.",statistical inference errors often stem insufficient inaccurate information highlighting need inaccuracy measures mitigate challenges analysis system inactivity time particularly important reliability data analysis providing valuable insights system failures overall performance study extends cumulative past inaccuracy measure bivariate framework addressing prediction errors bivariate random processes examining properties absence known probability density function additionally introduce conditionally weighted extension conditional weighted cumulative past inaccuracy measure explore theoretical properties within conditionally specified models probability density function unknown adopt nonparametric approach using kernel estimation techniques approximate measure effectively furthermore investigate asymptotic properties proposed kernel estimator ensure reliability demonstrate practical utility estimator provide numerical illustration showcasing application reliability survival analysis
"This research examines the effect of Structured Academic Controversy (SAC) on alleviating foreign language speaking anxiety among Iranian female intermediate to upper-intermediate EFL learners. In this study, a pretest-posttest experimental design was employed, which was implemented over three months. The participants were split into control and experiment groups. The experimental group performed SAC tasks via structured debates on controversial issues, while the control group performed traditional speaking practices. Data were collected using a standardized questionnaire regarding speaking anxiety, which was administered during the pre- and post-experiment phases. The results revealed that the SAC participants had significantly lower speaking anxiety and greater speaking confidence than the control group. The findings support the notion that SAC creates a safe and cooperative atmosphere that reduces anxiety, promotes engagement, and develops speaking ability. Furthermore, this study adds to the emerging research on innovative methods in language teaching practice and the notion that SAC not only enhances linguistic ability but also emotional autonomy and agency. The results have practical implications for teachers who are searching for an effective way to reduce communication apprehension of their foreign language learners in pursuit of more confident and competent speakers.",research examines effect structured academic controversy sac alleviating foreign language speaking anxiety among iranian female intermediate upper intermediate efl learners study pretest posttest experimental design employed implemented three months participants split control experiment groups experimental group performed sac tasks via structured debates controversial issues control group performed traditional speaking practices data collected using standardized questionnaire regarding speaking anxiety administered pre post experiment phases results revealed sac participants significantly lower speaking anxiety greater speaking confidence control group findings support notion sac creates safe cooperative atmosphere reduces anxiety promotes engagement develops speaking ability furthermore study adds emerging research innovative methods language teaching practice notion sac enhances linguistic ability also emotional autonomy agency results practical implications teachers searching effective way reduce communication apprehension foreign language learners pursuit confident competent speakers
"The significance of creativity in the educational sphere is widely recognized. However, this is a topic that has yet to receive much attention in the post-Soviet context and more specifically Kazakhstan. Currently, there is little international evidence of research on upper-secondary school teachers’ beliefs about creativity. Thus, the aim of the study was to explore how these teachers conceptualize a creative learning environment, including their beliefs about what constitutes such an environment, and how creativity can be developed in the classroom. The research was designed as a qualitative, multiple-case study comprising four different types of Kazakhstani secondary schools. Data collection included 15 in-depth semi-structured interviews. A modified conceptual framework for teachers’ beliefs about creativity (Bereczki & Kárpáti, 2018) was employed in this study. The findings revealed that teachers’ beliefs about developing a creative environment align with many empirical findings worldwide. However, a noteworthy finding of this study was that the creative environment construct of this study’s initial conceptual framework was expanded with an additional subconstruct: students’ needs. Unique to this study was that participants believe a creative environment cannot be fully effective if students’ needs (basic and psychological) are not met and if the physical environment is unsatisfactory.",significance creativity educational sphere widely recognized however topic yet receive much attention post soviet context specifically kazakhstan currently little international evidence research upper secondary school teachers beliefs creativity thus aim study explore teachers conceptualize creative learning environment including beliefs constitutes environment creativity developed classroom research designed qualitative multiple case study comprising four different types kazakhstani secondary schools data collection included depth semi structured interviews modified conceptual framework teachers beliefs creativity bereczki kárpáti employed study findings revealed teachers beliefs developing creative environment align many empirical findings worldwide however noteworthy finding study creative environment construct study initial conceptual framework expanded additional subconstruct students needs unique study participants believe creative environment fully effective students needs basic psychological met physical environment unsatisfactory
"Fostering critical thinking (CT) in higher education has become increasingly pertinent and challenging in the era of generative artificial intelligence (GenAI). However, empirical studies on CT skill cultivation in GenAI-assisted learning are limited, especially in prevalent multimodal contexts. Integrating the CT skill framework and visual grammar, this qualitative case study developed a teaching module to foster undergraduates’ CT skills in GenAI-assisted digital multimodal composing (DMC), focusing on non-proficient AI users. It further explored four groups’ learning experiences in the classroom through students’ GenAI-assisted PowerPoint products, reflective writings, interviews, and design process observation. The findings revealed students’ more thoughtful scrutiny, comprehensive evaluation, and selective adoption of multimodal AIGC to achieve communicative purposes, with group variations and limitations in each dimension. This progress was facilitated by collaborative, revision-based DMC classwork with specifically defined, multi-dimensional requirements, systematic framework teaching, and real-time scaffolding. In conclusion, collaborative GenAI-assisted DMC practice with systematic instructional support can significantly foster students’ CT skill cultivation. These findings illuminate future pedagogical innovations to encourage deep learning and higher-order cognitive skills among a broader range of students in GenAI-assisted multimodal contexts.",fostering critical thinking higher education become increasingly pertinent challenging generative artificial intelligence genai however empirical studies skill cultivation genai assisted learning limited especially prevalent multimodal contexts integrating skill framework visual grammar qualitative case study developed teaching module foster undergraduates skills genai assisted digital multimodal composing dmc focusing non proficient users explored four groups learning experiences classroom students genai assisted powerpoint products reflective writings interviews design process observation findings revealed students thoughtful scrutiny comprehensive evaluation selective adoption multimodal aigc achieve communicative purposes group variations limitations dimension progress facilitated collaborative revision based dmc classwork specifically defined multi dimensional requirements systematic framework teaching real time scaffolding conclusion collaborative genai assisted dmc practice systematic instructional support significantly foster students skill cultivation findings illuminate future pedagogical innovations encourage deep learning higher order cognitive skills among broader range students genai assisted multimodal contexts
"This study examines the effects of an intervention program aimed at fostering creativity in first-year students pursuing a degree in Early Childhood Education and Teaching in Primary Education (N = 181; 18.67 ± 1.41 years) in the context of a Spanish university by addressing socio-demographic aspects. A quasi-experimental design with a non-equivalent control group was employed, and creativity was assessed using the CREA test. The program was implemented over an academic year in three intervention groups, involving the utilization of various creative strategies applied to problem-solving in academic tasks. It spanned 14 weeks, with one 100-min session per week, in which ten activities were introduced. Results: The intervention program has achieved a significant development of creativity in all intervention groups, being especially relevant in one of them. The importance of sociodemographic variables, gender and age, was studied. Gender differences in creativity were not detected, neither in relation to age or academic performance of the students which suggest the need for further research in this direction. The results of the intervention program highlight the importance of teaching creativity for creative development in higher education by considering the creative learning and the characteristics of the students in order to adapt the interventions. The importance of implementing initiatives to foster creativity in university students is emphasized, highlighting the need for actions focused on teacher training and longitudinal studies to evaluate long-term effects. The research highlights the importance of creativity in higher education, showing significant improvements in the creativity of students in Early Childhood Education and in Teaching in Primary Education Degrees through specific interventions.",study examines effects intervention program aimed fostering creativity first year students pursuing degree early childhood education teaching primary education years context spanish university addressing socio demographic aspects quasi experimental design non equivalent control group employed creativity assessed using crea test program implemented academic year three intervention groups involving utilization various creative strategies applied problem solving academic tasks spanned weeks one min session per week ten activities introduced results intervention program achieved significant development creativity intervention groups especially relevant one importance sociodemographic variables gender age studied gender differences creativity detected neither relation age academic performance students suggest need research direction results intervention program highlight importance teaching creativity creative development higher education considering creative learning characteristics students order adapt interventions importance implementing initiatives foster creativity university students emphasized highlighting need actions focused teacher training longitudinal studies evaluate long term effects research highlights importance creativity higher education showing significant improvements creativity students early childhood education teaching primary education degrees specific interventions
"In design education, fostering students’ creativity is crucial because it directly impacts the quality of their design outcomes. As artificial intelligence develops rapidly, exploring the potential of artificial intelligence-generated content (AIGC) in fostering students’ creativity has become paramount. This study experimentally investigates the impact of design processes based on different design methods, including the AIGC-based method and the traditional design method, on the intrinsic motivation and creative outcomes of design students. Utilizing a mixed-methods research approach, this between-subjects experiment involved 52 student participants and examined the two methods across different stages of design learning. This study demonstrates that AIGC significantly enhances the design learning effect by improving usability, boosting students’ intrinsic motivation and learning experiences, and elevating creativity, while pointing out limitations like overdependence on AI and learning barriers that must be addressed for sustainable integration and long-term effectiveness. This study provides empirical evidence and theoretical guidance for the application of AIGC in design learning, offering support for future research on the impact of AIGC on creativity in interdisciplinary learning environments.",design education fostering students creativity crucial directly impacts quality design outcomes artificial intelligence develops rapidly exploring potential artificial intelligence generated content aigc fostering students creativity become paramount study experimentally investigates impact design processes based different design methods including aigc based method traditional design method intrinsic motivation creative outcomes design students utilizing mixed methods research approach subjects experiment involved student participants examined two methods across different stages design learning study demonstrates aigc significantly enhances design learning effect improving usability boosting students intrinsic motivation learning experiences elevating creativity pointing limitations like overdependence learning barriers must addressed sustainable integration long term effectiveness study provides empirical evidence theoretical guidance application aigc design learning offering support future research impact aigc creativity interdisciplinary learning environments
"The rapid development of artificial intelligence technology has made programming ability a core competency for college students. However, the existing single-indicator evaluation system struggles to accurately distinguish between different levels of ability, highlighting the urgent need for a scientific and multidimensional evaluation system. In this study, the AHP-TOPSIS method was adopted. Combined with the grades of programming-related courses and competition results, a multi-dimensional evaluation system for programming design ability was constructed, and a comprehensive score was given to 307 students. The results show that the weight of mathematical analysis ability is the highest (0.26), highlighting its core position in programming education. Furthermore, the comprehensive scores of students follow a normal distribution. 68 % of the students' scores are concentrated between 70 and 79 points, which can effectively distinguish students of different ability levels. Further analysis indicates that there is a strong positive correlation between course grades and competition grades (Pearson's r = 0.78), emphasizing the importance of a solid course foundation for the development of programming ability. Furthermore, there was no significant difference in the comprehensive score between genders (p = 0.22, Cohen's d = -0.15), indicating that programming education should focus on individual differences rather than gender. The evaluation system proposed in this paper not only provides scientific assessment tools for educational institutions, but also offers a basis for optimizing teaching strategies, improving teaching quality and the quality of talent cultivation, which is of great significance to the reform and innovation of computer education.",rapid development artificial intelligence technology made programming ability core competency college students however existing single indicator evaluation system struggles accurately distinguish different levels ability highlighting urgent need scientific multidimensional evaluation system study ahp topsis method adopted combined grades programming related courses competition results multi dimensional evaluation system programming design ability constructed comprehensive score given students results show weight mathematical analysis ability highest highlighting core position programming education furthermore comprehensive scores students follow normal distribution students scores concentrated points effectively distinguish students different ability levels analysis indicates strong positive correlation course grades competition grades pearson emphasizing importance solid course foundation development programming ability furthermore significant difference comprehensive score genders cohen indicating programming education focus individual differences rather gender evaluation system proposed paper provides scientific assessment tools educational institutions also offers basis optimizing teaching strategies improving teaching quality quality talent cultivation great significance reform innovation computer education
"According to regulatory focus theory, the Aha! experience and resilience represent two distinct psychological states that influence creative performance. However, limited research has explored how these states emerge and function in the context of youth innovation. To address this gap, the present study investigated participants in the Taiwan Youth Innovation Exhibition, an annual competition that selects top projects to advance to the International Exhibition for Young Inventors. The study examined how creative self-efficacy (CSE) predicts both the Aha! experience and resilience, and how these states mediate the relationship between CSE and innovation performance. Structural equation modelling (SEM) was conducted using data collected from student participants. Results indicated that all hypothesized relationships were supported: CSE positively predicted both the Aha! experience and resilience, and both in turn significantly predicted innovation performance. These findings suggest that encouraging students to cultivate Aha! moments and develop psychological resilience may enhance their creative outcomes in innovation-based competitions.",according regulatory focus theory aha experience resilience represent two distinct psychological states influence creative performance however limited research explored states emerge function context youth innovation address gap present study investigated participants taiwan youth innovation exhibition annual competition selects top projects advance international exhibition young inventors study examined creative self efficacy cse predicts aha experience resilience states mediate relationship cse innovation performance structural equation modelling sem conducted using data collected student participants results indicated hypothesized relationships supported cse positively predicted aha experience resilience turn significantly predicted innovation performance findings suggest encouraging students cultivate aha moments develop psychological resilience may enhance creative outcomes innovation based competitions
"Early adolescence has been underscored as a crucial period for creativity development. Based on the ecological perspective, this study aimed to uncover the joint roles of parents' and teachers' need-support on two creativity sub-dimensions (i.e., self-efficacy and performance), and identify the prominent forms of support for creativity. It further examined the generalizability of findings by conducting comparisons between China and Finland. In total, 6667 adolescents aged 10 (Finland 3034, China 3633) from the OECD Survey on Social and Emotional Skills were surveyed. Results demonstrated that (1) in both countries, only teachers' emotional support showed universal positive associations with two creativity sub-dimensions; (2) Other forms of support were particularly effective for either creative self-efficacy (e.g., parents' autonomy and emotional support) or for creative performance (e.g., teachers' autonomy and competence support); (3) While Chinese parents' autonomy support associated with both creative efficacy and creative performance, Finnish parents' emotional support associated more with creative self-efficacy. Findings highlighted the critical role of teachers' emotional support as well as the complementary roles of parents' and other teachers' support in adolescents' creativity.",early adolescence underscored crucial period creativity development based ecological perspective study aimed uncover joint roles parents teachers need support two creativity sub dimensions self efficacy performance identify prominent forms support creativity examined generalizability findings conducting comparisons china finland total adolescents aged finland china oecd survey social emotional skills surveyed results demonstrated countries teachers emotional support showed universal positive associations two creativity sub dimensions forms support particularly effective either creative self efficacy parents autonomy emotional support creative performance teachers autonomy competence support chinese parents autonomy support associated creative efficacy creative performance finnish parents emotional support associated creative self efficacy findings highlighted critical role teachers emotional support well complementary roles parents teachers support adolescents creativity
"Even in the time of artificial intelligence, creative thinking is considered an important 21st century skill. Nevertheless, our understanding of how contextual factors such as socio-economic status (SES) and gender affect creativity is still limited – especially from an international perspective. In the current study, we thus examined the impact of gender and SES on creative thinking across 62 countries, using data from the 2022 Programme for International Student Assessment (PISA). Creative thinking, alongside mathematics and reading, was analyzed using a two-stage meta-analytic approach, integrating effect sizes from country-specific samples with a total sample size of N = 493,660. Our results revealed consistent gender disparities, with females outperforming males in creative thinking and reading, a trend robust across countries but with considerable variability. Gender disparities were less pronounced in the mathematical domain. Moreover, SES was found to be a strong predictor of creative thinking, mathematics, and reading, with higher SES associated with better performance across all domains. There was no substantial interaction effect between gender and SES for creative thinking and reading, suggesting that SES advantages are consistent across genders. Our analyses indicated substantial heterogeneity between countries, emphasizing the need for context-specific educational policies. These findings highlight the pervasive influence of gender and SES on fundamental educational outcomes and hence stress the necessity of tailored interventions to address these disparities around the globe.",even time artificial intelligence creative thinking considered important century skill nevertheless understanding contextual factors socio economic status ses gender affect creativity still limited especially international perspective current study thus examined impact gender ses creative thinking across countries using data programme international student assessment pisa creative thinking alongside mathematics reading analyzed using two stage meta analytic approach integrating effect sizes country specific samples total sample size results revealed consistent gender disparities females outperforming males creative thinking reading trend robust across countries considerable variability gender disparities less pronounced mathematical domain moreover ses found strong predictor creative thinking mathematics reading higher ses associated better performance across domains substantial interaction effect gender ses creative thinking reading suggesting ses advantages consistent across genders analyses indicated substantial heterogeneity countries emphasizing need context specific educational policies findings highlight pervasive influence gender ses fundamental educational outcomes hence stress necessity tailored interventions address disparities around globe
"The aim of the study is to improve primary school teachers' mathematical problem-posing skills by using narrative texts. The explanatory mixed-method design was utilized in the study. While the quantitative data of the research were collected from 24 primary school teachers working in Trabzon, the qualitative data were obtained from 10 of these primary school teachers who were randomly selected. In the study, in order to improve the problem-posing skills of primary school teachers by using narrative texts, the teachers were given theoretical information about problem-posing, mind and intelligence games, dialogic reading, STEM, philosophy for children, orienteering, museum education, and animation creation with the help of Web 2.0 tools (each activity took three class hours) for six class hours a day for four days. The participants were asked to construct problems from the narrative text “The Cure for Unhappiness” as a pre- and post-test. In addition, semi-structured interviews were conducted with the participants to determine their thoughts on problem-posing. The Wilcoxon Ranked Signs test was conducted to determine the statistically significant increase in the mathematics content strand, type, and total number of problems that the teachers constructed in the post-test compared to the pre-test. The semi-structured interviews with the teachers were analyzed with descriptive analysis. The training on narrative-text-based problem-posing given to the teachers was found to increase their non-routine problem-posing skills, although there was a statistically significant decrease in the number of problems posed by the teachers and the number of routine problems they posed. The teachers, who stated that they were inadequate in problem-posing before the training, stated that the problem-posing training using narrative texts increased their problem-posing awareness.",aim study improve primary school teachers mathematical problem posing skills using narrative texts explanatory mixed method design utilized study quantitative data research collected primary school teachers working trabzon qualitative data obtained primary school teachers randomly selected study order improve problem posing skills primary school teachers using narrative texts teachers given theoretical information problem posing mind intelligence games dialogic reading stem philosophy children orienteering museum education animation creation help web tools activity took three class hours six class hours day four days participants asked construct problems narrative text cure unhappiness pre post test addition semi structured interviews conducted participants determine thoughts problem posing wilcoxon ranked signs test conducted determine statistically significant increase mathematics content strand type total number problems teachers constructed post test compared pre test semi structured interviews teachers analyzed descriptive analysis training narrative text based problem posing given teachers found increase non routine problem posing skills although statistically significant decrease number problems posed teachers number routine problems posed teachers stated inadequate problem posing training stated problem posing training using narrative texts increased problem posing awareness
"In recent literature, matrix product (MP) codes and their duals have gained significant attention due to their applications in constructing quantum stabilizer codes. In this paper, we present a general formula characterizing the Galois dual of MP codes. Using this formula, we establish necessary and sufficient conditions under which MP codes are Galois self-orthogonal and dual-containing. Unlike previous results that describe the Euclidean and Hermitian duals of MP codes only under specific assumptions on the defining matrix, our characterization applies to MP codes with defining matrices that are not necessarily square nor of full row rank. Moreover, we consider the more general framework of Galois duals, with Euclidean and Hermitian duals treated as special cases. To demonstrate the theoretical results, several numerical examples of MP codes with best-known parameters are provided.",recent literature matrix product codes duals gained significant attention due applications constructing quantum stabilizer codes paper present general formula characterizing galois dual codes using formula establish necessary sufficient conditions codes galois self orthogonal dual containing unlike previous results describe euclidean hermitian duals codes specific assumptions defining matrix characterization applies codes defining matrices necessarily square full row rank moreover consider general framework galois duals euclidean hermitian duals treated special cases demonstrate theoretical results several numerical examples codes best known parameters provided
"Sediment suspension and motion due to vessel traffic in navigation channels is a challenging problem in the design and management of waterways, ports and navigation channels. This study utilised computational fluid dynamics (CFD) simulations to investigate the impact of vessel motion on bed morphology and sediment suspension. The work was carried out for a navigation channel with sloped banks, subject to motion-induced currents and waves generated by a rectangular barge with a blunt bow, towed at constant speed. A fine sand channel bottom was modelled. The simulations, consisting of six different scenarios with two bank slope angles, two tow speeds and two vessel widths, have been carried out by FLOW-3D Hydro software. It was found out that the most significant parameter affecting sediment motion is the tow speed of the vessel, which is coupled with the vessels squat. The study further shows that ship width also plays a critical role in predicting the risk of sediment accumulation in harbour and navigation channel projects, as an increase of only 15 % in vessel width results in a large increase of 72 % in the total amount of suspended sediment. An increase in bank slopes, on the other hand, is found to have a smaller effect (15 %) on the total suspended sediment concentration.",sediment suspension motion due vessel traffic navigation channels challenging problem design management waterways ports navigation channels study utilised computational fluid dynamics cfd simulations investigate impact vessel motion bed morphology sediment suspension work carried navigation channel sloped banks subject motion induced currents waves generated rectangular barge blunt bow towed constant speed fine sand channel bottom modelled simulations consisting six different scenarios two bank slope angles two tow speeds two vessel widths carried flow hydro software found significant parameter affecting sediment motion tow speed vessel coupled vessels squat study shows ship width also plays critical role predicting risk sediment accumulation harbour navigation channel projects increase vessel width results large increase total amount suspended sediment increase bank slopes hand found smaller effect total suspended sediment concentration
"In this manuscript, we propose newly-derived exponential quadrature rules for stiff linear differential equations with time-dependent fractional sources in the form h ( t r ) , with 0 < r < 1 and h a sufficiently smooth function. To construct the methods, the source term is interpolated at ν collocation points by a suitable non-polynomial function, yielding to time marching schemes that we call Exponential Quadrature Rules for Fractional sources (EQRF ν ). We write the integrators in terms of special instances of the Mittag–Leffler functions that we call fractional φ functions. We perform the error analysis of the schemes in the abstract framework of strongly continuous semigroups. Compared to classical exponential quadrature rules, which in our case of interest converge with order 1 + r at most, we prove that the new methods may reach order 1 + ν r for proper choices of the collocation points. Several numerical experiments demonstrate the theoretical findings and highlight the effectiveness of the approach.",manuscript propose newly derived exponential quadrature rules stiff linear differential equations time dependent fractional sources form sufficiently smooth function construct methods source term interpolated collocation points suitable non polynomial function yielding time marching schemes call exponential quadrature rules fractional sources eqrf write integrators terms special instances mittag leffler functions call fractional functions perform error analysis schemes abstract framework strongly continuous semigroups compared classical exponential quadrature rules case interest converge order prove new methods may reach order proper choices collocation points several numerical experiments demonstrate theoretical findings highlight effectiveness approach
"In this paper, an immersed finite element method is proposed for solving parabolic problems on surfaces with static interfaces. Immersed finite element basis functions are constructed based on the triangular surface finite element. The advantage of the proposed method is that it allows interface across triangular elements, thereby avoiding the generation of complex fitted meshes on surfaces. Error estimates are derived for both the semi-discrete and fully-discrete schemes of the proposed method. Finally, some numerical examples are presented to illustrate the accuracy of the proposed method.",paper immersed finite element method proposed solving parabolic problems surfaces static interfaces immersed finite element basis functions constructed based triangular surface finite element advantage proposed method allows interface across triangular elements thereby avoiding generation complex fitted meshes surfaces error estimates derived semi discrete fully discrete schemes proposed method finally numerical examples presented illustrate accuracy proposed method
"Analysis of covariance is a crucial method for improving precision of statistical tests for factor effects in randomized experiments. However, existing solutions suffer from one or more of the following limitations: (i) they are not suitable for ordinal data (as endpoints or explanatory variables); (ii) they require semiparametric model assumptions; (iii) they are inapplicable to small data scenarios due to often poor type-I error control; or (iv) they provide only approximate testing procedures and (asymptotically) exact test are missing. A resampling approach to the NANCOVA framework is investigated. NANCOVA is a fully nonparametric model based on relative effects that allows for an arbitrary number of covariates and groups, where both outcome variable (endpoint) and covariates can be metric or ordinal. Novel NANCOVA tests and a nonparametric competitor test without covariate adjustment were evaluated in extensive simulations. Unlike approximate tests in the NANCOVA framework, the proposed resampling version showed good performance in small sample scenarios and maintained the nominal type-I error well. Resampling NANCOVA also provided consistently high power: up to 26 % higher than the test without covariate adjustment in a small sample scenario with 4 groups and two covariates. Moreover, it is shown that resampling NANCOVA provides an asymptotically exact testing procedure, which makes it the first one with good finite sample performance in the present NANCOVA framework. In summary, resampling NANCOVA can be considered a viable tool for analysis of covariance overcoming issues (i) - (iv).",analysis covariance crucial method improving precision statistical tests factor effects randomized experiments however existing solutions suffer one following limitations suitable ordinal data endpoints explanatory variables require semiparametric model assumptions iii inapplicable small data scenarios due often poor type error control provide approximate testing procedures asymptotically exact test missing resampling approach nancova framework investigated nancova fully nonparametric model based relative effects allows arbitrary number covariates groups outcome variable endpoint covariates metric ordinal novel nancova tests nonparametric competitor test without covariate adjustment evaluated extensive simulations unlike approximate tests nancova framework proposed resampling version showed good performance small sample scenarios maintained nominal type error well resampling nancova also provided consistently high power higher test without covariate adjustment small sample scenario groups two covariates moreover shown resampling nancova provides asymptotically exact testing procedure makes first one good finite sample performance present nancova framework summary resampling nancova considered viable tool analysis covariance overcoming issues
"The rapid integration of Generative AI (GenAI) in education presents both opportunities and challenges in fostering critical questioning – a skill essential for critical thinking and AI literacy. In the context of GenAI, critical questioning refers to the ability to question, probe, and critically assess information generated by GenAI that will equip students with the discernment necessary in a digital world. However, there is limited research on how students develop and apply critical questioning when interacting with GenAI. This study addresses the research gap by investigating the pedagogical and contextual conditions that support high school students in critical questioning with GenAI. Through an action research study situated in a Grade 10 English classroom, the study examines the key conditions that facilitated students’ critical questioning with GenAI. Ethnographic methods were used to generate data from classroom observations, interviews, and student chatlogs that captured how students engaged with GenAI in situ within the classroom environment. A prior critical questioning framework was modified and used to identify instances of critical questioning with GenAI in the data, which were coded along the dimensions of context, delivery, and competency. Findings highlight how the instructional design of AI-mediated interactions, role of the teacher, students’ knowledge and disposition, and the delivery of GenAI platform were crucial in shaping the quality and depth of students’ questioning. These findings indicate that the success of critical engagement with GenAI does not rest on its technological capabilities alone, but on the specific pedagogical and classroom conditions that enable students to use it purposefully and reflectively. By extending our understanding of critical questioning in AI-mediated learning environments, this study provides insights into the conditions that foster AI literacy, which can lead to students actively and critically engaging with AI-generated content rather than passively consuming it.",rapid integration generative genai education presents opportunities challenges fostering critical questioning skill essential critical thinking literacy context genai critical questioning refers ability question probe critically assess information generated genai equip students discernment necessary digital world however limited research students develop apply critical questioning interacting genai study addresses research gap investigating pedagogical contextual conditions support high school students critical questioning genai action research study situated grade english classroom study examines key conditions facilitated students critical questioning genai ethnographic methods used generate data classroom observations interviews student chatlogs captured students engaged genai situ within classroom environment prior critical questioning framework modified used identify instances critical questioning genai data coded along dimensions context delivery competency findings highlight instructional design mediated interactions role teacher students knowledge disposition delivery genai platform crucial shaping quality depth students questioning findings indicate success critical engagement genai rest technological capabilities alone specific pedagogical classroom conditions enable students use purposefully reflectively extending understanding critical questioning mediated learning environments study provides insights conditions foster literacy lead students actively critically engaging generated content rather passively consuming
"Crude oil is a highly strategic global resource, and price fluctuations significantly impact nearly all economic sectors. Therefore, accurate forecasting of its prices is essential for better financial stability and decision-making. This study aims to develop a robust model using monthly data from April 2004 to January 2024 to predict the price of crude oil. We propose a novel approach that blends ARIMAX and LSTM models using a weighted combination to leverage the strengths of econometric and machine learning methods. Unlike hybrid models, which are solely designed based on a decomposition-optimization structure, in our model, an explicit ensemble with weights via grid searching is used to enhance the model’s flexibility and performance. As ARIMAX is more efficient in dealing with linear relationships and exogenous variables, LSTM performs much better and effectively captures nonlinear patterns and long-range dependence. Weight hyperparameter tuning and cross-validation help reduce the risk of overfitting or underfitting in the model. Our empirical results indicate that the LSTM model provides a powerful forecasting baseline. The weighted ensemble model offers a marginal improvement on the chronological test set, and the Diebold-Mariano test confirms this advantage is statistically significant. Cross-validation reveals the standalone LSTM to be highly robust, highlighting the importance of component model selection. This study contributes to a more sophisticated framework for risk assessment in energy policy by revealing the crucial trade-off between a model's period-specific accuracy and its general robustness.",crude oil highly strategic global resource price fluctuations significantly impact nearly economic sectors therefore accurate forecasting prices essential better financial stability decision making study aims develop robust model using monthly data april january predict price crude oil propose novel approach blends arimax lstm models using weighted combination leverage strengths econometric machine learning methods unlike hybrid models solely designed based decomposition optimization structure model explicit ensemble weights via grid searching used enhance model flexibility performance arimax efficient dealing linear relationships exogenous variables lstm performs much better effectively captures nonlinear patterns long range dependence weight hyperparameter tuning cross validation help reduce risk overfitting underfitting model empirical results indicate lstm model provides powerful forecasting baseline weighted ensemble model offers marginal improvement chronological test set diebold mariano test confirms advantage statistically significant cross validation reveals standalone lstm highly robust highlighting importance component model selection study contributes sophisticated framework risk assessment energy policy revealing crucial trade model period specific accuracy general robustness
"This paper introduces a nonlocal size-modified Poisson–Boltzmann (NSMPB) model and an efficient finite element iterative solver for proteins with three-dimensional molecular structures in ionic solutions containing multiple ion species. This model is the first Poisson–Boltzmann variant to simultaneously account for both nonlocal dielectric correlations and ionic size effects. The solver is constructed from a novel solution decomposition to overcome the numerical difficulties caused by the strong singularity, nonlinearity, and nonlocality of the model. It features an efficient modified Newton iterative method, an effective damping parameter selection scheme, and two good initial iteration strategies. To facilitate and streamline protein simulations, the solver is implemented as a software package that integrates a mesh generation tool, a protein data bank file retrieval program, and the PDB2PQR package. Numerical experiments involving an ionic solution with four species, three proteins with up to 11,439 atoms, and irregular interface-fitted tetrahedral meshes with up to 1,188,840 vertices demonstrate the fast convergence of the modified Newton iterative method, the efficiency of the solver, and the high performance of the package. This package provides a valuable tool for protein simulations.",paper introduces nonlocal size modified poisson boltzmann nsmpb model efficient finite element iterative solver proteins three dimensional molecular structures ionic solutions containing multiple ion species model first poisson boltzmann variant simultaneously account nonlocal dielectric correlations ionic size effects solver constructed novel solution decomposition overcome numerical difficulties caused strong singularity nonlinearity nonlocality model features efficient modified newton iterative method effective damping parameter selection scheme two good initial iteration strategies facilitate streamline protein simulations solver implemented software package integrates mesh generation tool protein data bank file retrieval program pdb pqr package numerical experiments involving ionic solution four species three proteins atoms irregular interface fitted tetrahedral meshes vertices demonstrate fast convergence modified newton iterative method efficiency solver high performance package package provides valuable tool protein simulations
"A one dimensional two-phase Stefan problem is considered to model the solidification process of a semi-infinite material with power-type temperature-dependent thermal coefficients and a Dirichlet boundary condition at the fixed face. Through a similarity transformation, an equivalent system of ordinary differential equations is obtained, which will be shown to have a unique solution. Since the domain is unbounded, a novel condition is imposed to transform it into a finite domain, allowing the application of the Tau approximation method. This method is based on shifted Chebyshev operational matrix of differentiation. Some comparisons between exact and numerical solutions are shown in order to test the accuracy of the method.",one dimensional two phase stefan problem considered model solidification process semi infinite material power type temperature dependent thermal coefficients dirichlet boundary condition fixed face similarity transformation equivalent system ordinary differential equations obtained shown unique solution since domain unbounded novel condition imposed transform finite domain allowing application tau approximation method method based shifted chebyshev operational matrix differentiation comparisons exact numerical solutions shown order test accuracy method
"This article examines the mechanisms by which creativity training enhances creative abilities and confidence. An extensive creativity training program (5 months of weekly training for almost seven hundred participants aged 10-17, assigned to the intervention and control group) was used to examine two alternative hypotheses. The first predicted that the increase in creative confidence due to training is mediated by the rise in creative abilities: the abilities-shape-confidence-effect. An alternative hypothesis posited that the increase in creative abilities due to training is mediated by the rise in creative confidence: the to-believe-is-to-rise effect. Although both these effects occurred, the change in participants’ creative abilities fully mediated their growth in creative confidence. This pattern suggests metacognitive mechanisms through which creativity training works. We discuss the theoretical and practical implications of those results and provide recommendations for creativity training.",article examines mechanisms creativity training enhances creative abilities confidence extensive creativity training program months weekly training almost seven hundred participants aged assigned intervention control group used examine two alternative hypotheses first predicted increase creative confidence due training mediated rise creative abilities abilities shape confidence effect alternative hypothesis posited increase creative abilities due training mediated rise creative confidence believe rise effect although effects occurred change participants creative abilities fully mediated growth creative confidence pattern suggests metacognitive mechanisms creativity training works discuss theoretical practical implications results provide recommendations creativity training
"The newly introduced higher education curriculum focuses on skill-oriented instruction, encouraging students to engage in design-driven tasks. Despite the potential of the curriculum, its implementation faces challenges, particularly in integrating cost-effective pedagogical approaches to foster cognitive dispositions such as metacognition, creativity, and design thinking. This study investigates the effectiveness of peer and individual support mechanisms, alongside emerging AI technologies, in addressing these challenges. Specifically, it examines how these supports enhance students’ cognitive dispositions within the context of design education. The study adopts a quasi-experimental research design involving 142 undergraduate students enrolled in design-related courses. Participants are divided into groups receiving peer support, individual support, or a combination of these, with or without AI assistance. Data collection includes reflective journals, creative design tasks, and design artefacts, which are analysed using epistemic network analysis and MANOVA. Results show that peer support significantly enhances collaborative metacognitive strategies, creativity, and design thinking compared to individual support. The integration of AI provides additional support, with more potent effects in peer support groups. However, AI does not improve students' design thinking in either peer or individual support mechanisms. The findings demonstrate the potential of combining traditional support mechanisms with AI to enhance cognitive dispositions in design education and offer a scalable solution for addressing curriculum implementation challenges while equipping students with critical 21st-century skills.",newly introduced higher education curriculum focuses skill oriented instruction encouraging students engage design driven tasks despite potential curriculum implementation faces challenges particularly integrating cost effective pedagogical approaches foster cognitive dispositions metacognition creativity design thinking study investigates effectiveness peer individual support mechanisms alongside emerging technologies addressing challenges specifically examines supports enhance students cognitive dispositions within context design education study adopts quasi experimental research design involving undergraduate students enrolled design related courses participants divided groups receiving peer support individual support combination without assistance data collection includes reflective journals creative design tasks design artefacts analysed using epistemic network analysis manova results show peer support significantly enhances collaborative metacognitive strategies creativity design thinking compared individual support integration provides additional support potent effects peer support groups however improve students design thinking either peer individual support mechanisms findings demonstrate potential combining traditional support mechanisms enhance cognitive dispositions design education offer scalable solution addressing curriculum implementation challenges equipping students critical century skills
"Scholars have put a lot of emphasis on time-varying linear matrix equations (LMEs) problems because of its importance in science and engineering. The problem of determining the time-varying LME’s minimum-norm least-squares solution (MLLE) is therefore tackled in this work. This is achieved by the use of NHZNN, a recently developed neutrosophic logic/fuzzy adaptive high-order zeroing neural network technique. The NHZNN is an advancement on the conventional zeroing neural network (ZNN) technique, which has shown great promise in solving time-varying tasks. To address the MLLE task for arbitrary-dimensional time-varying matrices, three novel ZNN models are presented. The models perform exceptionally well, as demonstrated by two simulation studies and two real-world applications to acoustic source tracking.",scholars put lot emphasis time varying linear matrix equations lmes problems importance science engineering problem determining time varying lme minimum norm least squares solution mlle therefore tackled work achieved use nhznn recently developed neutrosophic logic fuzzy adaptive high order zeroing neural network technique nhznn advancement conventional zeroing neural network znn technique shown great promise solving time varying tasks address mlle task arbitrary dimensional time varying matrices three novel znn models presented models perform exceptionally well demonstrated two simulation studies two real world applications acoustic source tracking
"Anti-Gauss Laguerre quadrature formulas are based on the zeros of polynomials, we call them Anti-Gauss polynomials, defined in terms of Laguerre orthogonal polynomials. This paper establishes new properties of the Anti-Gauss Laguerre polynomials, and analyzes some “truncated” interpolation processes essentially based on their zeros. Estimates of the corresponding Lebesgue constants are proved, and error bounds in spaces of locally continuous functions, equipped with uniform weighted norms are given. Finally, some numerical tests are presented about the behavior of the Lebesgue functions and Lebesgue constants, and numerical experiments dealing with the approximation of functions having different smoothness are proposed. Comparisons with the results achieved by truncated Lagrange interpolation processes based on Laguerre zeros are shown.",anti gauss laguerre quadrature formulas based zeros polynomials call anti gauss polynomials defined terms laguerre orthogonal polynomials paper establishes new properties anti gauss laguerre polynomials analyzes truncated interpolation processes essentially based zeros estimates corresponding lebesgue constants proved error bounds spaces locally continuous functions equipped uniform weighted norms given finally numerical tests presented behavior lebesgue functions lebesgue constants numerical experiments dealing approximation functions different smoothness proposed comparisons results achieved truncated lagrange interpolation processes based laguerre zeros shown
"The aim of this paper is to numerically solve Fredholm Integral Equations (FIEs) of the second kind, when the right-hand side term and the kernel are known only at scattered sample points. The proposed method is of the Nyström type and leverages a cubature rule based on Radial Basis Function (RBF) interpolation, with the Leave-One-Out Cross-Validation (LOOCV) technique used to select the optimal RBF shape parameter. The convergence of the method is proven in the space of continuous functions. Finally, numerical tests demonstrate the performance of the method for various RBF choices and provide direct comparisons with other RBF-based techniques for FIEs available in the literature.",aim paper numerically solve fredholm integral equations fies second kind right hand side term kernel known scattered sample points proposed method nystr type leverages cubature rule based radial basis function rbf interpolation leave one cross validation loocv technique used select optimal rbf shape parameter convergence method proven space continuous functions finally numerical tests demonstrate performance method various rbf choices provide direct comparisons rbf based techniques fies available literature
"Recovery and forecast of network traffic data from incomplete observed data is an important issue in internet engineering and management. In this paper, by fully considering the temporal stability and periodicity features in internet traffic data, a novel optimization model for internet data recovery and forecast is proposed, which is based upon the newly introduced higher-order tensor decomposition form called tubal tensor train (TTT) decomposition. Moreover, by introducing auxiliary variables and penalty techniques, a relaxation of the proposed model is obtained. Then, an easy-to-operate and effective algorithm for solving the relaxation model is proposed. We prove that the sequence generated by the proposed algorithm converges to a stationary point of the established relaxation model. A series of numerical experiments about the recovery of structurally missing traffic data and the traffic data prediction on the widely used real-world datasets demonstrate that our approach have favorable performance than some state-of-the-art tensor/matrix based approaches.",recovery forecast network traffic data incomplete observed data important issue internet engineering management paper fully considering temporal stability periodicity features internet traffic data novel optimization model internet data recovery forecast proposed based upon newly introduced higher order tensor decomposition form called tubal tensor train ttt decomposition moreover introducing auxiliary variables penalty techniques relaxation proposed model obtained easy operate effective algorithm solving relaxation model proposed prove sequence generated proposed algorithm converges stationary point established relaxation model series numerical experiments recovery structurally missing traffic data traffic data prediction widely used real world datasets demonstrate approach favorable performance state art tensor matrix based approaches
"Critical thinking (CT) is a key education goal internationally, and enables people to navigate the growing amount of information and make informed decisions on increasingly complex issues. The term CT is frequently used in educational policy documents and curricula, and teachers play a key role in delivering effective CT instruction. Studies indicate that although teachers recognize the importance of CT, many lack an understanding of what CT really means, and feel unprepared to teach CT. Further research is needed to obtain a deeper understanding of what CT in primary school implies. As teachers tend to teach in accordance with their perception of CT, it is important to gain an insight into how teachers understand CT. This exploratory qualitative study investigates how a group of Norwegian primary school teachers (n = 56) perceive the concept of CT, both in general and in their teaching. Using reflexive thematic analysis of reflection notes (n = 56) and semi-structured group interviews (n = 15), we found that skills are emphasized more than dispositions. Teachers primarily have a skills-disposition view of CT, with limited attention to its social dimensions. Dispositions seem to be excluded from their views of CT in relation to teaching, which is considered problematic, as dispositions can be seen as the driving force for CT. As an educational implication of our study, we can conclude that there is a need for professional development for teachers to be aware of the necessity of teaching dispositions as well as skills, and for them to include the social aspects of CT.",critical thinking key education goal internationally enables people navigate growing amount information make informed decisions increasingly complex issues term frequently used educational policy documents curricula teachers play key role delivering effective instruction studies indicate although teachers recognize importance many lack understanding really means feel unprepared teach research needed obtain deeper understanding primary school implies teachers tend teach accordance perception important gain insight teachers understand exploratory qualitative study investigates group norwegian primary school teachers perceive concept general teaching using reflexive thematic analysis reflection notes semi structured group interviews found skills emphasized dispositions teachers primarily skills disposition view limited attention social dimensions dispositions seem excluded views relation teaching considered problematic dispositions seen driving force educational implication study conclude need professional development teachers aware necessity teaching dispositions well skills include social aspects
"Collaborative learning is important for assisting students in knowledge understanding and knowledge co-construction. Many studies use concept maps, a visualized tool, to aid in this process. It is crucial to look into the ways that groups of three or more people construct knowledge and to combine implicit physiological data with explicit data. In this study, we combined content analysis, ordered network analysis (ONA), and hyperscanning technology based on electroencephalograms (EEG) to analyze the data of 28 triads of high- and low-achievement groups that participated in collaborative concept mapping tasks. The frequency statistics showed that the low-achievement groups have significantly higher monitoring and adapting behavior frequencies at the individual level in the regulation dimension than the high-achievement groups. The ONA results further illustrated the sequence of knowledge transfer based on the frequency analysis. The cognitive patterns of high-achievement groups showed a cycle that began at the group level of cognition, moved to the peer level of cognition, then to the group level of regulation, and ultimately returned to the group level of cognition. The low-achievement groups' cognitive patterns also exhibited directionality, moving from the group level of regulation to the individual level of cognition and to the group level of cognition. From the implicit perspective, the theta band and temporal-parietal regions showed greater inter-brain synchrony in the high-achievement groups. Low-achievement groups, on the other hand, had comparatively lower synchrony. Their consensus was only superficial and concentrated on ensuring that everyone participated through regulation and thought integration to finish the task. Our study's findings can help teachers and instructional designers develop tailored interventions based on differences in learners' cognitive patterns during collaborative concept mapping tasks.",collaborative learning important assisting students knowledge understanding knowledge construction many studies use concept maps visualized tool aid process crucial look ways groups three people construct knowledge combine implicit physiological data explicit data study combined content analysis ordered network analysis ona hyperscanning technology based electroencephalograms eeg analyze data triads high low achievement groups participated collaborative concept mapping tasks frequency statistics showed low achievement groups significantly higher monitoring adapting behavior frequencies individual level regulation dimension high achievement groups ona results illustrated sequence knowledge transfer based frequency analysis cognitive patterns high achievement groups showed cycle began group level cognition moved peer level cognition group level regulation ultimately returned group level cognition low achievement groups cognitive patterns also exhibited directionality moving group level regulation individual level cognition group level cognition implicit perspective theta band temporal parietal regions showed greater inter brain synchrony high achievement groups low achievement groups hand comparatively lower synchrony consensus superficial concentrated ensuring everyone participated regulation thought integration finish task study findings help teachers instructional designers develop tailored interventions based differences learners cognitive patterns collaborative concept mapping tasks
"This study seeks to examine how secondary school students demonstrate abstraction skills while solving real-world problems using block-based programming platforms. The research, exploratory in nature, involved 6th-grade students in a computer science class engaging with real-world problems through Scratch. Screen recordings and interviews were used to examine the students’ ways of demonstrating abstraction. The findings showed that real-world problem contexts, combined with the features of Scratch as a block-based programming platform, supported the emergence of abstraction behaviors in problem solving process. Among the indicators of abstraction, ""eliminating unnecessary blocks"" and ""finding clear and practical solutions"" were the observed most frequently. The ""applying previous solutions"" has developed slightly less. These indicators of abstraction were demonstrated within the affordances of Scratch, particularly through its support for suggesting the necessity of blocks, reflecting on the complexity of solutions, presenting efficient alternative blocks, and structuring the solution for better interpretability. The results indicate that working with problems based on real-life contexts in block-based programming platforms may offer students rich opportunities to demonstrate abstraction in diverse ways through meaningful problem-solving. We hope this study will support more effective programming instruction on block-based platforms through the use of real-world problems.",study seeks examine secondary school students demonstrate abstraction skills solving real world problems using block based programming platforms research exploratory nature involved grade students computer science class engaging real world problems scratch screen recordings interviews used examine students ways demonstrating abstraction findings showed real world problem contexts combined features scratch block based programming platform supported emergence abstraction behaviors problem solving process among indicators abstraction eliminating unnecessary blocks finding clear practical solutions observed frequently applying previous solutions developed slightly less indicators abstraction demonstrated within affordances scratch particularly support suggesting necessity blocks reflecting complexity solutions presenting efficient alternative blocks structuring solution better interpretability results indicate working problems based real life contexts block based programming platforms may offer students rich opportunities demonstrate abstraction diverse ways meaningful problem solving hope study support effective programming instruction block based platforms use real world problems
"A recursive algorithm is proposed to estimate a set of distribution functions indexed by a regressor variable. The procedure is fully nonparametric and has a Bayesian motivation and interpretation. Indeed, the recursive algorithm follows a certain Bayesian update, defined by the predictive distribution of a Dirichlet process mixture of linear regression models. Consistency of the algorithm is demonstrated under mild assumptions, and numerical accuracy in finite samples is shown via simulations and real data examples. The algorithm is very fast to implement, it is parallelizable, sequential, and requires limited computing power.",recursive algorithm proposed estimate set distribution functions indexed regressor variable procedure fully nonparametric bayesian motivation interpretation indeed recursive algorithm follows certain bayesian update defined predictive distribution dirichlet process mixture linear regression models consistency algorithm demonstrated mild assumptions numerical accuracy finite samples shown via simulations real data examples algorithm fast implement parallelizable sequential requires limited computing power
"The problem of breakpoint detection is considered within a regression modeling framework. A novel method, the max-EM algorithm, is introduced, combining a constrained Hidden Markov Model with the Classification-EM algorithm. This algorithm has linear complexity and provides accurate detection of breakpoints and estimation of parameters. A theoretical result is derived, showing that the likelihood of the data, as a function of the regression parameters and the breakpoints location, increases at each step of the algorithm. Two initialization methods for the breakpoints location are also presented to address local maxima issues. Finally, a statistical test in the one breakpoint situation is developed. Simulation experiments based on linear, logistic, Poisson and Accelerated Failure Time regression models show that the final method that includes the initialization procedure and the max-EM algorithm has a strong performance both in terms of parameters estimation and breakpoints detection. The statistical test is also evaluated and exhibits a correct rejection rate under the null hypothesis and a strong power under various alternatives. Two real dataset are analyzed, the UCI bike sharing and the health disease data, where the interest of the method to detect heterogeneity in the distribution of the data is illustrated.",problem breakpoint detection considered within regression modeling framework novel method max algorithm introduced combining constrained hidden markov model classification algorithm algorithm linear complexity provides accurate detection breakpoints estimation parameters theoretical result derived showing likelihood data function regression parameters breakpoints location increases step algorithm two initialization methods breakpoints location also presented address local maxima issues finally statistical test one breakpoint situation developed simulation experiments based linear logistic poisson accelerated failure time regression models show final method includes initialization procedure max algorithm strong performance terms parameters estimation breakpoints detection statistical test also evaluated exhibits correct rejection rate null hypothesis strong power various alternatives two real dataset analyzed uci bike sharing health disease data interest method detect heterogeneity distribution data illustrated
"In this paper, we give the affirmative answers to open problems about structured pseudospectra, proposed by R. Ferro and J. A. Virtanen in [J. Comput. Appl. Math. 322 (2017) 18-24]. Our primary contribution establishes the equivalence between the structured pseudospectra and unstructured pseudospectra for double-structured persymmetric Hankel and symmetric Toeplitz matrices in the complex case. Furthermore, we extend our analysis to analogous problems for centrosymmetric, Hermitian, skew-Hermitian, and circulant double-structured block matrices. The double structures are that the blocks of the given matrix are the same structure as the block matrix.",paper give affirmative answers open problems structured pseudospectra proposed ferro virtanen comput appl math primary contribution establishes equivalence structured pseudospectra unstructured pseudospectra double structured persymmetric hankel symmetric toeplitz matrices complex case furthermore extend analysis analogous problems centrosymmetric hermitian skew hermitian circulant double structured block matrices double structures blocks given matrix structure block matrix
"Quasi-interpolation based on spline approximation methods is used in numerous applications. A quartic quasi-interpolating spline is a piecewise polynomial of degree four satisfying C 3 continuity and fifth-order approximation, if the function to be approximated is sufficiently smooth. However, if the function has jump discontinuities, we observe that the Gibbs phenomenon appears when approximating near discontinuities. In this paper, we present nonlinear modifications of such a spline, based on weighted essentially non-oscillatory (WENO) techniques to avoid this phenomenon near discontinuities and, at the same time, maintain the fifth-order accuracy in smooth regions. We also provide some numerical and graphical tests confirming the theoretical results.",quasi interpolation based spline approximation methods used numerous applications quartic quasi interpolating spline piecewise polynomial degree four satisfying continuity fifth order approximation function approximated sufficiently smooth however function jump discontinuities observe gibbs phenomenon appears approximating near discontinuities paper present nonlinear modifications spline based weighted essentially non oscillatory weno techniques avoid phenomenon near discontinuities time maintain fifth order accuracy smooth regions also provide numerical graphical tests confirming theoretical results
"Detection of glaucoma progression is crucial to managing patients, permitting individualized care plans and treatment. It is a challenging task requiring the assessment of structural changes to the optic nerve head and functional changes based on visual field testing. Artificial intelligence, especially deep learning techniques, has shown promising results in many applications, including glaucoma diagnosis. This paper proposes a two-stage computational learning pipeline for detecting glaucoma progression using only fundus photographs. In the first stage, a deep learning model takes a time series of fundus photographs as input and outputs a vector of predictions where each element represents the overall rate of change in visual field (VF) sensitivity values for a sector (region) of the optic nerve head (ONH). We implemented two deep learning models—ResNet50 and InceptionResNetV2—for this stage. In the second stage, a binary classifier (weighted logistic regression) takes the predicted vector as input to detect progression. We also propose a novel method for constructing annotated datasets from temporal sequences of clinical fundus photographs and corresponding VF data suitable for machine learning. Each dataset element comprises a temporal sequence of photographs together with a vector-valued label. The label is derived by computing the pointwise linear regression of VF sensitivity values at each VF test location, mapping these locations to eight ONH sectors, and assigning the overall rate of change in each sector to one of the elements of the vector. We used a retrospective clinical dataset with 82 patients collected at multiple timepoints over five years in our experiments. The InceptionResNetV2-based implementation yielded the best performance, achieving detection accuracies of 97.28 ± 1.10 % for unseen test data (i.e., each dataset element is unseen but originates from the same set of patients appearing in the training dataset), and 87.50 ± 0.70 % for test data from unseen patients (training and testing patients are entirely different). The testing throughput was 11.60 ms per patient. These results demonstrate the efficacy of the proposed method for detecting glaucoma progression from fundus photographs.",detection glaucoma progression crucial managing patients permitting individualized care plans treatment challenging task requiring assessment structural changes optic nerve head functional changes based visual field testing artificial intelligence especially deep learning techniques shown promising results many applications including glaucoma diagnosis paper proposes two stage computational learning pipeline detecting glaucoma progression using fundus photographs first stage deep learning model takes time series fundus photographs input outputs vector predictions element represents overall rate change visual field sensitivity values sector region optic nerve head onh implemented two deep learning models resnet inceptionresnetv stage second stage binary classifier weighted logistic regression takes predicted vector input detect progression also propose novel method constructing annotated datasets temporal sequences clinical fundus photographs corresponding data suitable machine learning dataset element comprises temporal sequence photographs together vector valued label label derived computing pointwise linear regression sensitivity values test location mapping locations eight onh sectors assigning overall rate change sector one elements vector used retrospective clinical dataset patients collected multiple timepoints five years experiments inceptionresnetv based implementation yielded best performance achieving detection accuracies unseen test data dataset element unseen originates set patients appearing training dataset test data unseen patients training testing patients entirely different testing throughput per patient results demonstrate efficacy proposed method detecting glaucoma progression fundus photographs
"Potential theory provides rigorous solutions to geodetic boundary value problems involving Laplace’s equation, often expressed through global integral transformations. However, the practical application of these solutions is hindered by the incomplete availability of high-resolution gravity data. This limitation necessitates the separate evaluation of far-zone contributions—commonly referred to as truncation errors. These errors are typically characterized using truncation coefficients, whose computation via recurrence formulas becomes numerically unstable at high altitudes and degrees when using double precision. This study presents a robust and comprehensive method for accurately computing truncation coefficients for the Poisson, Hotine, and Stokes integrals across all degrees and altitudes. The proposed approach integrates three specialized strategies: (1) an asymptotic expansion for high-degree terms to ensure both accuracy and efficiency; (2) a two-regime altitude model for low-degree terms, with binomial expansions applied at low altitudes and stabilized recurrence series used at high altitudes; and (3) double-precision numerical experiments that validate the method, showing relative errors below 10−5 for the Poisson integral and below 10−8 for the Hotine and Stokes integrals. These results demonstrate that the method is both accurate and reliable for geodetic applications at all altitudes—from airborne surveys to satellite missions—and across the full range of spherical harmonic degrees.",potential theory provides rigorous solutions geodetic boundary value problems involving laplace equation often expressed global integral transformations however practical application solutions hindered incomplete availability high resolution gravity data limitation necessitates separate evaluation far zone contributions commonly referred truncation errors errors typically characterized using truncation coefficients whose computation via recurrence formulas becomes numerically unstable high altitudes degrees using double precision study presents robust comprehensive method accurately computing truncation coefficients poisson hotine stokes integrals across degrees altitudes proposed approach integrates three specialized strategies asymptotic expansion high degree terms ensure accuracy efficiency two regime altitude model low degree terms binomial expansions applied low altitudes stabilized recurrence series used high altitudes double precision numerical experiments validate method showing relative errors poisson integral hotine stokes integrals results demonstrate method accurate reliable geodetic applications altitudes airborne surveys satellite missions across full range spherical harmonic degrees
"Scholars have long debated the existence of the research-teaching nexus. Guided by the Job Demands-Resources model, the present study pioneered the investigation of the predictive power of academics’ research agendas for their emotions in teaching, considering the mediating role of academic self-efficacy (i.e., teaching efficacy and research efficacy). Participants were 332 academics from nine research-oriented comprehensive universities in mainland China. The participants responded to the Multi-Dimensional Research Agendas Inventory-12, the Research-Teaching Efficacy Inventory-6, and the Emotions in Teaching Inventory-Revised-6. Academics who were more deeply engaged in creative research agendas tended to report more positive emotions in teaching, whereas those more engaged in conventional research agendas tended to report more negative teaching emotions. These statistical predictive findings were obtained, with academics’ age, academic rank, gender, and academic discipline controlled for. Moreover, teaching efficacy provided a pathway from creative research agendas to both positive and negative teaching emotions. These findings unveiled intricate interconnections between academics’ research and teaching. The study has made theoretical contributions and it has practical implications for both academics and university senior managers in their respective efforts to promote positive teaching emotions through fostering creativity in research agendas and boosting teaching efficacy. Furthermore, the limitations of the study can provide guidance for future research directions.",scholars long debated existence research teaching nexus guided job demands resources model present study pioneered investigation predictive power academics research agendas emotions teaching considering mediating role academic self efficacy teaching efficacy research efficacy participants academics nine research oriented comprehensive universities mainland china participants responded multi dimensional research agendas inventory research teaching efficacy inventory emotions teaching inventory revised academics deeply engaged creative research agendas tended report positive emotions teaching whereas engaged conventional research agendas tended report negative teaching emotions statistical predictive findings obtained academics age academic rank gender academic discipline controlled moreover teaching efficacy provided pathway creative research agendas positive negative teaching emotions findings unveiled intricate interconnections academics research teaching study made theoretical contributions practical implications academics university senior managers respective efforts promote positive teaching emotions fostering creativity research agendas boosting teaching efficacy furthermore limitations study provide guidance future research directions
We investigate stability of linear delay differential systems. Stability criteria of the systems are derived based on integrals of the fundamental matrix. They are necessary and sufficient conditions for delay-dependent stability of the systems. Numerical examples illustrate the main results.,investigate stability linear delay differential systems stability criteria systems derived based integrals fundamental matrix necessary sufficient conditions delay dependent stability systems numerical examples illustrate main results
"The paper proposes a original hybrid zonal RANS-LES model that allows using unmatched-grid fragments in RANS and LES computational domains. The proposed model is based on the numerical solution of a system of Navier-Stokes equations, the discretization of which is carried out by the finite volume method on an arbitrary unstructured grid, and the iterative solution is carried by the SIMPLE algorithm. During RANS calculations, an SST turbulence model is used to determine the flow parameters. In addition, developed before by authors an original method is applied to connect unmatched-grid fragments. The method accounts for the features of solving the hydrodynamic equations modeling a viscous incompressible fluid flow at the adjacent boundaries of the grid’s unmatched fragments. At the second stage of the calculation, to create artificial turbulence, the method of generating velocity fluctuations at the input boundaries of the LES region is used, adapted to account for the inconsistency of the grids between the RANS and LES regions. The developed hybrid model was applied to simulate the thermally-compressible viscous fluid flow experimentally investigated in a Vattenfall T-junction. The numerical experiments show the effectiveness and sufficient accuracy of predicting the average flow characteristics of the proposed method.",paper proposes original hybrid zonal rans model allows using unmatched grid fragments rans computational domains proposed model based numerical solution system navier stokes equations discretization carried finite volume method arbitrary unstructured grid iterative solution carried simple algorithm rans calculations sst turbulence model used determine flow parameters addition developed authors original method applied connect unmatched grid fragments method accounts features solving hydrodynamic equations modeling viscous incompressible fluid flow adjacent boundaries grid unmatched fragments second stage calculation create artificial turbulence method generating velocity fluctuations input boundaries region used adapted account inconsistency grids rans regions developed hybrid model applied simulate thermally compressible viscous fluid flow experimentally investigated vattenfall junction numerical experiments show effectiveness sufficient accuracy predicting average flow characteristics proposed method
"We derive a new Laplacian approximation for use in the Moving Particle Semi-Implicit method, ensuring a rigorous mathematical foundation based on Taylor expansion. The accuracy of this novel approximation is evaluated through comparison with other existing methods by studying the convergence rate of the numerical approximation in uniform and perturbed particle arrangements, and when solving a Poisson problem with different types of boundary conditions. The proposed model allows for easier implementation with respect to previous techniques, and always guarantees an accuracy of at least first order.",derive new laplacian approximation use moving particle semi implicit method ensuring rigorous mathematical foundation based taylor expansion accuracy novel approximation evaluated comparison existing methods studying convergence rate numerical approximation uniform perturbed particle arrangements solving poisson problem different types boundary conditions proposed model allows easier implementation respect previous techniques always guarantees accuracy least first order
"This paper proposes a novel approach to developing hybrid flux reconstruction (HFR) schemes through weighted correction functions. The methodology synthesizes correction functions via linear combinations of existing formulations, leading to enhanced numerical properties. The proposed framework is demonstrated by constructing hybrid schemes based on correction functions from the discontinuous Galerkin (DG), spectral difference (SD), and staggered-grid (SG) schemes, representing three well-established classes of FR schemes. Theoretical investigation through von Neumann analysis reveals the influence of weighting parameters on the dissipation and dispersion characteristics. Numerical experiments with the linear advection equation, linearized Euler equations and nonlinear Euler equations demonstrate that optimally weighted correction functions can achieve superior accuracy and stability compared to their constituent base schemes. Specifically, the optimal weighting coefficient in the HFR1 scheme enables it to achieve even higher accuracy than the DG scheme, whereas the optimal weighting coefficient in the HFR2 scheme yields a CFL limit exceeding that of the SG scheme. From a practical perspective, this work provides a straightforward approach for generating a wide range of FR schemes with predictable numerical characteristics.",paper proposes novel approach developing hybrid flux reconstruction hfr schemes weighted correction functions methodology synthesizes correction functions via linear combinations existing formulations leading enhanced numerical properties proposed framework demonstrated constructing hybrid schemes based correction functions discontinuous galerkin spectral difference staggered grid schemes representing three well established classes schemes theoretical investigation von neumann analysis reveals influence weighting parameters dissipation dispersion characteristics numerical experiments linear advection equation linearized euler equations nonlinear euler equations demonstrate optimally weighted correction functions achieve superior accuracy stability compared constituent base schemes specifically optimal weighting coefficient hfr scheme enables achieve even higher accuracy scheme whereas optimal weighting coefficient hfr scheme yields cfl limit exceeding scheme practical perspective work provides straightforward approach generating wide range schemes predictable numerical characteristics
"The efficient numerical computations of matrix exponentials in a tensor framework have been recently studied by some researchers. We consider this approach and apply it to exponential methods for stochastic differential equations (SDEs). As a result, we will show that the approach is available for the methods to solve high-dimensional SDEs efficiently.",efficient numerical computations matrix exponentials tensor framework recently studied researchers consider approach apply exponential methods stochastic differential equations sdes result show approach available methods solve high dimensional sdes efficiently
"We study the problem of guarding points on an x-monotone polygonal chain, called a terrain, using k watchtowers. A watchtower is a vertical segment whose bottom endpoint lies on the terrain. A point on the terrain is visible from a watchtower if the line segment connecting the point and the top endpoint of the watchtower does not cross the terrain. Given a sequence of point sites lying on a terrain, we aim to partition the sequence into k contiguous subsequences and place k watchtowers on the terrain such that every point site in a subsequence is visible from the same watchtower and the maximum length of the watchtowers is minimized. We present efficient algorithms for two variants of the problem.",study problem guarding points monotone polygonal chain called terrain using watchtowers watchtower vertical segment whose bottom endpoint lies terrain point terrain visible watchtower line segment connecting point top endpoint watchtower cross terrain given sequence point sites lying terrain aim partition sequence contiguous subsequences place watchtowers terrain every point site subsequence visible watchtower maximum length watchtowers minimized present efficient algorithms two variants problem
N/D,
"Intensity-Modulated Radiation Therapy enhances dose delivery by dynamically adjusting beam intensities to target tumorous tissues while preserving healthy organs. One of the most effective planning approaches uses the Generalized Equivalent Uniform Dose metric, which ensures high-quality treatment plans but requires tuning several hyperparameters for each anatomical structure. Traditionally, this process is performed manually by clinical experts, making it time-consuming and dependent on human expertise. To address these challenges, a previous method combined multi-objective evolutionary search with gradient-based optimization to automate the tuning process. However, this hybrid strategy incurs high computational cost, as each candidate solution must undergo a complete gradient-based optimization step, repeated thousands of times throughout the process. This study introduces two complementary strategies to improve the efficiency of this framework. First, we analyze alternative multi-objective evolutionary algorithms that converge more rapidly, thereby reducing the number of required function evaluations, and we compare three gradient-based optimization methods to identify the one that accelerates convergence without compromising plan quality. Second, we implement a parallel computing framework that distributes the function evaluations across heterogeneous multicore computing clusters using a static batch scheduling strategy adapted to each node’s computational capacity. Combined, these algorithmic and computational enhancements yield an acceleration factor of 4049 compared to the original implementation. As a result, high-quality radiotherapy treatment plans can be automatically generated in approximately one hour, making this approach viable for integration into time-constrained clinical workflows.",intensity modulated radiation therapy enhances dose delivery dynamically adjusting beam intensities target tumorous tissues preserving healthy organs one effective planning approaches uses generalized equivalent uniform dose metric ensures high quality treatment plans requires tuning several hyperparameters anatomical structure traditionally process performed manually clinical experts making time consuming dependent human expertise address challenges previous method combined multi objective evolutionary search gradient based optimization automate tuning process however hybrid strategy incurs high computational cost candidate solution must undergo complete gradient based optimization step repeated thousands times throughout process study introduces two complementary strategies improve efficiency framework first analyze alternative multi objective evolutionary algorithms converge rapidly thereby reducing number required function evaluations compare three gradient based optimization methods identify one accelerates convergence without compromising plan quality second implement parallel computing framework distributes function evaluations across heterogeneous multicore computing clusters using static batch scheduling strategy adapted node computational capacity combined algorithmic computational enhancements yield acceleration factor compared original implementation result high quality radiotherapy treatment plans automatically generated approximately one hour making approach viable integration time constrained clinical workflows
"This study aimed to analyze the creative experiences of early childhood pre-service teachers enrolled in a semester-long course designed to enhance their creative potential through experiential projects. Using a qualitative research design, this study explored the subjective experiences of 16 pre-service teachers, aged 21 to 23-years-old, who participated in a 14-week course focused on integrating creativity into early childhood education. Data were collected through weekly reflections, project reports, and multimedia documentation, capturing each pre-service teacher’s creative journey. The findings revealed four key themes which characterized the pre-service teachers’ creative experiences: (1) learning a new skill, (2) expressing creativity and experiencing a sense of accomplishment, (3) overcoming challenges related to fear and insecurity, and (4) achieving personal fulfillment and professional growth. These open-ended projects fostered an environment where the pre-service teachers could take risks, embrace failure, and reflect on their development, building essential skills such as problem-solving, creative confidence, and resilience. This approach equipped pre-service teachers with the skills and mindset necessary to foster creativity in their future classrooms, supporting both their personal and professional growth.",study aimed analyze creative experiences early childhood pre service teachers enrolled semester long course designed enhance creative potential experiential projects using qualitative research design study explored subjective experiences pre service teachers aged years old participated week course focused integrating creativity early childhood education data collected weekly reflections project reports multimedia documentation capturing pre service teacher creative journey findings revealed four key themes characterized pre service teachers creative experiences learning new skill expressing creativity experiencing sense accomplishment overcoming challenges related fear insecurity achieving personal fulfillment professional growth open ended projects fostered environment pre service teachers could take risks embrace failure reflect development building essential skills problem solving creative confidence resilience approach equipped pre service teachers skills mindset necessary foster creativity future classrooms supporting personal professional growth
"Psychological capital (PsyCap) is a critical personal resource that supports behavioral proactivity and resilience in the face of challenges, yet its developmental antecedents remain underexplored. Drawing upon Broaden-and-Build Theory, this study investigates how two core dimensions of trait curiosity, namely stretching (the drive to seek new knowledge) and embracing (the willingness to tolerate uncertainty), foster the development of PsyCap over time. Furthermore, the study examines how three distinct forms of imagination (initiating, conceiving, and transforming) moderate these relationships. Using a two-wave, time-lagged design, data were collected from 519 trained volunteers participating in science education programs. Results indicate that both stretching and embracing curiosity significantly predict subsequent PsyCap, but these effects are uniquely moderated by different facets of imagination. More importantly, imagination exhibits a dual influence: depending on its type, it can either amplify or constrain the positive impact of curiosity on PsyCap development. These findings advance our understanding of the cognitive pathways underpinning PsyCap and highlight practical implications for designing interventions in educational and professional learning environments.",psychological capital psycap critical personal resource supports behavioral proactivity resilience face challenges yet developmental antecedents remain underexplored drawing upon broaden build theory study investigates two core dimensions trait curiosity namely stretching drive seek new knowledge embracing willingness tolerate uncertainty foster development psycap time furthermore study examines three distinct forms imagination initiating conceiving transforming moderate relationships using two wave time lagged design data collected trained volunteers participating science education programs results indicate stretching embracing curiosity significantly predict subsequent psycap effects uniquely moderated different facets imagination importantly imagination exhibits dual influence depending type either amplify constrain positive impact curiosity psycap development findings advance understanding cognitive pathways underpinning psycap highlight practical implications designing interventions educational professional learning environments
"Teachers’ professional vision as ability to notice and interpret relevant classroom events is based on professional knowledge. With regard to the circumvention-of-limits hypothesis, pre-service teachers’ professional vision might as well be conditioned by individual differences in their general cognitive abilities. We investigated the relationship between pre-service teachers’ professional vision and general cognitive abilities as well as whether their professional knowledge moderates this relationship (N = 91, thereof 49 bachelor and 42 master students). Regression analyses confirmed the hypothesized effect of professional knowledge on professional vision. Furthermore, only selective attention as part of general cognitive abilities predicted professional vision in open-response video-based tasks, while the moderation effect could not be corroborated. The findings confirm professional vision as knowledge-based process, but also underline that individual differences in general cognitive abilities might affect and need to be considered when addressing professional vision with video clips.",teachers professional vision ability notice interpret relevant classroom events based professional knowledge regard circumvention limits hypothesis pre service teachers professional vision might well conditioned individual differences general cognitive abilities investigated relationship pre service teachers professional vision general cognitive abilities well whether professional knowledge moderates relationship thereof bachelor master students regression analyses confirmed hypothesized effect professional knowledge professional vision furthermore selective attention part general cognitive abilities predicted professional vision open response video based tasks moderation effect could corroborated findings confirm professional vision knowledge based process also underline individual differences general cognitive abilities might affect need considered addressing professional vision video clips
"Conventional teaching methods often emphasize teacher-centered activities, limiting opportunities for students to develop professional and transferable skills. This disconnection between academic knowledge and workplace readiness highlights the need for innovative approaches to education. This study explores the integration of computational thinking (CT) into project-based learning (PjBL) as a means of enhancing student learning outcomes, particularly in online learning environments. A quasi-experimental design was adopted to evaluate the effectiveness of this strategy. The study involved two groups of university students: an experimental group (24 students) exposed to CT-integrated PjBL and a control group (23 students) taught using conventional PjBL method. Results demonstrated that CT-integrated PjBL significantly enhanced students’ knowledge construction and affective-domain learning objectives, including motivation, attitude, and self-efficacy, compared to the conventional approach. These findings underscore the potential of combining CT with PjBL to bridge the gap between theoretical knowledge and practical application. The study offers valuable insights and practical implications for educators, researchers, and course designers, advocating for the adoption of innovative teaching strategies that foster holistic skill development and better prepare students for professional challenges.",conventional teaching methods often emphasize teacher centered activities limiting opportunities students develop professional transferable skills disconnection academic knowledge workplace readiness highlights need innovative approaches education study explores integration computational thinking project based learning pjbl means enhancing student learning outcomes particularly online learning environments quasi experimental design adopted evaluate effectiveness strategy study involved two groups university students experimental group students exposed integrated pjbl control group students taught using conventional pjbl method results demonstrated integrated pjbl significantly enhanced students knowledge construction affective domain learning objectives including motivation attitude self efficacy compared conventional approach findings underscore potential combining pjbl bridge gap theoretical knowledge practical application study offers valuable insights practical implications educators researchers course designers advocating adoption innovative teaching strategies foster holistic skill development better prepare students professional challenges
"Nurturing students' creative potential, particularly in STEM fields, is essential for fostering social progress and innovation. This study addresses the lack of available tests for creative potential in specific subjects by developing and validating a test instrument designed to measure divergent thinking (DT) in biology. The design-based research project consisted of two studies involving a total of N = 342 students (M age = 10.96). The developed DT test was initially evaluated with participants from an enrichment program during the first study. For validation, intelligence was measured using Raven's Progressive Matrices 2, as well as scientific inquiry skills in biology, inductive thinking skills in biology, situational interest, and subjective task values across different subjects. In the second study, the developed DT test was applied to a randomized sample from different schools. In both studies, the DT test demonstrated excellent inter-rater reliability. The investigation into the factor structure showed that each task represented a different factor, and the test has limited ability to measure individual DT constructs. Positive correlations were identified between fluency and scientific inquiry skills, as well as between fluency and scientific inductive thinking. The results indicate that the developed test is capable of measuring facets of subject-specific DT and provides an initial option for researchers and educators to assess DT in biology. Future research should explore the effects of different task types, instructions, and time limits. Replications for other STEM subjects, age groups, and comparisons to classic DT tests should be conducted.",nurturing students creative potential particularly stem fields essential fostering social progress innovation study addresses lack available tests creative potential specific subjects developing validating test instrument designed measure divergent thinking biology design based research project consisted two studies involving total students age developed test initially evaluated participants enrichment program first study validation intelligence measured using raven progressive matrices well scientific inquiry skills biology inductive thinking skills biology situational interest subjective task values across different subjects second study developed test applied randomized sample different schools studies test demonstrated excellent inter rater reliability investigation factor structure showed task represented different factor test limited ability measure individual constructs positive correlations identified fluency scientific inquiry skills well fluency scientific inductive thinking results indicate developed test capable measuring facets subject specific provides initial option researchers educators assess biology future research explore effects different task types instructions time limits replications stem subjects age groups comparisons classic tests conducted
"This study aims to investigate how proficient prospective mathematics teachers are in evaluating the truth value of statements that are true in Euclidean geometry in terms of non-Euclidean geometries. In addition, this study seeks to portray the strategies prospective mathematics teachers used during the inductive reasoning process as they evaluate the truth value of statements in terms of non-Euclidean geometries. To this end, 106 participants were asked to evaluate the truth value of six statements, which are true in Euclidean geometry, in terms of elliptic and hyperbolic geometries, and explain their reasoning. Through this comparative and evaluative task, participants were naturally engaged in an inductive reasoning process. According to the findings, at least a quarter of the participants presented incorrect answers for every statement. The strategies observed during the inductive reasoning process were classified under thirteen categories: visualizing geometry surface/concept(s) in the statement, drawing by considering geometry surface/concept(s) in the statement, drawing without explanation, considering the definition of the main concept in the statement, presenting/aiming to present a counterexample/countercase, stating the absence of a counterexample/countercase, pointing a similarity/difference with Euclidean geometry, associating with the fifth postulate, generalizing, relating to another discipline, considering the cases necessary for the statement to be true, offering the presence of various cases based on the predicate of the statement, and identifying a specific relation between the concepts of the statement.",study aims investigate proficient prospective mathematics teachers evaluating truth value statements true euclidean geometry terms non euclidean geometries addition study seeks portray strategies prospective mathematics teachers used inductive reasoning process evaluate truth value statements terms non euclidean geometries end participants asked evaluate truth value six statements true euclidean geometry terms elliptic hyperbolic geometries explain reasoning comparative evaluative task participants naturally engaged inductive reasoning process according findings least quarter participants presented incorrect answers every statement strategies observed inductive reasoning process classified thirteen categories visualizing geometry surface concept statement drawing considering geometry surface concept statement drawing without explanation considering definition main concept statement presenting aiming present counterexample countercase stating absence counterexample countercase pointing similarity difference euclidean geometry associating fifth postulate generalizing relating another discipline considering cases necessary statement true offering presence various cases based predicate statement identifying specific relation concepts statement
"Quadrature rules are a common numerical tool for approximating definite integrals. While most classical rules are based on polynomial interpolation, recent results reveal the efficiency and effectiveness of quadrature rules based on linear barycentric rational interpolants. In this paper, we derive new quadrature rules from barycentric rational Hermite interpolants and prove their convergence orders. We then use the proposed quadrature rules to construct direct quadrature methods for solving Volterra integral equations. We provide several numerical experiments that validate our theoretical results and illustrate the efficiency of our new quadrature rules and methods.",quadrature rules common numerical tool approximating definite integrals classical rules based polynomial interpolation recent results reveal efficiency effectiveness quadrature rules based linear barycentric rational interpolants paper derive new quadrature rules barycentric rational hermite interpolants prove convergence orders use proposed quadrature rules construct direct quadrature methods solving volterra integral equations provide several numerical experiments validate theoretical results illustrate efficiency new quadrature rules methods
"In this study, we develop a novel stage-structured fractional-order prey–predator model that captures more realistic ecological dynamics by incorporating the effects of juvenile hunting behavior—particularly those arising from structural habitat complexity and shoot bombardment. These factors influence the viability and survival of juvenile predators, leading to potential deviations in population predictions. To account for the inherent memory and hereditary properties in ecological interactions, we employ fractional-order derivatives, thereby extending the classical prey–predator framework. The proposed model accounts for maturation delays, stage-specific interactions, and differential survival rates, offering a more comprehensive understanding of population stabilization across multiple life stages. Our analysis highlights how structural complexity in the habitat critically affects juvenile hunting efficiency and the associated maladroit costs, such as increased mortality due to unsuccessful hunting attempts during key developmental periods. Both analytical and numerical techniques are employed to investigate the stability and bifurcation behavior of the system under varying ecological parameters. The results underscore the importance of habitat features and predator limitations in shaping population dynamics and ecological resilience. These insights contribute to the broader understanding of biodiversity conservation and the strategic management of ecosystems under natural and anthropogenic pressures.",study develop novel stage structured fractional order prey predator model captures realistic ecological dynamics incorporating effects juvenile hunting behavior particularly arising structural habitat complexity shoot bombardment factors influence viability survival juvenile predators leading potential deviations population predictions account inherent memory hereditary properties ecological interactions employ fractional order derivatives thereby extending classical prey predator framework proposed model accounts maturation delays stage specific interactions differential survival rates offering comprehensive understanding population stabilization across multiple life stages analysis highlights structural complexity habitat critically affects juvenile hunting efficiency associated maladroit costs increased mortality due unsuccessful hunting attempts key developmental periods analytical numerical techniques employed investigate stability bifurcation behavior system varying ecological parameters results underscore importance habitat features predator limitations shaping population dynamics ecological resilience insights contribute broader understanding biodiversity conservation strategic management ecosystems natural anthropogenic pressures
"In this paper we study a class of new dynamical systems composed of the Oseen linearization of Navier–Stokes equations with a nonmonotone slip boundary condition, a generalized leak boundary condition of frictional type and the generalized quasilinear reaction–diffusion equation with the Neumann boundary condition. The system is formulated as a problem coupling a variational–hemivariational inequality and a nonlinear parabolic equations. We obtain the existence of the global weak solution by the Rothe technique which is based on time semidiscrete approximation by backward Euler method and a feedback iteration technique firstly. Then we use a surjectivity theorem and nonsmooth analysis to prove existence of solution to the approximation problem and provide a priori estimates. Finally, the limit process is used to establish existence of global weak solution of the system.",paper study class new dynamical systems composed oseen linearization navier stokes equations nonmonotone slip boundary condition generalized leak boundary condition frictional type generalized quasilinear reaction diffusion equation neumann boundary condition system formulated problem coupling variational hemivariational inequality nonlinear parabolic equations obtain existence global weak solution rothe technique based time semidiscrete approximation backward euler method feedback iteration technique firstly use surjectivity theorem nonsmooth analysis prove existence solution approximation problem provide priori estimates finally limit process used establish existence global weak solution system
"Modeling data using manifold values is a powerful concept with numerous advantages, particularly in addressing nonlinear phenomena. This approach captures the intrinsic geometric structure of the data, leading to more accurate descriptors and more efficient computational processes. However, even fundamental tasks like compression and data enhancement present meaningful challenges in the manifold setting. This paper introduces a multiscale transform that aims to represent manifold-valued sequences at different scales, enabling novel data processing tools for various applications. Similar to traditional methods, our construction is based on a refinement operator that acts as an upsampling operator and a corresponding downsampling operator. Inspired by Wiener’s lemma, we term the latter as the reverse of the former. It turns out that some upsampling operators, for example, least-squares-based refinement, do not have a practical reverse. Therefore, we introduce the notion of pseudo-reversing and explore its analytical properties and asymptotic behavior. We derive analytical properties of the induced multiscale transform and conclude the paper with numerical illustrations showcasing different aspects of the pseudo-reversing and two data processing applications involving manifolds.",modeling data using manifold values powerful concept numerous advantages particularly addressing nonlinear phenomena approach captures intrinsic geometric structure data leading accurate descriptors efficient computational processes however even fundamental tasks like compression data enhancement present meaningful challenges manifold setting paper introduces multiscale transform aims represent manifold valued sequences different scales enabling novel data processing tools various applications similar traditional methods construction based refinement operator acts upsampling operator corresponding downsampling operator inspired wiener lemma term latter reverse former turns upsampling operators example least squares based refinement practical reverse therefore introduce notion pseudo reversing explore analytical properties asymptotic behavior derive analytical properties induced multiscale transform conclude paper numerical illustrations showcasing different aspects pseudo reversing two data processing applications involving manifolds
"A discrete-time Stackelberg–Cournot competition model is analyzed, in which n leader firms and m follower firms interact in an oligopolistic market subject to time delays. Two equilibrium states are identified: a boundary equilibrium and a positive (interior) equilibrium, whose existence depends on the relative cost structures of the firms. For the non-delayed case, we derive conditions for the local stability of both equilibria and demonstrate that a flip (period-doubling) bifurcation occurs as the adjustment speed increases. When time delays are introduced, the stability properties of the boundary equilibrium remain unaffected, while the dynamics of the interior equilibrium become sensitive to the overall delay. In particular, when the leader and follower updates are effectively synchronized, reflecting a collective timing of reactions, the stability region of the positive equilibrium expands, and a Neimark–Sacker bifurcation is observed; in contrast, asynchronous updates yield dynamics similar to those of the non-delayed model. Numerical simulations validate our theoretical findings and illustrate the various routes to chaotic behavior. These results emphasize the role of reaction timing and delay interactions in shaping market dynamics in oligopolistic settings.",discrete time stackelberg cournot competition model analyzed leader firms follower firms interact oligopolistic market subject time delays two equilibrium states identified boundary equilibrium positive interior equilibrium whose existence depends relative cost structures firms non delayed case derive conditions local stability equilibria demonstrate flip period doubling bifurcation occurs adjustment speed increases time delays introduced stability properties boundary equilibrium remain unaffected dynamics interior equilibrium become sensitive overall delay particular leader follower updates effectively synchronized reflecting collective timing reactions stability region positive equilibrium expands neimark sacker bifurcation observed contrast asynchronous updates yield dynamics similar non delayed model numerical simulations validate theoretical findings illustrate various routes chaotic behavior results emphasize role reaction timing delay interactions shaping market dynamics oligopolistic settings
"The dynamic fracture toughness of nickel-based alloys is closely related to both the internal strengthening phases and external impact loads. However, the prediction of fracture toughness is challenging due to complex factors such as multi-scale effects, non-linearity, and strong correlations. To address these challenges, a novel prediction method for the dynamic fracture toughness of nickel-based alloys has been proposed, which integrates multi-scale numerical simulations (physical models) with machine learning techniques. This method considers both external factors (impact velocity) and internal factors (the size and distribution of strengthening phases).The study analyzes the correlation between impact velocity, the size and distribution of strengthening phases, and dynamic fracture toughness. It also proposes an optimization strategy for the hidden layer depth and the number of neurons in the neural network, resulting in an optimal network structure (8 layers with 25 neurons per layer) for predicting dynamic fracture toughness. Based on this optimized network structure, the study identifies the optimal sample size (N = 90) for ensuring a balance between computational efficiency and prediction accuracy in the fracture toughness model. Furthermore, a comparative analysis is conducted on the fitting and prediction performance of various machine learning models, including artificial neural networks (ANN), support vector machines (SVM), k-nearest neighbors (KNN), and decision trees (DT). Among these, the ANN model demonstrates the best performance, with an average absolute error of 6.73 and a coefficient of determination (R2) of 0.98. Additional validation confirms the model's excellent capability for both interpolation and extrapolation.This research successfully achieves accurate predictions of the dynamic fracture toughness of nickel-based alloys, incorporating physical information transfer. It provides new insights for the design and service performance optimization of nickel-based alloy materials.",dynamic fracture toughness nickel based alloys closely related internal strengthening phases external impact loads however prediction fracture toughness challenging due complex factors multi scale effects non linearity strong correlations address challenges novel prediction method dynamic fracture toughness nickel based alloys proposed integrates multi scale numerical simulations physical models machine learning techniques method considers external factors impact velocity internal factors size distribution strengthening phases study analyzes correlation impact velocity size distribution strengthening phases dynamic fracture toughness also proposes optimization strategy hidden layer depth number neurons neural network resulting optimal network structure layers neurons per layer predicting dynamic fracture toughness based optimized network structure study identifies optimal sample size ensuring balance computational efficiency prediction accuracy fracture toughness model furthermore comparative analysis conducted fitting prediction performance various machine learning models including artificial neural networks ann support vector machines svm nearest neighbors knn decision trees among ann model demonstrates best performance average absolute error coefficient determination additional validation confirms model excellent capability interpolation extrapolation research successfully achieves accurate predictions dynamic fracture toughness nickel based alloys incorporating physical information transfer provides new insights design service performance optimization nickel based alloy materials
"In modern society, programming has become a valuable skill for everyone. However, a shortage of computer science teachers in schools limits students’ opportunities to develop these skills. To address this gap, developing computer programming MOOCs can be a scalable solution, enabling students to transition from technology users to creators. This paper aims to analyze, describe, and evaluate the development process of the computer programming MOOC “From Technology Consumer to Creator” (TCC) for youths, highlighting its potential to mitigate the shortage of computer science education opportunities. Developmental research design is used to research the TCC course development process. Stages of the ADDIE model (Analyze, Design, Development, Implementation, Evaluation) are used to describe the process. 1386 students from more than half of the secondary-level schools in Estonia participated in the course, and more than 40 % of participants finished the course successfully. Results show that both learners and mentors were satisfied with the topics and course materials and found the course to be useful, interesting, and feasible. Human support from mentors and organizers is more valuable to adolescents than machine support, such as automated feedback. Overall, the TCC course is a good example of an introductory programming MOOC for adolescents, where successful students can continue their studies at the university. The results of our study provide valuable insights for developing programming MOOCs with various engagement strategies for increasing course completion rates for young people, which can benefit other MOOC developers and instructors planning computer science courses for youths.",modern society programming become valuable skill everyone however shortage computer science teachers schools limits students opportunities develop skills address gap developing computer programming moocs scalable solution enabling students transition technology users creators paper aims analyze describe evaluate development process computer programming mooc technology consumer creator tcc youths highlighting potential mitigate shortage computer science education opportunities developmental research design used research tcc course development process stages addie model analyze design development implementation evaluation used describe process students half secondary level schools estonia participated course participants finished course successfully results show learners mentors satisfied topics course materials found course useful interesting feasible human support mentors organizers valuable adolescents machine support automated feedback overall tcc course good example introductory programming mooc adolescents successful students continue studies university results study provide valuable insights developing programming moocs various engagement strategies increasing course completion rates young people benefit mooc developers instructors planning computer science courses youths
"This paper investigates the construction of rational motions of a minimal quaternionic degree that generate a prescribed plane trajectory (a “rational torse”). Using the algebraic framework of dual quaternions, we formulate the problem as a system of polynomial equations. We derive necessary and sufficient conditions for the existence of such motions, establish a method to compute solutions and characterize solutions of minimal degree. Our findings reveal that a rational torse is realizable as a trajectory of a rational motion if and only if its Gauss map is rational. Furthermore, we demonstrate that the minimal degree of a motion polynomial is geometrically related to a drop of degree of the Gauss and algebraically determined by the structure of the torse’s associated plane polynomial and the real greatest common divisor of its vector part. The developed theoretical framework has potential applications in robotics, computer-aided design, and computational kinematic, offering a systematic approach to constructing rational motions of small algebraic complexity.",paper investigates construction rational motions minimal quaternionic degree generate prescribed plane trajectory rational torse using algebraic framework dual quaternions formulate problem system polynomial equations derive necessary sufficient conditions existence motions establish method compute solutions characterize solutions minimal degree findings reveal rational torse realizable trajectory rational motion gauss map rational furthermore demonstrate minimal degree motion polynomial geometrically related drop degree gauss algebraically determined structure torse associated plane polynomial real greatest common divisor vector part developed theoretical framework potential applications robotics computer aided design computational kinematic offering systematic approach constructing rational motions small algebraic complexity
"Determining multiscale properties of the media is a challenging task because the observables often are of a macroscopic nature, e.g., production data in porous heterogeneous flows. This paper proposes a new approach for determining effective properties for multicontinuum models that arise in multiscale problems. Multicontinuum models use multiple macroscopic parameters in each coarse grid block for accurate prediction. Each macroscopic variable is associated with a local constraint problem for the averages. By changing the number of continua, one can achieve better accuracy while having continuous coarse-scale quantities (with physical meaning), such as solutions and effective properties. As a result, the inversion is performed on a coarse grid using smoothing constraints. In the proposed approach, the problem of computing is transformed into a problem of parameter identification and determining the macroscopic model representing the underlying multiscale problem. An optimization approach called an artificial bee colony algorithm is used for parameter identification. As an example, a dual continuum model is considered. A regular algorithm with fixed weight coefficients of smoothing constraints and a balanced algorithm in which the weight coefficients are updated to keep the objective function members balanced are proposed. To verify the proposed multicontinuum inversion approach, model problems with different heterogeneous properties and source terms are considered. Both optimization algorithms can efficiently identify the effective properties of the multicontinuum model. The balanced algorithm achieves faster convergence, while the regular algorithm provides smoother effective properties.",determining multiscale properties media challenging task observables often macroscopic nature production data porous heterogeneous flows paper proposes new approach determining effective properties multicontinuum models arise multiscale problems multicontinuum models use multiple macroscopic parameters coarse grid block accurate prediction macroscopic variable associated local constraint problem averages changing number continua one achieve better accuracy continuous coarse scale quantities physical meaning solutions effective properties result inversion performed coarse grid using smoothing constraints proposed approach problem computing transformed problem parameter identification determining macroscopic model representing underlying multiscale problem optimization approach called artificial bee colony algorithm used parameter identification example dual continuum model considered regular algorithm fixed weight coefficients smoothing constraints balanced algorithm weight coefficients updated keep objective function members balanced proposed verify proposed multicontinuum inversion approach model problems different heterogeneous properties source terms considered optimization algorithms efficiently identify effective properties multicontinuum model balanced algorithm achieves faster convergence regular algorithm provides smoother effective properties
"Fluorescent probes are widely employed in metal ion detection because of their high responsiveness and ability to distinguish between different ions. These probes provide continuous, non-invasive monitoring of ions, making them valuable for diagnostic and therapeutic applications. Herein, we synthesized a fluorescent probe 1 having a naphthalimide-benzothiazole skeleton for the selective detection of Fe3+ ions. The optical characteristics of the probe exhibit ratiometric behavior in absorption spectroscopy, whereas a “turn-off” fluorescence response is observed upon interaction with Fe3+ ions. These perturbations could be attributed to the formation of a metal-probe complex, thereby altering the electronic configuration. Further, the LOD for Fe3+ was determined to be 1.05 μM. The probe was found to exhibit reversibility and recyclability in the presence of a chelating agent, EDTA. The electronic structure analysis of the probe and its Fe3+ complex was conducted using Density Functional Theory (DFT) methods. It exhibits a decrease in HOMO-LUMO energy gap upon metal coordination, suggesting enhanced stability and reactivity. The probe also exhibits favorable nonlinear optical properties, with a significant hyperpolarizability value, highlighting its potential for optical applications. Additionally, the topological analysis revealed the spatial localization of the electrons in the probe and its complex. To gain biological insights, in silico studies of probe 1 were carried out with DNA and serum proteins such as HSA and BSA, underscoring its dual functionality as both a probe and a potential therapeutic agent. Molecular docking studies further substantiated the favorable interactions between probe 1 and its biological targets (HSA and BSA), thereby confirming its biological relevance and supporting its potential for efficient target-specific delivery in the human body when used as a therapeutic agent.",fluorescent probes widely employed metal ion detection high responsiveness ability distinguish different ions probes provide continuous non invasive monitoring ions making valuable diagnostic therapeutic applications herein synthesized fluorescent probe naphthalimide benzothiazole skeleton selective detection ions optical characteristics probe exhibit ratiometric behavior absorption spectroscopy whereas turn fluorescence response observed upon interaction ions perturbations could attributed formation metal probe complex thereby altering electronic configuration lod determined probe found exhibit reversibility recyclability presence chelating agent edta electronic structure analysis probe complex conducted using density functional theory dft methods exhibits decrease homo lumo energy gap upon metal coordination suggesting enhanced stability reactivity probe also exhibits favorable nonlinear optical properties significant hyperpolarizability value highlighting potential optical applications additionally topological analysis revealed spatial localization electrons probe complex gain biological insights silico studies probe carried dna serum proteins hsa bsa underscoring dual functionality probe potential therapeutic agent molecular docking studies substantiated favorable interactions probe biological targets hsa bsa thereby confirming biological relevance supporting potential efficient target specific delivery human body used therapeutic agent
"In this study, we investigated how individual differences in cognitive abilities (i.e., fluid intelligence, broad retrieval ability, and similarity reasoning ability) relate to the human metaphor comprehension process using the prediction model with two processes (spreading activation of the vehicle, m, and constraint in the topic, k) to compute metaphor interpretations. The participants (N = 85) completed cognitive ability tasks (Japanese version of the Semantic Similarities Test, Raven Progressive Matrices Short, and Ideation fluency task) and a metaphorical interpretation task, in which they listed multiple features of the presented metaphors. Each participant’s interpretation of the metaphors was modeled using the predication model, and the parameters (k and m) were determined to maximize the evaluation metrics (Mean Average Precision and Mean Reciprocal Rank). We found that the similarity reasoning ability affected the spreading activation of the vehicle concept and the degree of constraints on the topic. Compared to previous studies that found that the outcome of metaphor comprehension relates to similarity reasoning ability, our results showed that similarity reasoning ability is also related to the processes of metaphor comprehension.",study investigated individual differences cognitive abilities fluid intelligence broad retrieval ability similarity reasoning ability relate human metaphor comprehension process using prediction model two processes spreading activation vehicle constraint topic compute metaphor interpretations participants completed cognitive ability tasks japanese version semantic similarities test raven progressive matrices short ideation fluency task metaphorical interpretation task listed multiple features presented metaphors participant interpretation metaphors modeled using predication model parameters determined maximize evaluation metrics mean average precision mean reciprocal rank found similarity reasoning ability affected spreading activation vehicle concept degree constraints topic compared previous studies found outcome metaphor comprehension relates similarity reasoning ability results showed similarity reasoning ability also related processes metaphor comprehension
"As part of their broader societal responsibilities, higher education institutions face increasing pressure to contribute to societal challenges while also training our future professionals to address these issues. Given the growing complexity of today’s societal challenges, tackling them both requires inter- and transdisciplinary approaches to research and teaching. This study shows how an innovative teaching approach, which integrated street-art into an inter- and transdisciplinary course, facilitated new opportunities for learning to address complex societal issues in a higher education setting. The application of street-art as a ‘boundary object’ catalysed knowledge integration and interdisciplinary collaboration by facilitating communication, bridging disciplinary and cultural divides, and forging a shared language among students. We found that engagement with, and the production of, street-art fostered inter- and transdisciplinary learning in the following ways: 1) street-art supported the development of new competencies for students, such as humility and a deeper understanding of diverse perspectives, 2) street-art served as a means for learning to collaborate effectively in a team, and 3) street-art enabled the bridging of cultural differences, challenging students’ prior views on art and its purposes. Additionally, we found that the use of a non-traditional medium, the course design, and the collaboration with an art-based community partner were crucial in facilitating these learning processes. We conclude that street-art is a valuable tool for promoting inter- and transdisciplinary approaches to teaching, in which diverse perspectives can come together, and independent, critical, and creative thinking are fostered.",part broader societal responsibilities higher education institutions face increasing pressure contribute societal challenges also training future professionals address issues given growing complexity today societal challenges tackling requires inter transdisciplinary approaches research teaching study shows innovative teaching approach integrated street art inter transdisciplinary course facilitated new opportunities learning address complex societal issues higher education setting application street art boundary object catalysed knowledge integration interdisciplinary collaboration facilitating communication bridging disciplinary cultural divides forging shared language among students found engagement production street art fostered inter transdisciplinary learning following ways street art supported development new competencies students humility deeper understanding diverse perspectives street art served means learning collaborate effectively team street art enabled bridging cultural differences challenging students prior views art purposes additionally found use non traditional medium course design collaboration art based community partner crucial facilitating learning processes conclude street art valuable tool promoting inter transdisciplinary approaches teaching diverse perspectives come together independent critical creative thinking fostered
"The primary inspiration behind shrinkage methods is the desire to reduce variance without significantly increasing bias. At the heart of shrinkage estimators lies the concept of the bias-variance trade-off. Shrinkage estimators intentionally introduce a small bias by pulling estimates towards a central value, significantly reducing variance and thereby lowering the mean squared error. Many industries, such as banking, healthcare, and environmental research, where precise parameter estimation is necessary for modelling and decision-making, benefit from shrinkage estimation in gamma density utilizing prior information. This manuscript presents a class of interval shrinkage estimators to estimate the scale parameter of gamma density when prior information or guessed values about the parameter in the form of lower and upper bounds are available, and studies their properties. In particular, three classes of shrinkage estimators of the scale parameter based on arithmetic mean, geometric mean, and harmonic mean of interval estimators are also derived.Theoretical comparison given to show the performance over unbiased and MMSE estimator. An empirical study is carried out in order to judge the merits of the suggested interval shrinkage estimation over others.",primary inspiration behind shrinkage methods desire reduce variance without significantly increasing bias heart shrinkage estimators lies concept bias variance trade shrinkage estimators intentionally introduce small bias pulling estimates towards central value significantly reducing variance thereby lowering mean squared error many industries banking healthcare environmental research precise parameter estimation necessary modelling decision making benefit shrinkage estimation gamma density utilizing prior information manuscript presents class interval shrinkage estimators estimate scale parameter gamma density prior information guessed values parameter form lower upper bounds available studies properties particular three classes shrinkage estimators scale parameter based arithmetic mean geometric mean harmonic mean interval estimators also derived theoretical comparison given show performance unbiased mmse estimator empirical study carried order judge merits suggested interval shrinkage estimation others
"In this paper, we establish existence results for generalized equilibrium problems. The constrained set is an arbitrary nonempty set without any algebraic or topological structure. Then, we study generalized quasi-equilibrium problems under a weak equilibrium condition. In this study, we apply the KKM technique, a basic result in fixed point theory. Our result is an extension of Balaj result in Balaj (2021).",paper establish existence results generalized equilibrium problems constrained set arbitrary nonempty set without algebraic topological structure study generalized quasi equilibrium problems weak equilibrium condition study apply kkm technique basic result fixed point theory result extension balaj result balaj
"In the 21st century, the ability to think critically has become an essential skill that can be effectively fostered by dialogic teaching methods. However, teachers frequently struggle to engage students in instructional settings that require interaction and dialogue within classroom discourse. Using a quasi-experimental, mixed-methods research design, this study explored how a planned whole-class teaching discourse strategy affected the critical thinking disposition of 102 eighth-grade students from two Hong Kong schools. A validated critical thinking disposition questionnaire was used for pre- and post-test data collection to assess the effect of the teaching discourse intervention. Paired t-test results revealed significant pre- and post-test differences in the overall score and Truth-seeking, Systematicity, and Maturity of Judgment subset scores in the experimental group. However, no pre- and post-test changes were found in the control group. Further analysis of 32 classroom discourse video recordings conducted using a talk move framework identified three contributing factors that seemed to boost students’ critical thinking disposition in the experimental group: a range of talk moves, the turn-taking pattern between teachers and students, and timely, descriptive, and specific praise from teachers. This study contributes to the existing literature on the effectiveness of deliberately employing classroom talk wherein the talk move strategy encourages both students and teachers to engage in critical thinking, thereby fostering cognitive engagement in a nurturing learning environment.",century ability think critically become essential skill effectively fostered dialogic teaching methods however teachers frequently struggle engage students instructional settings require interaction dialogue within classroom discourse using quasi experimental mixed methods research design study explored planned whole class teaching discourse strategy affected critical thinking disposition eighth grade students two hong kong schools validated critical thinking disposition questionnaire used pre post test data collection assess effect teaching discourse intervention paired test results revealed significant pre post test differences overall score truth seeking systematicity maturity judgment subset scores experimental group however pre post test changes found control group analysis classroom discourse video recordings conducted using talk move framework identified three contributing factors seemed boost students critical thinking disposition experimental group range talk moves turn taking pattern teachers students timely descriptive specific praise teachers study contributes existing literature effectiveness deliberately employing classroom talk wherein talk move strategy encourages students teachers engage critical thinking thereby fostering cognitive engagement nurturing learning environment
"While previous research has predominantly concentrated on the immediate impact of trauma on creativity, the current study delves into the enduring impact of traumatic experiences on creative thinking and behaviors. Moreover, this investigation explores the potential roles that trait resilience and psychological richness may play in the interplay between trauma and creativity. The data collection process involved administering questionnaires to assess participants’ trauma history, psychological richness, trait resilience, and creative activities and achievements. Additionally, the Alternative Uses Test was utilized to evaluate creative thinking. This study gathered data from a sample of 649 college students (228 males and 421 females). Findings from the path analysis model reveal that: (1) exposure to trauma can positively predict the four facets of creativity — fluency (β = 0.146), originality (β = 0.193), engagement in creative activities (β = 0.170), and creative accomplishments (β = 0.200); (2) trait resilience is positively related to both fluency (β = 0.198) and originality (β = 0.185), and it serves as a moderator in the relationship between trauma exposure and creative accomplishments and activities; and (3) psychological richness acts as a mediator in the effects of trauma exposure on originality (β = 0.012), creative accomplishments (β = 0.014), and the frequency of creative activities (β = 0.025). By extending the scope of existing research on trauma and creativity, this study offers valuable insights into the development of effective psychological interventions for individuals coping with trauma's aftermath.",previous research predominantly concentrated immediate impact trauma creativity current study delves enduring impact traumatic experiences creative thinking behaviors moreover investigation explores potential roles trait resilience psychological richness may play interplay trauma creativity data collection process involved administering questionnaires assess participants trauma history psychological richness trait resilience creative activities achievements additionally alternative uses test utilized evaluate creative thinking study gathered data sample college students males females findings path analysis model reveal exposure trauma positively predict four facets creativity fluency originality engagement creative activities creative accomplishments trait resilience positively related fluency originality serves moderator relationship trauma exposure creative accomplishments activities psychological richness acts mediator effects trauma exposure originality creative accomplishments frequency creative activities extending scope existing research trauma creativity study offers valuable insights development effective psychological interventions individuals coping trauma aftermath
"This study applied a multi-CDM approach to diagnose EFL learners’ inferential ability via an online diagnostic reading test covering six inferential subskills. By integrating data-driven approach with linguistically informed modifications, the multi-CDM was validated using responses from 886 Chinese university students. Results demonstrated the superiority of multi-CDM over six single CDMs: it achieved a decent model-data fit, the highest test-level classification accuracy (72.1%) and subskill-level reliability (87%-95.8%), showing robust diagnostic performance. Subsequent diagnosis revealed pronounced disparities: high-proficiency learners excelled in literal comprehension and local inferences, yet struggled with temporal inference. Low-proficiency learners exhibited systemic deficiencies, particularly in global inferences and literal comprehension, underscoring cognitive overload challenges. Instructional implications were provided to different groups. Finally, limitations and future directions were discussed.",study applied multi cdm approach diagnose efl learners inferential ability via online diagnostic reading test covering six inferential subskills integrating data driven approach linguistically informed modifications multi cdm validated using responses chinese university students results demonstrated superiority multi cdm six single cdms achieved decent model data fit highest test level classification accuracy subskill level reliability showing robust diagnostic performance subsequent diagnosis revealed pronounced disparities high proficiency learners excelled literal comprehension local inferences yet struggled temporal inference low proficiency learners exhibited systemic deficiencies particularly global inferences literal comprehension underscoring cognitive overload challenges instructional implications provided different groups finally limitations future directions discussed
"While most literature on critical thinking emphasizes its cognitive dimension, this study explores criticality as a metacognitive experience in cultivating critical citizenship. Drawing on Paul Ricœur’s (1965/1970) hermeneutics of suspicion, it examines how criticality influences citizenship by encouraging individuals to evolve their interpretations of official discourses. Using Chinese citizenship education as a case study, narrative interviews with 12 purposively selected graduates from elite Chinese universities investigate changes in their interpretive stances over time. Thematic analysis identifies a four-stage, semi-iterative model of worldview transformation: participants initially accepted curricular content uncritically but subsequently encountered moments of suspicion that triggered metacognitive reflection and prompted reconstruction of their worldviews. Academic training contributed variably to this process. Confronted with everyday suspicions in social interactions and tensions between state narratives and personal interpretations, participants transformed themselves into critical beings through hermeneutic circle to reconcile these discourse gaps. Thus, the study advances understanding of how criticality can be unexpectedly activated, reflexively exercised, and intellectually directed to foster critical, reflexive, and deliberative citizenship. It also highlights the need for further research on the unintended civic effects of cultivating critical thinking, particularly its role in demystifying ideology, promoting epistemic autonomy, and constructing alternative worldviews.",literature critical thinking emphasizes cognitive dimension study explores criticality metacognitive experience cultivating critical citizenship drawing paul ric hermeneutics suspicion examines criticality influences citizenship encouraging individuals evolve interpretations official discourses using chinese citizenship education case study narrative interviews purposively selected graduates elite chinese universities investigate changes interpretive stances time thematic analysis identifies four stage semi iterative model worldview transformation participants initially accepted curricular content uncritically subsequently encountered moments suspicion triggered metacognitive reflection prompted reconstruction worldviews academic training contributed variably process confronted everyday suspicions social interactions tensions state narratives personal interpretations participants transformed critical beings hermeneutic circle reconcile discourse gaps thus study advances understanding criticality unexpectedly activated reflexively exercised intellectually directed foster critical reflexive deliberative citizenship also highlights need research unintended civic effects cultivating critical thinking particularly role demystifying ideology promoting epistemic autonomy constructing alternative worldviews
"We give a short proof of almost sure invertibility of unsymmetric random Kansa collocation matrices by a class of analytic RBF vanishing at infinity, for the Poisson equation with Dirichlet boundary conditions. Such a class includes popular Positive Definite instances such as Gaussians, Generalized Inverse MultiQuadrics and Matérn RBF. The proof works on general domains in any dimension and with any distribution of boundary collocation points, assuming that the internal collocation points are i.i.d. continuous random variables with respect to any probability density.",give short proof almost sure invertibility unsymmetric random kansa collocation matrices class analytic rbf vanishing infinity poisson equation dirichlet boundary conditions class includes popular positive definite instances gaussians generalized inverse multiquadrics matérn rbf proof works general domains dimension distribution boundary collocation points assuming internal collocation points continuous random variables respect probability density
"Numerous practical problems give rise to nonlinear differential equations that may exhibit multiple nontrivial solutions relevant to applications. Efficiently computing these solutions is crucial for a profound understanding of these problems and enhancing various applications. Therefore, the development of a numerical method capable of finding multiple solutions efficiently is imperative. Additionally, the provision of an efficient iteration process is vital for promptly obtaining multiple solutions. In the current paper, we introduce a novel algorithm for identifying multiple solutions of semilinear elliptic systems, where the trust region Levenberg–Marquardt method, combined with the deflation technique, is designed to compute multiple solutions for the first time. Based on several numerical experiments, our algorithm demonstrates efficacy in efficiently identifying multiple solutions, even when the nonlinear term appearing in these equations involves solely the first derivative. Moreover, we validate the efficiency of our algorithm and unveil previously undiscovered solutions in the existing literature",numerous practical problems give rise nonlinear differential equations may exhibit multiple nontrivial solutions relevant applications efficiently computing solutions crucial profound understanding problems enhancing various applications therefore development numerical method capable finding multiple solutions efficiently imperative additionally provision efficient iteration process vital promptly obtaining multiple solutions current paper introduce novel algorithm identifying multiple solutions semilinear elliptic systems trust region levenberg marquardt method combined deflation technique designed compute multiple solutions first time based several numerical experiments algorithm demonstrates efficacy efficiently identifying multiple solutions even nonlinear term appearing equations involves solely first derivative moreover validate efficiency algorithm unveil previously undiscovered solutions existing literature
N/D,
"We extend the Reynolds Transport Theorem to a weak formulation using duality pairings in Sobolev spaces, thereby enabling the analysis of time-dependent domains with irregular geometries and moving boundaries. This approach naturally incorporates interface dynamics and local conservation laws, providing a robust framework for multiphysics problems such as fluid–structure interactions and phase transitions. Moreover, the duality-based formulation facilitates the development of stable numerical methods, further broadening its applicability to contemporary challenges in engineering and mathematical physics.",extend reynolds transport theorem weak formulation using duality pairings sobolev spaces thereby enabling analysis time dependent domains irregular geometries moving boundaries approach naturally incorporates interface dynamics local conservation laws providing robust framework multiphysics problems fluid structure interactions phase transitions moreover duality based formulation facilitates development stable numerical methods broadening applicability contemporary challenges engineering mathematical physics
"This paper investigates the B-Fredholm spectral properties of unbounded block operator matrices defined on Banach spaces, motivated by their foundational role in spectral theory and their occurrence in physical models governed by evolution equations. Under a relaxed set of assumptions, the analysis provides a refined characterization of the B-essential spectra. The study introduces pivotal theorems that establish relationships between the spectra of the full operator matrix and its diagonal components, using Schur complement techniques. Numerical examples are included to illustrate the theoretical results, with explicit computations of resolvents and B-essential spectra for selected operator structures. The results contribute to advancing the spectral theory of unbounded operator matrices and open new directions that link abstract results to concrete applications within a numerical framework.",paper investigates fredholm spectral properties unbounded block operator matrices defined banach spaces motivated foundational role spectral theory occurrence physical models governed evolution equations relaxed set assumptions analysis provides refined characterization essential spectra study introduces pivotal theorems establish relationships spectra full operator matrix diagonal components using schur complement techniques numerical examples included illustrate theoretical results explicit computations resolvents essential spectra selected operator structures results contribute advancing spectral theory unbounded operator matrices open new directions link abstract results concrete applications within numerical framework
"This study investigates the professional competency development of primary English education (PEE) majors and explores strategies to foster sustainable competence development (SCD) through qualitative analysis of semi-structured interviews with 55 PEE majors at a university in the People’s Republic of China. Participants addressed three core themes: (a) challenges in competency development, (b) barriers to sustainable growth, and (c) institutional/non-institutional strategies for promoting SCD. Using grounded theory, the analysis reveals that institutional and non-institutional factors collectively constrain students’ SCD, including curricular gaps, pedagogical limitations, and underdeveloped autonomous learning practices. The findings further highlight disparities in faculty expertise, inadequate alignment between coursework and practical teaching demands, and limited support for lifelong learning competencies. To address these issues, this study proposes a series of evidence-based countermeasures: reforming teaching methodologies in accordance with self-determination theory (SDT), enhancing faculty training through the integration of the TPACK framework, redesigning curricula to incorporate sustainable development perspectives, establishing collaborative learning environments to promote resource equity and improve teaching conditions, and cultivating students’ self-directed learning skills grounded in SDT principles. These recommendations are designed to ensure that PEE programs remain aligned with the evolving requirements of primary English education and facilitate the long-term professional development of graduates.",study investigates professional competency development primary english education pee majors explores strategies foster sustainable competence development scd qualitative analysis semi structured interviews pee majors university people republic china participants addressed three core themes challenges competency development barriers sustainable growth institutional non institutional strategies promoting scd using grounded theory analysis reveals institutional non institutional factors collectively constrain students scd including curricular gaps pedagogical limitations underdeveloped autonomous learning practices findings highlight disparities faculty expertise inadequate alignment coursework practical teaching demands limited support lifelong learning competencies address issues study proposes series evidence based countermeasures reforming teaching methodologies accordance self determination theory sdt enhancing faculty training integration tpack framework redesigning curricula incorporate sustainable development perspectives establishing collaborative learning environments promote resource equity improve teaching conditions cultivating students self directed learning skills grounded sdt principles recommendations designed ensure pee programs remain aligned evolving requirements primary english education facilitate long term professional development graduates
"Creativity is an essential skill in contemporary education systems, particularly in early childhood, where it is stimulated by exploratory activities and the expression of thought through the creation of ideas or the production of new objects. However, its assessment remains a major challenge, particularly for future teachers, who have to reconcile the emergence of creativity with the requirements of training plans. This dual injunction reveals a hesitancy on the part of professionals regarding the definition of creativity and the ways in which its manifestations in students can be assessed. This study explores how future teachers of early childhood integrate evaluation criteria measuring the development of creativity into their evaluation documents. It aims to understand which dimensions of the creative process and theoretical models, such as the 3Ps and the OECD Creative Assessment Grid, are mobilised in an evaluative context. Two hypotheses guided this work: (1) trainee teachers give priority to technical and observable dimensions, often to the detriment of the creative process, and (2) transversal aspects, such as originality and collaboration, are underrepresented. To test these hypotheses, 153 evaluation documents produced by student teachers at the HEP Vaud were analysed using a specific grid. The study combined an analysis of proportions, correlations and a hierarchical classification of evaluation criteria. The results reveal that emphasis is placed on technical and methodological criteria, such as respect for constraints and diversity of materials, while less tangible dimensions, such as originality and collaboration, are marginally taken into account. These trends reflect the systemic challenges involved in assessing creativity that is non-linear and exploratory in nature in early childhood. These initial elements are in line with previous research which emphasises the predominance of observable and measurable aspects of the evaluation of creativity, to the detriment of the creative process, which is more complex to identify. The study also highlights a preference for assessing individual performance to the detriment of more collaborative dynamics. This observation highlights the need to reassess the way in which creativity is understood and taught in teacher training programmes. In conclusion, this research highlights the importance of equipping future teachers with tools to assess both tangible outcomes and intangible processes, paving the way for a balanced and effective approach to promoting creativity in young learners in a rapidly changing educational landscape.",creativity essential skill contemporary education systems particularly early childhood stimulated exploratory activities expression thought creation ideas production new objects however assessment remains major challenge particularly future teachers reconcile emergence creativity requirements training plans dual injunction reveals hesitancy part professionals regarding definition creativity ways manifestations students assessed study explores future teachers early childhood integrate evaluation criteria measuring development creativity evaluation documents aims understand dimensions creative process theoretical models oecd creative assessment grid mobilised evaluative context two hypotheses guided work trainee teachers give priority technical observable dimensions often detriment creative process transversal aspects originality collaboration underrepresented test hypotheses evaluation documents produced student teachers hep vaud analysed using specific grid study combined analysis proportions correlations hierarchical classification evaluation criteria results reveal emphasis placed technical methodological criteria respect constraints diversity materials less tangible dimensions originality collaboration marginally taken account trends reflect systemic challenges involved assessing creativity non linear exploratory nature early childhood initial elements line previous research emphasises predominance observable measurable aspects evaluation creativity detriment creative process complex identify study also highlights preference assessing individual performance detriment collaborative dynamics observation highlights need reassess way creativity understood taught teacher training programmes conclusion research highlights importance equipping future teachers tools assess tangible outcomes intangible processes paving way balanced effective approach promoting creativity young learners rapidly changing educational landscape
"Creativity plays a vital role in various domains of life, and the rise of social media has changed how people generate, share, and engage with creative content. Previous cross-sectional studies yielded mixed findings on the relationship between social media use (SMU) and self-perceived creativity. It remains unclear how multiple aspects of SMU are associated with self-perceived creativity over time. This study aims to investigate the links between self-perceived creativity and various facets of SMU, including duration, form, and purpose. We conducted a 14-day diary study (N = 208) and a six-month interval survey (N = 204). Residual dynamic structural equation modeling results revealed that active SMU, social-oriented, and cognitive-oriented SMU positively predicted self-perceived creativity on the same day. Conversely, the duration of SMU, passive SMU, and hedonic-oriented SMU had no significant effects. Furthermore, longitudinal regression analyses showed that only social-oriented and cognitive-oriented SMU positively predicted self-perceived creativity six months later. Notably, no significant predictive effects were found from self-perceived creativity to SMU. These findings suggest that the impact of SMU on self-perceived creativity depends on the specific ways people engage with social media.",creativity plays vital role various domains life rise social media changed people generate share engage creative content previous cross sectional studies yielded mixed findings relationship social media use smu self perceived creativity remains unclear multiple aspects smu associated self perceived creativity time study aims investigate links self perceived creativity various facets smu including duration form purpose conducted day diary study six month interval survey residual dynamic structural equation modeling results revealed active smu social oriented cognitive oriented smu positively predicted self perceived creativity day conversely duration smu passive smu hedonic oriented smu significant effects furthermore longitudinal regression analyses showed social oriented cognitive oriented smu positively predicted self perceived creativity six months later notably significant predictive effects found self perceived creativity smu findings suggest impact smu self perceived creativity depends specific ways people engage social media
"In this paper, we propose an adaptive finite element method (AFEM) for simulating functional metasurfaces, based on generalized sheet transition conditions (GSTC). By exploiting the GSTC-characterized metasurface properties, we develop a residual-type posteriori error estimator, and rigorously prove its boundedness. Using an absorbing metasurface as a benchmark case, we demonstrate the convergence of the AFEM based on this error estimator. Numerical experiments on diverse functional metasurfaces verify the effectiveness of the proposed approach. Our results indicate that the proposed method significantly improves both the efficiency and accuracy of numerical simulations, providing a reliable theoretical framework and numerical tools for the design and analysis of functional metasurfaces.",paper propose adaptive finite element method afem simulating functional metasurfaces based generalized sheet transition conditions gstc exploiting gstc characterized metasurface properties develop residual type posteriori error estimator rigorously prove boundedness using absorbing metasurface benchmark case demonstrate convergence afem based error estimator numerical experiments diverse functional metasurfaces verify effectiveness proposed approach results indicate proposed method significantly improves efficiency accuracy numerical simulations providing reliable theoretical framework numerical tools design analysis functional metasurfaces
"A Petrov–Galerkin finite element method on a rectangular Shishkin mesh is constructed for a singularly perturbed elliptic problem in two space dimensions. The solution contains a regular boundary layer and two characteristic boundary layers. The test functions are taken to be a tensor product of exponential splines in one coordinate direction and hat functions in the other direction, which results in a stable higher (than first) order numerical method. The differential equation contains a zero order term. Compared to the case of no zero order term, the character of the exponential splines are more complicated and the associated pointwise error analysis relies more on the finite element formulation. Error bounds are given in a global pointwise norm. Numerical results are presented to illustrate the performance of the method.",petrov galerkin finite element method rectangular shishkin mesh constructed singularly perturbed elliptic problem two space dimensions solution contains regular boundary layer two characteristic boundary layers test functions taken tensor product exponential splines one coordinate direction hat functions direction results stable higher first order numerical method differential equation contains zero order term compared case zero order term character exponential splines complicated associated pointwise error analysis relies finite element formulation error bounds given global pointwise norm numerical results presented illustrate performance method
"Generally, a perpetual American strangle option is an investment strategy integrating the characteristics of call and put options under an underlying asset with an infinite time horizon. Investors commonly use this trading strategy as they anticipate the underlying asset to fluctuate considerably but are uncertain about an increase or decrease. In this study, we consider the perpetual American strangle options under the Stochastic Volatility Constant Elasticity of Variance (SVCEV) model and examine the approximated option prices and free boundary values using an asymptotic analysis. Moreover, we verify the pricing accuracy of the approximated solutions for perpetual American strangle options under SVCEV by comparing our solutions with the prices derived from Monte Carlo simulations. Finally, we analyze the price sensitivities of the options and free boundaries in terms of several model parameters. Our findings emphasize that the influence of the SV factor on the option price or the optimal exercise boundary is significant for the effective volatility and the elasticity parameter.",generally perpetual american strangle option investment strategy integrating characteristics call put options underlying asset infinite time horizon investors commonly use trading strategy anticipate underlying asset fluctuate considerably uncertain increase decrease study consider perpetual american strangle options stochastic volatility constant elasticity variance svcev model examine approximated option prices free boundary values using asymptotic analysis moreover verify pricing accuracy approximated solutions perpetual american strangle options svcev comparing solutions prices derived monte carlo simulations finally analyze price sensitivities options free boundaries terms several model parameters findings emphasize influence factor option price optimal exercise boundary significant effective volatility elasticity parameter
"A fractional partition of unity finite element method is proposed for the solution of the transient anomalous diffusion equation. The Caputo integro-differential operator is employed to represent the fractional time-derivative in these problems. To approximate the Caputo fractional derivative, we propose a new numerical differentiation formula using quadratic splines. For the spatial discretization, we implement an enriched finite element method on unstructured meshes. In the present study, a category of exponential functions incorporating fractional orders is introduced as enrichment functions to refine the finite element approximation. These functions are designed to capture the fractional characteristics of the solution more effectively. By integrating these enrichment functions through the partition of unity framework, the method utilizes prior knowledge of the fractional problem, leading to a substantial enhancement in approximation accuracy while preserving the fundamental advantages of the traditional finite element method. Consequently, the proposed approach delivers precise numerical solutions even with coarse meshes and requires significantly fewer degrees of freedom compared to conventional finite element techniques. Moreover, the mesh resolution remains unaffected by variations in the fractional order, allowing for a consistent mesh structure regardless of changes in fractional parameters. Through extensive numerical simulations, we consistently verify the effectiveness of the proposed technique in achieving high levels of accuracy. This approach not only ensures reliable and precise results but also broadens the applicability of the finite element method, making it more capable of handling time-fractional transient diffusion problems that have traditionally been challenging for standard methods.",fractional partition unity finite element method proposed solution transient anomalous diffusion equation caputo integro differential operator employed represent fractional time derivative problems approximate caputo fractional derivative propose new numerical differentiation formula using quadratic splines spatial discretization implement enriched finite element method unstructured meshes present study category exponential functions incorporating fractional orders introduced enrichment functions refine finite element approximation functions designed capture fractional characteristics solution effectively integrating enrichment functions partition unity framework method utilizes prior knowledge fractional problem leading substantial enhancement approximation accuracy preserving fundamental advantages traditional finite element method consequently proposed approach delivers precise numerical solutions even coarse meshes requires significantly fewer degrees freedom compared conventional finite element techniques moreover mesh resolution remains unaffected variations fractional order allowing consistent mesh structure regardless changes fractional parameters extensive numerical simulations consistently verify effectiveness proposed technique achieving high levels accuracy approach ensures reliable precise results also broadens applicability finite element method making capable handling time fractional transient diffusion problems traditionally challenging standard methods
"In this work, we propose a new neural network framework to interface problems. The highlight of this framework lies in its incorporation of a neural network structure composed of multi-tensor neural networks and a loss function integrated with the Nitsche method. Meanwhile, we extend the application scope of tensor neural networks from computational regions that are hypercubes to that are unions of a finite number of disjoint hypercubes. We also propose a method for a special eigenvalue interface problem in engineering, the two-group neutron diffusion problem. We incorporates the idea of decoupling energy loss functions and changes the eigenvalue equations into fixed-source equations. As the neural network is optimized, the fixed-source term evolves with the iterative steps. Finally, ample numerical experiments are presented to validate our methods.",work propose new neural network framework interface problems highlight framework lies incorporation neural network structure composed multi tensor neural networks loss function integrated nitsche method meanwhile extend application scope tensor neural networks computational regions hypercubes unions finite number disjoint hypercubes also propose method special eigenvalue interface problem engineering two group neutron diffusion problem incorporates idea decoupling energy loss functions changes eigenvalue equations fixed source equations neural network optimized fixed source term evolves iterative steps finally ample numerical experiments presented validate methods
"In recent years, computational thinking (CT) has gained growing attention in education, especially as societies become more reliant on digital technologies. Despite the growing emphasis on integrating CT into K − 12 curricula, there is still a shortage of validated tools for assessing CT practices among younger students. This study addresses this gap by developing and validating a game-based assessment tool called Critters Puzzle (CP), specifically designed to evaluate abstraction skills, a key dimension of CT practices, in lower primary school students (Grades 1–3). Based on a principled approach known as evidence-centered game design (ECGD), a set of game tasks independent of programming platforms was developed. The tool’s content validity was confirmed through expert reviews and cognitive interviews, followed by field tests. Psychometric analyses were performed with both Classical Test Theory (CTT) and Confirmatory Factor Analysis (CFA), and the results demonstrate adequate reliability and validity indicators. Further analysis of students’ game performance revealed that female students exhibited stronger abstraction skills. Then, the process data was used to identify students’ problem-solving strategies, and the analysis revealed distinct strategies among students, with observable gender disparities. This study introduces a carefully developed and validated assessment tool tailored for early-grade students within the context of Chinese primary school, focusing on the evaluation of abstraction skills in CT practices. The research process has the potential to support future assessment development in CT education and may serve as a reference for similar educational settings. The findings of process data highlight the importance of designing assessment tools that accommodate different problem-solving strategies and provide valuable insights for enhancing lower primary CT education.",recent years computational thinking gained growing attention education especially societies become reliant digital technologies despite growing emphasis integrating curricula still shortage validated tools assessing practices among younger students study addresses gap developing validating game based assessment tool called critters puzzle specifically designed evaluate abstraction skills key dimension practices lower primary school students grades based principled approach known evidence centered game design ecgd set game tasks independent programming platforms developed tool content validity confirmed expert reviews cognitive interviews followed field tests psychometric analyses performed classical test theory ctt confirmatory factor analysis cfa results demonstrate adequate reliability validity indicators analysis students game performance revealed female students exhibited stronger abstraction skills process data used identify students problem solving strategies analysis revealed distinct strategies among students observable gender disparities study introduces carefully developed validated assessment tool tailored early grade students within context chinese primary school focusing evaluation abstraction skills practices research process potential support future assessment development education may serve reference similar educational settings findings process data highlight importance designing assessment tools accommodate different problem solving strategies provide valuable insights enhancing lower primary education
"Prior research has suggested that larger spaces might enhance creative performance in divergent thinking tasks. To further investigate this spatial effect and elucidate the underlying mechanism, the current study used a controlled manipulation of space size. We aimed at replicating the influence of space size on divergent and convergent thinking, while also examining defocused attention as a potential underlying mechanism through the Auditory Stroop Task. In Experiment 1, participants performed the Alternate Uses Task, Remote Association Task, and Auditory Stroop Task in both large and small physical spaces. Results indicated no significant differences in task performance across spatial conditions, with Bayesian analyses strongly supporting the null hypothesis. Experiment 2 employed a virtual reality environment, presenting participants with large and small virtual rooms where they completed the same three cognitive tasks using a virtual reality headset. Findings demonstrated higher novelty scores in divergent thinking in the smaller (vs. larger) virtual space, but no significant differences in other tasks. Bayesian analyses again favored the null hypothesis. Overall, the findings of the two experiments suggest that space size, whether manipulated in the physical or virtual environment, does not influence divergent thinking, convergent thinking, or defocused attention. The implications of these findings on embodied metaphor and defocused attention accounts are discussed.",prior research suggested larger spaces might enhance creative performance divergent thinking tasks investigate spatial effect elucidate underlying mechanism current study used controlled manipulation space size aimed replicating influence space size divergent convergent thinking also examining defocused attention potential underlying mechanism auditory stroop task experiment participants performed alternate uses task remote association task auditory stroop task large small physical spaces results indicated significant differences task performance across spatial conditions bayesian analyses strongly supporting null hypothesis experiment employed virtual reality environment presenting participants large small virtual rooms completed three cognitive tasks using virtual reality headset findings demonstrated higher novelty scores divergent thinking smaller larger virtual space significant differences tasks bayesian analyses favored null hypothesis overall findings two experiments suggest space size whether manipulated physical virtual environment influence divergent thinking convergent thinking defocused attention implications findings embodied metaphor defocused attention accounts discussed
"This study looked at how combining connected learning and STEAM activities affected elementary school pupils' digital skills. The main topics were learning and innovation skills for the 21st century, social entrepreneurship goals, and digital media literacy. The study employed a mixed-method exploratory sequential design, initially identifying student interests through interviews with both parents and students. These observations directly influenced the development of a 10-week STEAM-oriented scientific curriculum based on the ideas of connected learning. We used the 21st Century Learning and Innovation Skills Scale, the Social Entrepreneurship Intentions Scale, and the Digital Literacy Scale to test people before and after training. We used structural equation modeling to look at the data. The results showed that combining STEAM activities with connected learning greatly improved students' digital media literacy. This was made even better by their gaining of 21st-century skills and social entrepreneurship abilities. This research validates the efficacy of integrating STEAM with linked learning to cultivate crucial digital competences and innovative talents among young learners.",study looked combining connected learning steam activities affected elementary school pupils digital skills main topics learning innovation skills century social entrepreneurship goals digital media literacy study employed mixed method exploratory sequential design initially identifying student interests interviews parents students observations directly influenced development week steam oriented scientific curriculum based ideas connected learning used century learning innovation skills scale social entrepreneurship intentions scale digital literacy scale test people training used structural equation modeling look data results showed combining steam activities connected learning greatly improved students digital media literacy made even better gaining century skills social entrepreneurship abilities research validates efficacy integrating steam linked learning cultivate crucial digital competences innovative talents among young learners
"This work aims to assess the predictive effectiveness of machine learning models for four Saudi Arabian cities—AdDarb, AdDilam, AdDirIyyah, and AdDuwadimi—COVID-19 case forecasting. Using RMSE, MAPE, R², training and prediction durations, and computational variance, three models—Random Forest, SVR, and XGBoost—are examined. Although Random Forest exhibits increased temporal complexity and unpredictability in prediction timeframes, it achieves the highest predictive accuracy with a R² score of 0.999998 in AdDarb and comparable results across other cities. For quick and consistent forecasting, XGBoost is perfect as it provides competitive accuracy, low prediction time variation, and computational economy. Although less accurate, SVR performs best in low training and prediction timeframes, which qualifies it for environments with limited resources. This work offers a complete framework for choosing machine learning models fit to the dynamic and high-stakes criteria of COVID-19 prediction by including temporal variance and computational trends. XGBoost's balanced performance makes it advised to provide top priority for real-time forecasting situations; Random Forest might be saved for tasks stressing great predictive accuracy. SVR may also be effectively used in environments with low processing capacity. The main novelty of this study lies in integrating model scalability, prediction consistency, and time-complexity metrics into a unified framework for real-time public health forecasting in Saudi Arabia.",work aims assess predictive effectiveness machine learning models four saudi arabian cities addarb addilam addiriyyah adduwadimi covid case forecasting using rmse mape training prediction durations computational variance three models random forest svr xgboost examined although random forest exhibits increased temporal complexity unpredictability prediction timeframes achieves highest predictive accuracy score addarb comparable results across cities quick consistent forecasting xgboost perfect provides competitive accuracy low prediction time variation computational economy although less accurate svr performs best low training prediction timeframes qualifies environments limited resources work offers complete framework choosing machine learning models fit dynamic high stakes criteria covid prediction including temporal variance computational trends xgboost balanced performance makes advised provide top priority real time forecasting situations random forest might saved tasks stressing great predictive accuracy svr may also effectively used environments low processing capacity main novelty study lies integrating model scalability prediction consistency time complexity metrics unified framework real time public health forecasting saudi arabia
"To solve the scale ambiguity problem that exists when outputting separated signals in convolutional mixture blind source separation, this paper proposes a new algorithm called maximum a posteriori scale-certainty geometrically constrained auxiliary function-based independent vector analysis (MSCGC-AuxIVA). The MSCGC-AuxIVA algorithm is derived from SCGC-AuxIVA by adding a priori probability density function to the objective function. This algorithm allows combining prior knowledge about the hierarchical system through prior probability density, effectively avoiding the problem of missing prior information caused by deterministic geometric parameters, and effectively solving the problem of scale ambiguity in signal output. Simulation experiments results corroborate that the proposed method is superior to other algorithms in terms of separation performance and computational efficiency.",solve scale ambiguity problem exists outputting separated signals convolutional mixture blind source separation paper proposes new algorithm called maximum posteriori scale certainty geometrically constrained auxiliary function based independent vector analysis mscgc auxiva mscgc auxiva algorithm derived scgc auxiva adding priori probability density function objective function algorithm allows combining prior knowledge hierarchical system prior probability density effectively avoiding problem missing prior information caused deterministic geometric parameters effectively solving problem scale ambiguity signal output simulation experiments results corroborate proposed method superior algorithms terms separation performance computational efficiency
"A novel method is introduced for denoising partially observed signals over networks using graph total variation (TV) regularization, a technique adapted from signal processing to handle binary data. This approach extends existing results derived for Gaussian data to the discrete, binary case — a method hereafter referred to as “one-bit TV denoising.” The framework considers a network represented as a set of nodes with binary observations, where edges encode pairwise relationships between nodes. A key theoretical contribution is the establishment of consistency guarantees of graph TV denoising for the recovery of underlying node-level probabilities. The method is well suited for settings with missing data, enabling robust inference from incomplete observations. Extensive numerical experiments and real-world applications further highlight its effectiveness, underscoring its potential in various practical scenarios that require denoising and prediction on networks with binary-valued data. Finally, applications to two real-world epidemic scenarios demonstrate that one-bit total variation denoising significantly enhances the accuracy of network-based nowcasting and forecasting.",novel method introduced denoising partially observed signals networks using graph total variation regularization technique adapted signal processing handle binary data approach extends existing results derived gaussian data discrete binary case method hereafter referred one bit denoising framework considers network represented set nodes binary observations edges encode pairwise relationships nodes key theoretical contribution establishment consistency guarantees graph denoising recovery underlying node level probabilities method well suited settings missing data enabling robust inference incomplete observations extensive numerical experiments real world applications highlight effectiveness underscoring potential various practical scenarios require denoising prediction networks binary valued data finally applications two real world epidemic scenarios demonstrate one bit total variation denoising significantly enhances accuracy network based nowcasting forecasting
"In this paper, a deteriorating inventory model is developed with price dependent demand and time dependent holding cost in fuzzy environment. It is assumed that, deterioration does not start for a time period which is normal scenario in construction materials like cement. Shortages are allowed to happen and assumed to be partially backlogged. Investment in preservation technology is made to combat the effect of deterioration. Due to limited transportation capacity of the model, a transportation company is hired for delivering the materials. To neutralize the fluctuation in various types of costs in present economic conditions, crisp and fuzzy models are created. Genetic algorithm is used to find the optimal strategies of the inventory model such as minimizing the total cost and optimal order quantity. The model is validated through numerical experimentation along with the sensitivity analysis. Real life applications in terms of managerial insights are given for the proposed model.",paper deteriorating inventory model developed price dependent demand time dependent holding cost fuzzy environment assumed deterioration start time period normal scenario construction materials like cement shortages allowed happen assumed partially backlogged investment preservation technology made combat effect deterioration due limited transportation capacity model transportation company hired delivering materials neutralize fluctuation various types costs present economic conditions crisp fuzzy models created genetic algorithm used find optimal strategies inventory model minimizing total cost optimal order quantity model validated numerical experimentation along sensitivity analysis real life applications terms managerial insights given proposed model
"The integration of Design Thinking (DT) and Maker Education (ME), known as DT-Making pedagogy (DTMP), has become a prominent practice in ME settings. While previous studies have highlighted its benefits for students, there remains a gap in understanding how teachers can effectively implement it to promote students’ maker mindsets. To address this gap, we collaborated with four elementary STEM teachers engaged in a Lesson Study (LS) cycle. Our aim was to delve deeper into DTMP and analyze teachers' pedagogical practices in utilizing it. We conducted an analysis of a real classroom session, employing a hierarchical and nested coding scheme to assess the teachers' use of artifacts and questioning techniques. Our analysis uncovered three significant ways in which DT contributes to reframing ME provided by teachers engaged in the LS cycle: 1) utilizing DT process models to structure learning activities; 2) using DT toolkits to scaffold, visualize, and assess learning; 3) employing DT strategies to support questioning practices. Building on these findings, we developed practical recommendations to support teachers effectively implementing DTMP to develop students’ maker mindsets, bridging the theory-practice gap and enhancing student learning experiences.",integration design thinking maker education known making pedagogy dtmp become prominent practice settings previous studies highlighted benefits students remains gap understanding teachers effectively implement promote students maker mindsets address gap collaborated four elementary stem teachers engaged lesson study cycle aim delve deeper dtmp analyze teachers pedagogical practices utilizing conducted analysis real classroom session employing hierarchical nested coding scheme assess teachers use artifacts questioning techniques analysis uncovered three significant ways contributes reframing provided teachers engaged cycle utilizing process models structure learning activities using toolkits scaffold visualize assess learning employing strategies support questioning practices building findings developed practical recommendations support teachers effectively implementing dtmp develop students maker mindsets bridging theory practice gap enhancing student learning experiences
"The analysis of binary outcomes and features, such as the effect of vaccination on health, often rely on 2  ×  2 contingency tables. However, confounding factors such as age or gender call for stratified analysis, by creating sub-tables, which is common in bioscience, epidemiological, and social research, as well as in meta-analyses. Traditional methods for testing associations across strata, such as the Cochran-Mantel-Haenszel (CMH) test, struggle with small sample sizes and heterogeneity of effects between strata. Exact tests can address these issues, but are computationally expensive. To address these challenges, the Gamma Approximation of Stratified Truncated Exact (GASTE) test is proposed. It approximates the exact statistic of the combination of p-values with discrete support, leveraging the gamma distribution to approximate the distribution of the test statistic under stratification, providing fast and accurate p-value calculations, even when effects vary between strata. The GASTE method maintains high statistical power and low type I error rates, outperforming traditional methods by offering more sensitive and reliable detection. It is computationally efficient and broadens the applicability of exact tests in research fields with stratified binary data. The GASTE method is demonstrated through two applications: an ecological study of Alpine plant associations and a 1973 case study on admissions at the University of California, Berkeley. The GASTE method offers substantial improvements over traditional approaches. The GASTE method is available as an open-source package at https://github.com/AlexandreWen/gaste. A Python package is available on PyPI at https://pypi.org/project/gaste-test/",analysis binary outcomes features effect vaccination health often rely contingency tables however confounding factors age gender call stratified analysis creating sub tables common bioscience epidemiological social research well meta analyses traditional methods testing associations across strata cochran mantel haenszel cmh test struggle small sample sizes heterogeneity effects strata exact tests address issues computationally expensive address challenges gamma approximation stratified truncated exact gaste test proposed approximates exact statistic combination values discrete support leveraging gamma distribution approximate distribution test statistic stratification providing fast accurate value calculations even effects vary strata gaste method maintains high statistical power low type error rates outperforming traditional methods offering sensitive reliable detection computationally efficient broadens applicability exact tests research fields stratified binary data gaste method demonstrated two applications ecological study alpine plant associations case study admissions university california berkeley gaste method offers substantial improvements traditional approaches gaste method available open source package https github com alexandrewen gaste python package available pypi https pypi org project gaste test
"Acid sphingomyelinases (aSMases) are enzymes involved in the repair of the plasma membrane in eukaryotic cells. However, neutral sphingomyelinases (nSMases) have also been shown to possess other roles in bacteria and eukaryotic microorganisms, especially as virulence factors. These enzymes exhibit structural conservation but are characterized by elusive homology and the lack of sequence signatures or motifs. In a previous study, we reported the structural features of the complete set of sphingomyelinases (SMases) in Entamoeba histolytica and Trichomonas vaginalis, showing structural homology and functional differences in two aSMases from E. histolytica (EhSMase). However, the approach was limited due to the AlphaFold3 source code not being publicly available at the time. In this report, the structural transitions in the aSMases from T. vaginalis (TvSMase) were measured using open-source AlphaFold3 and collective motions of proteins via Normal Mode Analysis in internal coordinates. They compared them with the models from aSMase4 (EHI_100080) and aSMase6 (EHI_125660) from E. histolytica, containing different combinations of ligands. Using full-length sphingomyelin and the Mg2+ and Co2+ ions, where Co2+ was shown to inhibit the enzymes of both organisms, we demonstrate that the enzymes exhibit limited flexibility and deformability, except for the T. vaginalis TVAG_271580 enzyme, which displays high structural deformability. This contrasts with the inhibitory mechanism elicited by Co2+ as shown previously. TVSMase3 (TVAG_222460) could not be modelled with the sphingomyelin in the active site pocket, suggesting a regulatory role rather than a functional active enzyme. Additional physicochemical parameters calculated for T. vaginalis enzymes suggest unstable structures and high internal mobility (estimated using the Internal Coordinate method), which may be associated with the functional role of these enzymes. The results presented here open an avenue for searching for novel inhibitors of aSMases that target their physical properties, which could potentially complement treatment to control the parasite burden. These inhibitors could be valuable for further studying the role of these enzymes in parasite pathobiology and, potentially, as therapeutic targets.",acid sphingomyelinases asmases enzymes involved repair plasma membrane eukaryotic cells however neutral sphingomyelinases nsmases also shown possess roles bacteria eukaryotic microorganisms especially virulence factors enzymes exhibit structural conservation characterized elusive homology lack sequence signatures motifs previous study reported structural features complete set sphingomyelinases smases entamoeba histolytica trichomonas vaginalis showing structural homology functional differences two asmases histolytica ehsmase however approach limited due alphafold source code publicly available time report structural transitions asmases vaginalis tvsmase measured using open source alphafold collective motions proteins via normal mode analysis internal coordinates compared models asmase ehi asmase ehi histolytica containing different combinations ligands using full length sphingomyelin ions shown inhibit enzymes organisms demonstrate enzymes exhibit limited flexibility deformability except vaginalis tvag enzyme displays high structural deformability contrasts inhibitory mechanism elicited shown previously tvsmase tvag could modelled sphingomyelin active site pocket suggesting regulatory role rather functional active enzyme additional physicochemical parameters calculated vaginalis enzymes suggest unstable structures high internal mobility estimated using internal coordinate method may associated functional role enzymes results presented open avenue searching novel inhibitors asmases target physical properties could potentially complement treatment control parasite burden inhibitors could valuable studying role enzymes parasite pathobiology potentially therapeutic targets
"Neurological disorders are the major factor of dementia worldwide, presenting significant and escalating challenges to global healthcare systems. Alzheimer's disease (AD) is a leading cause of dementia, creating substantial challenges for international healthcare. Medicinal and aromatic plants offer diverse pharmaceutical properties owing to their richness in chemical constituents. This study aimed to evaluate the in vitro anti-acetylcholinesterase activity of essential oil from Pogostemon cablin (patchouli), which demonstrated inhibitory potential, indicating its possible role in AD management. Preliminary phytochemical screening, followed by Gas Chromatography-Mass Spectrometry (GC-MS) analysis, led to the identification of seventeen compounds, collectively accounting for 97.70 % of the total area, with patchouli alcohol (26.21 %) emerging as the predominant constituent. In-vitro anti-acetylcholinesterase activity tests revealed that sesquiterpene-rich patchouli essential oil had a lower (IC50 24.15 µg/mL) than the standard galantamine (IC50 25.66 µg/mL), highlighting the higher inhibition potential of patchouli essential oil on acetylcholinesterase. In-silico studies revealed that patchouli alcohol exhibited the highest binding affinity with a docking score of −8.5 kcal/mol, while 1H-Cycloprop[e]azulene, 1a,2,3,4,6,7,7a,7b-octahydro-1,1,4,7-tetramethyl showed the lowest docking score of −6.8 kcal/mol, highlighting the differential binding interactions of the compounds with the acetylcholinesterase enzyme. All key compounds demonstrated acceptable ADME properties, including compliance with Lipinski's rule of five, favorable topological polar surface area (TPSA), high gastrointestinal (GI) absorption, blood-brain barrier (BBB) permeability, and suitable bioavailability scores. Molecular dynamics simulations (600 ns) confirmed stable AChE-ligand interactions (six complexes), with RMSD values of ∼2.5–3.5 Å, SASA around 2050–2200 Å², and Rg between 21.8 and 23.4 Å, indicating compact and stable protein-ligand complexes, indicating its potential as an AD therapeutic agent.",neurological disorders major factor dementia worldwide presenting significant escalating challenges global healthcare systems alzheimer disease leading cause dementia creating substantial challenges international healthcare medicinal aromatic plants offer diverse pharmaceutical properties owing richness chemical constituents study aimed evaluate vitro anti acetylcholinesterase activity essential oil pogostemon cablin patchouli demonstrated inhibitory potential indicating possible role management preliminary phytochemical screening followed gas chromatography mass spectrometry analysis led identification seventeen compounds collectively accounting total area patchouli alcohol emerging predominant constituent vitro anti acetylcholinesterase activity tests revealed sesquiterpene rich patchouli essential oil lower standard galantamine highlighting higher inhibition potential patchouli essential oil acetylcholinesterase silico studies revealed patchouli alcohol exhibited highest binding affinity docking score kcal mol cycloprop azulene octahydro tetramethyl showed lowest docking score kcal mol highlighting differential binding interactions compounds acetylcholinesterase enzyme key compounds demonstrated acceptable adme properties including compliance lipinski rule five favorable topological polar surface area tpsa high gastrointestinal absorption blood brain barrier bbb permeability suitable bioavailability scores molecular dynamics simulations confirmed stable ache ligand interactions six complexes rmsd values sasa around indicating compact stable protein ligand complexes indicating potential therapeutic agent
"Nearly all progressive renal disorders emerge through the renal interstitial fibrosis (RIF) pathway, yet its underlying mechanisms remain unclear. This study employed bioinformatics approaches to identify crucial targets and immune-infiltrating patterns in RIF. By screening the microarray datasets GSE22459 and GSE76882, we identified 163 and 418 differentially expressed genes (DEGs), respectively, with 80 core genes in common between RIF and healthy specimens. Gene Ontology (GO), Kyoto Encyclopedia of Genes and Genomes (KEGG), and protein-protein interaction (PPI) network analyses identified five hub genes—G protein-coupled receptor 18 (GPR18), chemokine ligand 5 (CCL5), chemokine receptor type 5 (CCR5), chemokine ligand 19 (CCL19), and Chemokine (C-X-C motif) ligand 9 (CXCL9). These were clustered into a functional group related to chemokine signaling, lymphocyte activation modulation, and leukocyte activation modulation. Immune cell infiltration analysis further revealed that the enrichment levels of 13 immune cell subtypes were significantly different in RIF samples, with Th1 cell enrichment being notably downregulated. To validate these computational findings, a unilateral ureteral obstruction (UUO)-induced renal fibrosis model was established in mice. Immunohistochemical (IHC) staining revealed that in UUO-induced mouse kidneys, the protein levels of GPR18, CCL5, CCR5, CCL19, and CXCL9 were remarkably elevated compared with the normal control. These results confirm the potential of these 5 hub genes as therapeutic targets in RIF. This study helps reveal the underlying mechanisms of RIF.",nearly progressive renal disorders emerge renal interstitial fibrosis rif pathway yet underlying mechanisms remain unclear study employed bioinformatics approaches identify crucial targets immune infiltrating patterns rif screening microarray datasets gse gse identified differentially expressed genes degs respectively core genes common rif healthy specimens gene ontology kyoto encyclopedia genes genomes kegg protein protein interaction ppi network analyses identified five hub genes protein coupled receptor gpr chemokine ligand ccl chemokine receptor type ccr chemokine ligand ccl chemokine motif ligand cxcl clustered functional group related chemokine signaling lymphocyte activation modulation leukocyte activation modulation immune cell infiltration analysis revealed enrichment levels immune cell subtypes significantly different rif samples cell enrichment notably downregulated validate computational findings unilateral ureteral obstruction uuo induced renal fibrosis model established mice immunohistochemical ihc staining revealed uuo induced mouse kidneys protein levels gpr ccl ccr ccl cxcl remarkably elevated compared normal control results confirm potential hub genes therapeutic targets rif study helps reveal underlying mechanisms rif
"A newly synthesized 2,6-bis-(2-(ethylthio)ethylimino)methyl-4-methylphenol (TMP) shows solvent dependent keto-enol tautomerism followed by an exciting structural isomerism in enolic tautomer. In solvent medium, as synthesized TMP molecule preferably oriented in its keto form (ketonic TMP) or in enol form (enolic TMP) such a way that the keto form is predominant in polar medium and enol form in non-polar medium. Irrespective of high polarity, interestingly in aqueous solvent medium the leading single peak ketonic form converted slowly in real time into its enolic form through the water mediated intermolecular proton transfer procedure. In emission, ketonic TMP shows high fluorescence in its existing solvents for extended π conjugations whereas normal enolic TMP is non-fluorescent in protic solvents and very weekly fluorescent in aprotic solvents due to forbidden transitions. In water only, after a certain time of keto to enol conversion i.e. at low ketonic concentration, a bathochromic shift of low intense ketonic emission is identified as enolic emission after excited state proton transfer. As well, in aprotic solvents, weekly fluorescent enolic TMP twisted in such a way that a new fluorescence active twisted structural isomer is developed as the forbidden excited state enolic transition is wiped out in its twisted isomer. In addition to single ketonic emissive state, two different emissive states for enolic TMP in aprotic solvents are also clearly distinguished in lifetime analysis. A detailed density functional theory (DFT) and time dependent density functional theory (TDDFT) calculations have clearly demonstrated the energy levels of the tautomers in aqueous medium, and polarity dependent inter-convertible intramolecular proton transfer between keto-enol state in non-aqueous medium through potential energy curves or surfaces. Dihedral angle plot also show the existence of two stable enolic states in S0 and S1 potential curves as normal and twisted enolic TMP in aprotic solvents. Therefore, in this article, the synthesized TMP molecule with widely used fundamental moiety and having potential activity of various applications is exposed from all its fundamental properties and isomeric structures for future references.",newly synthesized bis ethylthio ethylimino methyl methylphenol tmp shows solvent dependent keto enol tautomerism followed exciting structural isomerism enolic tautomer solvent medium synthesized tmp molecule preferably oriented keto form ketonic tmp enol form enolic tmp way keto form predominant polar medium enol form non polar medium irrespective high polarity interestingly aqueous solvent medium leading single peak ketonic form converted slowly real time enolic form water mediated intermolecular proton transfer procedure emission ketonic tmp shows high fluorescence existing solvents extended conjugations whereas normal enolic tmp non fluorescent protic solvents weekly fluorescent aprotic solvents due forbidden transitions water certain time keto enol conversion low ketonic concentration bathochromic shift low intense ketonic emission identified enolic emission excited state proton transfer well aprotic solvents weekly fluorescent enolic tmp twisted way new fluorescence active twisted structural isomer developed forbidden excited state enolic transition wiped twisted isomer addition single ketonic emissive state two different emissive states enolic tmp aprotic solvents also clearly distinguished lifetime analysis detailed density functional theory dft time dependent density functional theory tddft calculations clearly demonstrated energy levels tautomers aqueous medium polarity dependent inter convertible intramolecular proton transfer keto enol state non aqueous medium potential energy curves surfaces dihedral angle plot also show existence two stable enolic states potential curves normal twisted enolic tmp aprotic solvents therefore article synthesized tmp molecule widely used fundamental moiety potential activity various applications exposed fundamental properties isomeric structures future references
"In this paper, we extend the constrained mock-Chebyshev least squares operator to quasi-uniform grids, preserving the accuracy and applicability of the original approach while addressing the practical challenge of quasi-equispaced nodes. Additionally, we provide an improved bound for the norm of the operator by using a refined bound for its coefficient vector through the direct elimination method. Numerical experiments confirm the efficiency of the proposed technique.",paper extend constrained mock chebyshev least squares operator quasi uniform grids preserving accuracy applicability original approach addressing practical challenge quasi equispaced nodes additionally provide improved bound norm operator using refined bound coefficient vector direct elimination method numerical experiments confirm efficiency proposed technique
"Cardiotoxicity remains a major clinical challenge associated with various environmental and chemotherapeutic toxicants. Sunitinib (SNB) is a potent targeted cancer drug that is reported to induce severe organ damage including renal failure. Cirsiliol (CSL) is a natural flavone that exhibits marvelous pharmacological properties. In this investigation, we explored the potential cardioprotective nature of CSL to counter SNB induced cardiac impairments. Sprague Dawley rats (n = 36) were categorized into control, SNB (25 mgkg−1), SNB (25 mgkg−1) + CSL (10 mgkg−1), and CSL (10 mgkg−1) alone experimented group. SNB exposure led to a notable reduction in the expression of Endothelin Receptor Type B (EDNRB), Forkhead box O3a (FOXO3a), and sirtuin 1 (SIRT1) while exacerbating the expression of P21, P53, Endothelin-1 (EDN-1), Endothelin Receptor Type A (EDNRA). The levels of reactive oxygen species (ROS) and malondialdehyde (MDA) were promoted while the enzymatic activities of hemeoxygenase-1 (HO-1), superoxide dismutase (SOD), glutathione peroxidase (GPx), glutathione S-transferase (GST), glutathione reductase (GSR), catalase (CAT), and glutathione (GSH) contents were reduced following the SNB exposure. Moreover, SNB intoxication led to a marked elevation in the concentrations of C-reactive protein, creatine kinase-myocardial band (CK-MB), Pro-B-Type natriuretic peptide (ProBNP), troponin-T, Lactate dehydrogenase (LDH), Creatine phosphokinase (CPK), troponin-I and Brain natriuretic peptide (BNP). Cardiac tissues showed sever immune-inflammatory responses after SNB intoxication as confirmed by augmented levels and expressions of cyclooxygenase-2 (COX-2), tumor necrosis factor-alpha (TNF-α), interleukin-1 beta (IL-1β), interleukin-6 (IL-6), and total fraction of nuclear factor-kappa B (NF-κB). Furthermore, the SNB administration upregulated the concentrations of cysteine-aspartic proteases-9 (Caspase-9), cysteine-aspartic proteases-3 (Capase-3), and Bcl-2–associated X protein (Bax) while declining the concentration of B-cell lymphoma 2 (Bcl-2). SNB intoxication caused histological disarrays including myofibrillar degeneration, capillary dilation, wavy fibers, necrosis of focal regions, hypertrophy of cardiomyocytes, inflammation and interstitial edema. Nonetheless, CSL therapy notably reversed these pathological changes via upregulating SIRT1/FOXO3a and endothelin pathways while reducing cardiac inflammation, apoptosis and cardiac function markers. Our results are validated through in-silico which showed that CSL showed high binding affinity with key regulatory pathways.",cardiotoxicity remains major clinical challenge associated various environmental chemotherapeutic toxicants sunitinib snb potent targeted cancer drug reported induce severe organ damage including renal failure cirsiliol csl natural flavone exhibits marvelous pharmacological properties investigation explored potential cardioprotective nature csl counter snb induced cardiac impairments sprague dawley rats categorized control snb mgkg snb mgkg csl mgkg csl mgkg alone experimented group snb exposure led notable reduction expression endothelin receptor type ednrb forkhead box foxo sirtuin sirt exacerbating expression endothelin edn endothelin receptor type ednra levels reactive oxygen species ros malondialdehyde mda promoted enzymatic activities hemeoxygenase superoxide dismutase sod glutathione peroxidase gpx glutathione transferase gst glutathione reductase gsr catalase cat glutathione gsh contents reduced following snb exposure moreover snb intoxication led marked elevation concentrations reactive protein creatine kinase myocardial band pro type natriuretic peptide probnp troponin lactate dehydrogenase ldh creatine phosphokinase cpk troponin brain natriuretic peptide bnp cardiac tissues showed sever immune inflammatory responses snb intoxication confirmed augmented levels expressions cyclooxygenase cox tumor necrosis factor alpha tnf interleukin beta interleukin total fraction nuclear factor kappa furthermore snb administration upregulated concentrations cysteine aspartic proteases caspase cysteine aspartic proteases capase bcl associated protein bax declining concentration cell lymphoma bcl snb intoxication caused histological disarrays including myofibrillar degeneration capillary dilation wavy fibers necrosis focal regions hypertrophy cardiomyocytes inflammation interstitial edema nonetheless csl therapy notably reversed pathological changes via upregulating sirt foxo endothelin pathways reducing cardiac inflammation apoptosis cardiac function markers results validated silico showed csl showed high binding affinity key regulatory pathways
"With rapid developments in artificial intelligence (AI), the discussion about and applications of generative AI have increased substantially. Generative AI has extensive and valuable applications in many industrial and medical fields and is a possible solution for industries that struggle to collect large quantities of data. The present study evaluated the use of generative AI in eye disease prediction. Because retinal images are difficult to acquire, this study used a generative AI model [i.e., the denoising diffusion implicit model (DDIM)] to conduct data augmentation, thereby improving the accuracy of a convolutional neural network (CNN) model developed for eye disease detection. This study adopted the DDIM primarily for its high inference speed and ability to consistently generate high-quality samples in a limited number of steps, making it suitable for tasks that require high-quality medical images. With the increasing prevalence of electronic products, the number of patients with retinopathy or optic neuropathy is increasing annually, and patients are experiencing these diseases at increasingly younger ages. Moreover, eye diseases such as glaucoma and macular degeneration are becoming increasingly common in modern society. The developed CNN model exhibited a 3 % higher accuracy when it was trained using the data generated by the DDIM than when it was trained without these data. This CNN model can screen eye disease symptoms early to enable patients to receive timely treatment, thereby mitigating the risk and consequences of eye diseases. The results of this study indicate that the training data generated using the DDIM can enhance the accuracy of early eye disease detection.",rapid developments artificial intelligence discussion applications generative increased substantially generative extensive valuable applications many industrial medical fields possible solution industries struggle collect large quantities data present study evaluated use generative eye disease prediction retinal images difficult acquire study used generative model denoising diffusion implicit model ddim conduct data augmentation thereby improving accuracy convolutional neural network cnn model developed eye disease detection study adopted ddim primarily high inference speed ability consistently generate high quality samples limited number steps making suitable tasks require high quality medical images increasing prevalence electronic products number patients retinopathy optic neuropathy increasing annually patients experiencing diseases increasingly younger ages moreover eye diseases glaucoma macular degeneration becoming increasingly common modern society developed cnn model exhibited higher accuracy trained using data generated ddim trained without data cnn model screen eye disease symptoms early enable patients receive timely treatment thereby mitigating risk consequences eye diseases results study indicate training data generated using ddim enhance accuracy early eye disease detection
"The impact of the steric bulk of the heteroaryl moiety on the hydroxy(tosyloxy)iodobenzene (HTIB)-mediated rearrangement of pyrazolyl chalcones under non-nucleophilic conditions is studied herein. The results demonstrate that the pyrazolyl group of chalcones readily undergoes a 1,2-heteroaryl migration upon treatment with HTIB in dichloromethane solvent. The product of the migration leads to the formation of a series of α-heteroaryl-β,β-ditosyloxy ketones. The structure of these ketones is fully characterized by spectral data (FT-IR, 1H-NMR, 13C-NMR), single-crystal XRD and HRMS analysis. The molecular structure is further analysed in-depth in terms of the characteristics of frontier molecular orbitals, global descriptive parameters, molecular electrostatic potentials, Mulliken population analysis, Hirshfeld surface analysis and 3D-energy framework studies. Moreover, the synthetic utility of these α-heteroaryl-β,β-ditosyloxy ketones is also explored in the regioselective synthesis of five and six-membered heterocyclic compounds as a specimen reaction.",impact steric bulk heteroaryl moiety hydroxy tosyloxy iodobenzene htib mediated rearrangement pyrazolyl chalcones non nucleophilic conditions studied herein results demonstrate pyrazolyl group chalcones readily undergoes heteroaryl migration upon treatment htib dichloromethane solvent product migration leads formation series heteroaryl ditosyloxy ketones structure ketones fully characterized spectral data nmr nmr single crystal xrd hrms analysis molecular structure analysed depth terms characteristics frontier molecular orbitals global descriptive parameters molecular electrostatic potentials mulliken population analysis hirshfeld surface analysis energy framework studies moreover synthetic utility heteroaryl ditosyloxy ketones also explored regioselective synthesis five six membered heterocyclic compounds specimen reaction
"Prostate cancer is one of the most common malignancies in men worldwide and is driven in large part by aberrant androgen receptor (AR) signaling. Upon sufficient androgenic concentration, the AR undergoes conformational changes and translocates to the nucleus, where it promotes the transcription of genes essential for tumor growth and progression. In this study, we performed a structure-based virtual screening of 34 phytochemicals from Euphorbia resinifera against the crystal structure of the AR ligand-binding domain (PDB ID: 1E3G). This screening identified two leading compounds, Resiniferatoxin and Euphol, both demonstrated favorable binding poses and docking scores of −10 kcal/mol and −8.6 kcal/mol, respectively, surpassing Flutamide, the standard antiandrogen (−7.7 kcal/mol). Furthermore, a 100 ns molecular dynamics simulation showed that these two molecules formed stable complexes with AR, suggesting their potential utility as natural AR inhibitors in prostate cancer therapy. However, additional experimental studies are essential to validate their efficacy, elucidate their mechanisms of action, and assess their safety profiles, potentially paving the way for innovative plant-based treatments targeting AR-driven prostate cancer.",prostate cancer one common malignancies men worldwide driven large part aberrant androgen receptor signaling upon sufficient androgenic concentration undergoes conformational changes translocates nucleus promotes transcription genes essential tumor growth progression study performed structure based virtual screening phytochemicals euphorbia resinifera crystal structure ligand binding domain pdb screening identified two leading compounds resiniferatoxin euphol demonstrated favorable binding poses docking scores kcal mol kcal mol respectively surpassing flutamide standard antiandrogen kcal mol furthermore molecular dynamics simulation showed two molecules formed stable complexes suggesting potential utility natural inhibitors prostate cancer therapy however additional experimental studies essential validate efficacy elucidate mechanisms action assess safety profiles potentially paving way innovative plant based treatments targeting driven prostate cancer
"Accurate prediction of molecular properties plays a crucial role in drug analysis and discovery. Especially for neurotherapeutic drugs developed for the treatment of neurological disorders, one of today's most critical challenges, this process becomes even more crucial. The prediction of the blood-brain barrier penetration (BBBP) ability of the molecules contained in the development process of a drug is the primary condition for knowing the effect of drugs on the central nervous system. In this study, we developed an innovative deep-learning architecture for predicting BBBP using well-characterised molecular properties. The new method, called XP-GCN (Extreme Parallel Graph Convolutional Network), combines Graph Convolutional Networks (GCN) and Extreme Learning Machines (ELM) to predict molecular properties. It achieves multidimensional feature fusion for BBBP prediction without relying on back-propagation algorithms. XP-GCN offers a comprehensive approach by combining graphical representations and molecular fingerprints while evaluating molecules through multidimensional analysis of SMILES strings, enhanced with data augmentation techniques. The model analyses various attributes of molecules through parallel GCN layers. Trained on a notably extensive molecular dataset compared to many literature studies, it has achieved high performance and computational efficiency by fusing information obtained from molecular fingerprints. Due to ELM's forward random weight and bias assignment features, the classification task without backpropagation-based training showed a classification performance of over 95 % in less than one millisecond, even in models with a low number of hidden neurons. XP-GCN, which shows a peak ROC-AUC performance of 0.9846 across different hyperparameters, offers a novel approach to molecular feature prediction with its computational efficiency, high accuracy, and classification capability. Furthermore, the robustness of the XP-GCN model has been demonstrated by Friedman and Nemenyi's post-hoc statistical analyses.",accurate prediction molecular properties plays crucial role drug analysis discovery especially neurotherapeutic drugs developed treatment neurological disorders one today critical challenges process becomes even crucial prediction blood brain barrier penetration bbbp ability molecules contained development process drug primary condition knowing effect drugs central nervous system study developed innovative deep learning architecture predicting bbbp using well characterised molecular properties new method called gcn extreme parallel graph convolutional network combines graph convolutional networks gcn extreme learning machines elm predict molecular properties achieves multidimensional feature fusion bbbp prediction without relying back propagation algorithms gcn offers comprehensive approach combining graphical representations molecular fingerprints evaluating molecules multidimensional analysis smiles strings enhanced data augmentation techniques model analyses various attributes molecules parallel gcn layers trained notably extensive molecular dataset compared many literature studies achieved high performance computational efficiency fusing information obtained molecular fingerprints due elm forward random weight bias assignment features classification task without backpropagation based training showed classification performance less one millisecond even models low number hidden neurons gcn shows peak roc auc performance across different hyperparameters offers novel approach molecular feature prediction computational efficiency high accuracy classification capability furthermore robustness gcn model demonstrated friedman nemenyi post hoc statistical analyses
"This paper aims to asymptotically evaluate a class of integrals involving products of two oscillatory Bessel functions. By leveraging the properties of multiple integrals and integration by parts, we develop an approximation method for these integrals. Numerical examples are provided to validate the theoretical results and demonstrate the efficacy of the proposed approach.",paper aims asymptotically evaluate class integrals involving products two oscillatory bessel functions leveraging properties multiple integrals integration parts develop approximation method integrals numerical examples provided validate theoretical results demonstrate efficacy proposed approach
"Ethyl 4-methyl-2-(phenylamino)thiophene-3-carboxylate (8) was synthesized via a one-pot three-component reaction of ethyl 3-oxobutanoate, PhNCS, and 1-chloropropan-2-one in NaOEt. This process offers several advantages over previously reported synthetic method, including high yield, less reaction time and availability of the starting materials. Spectral data and X-ray analysis were used to elucidate the structure. Its structure is stabilized by a strong intramolecular NH…O hydrogen bond. Hirshfeld analysis indicated the most dominant non-covalent interactions are C…H (26.3 %) and H…H (53.9 %), where the C11…H2 (2.760 Å) and C7…C7 (3.314 Å) contacts are the shortest contacts. In addition, DFT-based computational studies were performed to assess the electronic structure and local reactivity of thiophene 8. HOMO–LUMO energy gaps, MEP surface mapping, Mulliken charge distribution, and Fukui function analyses were used to identify the key electrophilic and nucleophilic centers within the molecules. Antitumor activity of thiophene 8 was assessed against HepG2 (liver cancer), MCF-7 (breast cancer), and HCT116 (colorectal cancer) by means of the sulforhodamine B (SRB) assay. The compound exhibited promising anticancer activity, showing its highest potency against HepG2 (IC50 =40.1 ± 1.3 μg/mL) compared to MCF-7 (76.3 ± 2.5) and HCT-116 (92.9 ± 2.02 μg/mL) cell lines.",ethyl methyl phenylamino thiophene carboxylate synthesized via one pot three component reaction ethyl oxobutanoate phncs chloropropan one naoet process offers several advantages previously reported synthetic method including high yield less reaction time availability starting materials spectral data ray analysis used elucidate structure structure stabilized strong intramolecular hydrogen bond hirshfeld analysis indicated dominant non covalent interactions contacts shortest contacts addition dft based computational studies performed assess electronic structure local reactivity thiophene homo lumo energy gaps mep surface mapping mulliken charge distribution fukui function analyses used identify key electrophilic nucleophilic centers within molecules antitumor activity thiophene assessed hepg liver cancer mcf breast cancer hct colorectal cancer means sulforhodamine srb assay compound exhibited promising anticancer activity showing highest potency hepg compared mcf hct cell lines
"Food insecurity and the spread of infectious disease are among the two major problems facing the world today especially in poor rural communities. Unfortunately, these two problems are related as many poor rural communities with food insecurity issues are also endemic to some food and waterborne diseases. A mathematical model that takes into consideration the major factors affecting food insecurity and disease is developed and used to analyse the problems. The essential features of the model are obtained and analysed analytically. Model fitting and parameter estimation is performed using Nigeria as a case study. To determine how to reduce food insecurity and disease, the model is extended by introducing appropriate control intervention. Optimal control analysis is conducted to determine how to reduce food insecurity and disease with minimum cost. The study reveals the interrelated issues of food insecurity and disease dynamics which is crucial for proper management and better decision making.",food insecurity spread infectious disease among two major problems facing world today especially poor rural communities unfortunately two problems related many poor rural communities food insecurity issues also endemic food waterborne diseases mathematical model takes consideration major factors affecting food insecurity disease developed used analyse problems essential features model obtained analysed analytically model fitting parameter estimation performed using nigeria case study determine reduce food insecurity disease model extended introducing appropriate control intervention optimal control analysis conducted determine reduce food insecurity disease minimum cost study reveals interrelated issues food insecurity disease dynamics crucial proper management better decision making
"Gene regulatory networks (GRNs) govern gene expression and cellular identity, but accurately inferring their structure from high-dimensional single-cell RNA sequencing (scRNA-seq) data remains a major challenge. Here, we present EnsembleRegNet, a deep learning framework that infers transcription factor (TF)–target gene relationships by integrating an ensemble encoder-decoder and multilayer perceptron (MLP) architecture. EnsembleRegNet utilizes Hodges-Lehmann estimator (HLE)-based binarization, case-deletion analysis, motif enrichment using RcisTarget, and regulon activity scoring with AUCell to enhance both robustness and biological interpretability. Extensive evaluations across simulated and real scRNA-seq datasets demonstrate that EnsembleRegNet outperforms existing GRN inference methods, including SCENIC and SIGNET, in both clustering performance and regulatory accuracy. By uncovering cell-type-specific regulatory modules and enhancing interpretability, EnsembleRegNet offers a scalable and biologically grounded framework for exploring transcriptional regulation. Its demonstrated performance establishes a new benchmark for GRN inference and highlights its promise for applications in disease modeling, biomarker discovery, and cellular reprogramming.",gene regulatory networks grns govern gene expression cellular identity accurately inferring structure high dimensional single cell rna sequencing scrna seq data remains major challenge present ensembleregnet deep learning framework infers transcription factor target gene relationships integrating ensemble encoder decoder multilayer perceptron mlp architecture ensembleregnet utilizes hodges lehmann estimator hle based binarization case deletion analysis motif enrichment using rcistarget regulon activity scoring aucell enhance robustness biological interpretability extensive evaluations across simulated real scrna seq datasets demonstrate ensembleregnet outperforms existing grn inference methods including scenic signet clustering performance regulatory accuracy uncovering cell type specific regulatory modules enhancing interpretability ensembleregnet offers scalable biologically grounded framework exploring transcriptional regulation demonstrated performance establishes new benchmark grn inference highlights promise applications disease modeling biomarker discovery cellular reprogramming
"Cancer is a significant public health issue that has a global impact. Significant mortality rates have already been observed due to this disease, and more mortalities are expected in the future. In recent times, there has been a growing interest among otolaryngologists and oncologists in the development of appropriate treatment regimens for patients with recurrent nasopharyngeal carcinoma (NPC). The primary objective of these treatment modalities is to extend the lifespan of patients following recurrence and enhance their overall survival and quality of life. For instance, metaheuristic algorithms (MH), a form of soft computing technology, are commonly utilized in healthcare data due to their effectiveness. Furthermore, metaheuristics rely on the evolutionary search principle. They direct the search process to effectively explore the search space in order to find near-optimal solutions for solving global optimization problems. Tabu search (TS) is a method used in optimization problems and falls under metaheuristic techniques. An essential element of TS is its utilization of adaptive memory, which enhances search efficiency by avoiding local optimality and promoting flexibility. Another example is data mining, which is a subset of artificial intelligence that utilizes data to extract meaningful information from previously unknown patterns. It has been increasingly used in healthcare to aid clinical diagnostics and disease prediction. The proposed technique treated data mining problems as combinatorial optimization problems and used metaheuristics to address data mining challenges, such as classification for unknown data and finding association rules for significant patterns. The Tabu Search Classifier Method (TSCM) outlined in this paper primarily utilizes the Tabu Search (TS) algorithm, enhanced with the incorporation of Dynamic Neighborhood Structure (DNHS), which contributes to better discovery of the search space. The TSCM algorithm identifies three rules based on the patients’ data and generates three precise artificial predictive models to determine and categorize individuals who are at risk of recurrent NPC. With each stage of the treatment, additional features become accessible. The first model relies on a primary data set that includes descriptive data. The second model incorporates more features than the first model but does not include the response feature. The third model utilizes all existing features and includes the response feature, which is observed three months after the treatment phase concludes, the third model is considered a post-treatment monitoring. This paper introduces an Artificial Advisory Healthcare System (AAHS) that utilizes these models to accurately predict the occurrence of recurrence during each stage of treatment and after the treatment as a post-treatment monitoring. This prediction enables the adjustment of the treatment plan and the implementation of additional measures in accordance with the system’s outputs. Given the growing prevalence of artificial intelligence in medical research, the proposed system aims to predict the likelihood of NPC recurrence in certain patients. Therefore, this will enable oncologists to take additional medical precautions to prevent recurrence and enhance their understanding of cancer. The experimental results indicate that the three proposed predictive models outperform the existing prognoses for NPC recurrence.",cancer significant public health issue global impact significant mortality rates already observed due disease mortalities expected future recent times growing interest among otolaryngologists oncologists development appropriate treatment regimens patients recurrent nasopharyngeal carcinoma npc primary objective treatment modalities extend lifespan patients following recurrence enhance overall survival quality life instance metaheuristic algorithms form soft computing technology commonly utilized healthcare data due effectiveness furthermore metaheuristics rely evolutionary search principle direct search process effectively explore search space order find near optimal solutions solving global optimization problems tabu search method used optimization problems falls metaheuristic techniques essential element utilization adaptive memory enhances search efficiency avoiding local optimality promoting flexibility another example data mining subset artificial intelligence utilizes data extract meaningful information previously unknown patterns increasingly used healthcare aid clinical diagnostics disease prediction proposed technique treated data mining problems combinatorial optimization problems used metaheuristics address data mining challenges classification unknown data finding association rules significant patterns tabu search classifier method tscm outlined paper primarily utilizes tabu search algorithm enhanced incorporation dynamic neighborhood structure dnhs contributes better discovery search space tscm algorithm identifies three rules based patients data generates three precise artificial predictive models determine categorize individuals risk recurrent npc stage treatment additional features become accessible first model relies primary data set includes descriptive data second model incorporates features first model include response feature third model utilizes existing features includes response feature observed three months treatment phase concludes third model considered post treatment monitoring paper introduces artificial advisory healthcare system aahs utilizes models accurately predict occurrence recurrence stage treatment treatment post treatment monitoring prediction enables adjustment treatment plan implementation additional measures accordance system outputs given growing prevalence artificial intelligence medical research proposed system aims predict likelihood npc recurrence certain patients therefore enable oncologists take additional medical precautions prevent recurrence enhance understanding cancer experimental results indicate three proposed predictive models outperform existing prognoses npc recurrence
"This study developed a high-efficiency screening method based on Hansen Solubility Parameters (HSP) for selecting optimal back-extractants from Phenol-rich phases after Phenol extraction using deep eutectic solvents (DESs). The spatial distance between organic solvents and Phenol in the three-dimensional solubility space was calculated, and diethyl ether (DE) was used as a benchmark for preliminary screening. Further multi-dimensional optimization was performed by integrating ecotoxicity, physical properties (boiling point and viscosity), and environmental impacts (mutagenicity and bioaccumulation), ultimately identifying tetrahydrofuran (THF) and several ester solvents (MF, MA, EA) as potential back-extractants. Experimental verification showed that THF exhibited the best back-extraction performance, and the order of extraction efficiency (THF > MF > EA > MA > DE) was highly consistent with the calculation results of spatial distance and interaction energy. Mechanistic studies revealed that back-extraction efficiency mainly depends on hydrogen bonding interactions and hydrophobic effects between the solvent and Phenol. After 3 cycles of experiments, THF still maintained high regeneration rate of DESs and high extraction rate of phenol, confirming the effectiveness and reliability of this screening method.",study developed high efficiency screening method based hansen solubility parameters hsp selecting optimal back extractants phenol rich phases phenol extraction using deep eutectic solvents dess spatial distance organic solvents phenol three dimensional solubility space calculated diethyl ether used benchmark preliminary screening multi dimensional optimization performed integrating ecotoxicity physical properties boiling point viscosity environmental impacts mutagenicity bioaccumulation ultimately identifying tetrahydrofuran thf several ester solvents potential back extractants experimental verification showed thf exhibited best back extraction performance order extraction efficiency thf highly consistent calculation results spatial distance interaction energy mechanistic studies revealed back extraction efficiency mainly depends hydrogen bonding interactions hydrophobic effects solvent phenol cycles experiments thf still maintained high regeneration rate dess high extraction rate phenol confirming effectiveness reliability screening method
"In this contribution a novel basis set obtained from the composition of the sinc function and the inverse of a cubic spline specifying the desired nodes is discussed. As a test case the ground state energy for the electronic wave function of the Coulomb two centre problem is considered. It is found, that the ground state energy can be calculated quite accurately.",contribution novel basis set obtained composition sinc function inverse cubic spline specifying desired nodes discussed test case ground state energy electronic wave function coulomb two centre problem considered found ground state energy calculated quite accurately
"Oxidative stress is an etiologic factor responsible of many degenerative diseases. In this study, we report the design and synthesize of new hybrids 4a–f, containing indolin-2-one, imidazo-isoxazole and phenoxyacetohydrazide systems, as promising antioxidant substances using 2,2-diphenyl-1-picryl-hydrazyl-hydrate (DPPH), 2,2′-azino-bis(3-ethylbenzothiazoline-6-sulfonic acid (ABTS), and ferric reducing antioxidant power (FRAP) assays. All analogues were characterized using multispectroscopic techniques. Most of the newly synthesized derivatives demonstrated excellent to moderate antioxidant activities, especially 4b with IC50 value of 6.51 μM, 2.04 μM and 4.90 μM, followed by 4f displaying IC50 of 8.65 μM, 3.06 μM and 3.74 μM, against DPPH, ABTS and FRAP assays, respectively. The structure–Activity Relationship (SAR) study pointed out the effectiveness of substituted ring of the indolin-2-one moiety in enhancing the activity. Density functional theory (DFT) at the theoretical level of B3LYP/6–31(G) revealed that 4b was achieved the highest HOMO energy (–0.215 eV) and the lowest IP (0.215 eV), suggesting the strongest electron-donating ability, making it favorable for antioxidant action. In addition, 4f was emerged as a strong antioxidant candidate. Furthermore, molecular docking of the most potent ligand (4b) inside the active pocket of human cytochrome P450 (PDB:1OG5) and human carbonyl reductase-1(PDB:4Z3D) receptor enzymes suggest strong interactions. Molecular dynamic (MD) simulation on 4b at 100 ns revealed stronger and more stable binding affinity toward target enzyme. Drug-likeness study suggested that the designed molecules hold suitable pharmacokinetic profiles. These findings envisage that these analogs appear as promising antioxidant drugs.",oxidative stress etiologic factor responsible many degenerative diseases study report design synthesize new hybrids containing indolin one imidazo isoxazole phenoxyacetohydrazide systems promising antioxidant substances using diphenyl picryl hydrazyl hydrate dpph azino bis ethylbenzothiazoline sulfonic acid abts ferric reducing antioxidant power frap assays analogues characterized using multispectroscopic techniques newly synthesized derivatives demonstrated excellent moderate antioxidant activities especially value followed displaying dpph abts frap assays respectively structure activity relationship sar study pointed effectiveness substituted ring indolin one moiety enhancing activity density functional theory dft theoretical level lyp revealed achieved highest homo energy lowest suggesting strongest electron donating ability making favorable antioxidant action addition emerged strong antioxidant candidate furthermore molecular docking potent ligand inside active pocket human cytochrome pdb human carbonyl reductase pdb receptor enzymes suggest strong interactions molecular dynamic simulation revealed stronger stable binding affinity toward target enzyme drug likeness study suggested designed molecules hold suitable pharmacokinetic profiles findings envisage analogs appear promising antioxidant drugs
"The World Health Organization has designated the Methicillin-resistant Staphylococcus aureus (MRSA) and its variants as high-priority threats owing to their enhanced virulence and pathogenic potential. Staphyloxanthin (STX), a prominent virulence factor of S. aureus, plays a dual role: it shields the bacterium from oxidative stress generated by the host immune response and preserves the cell membrane integrity. Dehydrosqualene synthase (CrtM), a prenyl transferase, is essential for catalyzing the first step of STX biosynthesis. In this study, we evaluated 144,000 compounds, including anticancer agents, inhibitors and approved drugs, and 3D bioactive molecules to inhibit the CrtM using computational approaches. Virtual screening was performed on the prepared compound library, followed by relative binding free energy calculations based on MM/GBSA for hit compounds and 100 ns molecular dynamics (MD) simulations for top 3 hit candidates. BPH-652, a known CrtM inhibitor, was used as the reference. Our results revealed that Cmpd1 and Cmpd2 exhibit docking scores of −13.113 kcal/mol and −13.015 kcal/mol, respectively compared to BPH-652(-10.74 kcal/mol) against the CrtM. The stability was further confirmed with relative binding free energies of −57.70 kcal/mol for BPH-652, and −104.74 and −113.20 kcal/mol for Cmpd1 and Cmpd2, respectively. MD simulations demonstrated stable behavior of Cmpd1 and Cmpd2 inside active site of CrtM with minimal fluctuations, the binding energy calculated from MD trajectories also support strong affinity of these compounds. Their favorable ADME properties suggest the potential for further validation in in vitro and in vivo levels.",world health organization designated methicillin resistant staphylococcus aureus mrsa variants high priority threats owing enhanced virulence pathogenic potential staphyloxanthin stx prominent virulence factor aureus plays dual role shields bacterium oxidative stress generated host immune response preserves cell membrane integrity dehydrosqualene synthase crtm prenyl transferase essential catalyzing first step stx biosynthesis study evaluated compounds including anticancer agents inhibitors approved drugs bioactive molecules inhibit crtm using computational approaches virtual screening performed prepared compound library followed relative binding free energy calculations based gbsa hit compounds molecular dynamics simulations top hit candidates bph known crtm inhibitor used reference results revealed cmpd cmpd exhibit docking scores kcal mol kcal mol respectively compared bph kcal mol crtm stability confirmed relative binding free energies kcal mol bph kcal mol cmpd cmpd respectively simulations demonstrated stable behavior cmpd cmpd inside active site crtm minimal fluctuations binding energy calculated trajectories also support strong affinity compounds favorable adme properties suggest potential validation vitro vivo levels
"Detecting neurological diseases is an important task in modern medicine, for which it is crucial to accurately model the temporal distributions of disease genesis. In prior methodologies, temporal patterns are used in feature effects and limiting assumptions such as proportionate risks. We introduce a new methodology for neural disease diagnosis, known as LSA-DGNet (Lightweight Self-Attention based on Deep Gated Network). LSA-DGNet utilizes a deep gated neural network module to model nonlinear and time-lagged effects of variables on disease outcomes. We combined multi-scale time-aware self-attention modules with scaled dot-product self-attention modules so that the parallel structures could provide an integrated self-attention mechanism to improve data perception. LSA-DGNet addresses both issues and, hence, sets a new benchmark for real-time, accurate detection of neurological diseases. Unlike existing approaches, LSA-DGNet integrates a lightweight multi-scale time-aware self-attention mechanism with deep gated neural networks, enabling improved modeling of temporal dependencies in noisy EEG data. This design allows for accurate and efficient detection of multiple neurological diseases, validated on five real-world datasets, setting new benchmarks in classification performance. With up to 250 frames a second, it indicant large progress in computational efficiency—game-changer potential—and clinical applications. The entire framework opens up new opportunities for early diagnosis and more tailored treatment strategies and simply revolutionizes how neurological diseases are detected and treated.",detecting neurological diseases important task modern medicine crucial accurately model temporal distributions disease genesis prior methodologies temporal patterns used feature effects limiting assumptions proportionate risks introduce new methodology neural disease diagnosis known lsa dgnet lightweight self attention based deep gated network lsa dgnet utilizes deep gated neural network module model nonlinear time lagged effects variables disease outcomes combined multi scale time aware self attention modules scaled dot product self attention modules parallel structures could provide integrated self attention mechanism improve data perception lsa dgnet addresses issues hence sets new benchmark real time accurate detection neurological diseases unlike existing approaches lsa dgnet integrates lightweight multi scale time aware self attention mechanism deep gated neural networks enabling improved modeling temporal dependencies noisy eeg data design allows accurate efficient detection multiple neurological diseases validated five real world datasets setting new benchmarks classification performance frames second indicant large progress computational efficiency game changer potential clinical applications entire framework opens new opportunities early diagnosis tailored treatment strategies simply revolutionizes neurological diseases detected treated
"The flow-induced noise generated during the operation of the regulator spray valve, a critical component for pressure control in pressurized water reactor nuclear power plants, can significantly impact its reliability. In this study, Large Eddy Simulation (LES) is coupled with Lighthill’s acoustic analogy to model the noise-generation mechanism, showing that the sound arising from vortex structures and turbulent pulsations exhibits distinct frequency-domain characteristics. Machine learning is then employed to optimize a multi-objective noise-reduction design for the downstream orifice plate; this yields a 12.4 dB(A) noise reduction while decreasing the flow coefficient by only 7.5%. The optimized configuration increases structural safety without impairing valve operability, offering critical insights for enhancing performance and extending service life.",flow induced noise generated operation regulator spray valve critical component pressure control pressurized water reactor nuclear power plants significantly impact reliability study large eddy simulation coupled lighthill acoustic analogy model noise generation mechanism showing sound arising vortex structures turbulent pulsations exhibits distinct frequency domain characteristics machine learning employed optimize multi objective noise reduction design downstream orifice plate yields noise reduction decreasing flow coefficient optimized configuration increases structural safety without impairing valve operability offering critical insights enhancing performance extending service life
"Dengue is an arthropod-borne viral disease caused by four distinct dengue virus (DENV) serotypes. The nonstructural protein-1 (NS1) plays a critical role in viral replication and modulation of host immune responses through Toll-like receptor 4 (TLR4) and its co-receptor myeloid differentiation factor 2 (MD2). Despite its importance, the molecular interactions between NS1 and the TLR4/MD2 receptor complex remain incompletely characterized. In this study, molecular docking and molecular dynamics simulations were used to investigate NS1 interactions from DENV1 and DENV2 with TLR4/MD2. DENV1-NS1 preferentially engages MD2 through extensive hydrogen bonds and hydrophobic contacts, while DENV2-NS1 exhibits stronger interactions with TLR4, forming distinct salt bridges and polar contacts. The wing and β-ladder domains from DENV1-NS1 receptor complex were identified as key interaction hotspots, with the Asn207 glycosylation site positioned at the NS1–MD2 interface, potentially mimicking LPS binding. MM-GBSA binding energy calculations validate these findings, showing MD2 as the main stabilizing partner for DENV1-NS1 and TLR4 for DENV2-NS1. Despite ∼72 % sequence identity, serotype-specific differences in binding geometry and domain engagement were observed, suggesting divergent mechanisms of receptor modulation. These computational results highlight the NS1–TLR4/MD2 interface as a critical site for immune interaction and support the rationale for serotype-specific therapeutic strategies targeting NS1-mediated receptor engagement, while experimental validation is necessary to confirm functional outcomes.",dengue arthropod borne viral disease caused four distinct dengue virus denv serotypes nonstructural protein plays critical role viral replication modulation host immune responses toll like receptor tlr receptor myeloid differentiation factor despite importance molecular interactions tlr receptor complex remain incompletely characterized study molecular docking molecular dynamics simulations used investigate interactions denv denv tlr denv preferentially engages extensive hydrogen bonds hydrophobic contacts denv exhibits stronger interactions tlr forming distinct salt bridges polar contacts wing ladder domains denv receptor complex identified key interaction hotspots asn glycosylation site positioned interface potentially mimicking lps binding gbsa binding energy calculations validate findings showing main stabilizing partner denv tlr denv despite sequence identity serotype specific differences binding geometry domain engagement observed suggesting divergent mechanisms receptor modulation computational results highlight tlr interface critical site immune interaction support rationale serotype specific therapeutic strategies targeting mediated receptor engagement experimental validation necessary confirm functional outcomes
"The fluid flow through a 2D porous medium is typically described by the Darcy equations with appropriate boundary condition for the pressure. Such an approximation can be rigorously derived using two-scale asymptotic expansions and boundary layer correctors. Generalization of that approach leads to the correctors to the Darcy PDE and the corresponding boundary conditions. The goal of this paper is to compute and analyze first and second order terms in that expansion, leading to the fourth order PDE and the third order boundary condition, correcting the classical Darcy model. The derived approximation is justified by an error estimate.",fluid flow porous medium typically described darcy equations appropriate boundary condition pressure approximation rigorously derived using two scale asymptotic expansions boundary layer correctors generalization approach leads correctors darcy pde corresponding boundary conditions goal paper compute analyze first second order terms expansion leading fourth order pde third order boundary condition correcting classical darcy model derived approximation justified error estimate
"We introduce finite difference time domain (FDTD) methods for the Schrödinger equation interacting with the Chern–Simons gauge fields, subjected to the Lorenz or Coulomb gauge condition. We consider two types of FDTD methods: the Crank–Nicolson finite difference (CNFD) method and the leaf-frog finite difference (LFFD) method. In particular, the CNFD method is carefully devised on the staggered spatial and temporal grid points to preserve the total charge of the system. We provide several numerical experiments that validate the second-order convergence of the FDTD methods, the conservation of total charge and energy, and the dependencies of the solutions on the model parameters. We also compare the differences between the solutions of the CSS equations and the Schrödinger equation.",introduce finite difference time domain fdtd methods schr dinger equation interacting chern simons gauge fields subjected lorenz coulomb gauge condition consider two types fdtd methods crank nicolson finite difference cnfd method leaf frog finite difference lffd method particular cnfd method carefully devised staggered spatial temporal grid points preserve total charge system provide several numerical experiments validate second order convergence fdtd methods conservation total charge energy dependencies solutions model parameters also compare differences solutions css equations schr dinger equation
"Over the past few decades, kernel-based approximation methods had achieved astonishing success in solving different problems in the field of science and engineering. However, when employing the direct or standard method of performing computations using infinitely smooth kernels, a conflict arises between the accuracy that can be theoretically attained and the numerical stability. In other words, when the shape parameter tends to zero, the operational matrix for the standard bases with infinitely smooth kernels become severely ill-conditioned. This conflict can be managed applying hybrid kernels. The hybrid kernels extend the approximation space and provide high flexibility to strike the best possible balance between accuracy and stability. In the current study, an innovative approach using hybrid radial kernels (HRKs) is provided to solve weakly singular Fredholm integral equations (WSFIEs) of the second kind in a meshless scheme. The approach employs hybrid kernels built on dispersed nodes as a basis within the discrete collocation technique. This method transforms the problem being studied into a linear system of algebraic equations. Also, the particle swarm optimization (PSO) algorithm is utilized to calculate the optimal parameters for the hybrid kernels, which is based on minimizing the maximum absolute error (MAE). We also study the error estimate of the suggested scheme. Lastly, we assess the accuracy and validity of the hybrid technique by carrying out various numerical experiments. The numerical findings show that the estimates obtained from hybrid kernels are significantly more accurate in solving WSFIEs compared to pure kernels. Additionally, it was revealed that the hybrid bases remain stable across various values of the shape parameters.",past decades kernel based approximation methods achieved astonishing success solving different problems field science engineering however employing direct standard method performing computations using infinitely smooth kernels conflict arises accuracy theoretically attained numerical stability words shape parameter tends zero operational matrix standard bases infinitely smooth kernels become severely ill conditioned conflict managed applying hybrid kernels hybrid kernels extend approximation space provide high flexibility strike best possible balance accuracy stability current study innovative approach using hybrid radial kernels hrks provided solve weakly singular fredholm integral equations wsfies second kind meshless scheme approach employs hybrid kernels built dispersed nodes basis within discrete collocation technique method transforms problem studied linear system algebraic equations also particle swarm optimization pso algorithm utilized calculate optimal parameters hybrid kernels based minimizing maximum absolute error mae also study error estimate suggested scheme lastly assess accuracy validity hybrid technique carrying various numerical experiments numerical findings show estimates obtained hybrid kernels significantly accurate solving wsfies compared pure kernels additionally revealed hybrid bases remain stable across various values shape parameters
"We construct over a given bilinear multi-patch domain a novel C s -smooth mixed degree and regularity isogeometric spline space, which possesses the degree p = 2 s + 1 and regularity r = s in a small neighborhood around the edges and vertices, and the degree p ˜ ≤ p with regularity r ˜ = p ˜ − 1 ≥ r in all other parts of the domain. Our proposed approach relies on the technique Kapl and Vitrih (2021), which requires for the C s -smooth isogeometric spline space a degree at least p = 2 s + 1 on the entire multi-patch domain. Similar to Kapl and Vitrih (2021), the C s -smooth mixed degree and regularity spline space is generated as the span of basis functions that correspond to the individual patches, edges and vertices of the domain. The reduction of degrees of freedom for the functions in the interior of the patches is achieved by introducing an appropriate mixed degree and regularity underlying spline space over [ 0 , 1 ] 2 to define the functions on the single patches. We further extend our construction with a few examples to the class of bilinear-like G s multi-patch parameterizations (Kapl and Vitrih (2018); Kapl and Vitrih (2021)), which enables the design of multi-patch domains having curved boundaries and interfaces. Finally, the great potential of the C s -smooth mixed degree and regularity isogeometric spline space for performing isogeometric analysis is demonstrated by several numerical examples of solving two particular high order partial differential equations, namely the biharmonic and triharmonic equation, via the isogeometric Galerkin method.",construct given bilinear multi patch domain novel smooth mixed degree regularity isogeometric spline space possesses degree regularity small neighborhood around edges vertices degree regularity parts domain proposed approach relies technique kapl vitrih requires smooth isogeometric spline space degree least entire multi patch domain similar kapl vitrih smooth mixed degree regularity spline space generated span basis functions correspond individual patches edges vertices domain reduction degrees freedom functions interior patches achieved introducing appropriate mixed degree regularity underlying spline space define functions single patches extend construction examples class bilinear like multi patch parameterizations kapl vitrih kapl vitrih enables design multi patch domains curved boundaries interfaces finally great potential smooth mixed degree regularity isogeometric spline space performing isogeometric analysis demonstrated several numerical examples solving two particular high order partial differential equations namely biharmonic triharmonic equation via isogeometric galerkin method
"High-dimensional data is ubiquitous in studies involving omics, human movement, and imaging. A multivariate comparison method is proposed for such types of data when either the dimension or the replication size substantially exceeds the other. A testing procedure is introduced that centers and scales a composite measure of distance statistic among the samples to appropriately account for high dimensions and/or large sample sizes. The properties of the test statistic are examined both theoretically and empirically. The proposed procedure demonstrates superior performance in simulation studies and an application to confirm the involvement of previously identified genes in the stages of invasive breast cancer.",high dimensional data ubiquitous studies involving omics human movement imaging multivariate comparison method proposed types data either dimension replication size substantially exceeds testing procedure introduced centers scales composite measure distance statistic among samples appropriately account high dimensions large sample sizes properties test statistic examined theoretically empirically proposed procedure demonstrates superior performance simulation studies application confirm involvement previously identified genes stages invasive breast cancer
"In the present work, a novel organic single crystal (E)-N′-(1-(3-aminophenyl)ethylidene)isonicotinohydrazide was synthesized. The compound was obtained through the condensation of 3-aminoacetophenone with isonicotinic hydrazide under reflux at 60–80 °C. High-quality single crystals suitable for characterization were grown by slow evaporation at ambient temperature. The molecular and crystal structure was elucidated using single-crystal X-ray diffraction revealing a monoclinic system with space group P1 c1. The presence of hydrazide isomers confirmed by ¹H and ¹³C NMR spectroscopy. Formation of the hydrazone moiety was confirmed by a sharp IR band at 1620 cm⁻¹, while UV–Vis and emission spectra showed peaks at 237 nm and emission spectra at 421 nm in the blue region. Thermal analysis indicated exothermic transitions. The E-configuration was predicted to be predominant based on NMR chemical shifts and molecular stability. DFT-B3LYP/6–311++G(d,p) calculations provided insights into optimized geometry, electronic structure, molecular electrostatic potential and theoretical vibrational assignments. Docking studies against the 6WCF protein revealed a binding score of -14.43 kcal mol⁻¹. The bioavailability score (0.55) and favorable synthetic accessibility predicted using SwissADME, suggest potential drug-likeness. Nonlinear optical properties were evaluated via first-order hyperpolarizability and visualization of non-covalent interactions, Electron Localization Function and Localized Orbital Locator highlighted features favorable for NLO applications.",present work novel organic single crystal aminophenyl ethylidene isonicotinohydrazide synthesized compound obtained condensation aminoacetophenone isonicotinic hydrazide reflux high quality single crystals suitable characterization grown slow evaporation ambient temperature molecular crystal structure elucidated using single crystal ray diffraction revealing monoclinic system space group presence hydrazide isomers confirmed nmr spectroscopy formation hydrazone moiety confirmed sharp band vis emission spectra showed peaks emission spectra blue region thermal analysis indicated exothermic transitions configuration predicted predominant based nmr chemical shifts molecular stability dft lyp calculations provided insights optimized geometry electronic structure molecular electrostatic potential theoretical vibrational assignments docking studies wcf protein revealed binding score kcal mol bioavailability score favorable synthetic accessibility predicted using swissadme suggest potential drug likeness nonlinear optical properties evaluated via first order hyperpolarizability visualization non covalent interactions electron localization function localized orbital locator highlighted features favorable nlo applications
"Introduction In the pharmaceutical field, the rapid and accurate characterization of physicochemical properties is essential for drug development. In this context, in silico methodologies facilitate the early-stage prediction of ADME (Absorption, Distribution, Metabolism, and Excretion) and toxicity (ADME-Tox) parameters, reducing experimental costs and accelerating the screening of viable drug candidates. Computational approaches such as QSAR, SAR, and QSPR enable the assessment of biological activity and pharmacokinetic behavior. Objective This study aimed to evaluate the ADME(T) profiles of 58 organic compounds using computational tools, establish predictive models for toxicity, and assess inhibitory potential against the TLK2 kinase domain (PDB: 5O0Y)—a protein implicated in breast cancer and intellectual disability. Methodology Chemical structures were optimized using the MMFF94 force field. ADME-Tox descriptors—including Log P, Log S, Caco-2 permeability, CYP450 interactions, hERG inhibition, LD50, and DILI—were calculated using SwissADME and PreADMET. Data analysis included Pearson correlation, PCA, hierarchical clustering, and construction of a cosine similarity network. A Random Forest regression model was implemented to predict LD₅₀ values, and molecular docking simulations were conducted using PyRx and Discovery Studio. Results Correlation and PCA analyses revealed key trends, including a strong relationship between Log P and Log D, and groupings based on structural similarity. The Random Forest model demonstrated strong predictive performance for LD50 (r2 = 0.8410; RMSE = 0.1112), with five-fold cross-validation confirming robustness. Molecular docking identified several compounds with favorable binding affinities to TLK2, notably compound CC-43, which showed the strongest interaction (–8.2 kcal/mol) and a moderate theoretical toxicity profile (LD50 = 3.186). Conclusions The integration of ADME(T) profiling, machine learning, and molecular docking provides a comprehensive and reproducible computational strategy for drug discovery. The approach enabled the identification of compounds with favorable pharmacokinetic properties and selective inhibitory potential, supporting compound CC-43 as a promising candidate for further exploration in breast cancer therapeutics.",introduction pharmaceutical field rapid accurate characterization physicochemical properties essential drug development context silico methodologies facilitate early stage prediction adme absorption distribution metabolism excretion toxicity adme tox parameters reducing experimental costs accelerating screening viable drug candidates computational approaches qsar sar qspr enable assessment biological activity pharmacokinetic behavior objective study aimed evaluate adme profiles organic compounds using computational tools establish predictive models toxicity assess inhibitory potential tlk kinase domain pdb protein implicated breast cancer intellectual disability methodology chemical structures optimized using mmff force field adme tox descriptors including log log caco permeability cyp interactions herg inhibition dili calculated using swissadme preadmet data analysis included pearson correlation pca hierarchical clustering construction cosine similarity network random forest regression model implemented predict values molecular docking simulations conducted using pyrx discovery studio results correlation pca analyses revealed key trends including strong relationship log log groupings based structural similarity random forest model demonstrated strong predictive performance rmse five fold cross validation confirming robustness molecular docking identified several compounds favorable binding affinities tlk notably compound showed strongest interaction kcal mol moderate theoretical toxicity profile conclusions integration adme profiling machine learning molecular docking provides comprehensive reproducible computational strategy drug discovery approach enabled identification compounds favorable pharmacokinetic properties selective inhibitory potential supporting compound promising candidate exploration breast cancer therapeutics
"Heterocyclic scaffolds are central to drug discovery due to their diverse pharmacological properties. In this work, novel pyridine and 1,3,4-oxadiazole linked 1,3-thiazole hybrids were designed and synthesized as potential anticancer agents. Their structures were confirmed by various spectroscopic methods, and the compounds were evaluated for cytotoxicity against the MCF-7 breast cancer cell line. Among the series, compound 7a (IC₅₀ = 28.51 ± 1.70 µM) displayed the most potent activity, approaching that of cisplatin (23.44 ± 2.23 µM). Substituent effects revealed that a para-methyl group enhanced activity, whereas electron-withdrawing groups reduced potency. Computational studies (docking, MD simulations, MMGBSA, DFT, and drug-likeness predictions) further supported the experimental findings and highlighted favorable binding interactions. Overall, these results identify 1,3-thiazole hybrids as promising leads for further development in anticancer research.",heterocyclic scaffolds central drug discovery due diverse pharmacological properties work novel pyridine oxadiazole linked thiazole hybrids designed synthesized potential anticancer agents structures confirmed various spectroscopic methods compounds evaluated cytotoxicity mcf breast cancer cell line among series compound displayed potent activity approaching cisplatin substituent effects revealed methyl group enhanced activity whereas electron withdrawing groups reduced potency computational studies docking simulations mmgbsa dft drug likeness predictions supported experimental findings highlighted favorable binding interactions overall results identify thiazole hybrids promising leads development anticancer research
"We investigated the nucleophilic reactivity of a series of para-substituted anilines 2a-2e through a combined kinetic and theoretical approach, employing their reactions with thiophenes 1a-1c as electrophilic references in methanol at 20 °C. The satisfactorily correlations observed between the reaction rates and the oxidation potentials of the anilines provide compelling evidence for a single-electron transfer (SET) mechanism. Nucleophilicity parameters (N, sN) were determined in methanol following Mayr's empirical eq. A particularly compelling finding of our study is the good correlation observed between the nucleophilicity parameters (N) of anilines 2a-2e measured in methanol and those reported by Mayr in acetonitrile, demonstrating the role of solvent polarity in nucleophilic reactivity. Utilizing this correlation and the established relationship between N and the Hammett constant (σp), we predicted N values for five additional 4-X-anilines 2f-2j (X = NO₂, CN, CF₃, F, and N(CH₃)₂). In addition, density functional theory (DFT) transition-state (TS) analyses provided activation free energies (ΔG≠) for each para-substituent (X = OH, OCH3, CH3, H, Cl), all confirmed by a single imaginary frequency along the reaction coordinate. The computed ΔG≠ values inversely correlate with N: electron-donating groups decrease, and electron-withdrawing groups increase the activation barrier. Furthermore, DFT calculations were conducted to explore relationships between N and various reactivity descriptors, including the global nucleophilicity index (ω−1), dipole moment (μ), polarizability (α), and hyperpolarizability (β) for the ten para-X-substituted anilines 2a-2j. The data reveal a clear structure–property framework for tuning the NLO response of substituted anilines. Molecules substituted with strong EWGs are superior candidates for NLO applications due to their high dipole moments, elevated polarizabilities, and exceptionally large hyperpolarizabilities.",investigated nucleophilic reactivity series substituted anilines combined kinetic theoretical approach employing reactions thiophenes electrophilic references methanol satisfactorily correlations observed reaction rates oxidation potentials anilines provide compelling evidence single electron transfer set mechanism nucleophilicity parameters determined methanol following mayr empirical particularly compelling finding study good correlation observed nucleophilicity parameters anilines measured methanol reported mayr acetonitrile demonstrating role solvent polarity nucleophilic reactivity utilizing correlation established relationship hammett constant predicted values five additional anilines addition density functional theory dft transition state analyses provided activation free energies substituent och confirmed single imaginary frequency along reaction coordinate computed values inversely correlate electron donating groups decrease electron withdrawing groups increase activation barrier furthermore dft calculations conducted explore relationships various reactivity descriptors including global nucleophilicity index dipole moment polarizability hyperpolarizability ten substituted anilines data reveal clear structure property framework tuning nlo response substituted anilines molecules substituted strong ewgs superior candidates nlo applications due high dipole moments elevated polarizabilities exceptionally large hyperpolarizabilities
"The high heterogeneity of Hepatocellular Carcinoma (HCC) severely hampers clinical outcomes. Current classifications based on gene expression profiles may not adequately capture patients' overall biological mechanisms and phenotypic changes. Therefore, this study constructed HCC subtypes by integrating multi-omics data based on gene set pathways. We identified transcriptionally dysregulated genes, generated pathway scoring matrices for four gene sets using GSVA, and incorporated mutation data (with network smoothing) to define subtypes. We successfully constructed three subtypes (PS1, PS2, and PS3) that exhibited distinct patterns in immune infiltration, biological pathway characteristics, and genomic instability. Among these, PS3 showed the worst prognosis and was more enriched in pathways related to cell proliferation. The higher silhouette coefficient confirmed the validity of the classification. Moreover, our analysis identified six subtype-specific drugs, such as KU_55933 and Cyclophosphamide, that were more sensitive to PS1. Screening the differential pathways, we identified nine core pathways and performed mutation profiling on the extension of telomeres pathway, successfully identifying three telomere-related biomarkers: POLD1, RFC1, and TERF1. Meanwhile, we constructed a nomogram with important clinical features and validated the subtype of independent prognostic significance. Finally, by integrating 15 machine learning algorithms, we established a reproducible classification model (NN_MLP_10: AUC = 0.930) based on 10 genes. In conclusion, this study successfully constructed and evaluated a pathway-based molecular subtype and classification model for HCC, thoroughly investigated the biological and multi-omics differences between subtypes. Additionally, the identification of three telomere-associated biomarkers offers guidance and a theoretical basis for personalized treatment and clinical use of drugs for HCC patients.",high heterogeneity hepatocellular carcinoma hcc severely hampers clinical outcomes current classifications based gene expression profiles may adequately capture patients overall biological mechanisms phenotypic changes therefore study constructed hcc subtypes integrating multi omics data based gene set pathways identified transcriptionally dysregulated genes generated pathway scoring matrices four gene sets using gsva incorporated mutation data network smoothing define subtypes successfully constructed three subtypes exhibited distinct patterns immune infiltration biological pathway characteristics genomic instability among showed worst prognosis enriched pathways related cell proliferation higher silhouette coefficient confirmed validity classification moreover analysis identified six subtype specific drugs cyclophosphamide sensitive screening differential pathways identified nine core pathways performed mutation profiling extension telomeres pathway successfully identifying three telomere related biomarkers pold rfc terf meanwhile constructed nomogram important clinical features validated subtype independent prognostic significance finally integrating machine learning algorithms established reproducible classification model mlp auc based genes conclusion study successfully constructed evaluated pathway based molecular subtype classification model hcc thoroughly investigated biological multi omics differences subtypes additionally identification three telomere associated biomarkers offers guidance theoretical basis personalized treatment clinical use drugs hcc patients
"Robot programming is an age-appropriate means to cultivate children's computational thinking (CT) and executive function (EF). Project-based learning (PBL) is commonly applied to teach robot programming. PBL involves complex, large-scale projects consisting of multiple sub-projects. Each sub-project introduces new knowledge but offers few opportunities to connect it with previously learned knowledge, thus misaligning with children's cognitive development. In comparison, incremental PBL (IPBL) is a tailored design of PBL that begins with basic knowledge, with each new sub-project building on all learned knowledge from previous sub-projects and progressively adding new knowledge to achieve complete project learning. Hence, this research integrated IPBL into robot programming to develop an incremental project-based robot programming (I-PBRP) approach, and evaluated its effect on young children's CT, EF, and learning behavioral patterns. Ninety-five children aged 5–6 engaged in 12-week interventions. They were randomly assigned to the I-PBRP group and the conventional project-based robot programming (C-PBRP) group. Results manifested that the I-PBRP group achieved better performance than the C-PBRP group in CT and in the three components of EF (inhibition, working memory, and cognitive flexibility) over time. The progressive behavioral analysis manifested that the I-PBRP approach promoted superior performance in robot programming activities and more positive learning behaviors for children. This research has implications for robot programming teaching approaches for young children's CT and EF development.",robot programming age appropriate means cultivate children computational thinking executive function project based learning pbl commonly applied teach robot programming pbl involves complex large scale projects consisting multiple sub projects sub project introduces new knowledge offers opportunities connect previously learned knowledge thus misaligning children cognitive development comparison incremental pbl ipbl tailored design pbl begins basic knowledge new sub project building learned knowledge previous sub projects progressively adding new knowledge achieve complete project learning hence research integrated ipbl robot programming develop incremental project based robot programming pbrp approach evaluated effect young children learning behavioral patterns ninety five children aged engaged week interventions randomly assigned pbrp group conventional project based robot programming pbrp group results manifested pbrp group achieved better performance pbrp group three components inhibition working memory cognitive flexibility time progressive behavioral analysis manifested pbrp approach promoted superior performance robot programming activities positive learning behaviors children research implications robot programming teaching approaches young children development
"This study introduces a new strategy for pesticide's (dinotefuran) detection by employing amino acid–based eutectic mixtures (EMs) with graphene oxide (GO). Unlike conventional modifiers, the EMs were systematically designed and screened through DFT calculations to get the best EM in a particular ratio (lysine–choline chloride, C12, that is in 2:1 ratio), offering tunable polarity, strong hydrogen bonding, and eco-friendly functionalization. The EMs were synthesized and characterized using spectroscopic techniques and thermogravimetric analysis. A high-dipole-moment EM (C12) was selected for non-covalent functionalization of GO. DFT results highlighted significant optimization energy and dipole moment in the GO-EM composite. Electrochemical studies showed the compostie, GO-EM has enhanced sensing ability towards dinotefuran , demonstrating reduced dipole moment and optimization energy. The low LOD (5.6 µM) highlights the high sensitivity of the GO-EM electrode for dinotefuran detection. The results from the tafel plot, and cyclic voltammograms collectively demonstrate that the ability of GO-in the electrochemical sensing of dinotefuran.",study introduces new strategy pesticide dinotefuran detection employing amino acid based eutectic mixtures ems graphene oxide unlike conventional modifiers ems systematically designed screened dft calculations get best particular ratio lysine choline chloride ratio offering tunable polarity strong hydrogen bonding eco friendly functionalization ems synthesized characterized using spectroscopic techniques thermogravimetric analysis high dipole moment selected non covalent functionalization dft results highlighted significant optimization energy dipole moment composite electrochemical studies showed compostie enhanced sensing ability towards dinotefuran demonstrating reduced dipole moment optimization energy low lod highlights high sensitivity electrode dinotefuran detection results tafel plot cyclic voltammograms collectively demonstrate ability electrochemical sensing dinotefuran
"Esophageal squamous cell carcinoma (ESCC) is a major global health challenge, especially in Asia, due to its high incidence, mortality and poor prognosis. As there are no reliable early - diagnosis biomarkers, ESCC is often detected at an advanced stage, when radiotherapy becomes the main treatment. However, the emergence of radioresistance significantly compromises treatment efficacy, leading to tumor recurrence and metastasis. Although some research has been done on the mechanisms of ESCC radiation resistance, a comprehensive understanding remains elusive. To address this knowledge gap and identify more molecular targets for overcoming radiation resistance, we established a radioresistant ESCC cell model and conducted systematic 4D label-free proteomic profiling. Quantitative analysis revealed 364 differentially expressed proteins, predominantly enriched in nucleotide excision repair, glutathione metabolism, and insulin resistance pathways. Functional validation identified TXNDC12 as a critical regulator of radioresistance, and its overexpression is significantly associated with enhanced glutathione synthesis and intracellular ROS scavenging. This study provides the first proteomic evidence linking redox homeostasis modulation through TXNDC12-GSH axis activation to ESCC radioresistance, offering novel therapeutic targets for overcoming radiation resistance and improving clinical outcomes in advanced ESCC management.",esophageal squamous cell carcinoma escc major global health challenge especially asia due high incidence mortality poor prognosis reliable early diagnosis biomarkers escc often detected advanced stage radiotherapy becomes main treatment however emergence radioresistance significantly compromises treatment efficacy leading tumor recurrence metastasis although research done mechanisms escc radiation resistance comprehensive understanding remains elusive address knowledge gap identify molecular targets overcoming radiation resistance established radioresistant escc cell model conducted systematic label free proteomic profiling quantitative analysis revealed differentially expressed proteins predominantly enriched nucleotide excision repair glutathione metabolism insulin resistance pathways functional validation identified txndc critical regulator radioresistance overexpression significantly associated enhanced glutathione synthesis intracellular ros scavenging study provides first proteomic evidence linking redox homeostasis modulation txndc gsh axis activation escc radioresistance offering novel therapeutic targets overcoming radiation resistance improving clinical outcomes advanced escc management
"Cytological examination of serous effusion is critical for diagnosing malignancies, yet it heavily relies on subjective interpretation by pathologists, leading to inconsistent accuracy and misdiagnosis, especially in regions with limited medical resources. To address this challenge, we propose a two-step deep learning framework to standardize and enhance the diagnostic process. First, we improved the YOLOv8 model by integrating the Online Convolutional Reparameterization (OREPA) module, achieving a 93.09 % sensitivity for detecting abnormal cells. Second, we employed the Dual Attention Vision Transformer (DaViT) to classify normal cells (lymphocytes, mesothelial cells, histiocytes, neutrophils) with 98.74 % accuracy. By jointly deploying these models, our approach reduces missed diagnoses and provides granular insights into cell composition, offering a robust tool for rapid and objective cytopathological diagnosis. This work bridges the gap between AI-driven automation and clinical needs, particularly in resource-constrained settings.",cytological examination serous effusion critical diagnosing malignancies yet heavily relies subjective interpretation pathologists leading inconsistent accuracy misdiagnosis especially regions limited medical resources address challenge propose two step deep learning framework standardize enhance diagnostic process first improved yolov model integrating online convolutional reparameterization orepa module achieving sensitivity detecting abnormal cells second employed dual attention vision transformer davit classify normal cells lymphocytes mesothelial cells histiocytes neutrophils accuracy jointly deploying models approach reduces missed diagnoses provides granular insights cell composition offering robust tool rapid objective cytopathological diagnosis work bridges gap driven automation clinical needs particularly resource constrained settings
"The advent of Industry 4.0 has ushered in an era where data-driven decision-making is critical to operational excellence in manufacturing. Real-time data collection, predictive maintenance, and IoT-based automation are essential to maintain a competitive edge in industrial processes. Despite significant progress, adopting these technologies remains a challenge for many industries, particularly those that rely on legacy systems. Most industrial environments still operate with expensive, rigid and less flexibly monitoring systems. These systems are often difficult to scale and cannot adapt to rapidly changing operational needs. In this work, we propose a new solution that enables real-time data collection and processing using edge computing, reducing latency and guaranteeing quick decisions. Our system supports modular expansion, enabling the integration of additional sensors or processing capabilities to adapt to different industrial needs, including cooperation with older solutions.",advent industry ushered data driven decision making critical operational excellence manufacturing real time data collection predictive maintenance iot based automation essential maintain competitive edge industrial processes despite significant progress adopting technologies remains challenge many industries particularly rely legacy systems industrial environments still operate expensive rigid less flexibly monitoring systems systems often difficult scale adapt rapidly changing operational needs work propose new solution enables real time data collection processing using edge computing reducing latency guaranteeing quick decisions system supports modular expansion enabling integration additional sensors processing capabilities adapt different industrial needs including cooperation older solutions
"With the advancements of next-generation sequencing, publicly available pharmacogenomic datasets from cancer cell lines provide a handle for developing predictive models of drug responses and identifying associated biomarkers. However, many currently available predictive models are often just used as black boxes, lacking meaningful biological interpretations. In this study, we made use of open-source drug response data from cancer cell lines, in conjunction with KEGG pathway information, to develop sparse neural networks, K-net, enabling the prediction of drug response in EGFR signaling pathways and the identification of key biomarkers. To explore the rationality of identified biomarkers, we analyzed distribution patterns between drug-resistant and sensitive cell lines and performed simulated perturbation analysis on drug response. We compared K-net with commonly used interpretable algorithms in biomarker identification, such as lasso logistic regression and random forest classifiers. Our results suggested that K-net outperformed other algorithms in identifying key biomarkers linked to osimertinib response, such as KRAS and TP53 mutations, as well as AKT3 overexpression, accurately revealing their associations with osimertinib resistance. Moreover, K-net revealed subtype-specific top biomarkers for osimertinib resistance, with lung adenocarcinoma (LUAD) showing a predisposition to KRAS mutations and small cell lung cancer (SCLC) exhibiting AKT3 overexpression. Our study revealed that K-net was able to precisely identify critical biomarkers linked to drug responses, highlighting the potential to facilitate optimization of cancer treatment strategies.",advancements next generation sequencing publicly available pharmacogenomic datasets cancer cell lines provide handle developing predictive models drug responses identifying associated biomarkers however many currently available predictive models often used black boxes lacking meaningful biological interpretations study made use open source drug response data cancer cell lines conjunction kegg pathway information develop sparse neural networks net enabling prediction drug response egfr signaling pathways identification key biomarkers explore rationality identified biomarkers analyzed distribution patterns drug resistant sensitive cell lines performed simulated perturbation analysis drug response compared net commonly used interpretable algorithms biomarker identification lasso logistic regression random forest classifiers results suggested net outperformed algorithms identifying key biomarkers linked osimertinib response kras mutations well akt overexpression accurately revealing associations osimertinib resistance moreover net revealed subtype specific top biomarkers osimertinib resistance lung adenocarcinoma luad showing predisposition kras mutations small cell lung cancer sclc exhibiting akt overexpression study revealed net able precisely identify critical biomarkers linked drug responses highlighting potential facilitate optimization cancer treatment strategies
"Women are susceptible to hormonal imbalances and endocrine-related disorders such as Polycystic Ovary Syndrome (PCOS), Ovarian Cancer (OC), and Major Depressive Disorder (MDD). This study aims to identify gene-level interconnections among these conditions using omics-based bioinformatic approaches. Publicly available GEO datasets, viz., GSE226146 (PCOS), GSE18520 (OC), and GSE125664 (MDD), were analyzed, which in total resulted in 21,366 differentially expressed genes (DEGs), including 11,174 upregulated and 10,198 downregulated genes. Common genes PTTG1 and PID1 were identified using Venny 2.0. A protein–protein interaction (PPI) network was constructed using STRING, and 10 hub genes (ANAPC5, ANAPC2, PTTG1, FZR1, ANAPC4, CDC20, CDC27, ANAPC10, UBE2C, and BUB1) were identified using CytoHubba based on MCC scoring. Functional enrichment analysis showed significant involvement of these genes in oocyte meiosis, progesterone-mediated oocyte maturation, mitotic regulation, and metaphase-anaphase transition (p < 0.05). PTTG1, identified as both a common and hub gene, was downregulated in PCOS and upregulated in OC and MDD. Drug-gene interaction analysis using DSigDB via Enrichr identified Alvespimycin (for PCOS) and Gefitinib (for OC) as drugs targeting PTTG1. Molecular docking using AutoDock 4.2.6 showed that Alvespimycin and Ephedrone bind PTTG1 with a binding affinity of − 4.59 kcal/mol and − 5.81 kcal/mol, respectively, while Gefitinib showed − 4.92 kcal/mol, slightly less than Troglitazone (-5.3 kcal/mol) for OC. This study highlights PTTG1 as a shared molecular link among PCOS, OC, and MDD, suggesting its potential as a therapeutic target and providing insights into the genetic and physiological overlap of these conditions.",women susceptible hormonal imbalances endocrine related disorders polycystic ovary syndrome pcos ovarian cancer major depressive disorder mdd study aims identify gene level interconnections among conditions using omics based bioinformatic approaches publicly available geo datasets viz gse pcos gse gse mdd analyzed total resulted differentially expressed genes degs including upregulated downregulated genes common genes pttg pid identified using venny protein protein interaction ppi network constructed using string hub genes anapc anapc pttg fzr anapc cdc cdc anapc ube bub identified using cytohubba based mcc scoring functional enrichment analysis showed significant involvement genes oocyte meiosis progesterone mediated oocyte maturation mitotic regulation metaphase anaphase transition pttg identified common hub gene downregulated pcos upregulated mdd drug gene interaction analysis using dsigdb via enrichr identified alvespimycin pcos gefitinib drugs targeting pttg molecular docking using autodock showed alvespimycin ephedrone bind pttg binding affinity kcal mol kcal mol respectively gefitinib showed kcal mol slightly less troglitazone kcal mol study highlights pttg shared molecular link among pcos mdd suggesting potential therapeutic target providing insights genetic physiological overlap conditions
"Artificial intelligence (AI)-assisted thermostability prediction of proteins can significantly alleviate the burden of mutation screening, thereby enhancing the efficiency of protein engineering. To further improve prediction accuracy and shorten the development cycle of new proteins, we integrate protein sequences, mutation relationships, and physicochemical properties for encoding, introducing the innovative Sparse Convolutional Network driven by the self-attention mechanism, named SCSAddG. Experimental results demonstrate that SCSAddG achieves a prediction accuracy of 0.868, a precision of 0.710, a recall of 0.606, an F1 score of 0.653, and an area under the Receiver Operating Characteristic (AUROC) of 0.825 in the general dataset S2648. Compared to traditional Convolutional Neural Networks (CNN), SCSAddG exhibits slightly higher prediction accuracy and outperforms the Rosetta bioinformatics simulation software 12% in terms of accuracy. Furthermore, in the experimental transglutaminase dataset, SCSAddG exhibits significantly better prediction accuracy compared to CNN (0.744 vs. 0.667), achieving a precision of 1.000. The results of wet laboratory experiments are consistent with the model predictions. In the 5-fold cross-validation, the SCSAddG model outperformed the CNN across multiple evaluation metrics, demonstrating its superior predictive performance and robust reliability. These results indicate that SCSAddG can effectively evaluate the trends in protein thermostability and serve as a valuable tool to guide protein thermostability engineering.",artificial intelligence assisted thermostability prediction proteins significantly alleviate burden mutation screening thereby enhancing efficiency protein engineering improve prediction accuracy shorten development cycle new proteins integrate protein sequences mutation relationships physicochemical properties encoding introducing innovative sparse convolutional network driven self attention mechanism named scsaddg experimental results demonstrate scsaddg achieves prediction accuracy precision recall score area receiver operating characteristic auroc general dataset compared traditional convolutional neural networks cnn scsaddg exhibits slightly higher prediction accuracy outperforms rosetta bioinformatics simulation software terms accuracy furthermore experimental transglutaminase dataset scsaddg exhibits significantly better prediction accuracy compared cnn achieving precision results wet laboratory experiments consistent model predictions fold cross validation scsaddg model outperformed cnn across multiple evaluation metrics demonstrating superior predictive performance robust reliability results indicate scsaddg effectively evaluate trends protein thermostability serve valuable tool guide protein thermostability engineering
"The computation of integrals is a fundamental task in the analysis of functional data, where the data are typically considered as random elements in a space of squared integrable functions. Effective unbiased estimation and inference procedures are proposed for integrals of uni- and multivariate random functions. Applications to key problems in functional data analysis involving random design points are examined and illustrated. In the absence of noise, the proposed estimates converge faster than the sample mean and standard numerical integration algorithms. The estimator also supports effective inference by generally providing better coverage with shorter confidence and prediction intervals in both noisy and noiseless settings.",computation integrals fundamental task analysis functional data data typically considered random elements space squared integrable functions effective unbiased estimation inference procedures proposed integrals uni multivariate random functions applications key problems functional data analysis involving random design points examined illustrated absence noise proposed estimates converge faster sample mean standard numerical integration algorithms estimator also supports effective inference generally providing better coverage shorter confidence prediction intervals noisy noiseless settings
"The abnormal growth of cells leads to brain malignancy in humans, which is among the most prevalent causes of fatalities in adults worldwide. Patients' likelihood of survival increases, and therapeutic opportunities improve when brain tumors are identified early. Compared to other imaging techniques, Magnetic Resonance Imaging (MRI) scans provide more comprehensive information. A brain tumor can be diagnosed and differentiated from MRI images using a variety of brain tumor recognition and segmentation approaches. The utilization of deep learning-based models has proven effective in analyzing the vast volume of MRI data. The main purpose of this review is to provide an overview of brain tumor segmentation and detection techniques. To efficiently process the large volume of images, this review presents a detailed analysis of deep learning models. Furthermore, a chronological analysis is carried out to validate the robustness of the techniques. Following that, to better understand the performance of the models, the strengths and limitations of standard deep learning methods are discussed. In addition, the dataset details, performance evaluations, and simulation tools are discussed in this review. Finally, the challenges and research gaps in brain tumor segmentation and detection models are highlighted.",abnormal growth cells leads brain malignancy humans among prevalent causes fatalities adults worldwide patients likelihood survival increases therapeutic opportunities improve brain tumors identified early compared imaging techniques magnetic resonance imaging mri scans provide comprehensive information brain tumor diagnosed differentiated mri images using variety brain tumor recognition segmentation approaches utilization deep learning based models proven effective analyzing vast volume mri data main purpose review provide overview brain tumor segmentation detection techniques efficiently process large volume images review presents detailed analysis deep learning models furthermore chronological analysis carried validate robustness techniques following better understand performance models strengths limitations standard deep learning methods discussed addition dataset details performance evaluations simulation tools discussed review finally challenges research gaps brain tumor segmentation detection models highlighted
"The use of emerging micropollutants (EMPs) such as pesticides has raised numerous concerns and problems in the lives of humans and other living organisms. The entry of these pollutants into the environment through agricultural pesticides and subsequent water pollution has prompted mankind to separate these micropollutants from water using extraction methods. For this purpose, the green solvent extraction method is effective in this field. By using green solvents, such as deep eutectic solvents (DESs), the intermolecular interactions between these solvents and micropollutants can be investigated, and a new green solvent for separation can be proposed. In pursuit of this goal, in this paper, the extraction of 4 newly discovered micropollutants from water such as acetamiprid (Acet), imidacloprid (Imid), nitenpyram (Nite) and thiamethoxam (Thia) using a hydrophobic deep eutectic solvent (HDES, made of components with low water solubility for easy and good separation of micropollutants) consisting of thymol (Thy) and caprylic acid (Cap; C8) with a molar ratio of 2:3 from the aqueous phase, either as a mixture of micropollutants or individually, was simulated and structural and dynamic analyses including RDF, SDF, CDF, H-bonding, non-bonding energies showed encouraging results. Also, the formation of hydrogen bonds between HBA and HBD was identified and confirmed using 1H NMR and FTIR analysis. To understand the selectivity of adsorption (solubility) and diffusion of pesticides in eutectic mixtures, the effect of intermolecular interactions on dynamic properties at 298 K was investigated. The results revealed strong interactions between pesticides and DES compounds (high extraction efficiency), and two pollutants, Imid and Acet, were separated by green solvent more than Nite and Thia. In this way, the relative stability coefficient was also investigated and confirmed (DES with high stability (S = 3.94) that can be used in the separation of different types of pollutants). Properties such as σ profiles, σ potential, activity coefficient, partition coefficient, and LLE in the COSMO-RS model were calculated. The efficiency of the selected solvent in the micropollutants extraction was confirmed. This work, with a new approach to the extraction of insecticides, firstly introduces a DES that has not been reported for the extraction of such insecticides so far. Secondly, with an integrated computational strategy combining MD and COSMO-RS, elucidating the extraction mechanism at the molecular level, it allows for valuable predictions that significantly reduce the experimental costs.",use emerging micropollutants emps pesticides raised numerous concerns problems lives humans living organisms entry pollutants environment agricultural pesticides subsequent water pollution prompted mankind separate micropollutants water using extraction methods purpose green solvent extraction method effective field using green solvents deep eutectic solvents dess intermolecular interactions solvents micropollutants investigated new green solvent separation proposed pursuit goal paper extraction newly discovered micropollutants water acetamiprid acet imidacloprid imid nitenpyram nite thiamethoxam thia using hydrophobic deep eutectic solvent hdes made components low water solubility easy good separation micropollutants consisting thymol thy caprylic acid cap molar ratio aqueous phase either mixture micropollutants individually simulated structural dynamic analyses including rdf sdf cdf bonding non bonding energies showed encouraging results also formation hydrogen bonds hba hbd identified confirmed using nmr ftir analysis understand selectivity adsorption solubility diffusion pesticides eutectic mixtures effect intermolecular interactions dynamic properties investigated results revealed strong interactions pesticides des compounds high extraction efficiency two pollutants imid acet separated green solvent nite thia way relative stability coefficient also investigated confirmed des high stability used separation different types pollutants properties profiles potential activity coefficient partition coefficient lle cosmo model calculated efficiency selected solvent micropollutants extraction confirmed work new approach extraction insecticides firstly introduces des reported extraction insecticides far secondly integrated computational strategy combining cosmo elucidating extraction mechanism molecular level allows valuable predictions significantly reduce experimental costs
"High Temperature Gas Cooled Reactors (HTGRs) offer wide range of applications besides electricity generation therefore different designs are under development with multi-physics modeling. Although neutronic calculations of block type HTGRs are quite straight forward, thermal–hydraulic calculations are challenging due to complex heat transfer mechanism in the core. In addition, if high fidelity is applied, the computational power and time requirement is quite high. The high fidelity, loose, and explicit coupling multi-physics approach with Monte Carlo and computational fluid dynamics codes was proposed in this study that minimizes the computational power need without losing accuracy. The proposed methodology was tested with Holos Quad Core microreactor. The calculations showed that both neutronic and thermal–hydraulic simulation results of the proposed methodology are within 4% difference level with the results given for Holos microreactor therefore showing the proposed methodology’s reliability.",high temperature gas cooled reactors htgrs offer wide range applications besides electricity generation therefore different designs development multi physics modeling although neutronic calculations block type htgrs quite straight forward thermal hydraulic calculations challenging due complex heat transfer mechanism core addition high fidelity applied computational power time requirement quite high high fidelity loose explicit coupling multi physics approach monte carlo computational fluid dynamics codes proposed study minimizes computational power need without losing accuracy proposed methodology tested holos quad core microreactor calculations showed neutronic thermal hydraulic simulation results proposed methodology within difference level results given holos microreactor therefore showing proposed methodology reliability
"Cancer is a complex disease characterized by uncontrolled cell proliferation and metastasis, with breast cancer remaining a leading cause of mortality among women worldwide. Hypoxia-inducible factor (HIF) and vascular endothelial growth factor (VEGF) are key mediators of angiogenesis, sustaining tumor growth and progression. RNA interference (RNAi) has emerged as a promising gene-silencing strategy for targeted cancer therapy. In this study, we designed small interfering RNAs (siRNAs) against VEGF mRNA using computational approaches. VEGF gene sequences were retrieved from NCBI, and siRNAs were designed using siDirect v2.0 and i-Score Designer. Candidate siRNAs were screened based on GC content (30–52 %), secondary structure, and thermodynamic stability. Hybridization energy analysis revealed favourable binding to VEGF mRNA, ranging from –31.1 to –37.3 kcal/mol. Molecular docking with h-Argonaute-2 (h-Ago2) yielded docking scores between –330 and –351 kcal/mol, indicating efficient RISC loading. Molecular dynamics (MD) simulations further demonstrated stable siRNA–Ago2 complexes, with RMSD values stabilizing around 2.1–2.6 Å and RMSF fluctuations primarily localized to the PAZ and MID domains. These findings confirm strong binding affinity, structural stability, and specificity of the designed siRNAs. Overall, our results suggest that RNAi-based silencing of VEGF holds significant potential as a therapeutic strategy for inhibiting angiogenesis in breast cancer.",cancer complex disease characterized uncontrolled cell proliferation metastasis breast cancer remaining leading cause mortality among women worldwide hypoxia inducible factor hif vascular endothelial growth factor vegf key mediators angiogenesis sustaining tumor growth progression rna interference rnai emerged promising gene silencing strategy targeted cancer therapy study designed small interfering rnas sirnas vegf mrna using computational approaches vegf gene sequences retrieved ncbi sirnas designed using sidirect score designer candidate sirnas screened based content secondary structure thermodynamic stability hybridization energy analysis revealed favourable binding vegf mrna ranging kcal mol molecular docking argonaute ago yielded docking scores kcal mol indicating efficient risc loading molecular dynamics simulations demonstrated stable sirna ago complexes rmsd values stabilizing around rmsf fluctuations primarily localized paz mid domains findings confirm strong binding affinity structural stability specificity designed sirnas overall results suggest rnai based silencing vegf holds significant potential therapeutic strategy inhibiting angiogenesis breast cancer
"Alzheimer’s disease is an advanced neurodegenerative illness that disturbs cognitive behavior. Multiple factors are responsible for the etiology of Alzheimer’s disease and one of the cores neuropathologic finding is generation of hyper-phosphorylated tau. GSK-3β or Glycogen synthase kinase-3β a kinase leads to the hyperphosphorylation of tau protein at multiple sites and aggregates into neurofibrillary tangles in AD patient’s brain. Tideglusib is a drug which is under second phase of clinical trial (NCT01350362), impedes the GSK-3β at therapeutically concentration that has been established through various in silico techniques like scaffold morphing, pharmacokinetic, molecular docking and dynamic simulations studies. The Tideglusib based analogues showed good interactions with the catalytic dyed residue (Cys199) of GSK-3β GSK-3β, the main amino acid responsible for its tau hyperphosphorylation activity. Also, the designed analogues of Tideglusib are analyzed for its Multi targeting potential with three main receptors (GSK-3β, AChE, BACE) through molecular docking and molecular dynamic simulation approaches. SG-09 stands out with the best binding affinity and the stable ligand-protein interaction analogues in the time interval of 100 ns can be used as Multitargeting drug with further in silico, in vitro and in vivo clinical evaluation. This design strategy could thus reap considerable clinical and economic rewards.",alzheimer disease advanced neurodegenerative illness disturbs cognitive behavior multiple factors responsible etiology alzheimer disease one cores neuropathologic finding generation hyper phosphorylated tau gsk glycogen synthase kinase kinase leads hyperphosphorylation tau protein multiple sites aggregates neurofibrillary tangles patient brain tideglusib drug second phase clinical trial nct impedes gsk therapeutically concentration established various silico techniques like scaffold morphing pharmacokinetic molecular docking dynamic simulations studies tideglusib based analogues showed good interactions catalytic dyed residue cys gsk gsk main amino acid responsible tau hyperphosphorylation activity also designed analogues tideglusib analyzed multi targeting potential three main receptors gsk ache bace molecular docking molecular dynamic simulation approaches stands best binding affinity stable ligand protein interaction analogues time interval used multitargeting drug silico vitro vivo clinical evaluation design strategy could thus reap considerable clinical economic rewards
"A novel group of pyrimidine-based hybrids containing a 2,4-dichlorophenyl core was designed, synthesized, and evaluated as potential xanthine oxidase (XO) inhibitors for treating gouty arthritis. Through established medicinal chemistry synthetic methods, these compounds were successfully prepared and their structures confirmed using detailed spectroscopic analyses, including high-resolution mass spectrometry (HRMS), Fourier-transform infrared (FT-IR), and multinuclear NMR (¹H and ¹³C). In vitro screening, nine candidates (2, 3, 9, 10, 11a, 11d, 11e, 12a, and 12b) assessed their XO inhibitory activity, ability to prevent protein denaturation, and capability to stabilize erythrocyte membranes. Notably, compound 12b emerged as the most promising agent, displaying superior XO inhibition (IC₅₀ = 2.8 μM), effective protein denaturation suppression (89.4% at 100 μg/mL), and significant reduction in hemolysis (92.1% efficacy), surpassing standard drugs like allopurinol and diclofenac. Molecular docking simulations indicated strong binding between the compounds and the XO active site, with 12b showing optimal interactions through hydrogen bonding and hydrophobic contacts. These computational results were consistent with experimental outcomes, suggesting a mechanism of competitive inhibition. The overall findings highlight the therapeutic potential of 12b as a leading candidate for gout management, providing a foundation for further optimization in developing advanced XO inhibitors with improved efficacy and selectivity.",novel group pyrimidine based hybrids containing dichlorophenyl core designed synthesized evaluated potential xanthine oxidase inhibitors treating gouty arthritis established medicinal chemistry synthetic methods compounds successfully prepared structures confirmed using detailed spectroscopic analyses including high resolution mass spectrometry hrms fourier transform infrared multinuclear nmr vitro screening nine candidates assessed inhibitory activity ability prevent protein denaturation capability stabilize erythrocyte membranes notably compound emerged promising agent displaying superior inhibition effective protein denaturation suppression significant reduction hemolysis efficacy surpassing standard drugs like allopurinol diclofenac molecular docking simulations indicated strong binding compounds active site showing optimal interactions hydrogen bonding hydrophobic contacts computational results consistent experimental outcomes suggesting mechanism competitive inhibition overall findings highlight therapeutic potential leading candidate gout management providing foundation optimization developing advanced inhibitors improved efficacy selectivity
"Circular RNAs (circRNAs) play key roles in the development of various diseases and drug responses, and mining their potential association with drugs is of great significance in unraveling disease mechanisms and discovering new therapeutic targets. However, traditional hypergraph methods typically rely on a static composition strategy, which makes it challenging to dynamically adjust the structure according to the task, resulting in a limited ability to extract higher-order information. Accordingly, we present a learnable hypergraph reconstruction (LHRCDA) approach for predicting circRNA–drug sensitivity associations. Specifically, the hypergraph representation is initialized based on matrix decomposition, followed by a learnable hypergraph reconstruction mechanism to adaptively capture potential higher-order structural information. After reconstruction, a hierarchical hypergraph mapping approach encodes the hypergraph at multiple levels, thereby further enhancing the ability to model complex nonlinear interactions between hyperedges. To effectively integrate different view representations from heterogeneous graph and hypergraph views, we adopt a hierarchical perceptual fusion strategy that solves the problem of information redundancy or loss that exists in traditional single fusion methods. Ultimately, the fused node representations are nonlinearly mapped using an MLP to output the association probabilities between circRNAs and drugs. Experimental results show that the hypergraph approach based on higher-order structural modeling outperforms multiple existing state-of-the-art models in circRNA–drug sensitivity association prediction. In addition, case studies were conducted on the drugs vorinostat and cetuximab, further demonstrating the potential of our model as an efficient and reliable tool.",circular rnas circrnas play key roles development various diseases drug responses mining potential association drugs great significance unraveling disease mechanisms discovering new therapeutic targets however traditional hypergraph methods typically rely static composition strategy makes challenging dynamically adjust structure according task resulting limited ability extract higher order information accordingly present learnable hypergraph reconstruction lhrcda approach predicting circrna drug sensitivity associations specifically hypergraph representation initialized based matrix decomposition followed learnable hypergraph reconstruction mechanism adaptively capture potential higher order structural information reconstruction hierarchical hypergraph mapping approach encodes hypergraph multiple levels thereby enhancing ability model complex nonlinear interactions hyperedges effectively integrate different view representations heterogeneous graph hypergraph views adopt hierarchical perceptual fusion strategy solves problem information redundancy loss exists traditional single fusion methods ultimately fused node representations nonlinearly mapped using mlp output association probabilities circrnas drugs experimental results show hypergraph approach based higher order structural modeling outperforms multiple existing state art models circrna drug sensitivity association prediction addition case studies conducted drugs vorinostat cetuximab demonstrating potential model efficient reliable tool
"As an obligate intracellular parasite, Theileria parva is strictly dependent on its host for nutrient acquisition. Transport proteins are expected to play a crucial role in the influx of essential nutrients to sustain the parasite’s rapid growth. Unfortunately, the T. parva transportome is still not comprehensively elucidated, and plagued by the presence of uncharacterized proteins. In this study, we employed a combination of approaches including sequence orthology and structural similarity to identify 188 proteins predicted to be involved in transport-related processes. Among these, 24 were uncharacterized proteins, and 17 of them could be assigned a tentative annotation. Furthermore, the localization of these 188 proteins was investigated, resulting in their assignment to seven cellular compartments. Screening of the proteomes of other Theileria species, T. annulata, T. orientalis, and T. equi revealed that all 188 proteins were present in both transforming and non-transforming Theileria parasites. Among the 188 potential transport-related proteins, 45 were associated with transmembrane transport and most of them (87 %) are conserved across phylum Apicomplexa.",obligate intracellular parasite theileria parva strictly dependent host nutrient acquisition transport proteins expected play crucial role influx essential nutrients sustain parasite rapid growth unfortunately parva transportome still comprehensively elucidated plagued presence uncharacterized proteins study employed combination approaches including sequence orthology structural similarity identify proteins predicted involved transport related processes among uncharacterized proteins could assigned tentative annotation furthermore localization proteins investigated resulting assignment seven cellular compartments screening proteomes theileria species annulata orientalis equi revealed proteins present transforming non transforming theileria parasites among potential transport related proteins associated transmembrane transport conserved across phylum apicomplexa
"The development of single-cell multi-omics sequencing technologies provides new insights into cell heterogeneity. Cell clustering is a crucial step in the analysis of multi-omics data. However, existing methods often overlook variations in data quality across omics, leading to unreliable feature representations. To address this issue, we propose scUCAF, an uncertainty-aware network for multi-omics clustering. Specifically, to mitigate the impact of noise on cell feature extraction, we introduce a variational autoencoder with a negative binomial distribution. After extracting each omics feature, we propose a high-confidence cluster-guided contrastive learning method to ensure cross-omics feature consistency. Finally, an uncertainty-aware fusion and gating network dynamically integrates the omics features to mitigate biases from low-quality data and produce reliable cell representations for clustering. Clustering results on eight real single-cell multi-omics datasets demonstrate that scUCAF outperforms existing multi-omics clustering methods. We also conduct downstream analyses to validate the effectiveness of scUCAF for cell type annotation and biomarker identification in liver cancer.",development single cell multi omics sequencing technologies provides new insights cell heterogeneity cell clustering crucial step analysis multi omics data however existing methods often overlook variations data quality across omics leading unreliable feature representations address issue propose scucaf uncertainty aware network multi omics clustering specifically mitigate impact noise cell feature extraction introduce variational autoencoder negative binomial distribution extracting omics feature propose high confidence cluster guided contrastive learning method ensure cross omics feature consistency finally uncertainty aware fusion gating network dynamically integrates omics features mitigate biases low quality data produce reliable cell representations clustering clustering results eight real single cell multi omics datasets demonstrate scucaf outperforms existing multi omics clustering methods also conduct downstream analyses validate effectiveness scucaf cell type annotation biomarker identification liver cancer
"It is known that Hepatocellular carcinoma (HCC) accounts for the highest number of deaths due to primary liver cancer and is one of the leading global oncology cases. Currently, in spite of many other efforts to improve the treatment, the prognosis is rather poor with HCC patients, indicating the necessity of new biomarkers and therapeutic targets. In this study, we undertook a multi-omics approach including exome sequencing, GO analysis, PPI network analysis, bulk RNA-seq, single-cell RNA-seq, spatial transcriptomics, and Venn and survival analysis to search decisively for drivers of liver cancer. Our investigation focused on revealing particular somatic mutations in genes implicated in various fundamental processes of cellular biology. In view of this barrier, we were then able to single out FN1, JUN, H3C12, HSPA5, and FOS among all components at the molecular landscape of HCC, where the levels of expression in turn related to patient survival. The formal HSPA5 (Heat Shock Protein Family A Member 5) gene is a molecular chaperone participates in the unfolded protein response and transcriptional endoplasmic reticulum stress, which was highly elevated in HCC tissues. Bulk RNA-seq confirmed the overexpression of HSPA5, while single-cell RNA-seq showed the higher expression of this protein in some populations of tumor cells attesting to its importance during HCC progression at a cellular level. Meanwhile, spatial transcriptomics reinforced these results with more precise data, showing where inside the tumor microenvironment HSPA5 is highlighted. In addition, PPI analysis showed much interaction of the proteins identified, bringing emphasis on their functions in HCC. Increased HSPA5 expression levels are linked with low overall survival rates and so it raised the prospect of being a diagnostic tool. Thus, HSPA5 shows promise as a cancer biomarker as well as a target therapeutic agent for HCC and hence detailed studies on its role in liver cancer and application in precision medicine for better outcome of patients need to be carried out.",known hepatocellular carcinoma hcc accounts highest number deaths due primary liver cancer one leading global oncology cases currently spite many efforts improve treatment prognosis rather poor hcc patients indicating necessity new biomarkers therapeutic targets study undertook multi omics approach including exome sequencing analysis ppi network analysis bulk rna seq single cell rna seq spatial transcriptomics venn survival analysis search decisively drivers liver cancer investigation focused revealing particular somatic mutations genes implicated various fundamental processes cellular biology view barrier able single jun hspa fos among components molecular landscape hcc levels expression turn related patient survival formal hspa heat shock protein family member gene molecular chaperone participates unfolded protein response transcriptional endoplasmic reticulum stress highly elevated hcc tissues bulk rna seq confirmed overexpression hspa single cell rna seq showed higher expression protein populations tumor cells attesting importance hcc progression cellular level meanwhile spatial transcriptomics reinforced results precise data showing inside tumor microenvironment hspa highlighted addition ppi analysis showed much interaction proteins identified bringing emphasis functions hcc increased hspa expression levels linked low overall survival rates raised prospect diagnostic tool thus hspa shows promise cancer biomarker well target therapeutic agent hcc hence detailed studies role liver cancer application precision medicine better outcome patients need carried
"Triple-negative breast cancer (TNBC) remains one of the most aggressive and treatment-resistant subtypes of breast malignancies, defined by the absence of hormone receptors and HER2 amplification. This study introduces an integrated computer-aided drug design (CADD) framework that combines Monte Carlo–optimized QSAR modeling, molecular docking, and in silico pharmacokinetic assessment to identify dual c-Met/β-tubulin inhibitors with potential anti-TNBC activity. Robust conformation-independent QSAR models were developed using SMILES- and graph-based descriptors, achieving high internal consistency (R² > 0.84) and strong external predictivity (Q²ext > 0.77). Statistical validation and the CORAL-based “defect-of-correlation” approach confirmed reliability within the defined applicability domain. Fragment-level interpretation revealed molecular motifs associated with enhanced cytotoxicity, which were subsequently used in the CADD design of new multitarget analogs, enabling rational structure–activity relationship (SAR) refinement. Molecular docking against both c-Met and β-tubulin demonstrated complementary binding orientations and stable non-covalent interactions, supporting the proposed multitarget mechanism. In silico pharmacokinetic analysis indicated favorable drug-likeness, high gastrointestinal absorption, absence of major metabolic liabilities, and low predicted toxicity, suggesting a well-balanced safety–efficacy profile. Collectively, this study establishes a CADD-based multitarget modeling platform that integrates predictive, mechanistic, and pharmacokinetic insights, providing a rational foundation for the development of next-generation personalized therapeutics for TNBC.",triple negative breast cancer tnbc remains one aggressive treatment resistant subtypes breast malignancies defined absence hormone receptors amplification study introduces integrated computer aided drug design cadd framework combines monte carlo optimized qsar modeling molecular docking silico pharmacokinetic assessment identify dual met tubulin inhibitors potential anti tnbc activity robust conformation independent qsar models developed using smiles graph based descriptors achieving high internal consistency strong external predictivity ext statistical validation coral based defect correlation approach confirmed reliability within defined applicability domain fragment level interpretation revealed molecular motifs associated enhanced cytotoxicity subsequently used cadd design new multitarget analogs enabling rational structure activity relationship sar refinement molecular docking met tubulin demonstrated complementary binding orientations stable non covalent interactions supporting proposed multitarget mechanism silico pharmacokinetic analysis indicated favorable drug likeness high gastrointestinal absorption absence major metabolic liabilities low predicted toxicity suggesting well balanced safety efficacy profile collectively study establishes cadd based multitarget modeling platform integrates predictive mechanistic pharmacokinetic insights providing rational foundation development next generation personalized therapeutics tnbc
"This study proposes an efficient approach for nonlinear optimal control problems governed by variable-order fractional integro-differential equation. The presented approach is based on transformation of the main problem to solving system of nonlinear algebraic equations. The offered computational method is established upon the Dickson polynomials and their properties. Through the way, a new operational matrix of Caputo variable-order fractional derivative is derived for the mentioned polynomials. More precisely, the state and control variables are expanded in components of the Dickson polynomials with undetermined coefficients. Then, these expansions are substituted in the cost functional and the fractional dynamical system. Consequently, the procedure of the constrained extremum problem supported by the collocation method is utilized to extract some nonlinear algebraic equations from the transformation of the desired optimal control problem by a set of unknown Lagrange multipliers. One of the key points of this method is to proving the convergence analysis and error bound of its solution. The precision of the proposed approach is examined through three various types of test examples. The obtained results indicate the superiority of the proposed method and confirm the standard criteria with theoretically accurate results.",study proposes efficient approach nonlinear optimal control problems governed variable order fractional integro differential equation presented approach based transformation main problem solving system nonlinear algebraic equations offered computational method established upon dickson polynomials properties way new operational matrix caputo variable order fractional derivative derived mentioned polynomials precisely state control variables expanded components dickson polynomials undetermined coefficients expansions substituted cost functional fractional dynamical system consequently procedure constrained extremum problem supported collocation method utilized extract nonlinear algebraic equations transformation desired optimal control problem set unknown lagrange multipliers one key points method proving convergence analysis error bound solution precision proposed approach examined three various types test examples obtained results indicate superiority proposed method confirm standard criteria theoretically accurate results
"Gelatin, a biopolymer derived from collagen, is extensively employed in biomedical and food applications due to its biocompatibility and gel-forming properties. However, its physicochemical behavior is highly dependent on structural state, being a function of amino acid composition, molecular weight distribution, and processing conditions. The formation of polyelectrolyte complexes with polysaccharides, such as carrageenans, can significantly alter gelatin’s conformational stability and functional performance. In this work, we investigated the interactions between fish gelatin and sulfated polysaccharides carrageenans (κ-, ι-, and λ-types) at the molecular level using integrated computational and experimental approaches. Molecular docking and molecular dynamics simulations were applied to analyze the effect of carrageenan type, length, and charge on the structure and stability of polysaccharide-gelatin complexes, as well as on the dynamics and stability of gelatin molecule at acidic and neutral pH. This paper presents the first systematic comparison of the interaction of fish gelatin with all three types of carrageenans (k-, k- and λ-) at the molecular level and shows for the first time that carrageenans can form complexes with gelatin molecules of different stoichiometry (1:1, 1:2 and 1:3 carrageenan: gelatin). The complementary zeta potential analysis confirmed the pH-dependent charge neutralization upon complexation, supporting the MD-predicted electrostatic-driven association. Scanning electron microscopy demonstrated microstructural reorganization in composite gels as a result of strong carrageenan-gelatin interaction. These findings have highlighted the critical role of molecular structure and electrostatic complementarity in gelatin-carrageenan hydrogels and contribute to the rational design of gelatin-based gel systems with desired properties.",gelatin biopolymer derived collagen extensively employed biomedical food applications due biocompatibility gel forming properties however physicochemical behavior highly dependent structural state function amino acid composition molecular weight distribution processing conditions formation polyelectrolyte complexes polysaccharides carrageenans significantly alter gelatin conformational stability functional performance work investigated interactions fish gelatin sulfated polysaccharides carrageenans types molecular level using integrated computational experimental approaches molecular docking molecular dynamics simulations applied analyze effect carrageenan type length charge structure stability polysaccharide gelatin complexes well dynamics stability gelatin molecule acidic neutral paper presents first systematic comparison interaction fish gelatin three types carrageenans molecular level shows first time carrageenans form complexes gelatin molecules different stoichiometry carrageenan gelatin complementary zeta potential analysis confirmed dependent charge neutralization upon complexation supporting predicted electrostatic driven association scanning electron microscopy demonstrated microstructural reorganization composite gels result strong carrageenan gelatin interaction findings highlighted critical role molecular structure electrostatic complementarity gelatin carrageenan hydrogels contribute rational design gelatin based gel systems desired properties
"Metacognitive scaffolding plays a pivotal role in supporting the development of computational thinking (CT) among elementary students. Compared to planned scaffolding, adaptive scaffolding supported by generative AI (GAI) agents offers a promising option by providing real–time, adaptive and personalized response. This study investigated how adaptive metacognitive scaffolding (AMS) and planned metacognitive scaffolding (PMS) influenced elementary students’ CT in a GAI–supported programming environment. 74 students were assigned to AMS, PMS, or control conditions. Using a mixed-methods approach, we examined CT task performance, CT skill use, metacognitive regulation, and cognitive load through standardized assessments, programming artifacts, discourse logs, video-coded behaviors, classroom observation records, cognitive load ratings. Results showed that AMS significantly enhanced contextualized CT performance and promoted more complex, diverse skill use compared to PMS and control groups. AMS also fostered more recursive metacognitive behaviors and reduced mental efforts, while PMS supported higher meaningful engagement. These findings highlight the potential of adaptive scaffolding powered by GAI to support CT development in elementary programming education.",metacognitive scaffolding plays pivotal role supporting development computational thinking among elementary students compared planned scaffolding adaptive scaffolding supported generative gai agents offers promising option providing real time adaptive personalized response study investigated adaptive metacognitive scaffolding ams planned metacognitive scaffolding pms influenced elementary students gai supported programming environment students assigned ams pms control conditions using mixed methods approach examined task performance skill use metacognitive regulation cognitive load standardized assessments programming artifacts discourse logs video coded behaviors classroom observation records cognitive load ratings results showed ams significantly enhanced contextualized performance promoted complex diverse skill use compared pms control groups ams also fostered recursive metacognitive behaviors reduced mental efforts pms supported higher meaningful engagement findings highlight potential adaptive scaffolding powered gai support development elementary programming education
"This paper introduces a hybrid operator that combines Shepard operators with Lagrange polynomials, proving that the new operator exhibits superior approximation properties compared to the classical Shepard operator. In the linear case, our approach advances known results in the literature, providing a more effective framework for approximation. Building on this foundation, the method is also extended to nonlinear scenarios by employing max-product operations, demonstrating that the nonlinear operator achieves even better approximation characteristics than its linear counterpart. The theoretical findings are validated through numerical computations and graphical representations, strongly supporting the effectiveness of the hybrid operator in both linear and nonlinear contexts.",paper introduces hybrid operator combines shepard operators lagrange polynomials proving new operator exhibits superior approximation properties compared classical shepard operator linear case approach advances known results literature providing effective framework approximation building foundation method also extended nonlinear scenarios employing max product operations demonstrating nonlinear operator achieves even better approximation characteristics linear counterpart theoretical findings validated numerical computations graphical representations strongly supporting effectiveness hybrid operator linear nonlinear contexts
"Bayesian inference relies on the posterior distribution, which is often estimated with a Markov chain Monte Carlo sampler. The sampler produces a dependent stream of variates from the limiting distribution of the Markov chain, the posterior distribution. When one wishes to display the estimated posterior density, a natural choice is the histogram. However, abundant literature has shown that the kernel density estimator is more accurate than the histogram in terms of mean integrated squared error for an i.i.d. sample. With this as motivation, a kernel density estimation method is proposed that is appropriate for the dependence in the Markov chain Monte Carlo output. To account for the dependence, the cross-validation criterion is modified to select the bandwidth in standard kernel density estimation approaches. A data-driven adjustment to the biased cross-validation method is suggested with introducing the integrated autocorrelation time of the kernel. The convergence of the modified bandwidth to the optimal bandwidth is shown by adapting theorems from the time series literature. Simulation studies show that the proposed method finds the bandwidth close to the optimal value, while standard methods lead to smaller bandwidths under Markov chain samples and hence to undersmoothed density estimates. A study with real data shows that the proposed method has a considerably smaller integrated mean squared error than standard methods. The R package KDEmcmc to implement the suggested algorithm is available on the Comprehensive R Archive Network.",bayesian inference relies posterior distribution often estimated markov chain monte carlo sampler sampler produces dependent stream variates limiting distribution markov chain posterior distribution one wishes display estimated posterior density natural choice histogram however abundant literature shown kernel density estimator accurate histogram terms mean integrated squared error sample motivation kernel density estimation method proposed appropriate dependence markov chain monte carlo output account dependence cross validation criterion modified select bandwidth standard kernel density estimation approaches data driven adjustment biased cross validation method suggested introducing integrated autocorrelation time kernel convergence modified bandwidth optimal bandwidth shown adapting theorems time series literature simulation studies show proposed method finds bandwidth close optimal value standard methods lead smaller bandwidths markov chain samples hence undersmoothed density estimates study real data shows proposed method considerably smaller integrated mean squared error standard methods package kdemcmc implement suggested algorithm available comprehensive archive network
"Efficient solar light harvesting is essential for high-performance photocatalysts. Here, Rigorous Coupled-Wave Analysis (RCWA) computational method is used to investigate and optimize the optical absorption of TiO2-BiVO4 inverse opal (IO) structures under varying light incidence angles and pore-filling medium (air or water). Simulations were validated against experimental reflectance data. They revealed that small-pore IOs strongly absorb in the UV-C and UV-B regions due to the slow photon effect, making them ideal for sterilization and water disinfection. Medium- and large-pore IOs benefit from additional slow photon effect at the 2nd order photonic band gap, enhancing absorption across both UV and visible regions. Medium-pore IOs are suited for indoor air treatment and water purification, while large-pore IOs with the highest photon flux enhancement enable solar-driven photocatalysis such as outdoor pollutant removal and hydrogen production. For all tested IO designs, the absorbed photon flux exceeds that of equivalent planar slabs, highlighting the advantage of photonic structuring for sustainable photocatalytic applications.",efficient solar light harvesting essential high performance photocatalysts rigorous coupled wave analysis rcwa computational method used investigate optimize optical absorption tio bivo inverse opal structures varying light incidence angles pore filling medium air water simulations validated experimental reflectance data revealed small pore ios strongly absorb regions due slow photon effect making ideal sterilization water disinfection medium large pore ios benefit additional slow photon effect order photonic band gap enhancing absorption across visible regions medium pore ios suited indoor air treatment water purification large pore ios highest photon flux enhancement enable solar driven photocatalysis outdoor pollutant removal hydrogen production tested designs absorbed photon flux exceeds equivalent planar slabs highlighting advantage photonic structuring sustainable photocatalytic applications
"Hyperbolic conservation laws with source terms govern critical flow phenomena, including turbulence transport and hypersonic chemically reacting flows. Despite their engineering significance, traditional numerical schemes for these systems face strict CFL (≤1.0) constraints, limiting computational efficiency. This study addresses the computational challenges posed by geometric source terms in Euler equations governing compressible nozzle flows. The proposed approach decomposes the governing equations into convective and source components: while convection is resolved via an LTS Godunov scheme, geometric source terms—arising from nozzle area variations (quasi-1D) or cylindrical coordinate transformations (2D axisymmetric)—are discretized using a novel combination of explicit and exact schemes. Numerical validations include: (1) quasi-1D cases under supersonic start/non-start conditions (internal shocks near outlets) at CFL ≤8.0, demonstrating shock-capturing accuracy with 55 % RMS error reduction at CFL = 8.0; (2) 2D hypersonic nozzle flows (Mach 4.0 design), achieving stable simulations at CFL = 5.0 with <1 % exit Mach deviation and 66 % runtime reduction. Results confirm that the proposed LTS framework overcomes CFL ≤1.0 limitations while enhancing efficiency—computational costs and errors decrease monotonically with increasing CFL numbers. This method establishes a generalizable paradigm for stiff, multidimensional Euler systems, balancing robustness and accuracy without sacrificing explicit computation advantages.",hyperbolic conservation laws source terms govern critical flow phenomena including turbulence transport hypersonic chemically reacting flows despite engineering significance traditional numerical schemes systems face strict cfl constraints limiting computational efficiency study addresses computational challenges posed geometric source terms euler equations governing compressible nozzle flows proposed approach decomposes governing equations convective source components convection resolved via lts godunov scheme geometric source terms arising nozzle area variations quasi cylindrical coordinate transformations axisymmetric discretized using novel combination explicit exact schemes numerical validations include quasi cases supersonic start non start conditions internal shocks near outlets cfl demonstrating shock capturing accuracy rms error reduction cfl hypersonic nozzle flows mach design achieving stable simulations cfl exit mach deviation runtime reduction results confirm proposed lts framework overcomes cfl limitations enhancing efficiency computational costs errors decrease monotonically increasing cfl numbers method establishes generalizable paradigm stiff multidimensional euler systems balancing robustness accuracy without sacrificing explicit computation advantages
"In this paper, we obtain the European option price using the Pell-Lucas collocation numerical method. To do this, we present the long-memory version of the hybrid stochastic local volatility model to forecast asset prices based on futures market data. Next, we apply financial market concepts such as the self-financing portfolio and the no-arbitrage theorem to derive a partial differential equation (PDE) for evaluating the option price. The structure of the PDE is complex, so to solve it, we employ a spectral collocation method based on Pell-Lucas polynomials as basis functions. Since the coefficients of the governing PDE are variable, the collocation method offers several advantages over the Galerkin and Tau methods. To implement this approach, we compute the operational matrix of Pell-Lucas polynomials and approximate the first, second, and mixed partial derivatives of the model. By collocating the equation, we obtain a system of algebraic equations that can be solved using traditional numerical methods.",paper obtain european option price using pell lucas collocation numerical method present long memory version hybrid stochastic local volatility model forecast asset prices based futures market data next apply financial market concepts self financing portfolio arbitrage theorem derive partial differential equation pde evaluating option price structure pde complex solve employ spectral collocation method based pell lucas polynomials basis functions since coefficients governing pde variable collocation method offers several advantages galerkin tau methods implement approach compute operational matrix pell lucas polynomials approximate first second mixed partial derivatives model collocating equation obtain system algebraic equations solved using traditional numerical methods
"Time-dependent partial differential equations are a significant class of equations that describe the evolution of various physical phenomena over time. One of the open problems in scientific computing is predicting the behavior of the solution outside the given temporal region. Most traditional numerical methods are applied to a given time–space region and can only accurately approximate the solution of the given region. To address this problem, many deep learning-based methods, basically data-driven and data-free approaches, have been developed to solve these problems. However, most data-driven methods require a large amount of data, which consumes significant computational resources and fails to utilize all the necessary information embedded underlying the partial differential equations (PDEs). Moreover, data-free approaches such as Physics-Informed Neural Networks (PINNs) may not be that ideal in practice, as traditional PINNs, which primarily rely on multilayer perceptrons (MLPs) and convolutional neural networks (CNNs), tend to overlook the crucial temporal dependencies inherent in real-world physical systems. We propose a method denoted by PhysicsSolver that merges the strengths of two approaches: data-free methods that can learn the intrinsic properties of physical systems without using data, and data-driven methods, which are effective at making predictions. Extensive numerical experiments have demonstrated the efficiency and robustness of our proposed method. We provide the code at https://github.com/PhysicsSolver.",time dependent partial differential equations significant class equations describe evolution various physical phenomena time one open problems scientific computing predicting behavior solution outside given temporal region traditional numerical methods applied given time space region accurately approximate solution given region address problem many deep learning based methods basically data driven data free approaches developed solve problems however data driven methods require large amount data consumes significant computational resources fails utilize necessary information embedded underlying partial differential equations pdes moreover data free approaches physics informed neural networks pinns may ideal practice traditional pinns primarily rely multilayer perceptrons mlps convolutional neural networks cnns tend overlook crucial temporal dependencies inherent real world physical systems propose method denoted physicssolver merges strengths two approaches data free methods learn intrinsic properties physical systems without using data data driven methods effective making predictions extensive numerical experiments demonstrated efficiency robustness proposed method provide code https github com physicssolver
"Chikungunya virus (CHIKV) causes chronic arthritis affecting millions across over 100 countries, yet no approved antivirals exist, necessitating drug repurposing strategies. The CHIKV envelope protein E2 plays dual roles in viral entry and inflammatory responses but remains underexplored as a therapeutic target due to limited accessibility of binding cavities in the E2-E1 complex. This study aimed to identify druggable allosteric site in E2 and screen approved sulfonamide drugs as potential repurposing candidates. Computational analysis revealed a novel druggable cavity between domains A and B of E2, distinct from previously reported sites. Virtual screening of sulfonamide drug libraries using consensus scoring identified bosentan as the top candidate, demonstrating superior binding affinity, favorable bioavailability, low toxicity and established antiviral and anti-inflammatory properties. Molecular dynamics simulations validated bosentan's stability within the allosteric site, with real-time hydrogen bond analysis revealing sustained interactions with key residues Phe141, Ile136, Ser143 and Arg104, supported by hydrophobic contacts and sulfur-aromatic interactions. These findings suggest bosentan's potential as a CHIKV inhibitor through allosteric modulation of E2 function. The study demonstrates the viability of targeting allosteric sites in CHIKV E2 and prioritizes bosentan for experimental validation as a repurposed antiviral therapy.",chikungunya virus chikv causes chronic arthritis affecting millions across countries yet approved antivirals exist necessitating drug repurposing strategies chikv envelope protein plays dual roles viral entry inflammatory responses remains underexplored therapeutic target due limited accessibility binding cavities complex study aimed identify druggable allosteric site screen approved sulfonamide drugs potential repurposing candidates computational analysis revealed novel druggable cavity domains distinct previously reported sites virtual screening sulfonamide drug libraries using consensus scoring identified bosentan top candidate demonstrating superior binding affinity favorable bioavailability low toxicity established antiviral anti inflammatory properties molecular dynamics simulations validated bosentan stability within allosteric site real time hydrogen bond analysis revealing sustained interactions key residues phe ile ser arg supported hydrophobic contacts sulfur aromatic interactions findings suggest bosentan potential chikv inhibitor allosteric modulation function study demonstrates viability targeting allosteric sites chikv prioritizes bosentan experimental validation repurposed antiviral therapy
"Accurate prognostic stratification is essential for optimizing postoperative therapeutic strategies in oncology. While deep learning approaches have shown promise for survival prediction through unimodal analyses of histopathological images, transcriptomic profiles, and microbial signatures, their clinical utility remains limited due to fragmented biological insights. In this study, we introduce HMTsurv, a multimodal survival prediction framework that integrates digital histopathology, host transcriptomics, and tumor-associated microbiome features. Utilizing multi-omics datasets from four major malignancies—colorectal, gastric, hepatocellular, and breast cancers—our model exhibited superior prognostic accuracy (c-index: 0.68–0.72) when compared to single-modality benchmarks, as validated through rigorous cross-validation methods. Notably, our model achieved robust risk stratification (log-rank p < 0.001 across all cohorts) as demonstrated by Kaplan-Meier analysis, effectively distinguishing patients into distinct survival trajectories. Systematic examination of multimodal signatures identified 14 pan-cancer survival biomarkers, including MAGE family genes, which were consistently upregulated in high-risk subgroups. Additionally, we elucidated distinct histopathological patterns, dysregulated microbial communities, and altered gene-microbiota co-expression networks that were predictive of adverse outcomes. This study not only establishes a generalizable multimodal architecture for cancer prognosis but also elucidates the intricate interactions among histological, molecular, and ecological determinants of survival, providing a clinically actionable framework for precision oncology.",accurate prognostic stratification essential optimizing postoperative therapeutic strategies oncology deep learning approaches shown promise survival prediction unimodal analyses histopathological images transcriptomic profiles microbial signatures clinical utility remains limited due fragmented biological insights study introduce hmtsurv multimodal survival prediction framework integrates digital histopathology host transcriptomics tumor associated microbiome features utilizing multi omics datasets four major malignancies colorectal gastric hepatocellular breast cancers model exhibited superior prognostic accuracy index compared single modality benchmarks validated rigorous cross validation methods notably model achieved robust risk stratification log rank across cohorts demonstrated kaplan meier analysis effectively distinguishing patients distinct survival trajectories systematic examination multimodal signatures identified pan cancer survival biomarkers including mage family genes consistently upregulated high risk subgroups additionally elucidated distinct histopathological patterns dysregulated microbial communities altered gene microbiota expression networks predictive adverse outcomes study establishes generalizable multimodal architecture cancer prognosis also elucidates intricate interactions among histological molecular ecological determinants survival providing clinically actionable framework precision oncology
"This study aims to enhance early breast cancer prediction accuracy by utilizing machine learning classifiers and feature selection techniques. The Wisconsin Diagnostic Breast Cancer (WDBC) dataset was used to train and evaluate three popular machine learning classifiers: Support Vector Machine (SVM), Random Forest (RF), and k-Nearest Neighbors (k-NN). Feature selection methods were applied to optimize model performance, including Recursive Feature Elimination (RFE) and Mutual Information. Cross-validation and hyperparameter tuning were conducted to ensure the robustness and reliability of the models. The results showed that the SVM classifier achieved the highest performance with an accuracy of 98 %, compared to 95.8 % for RF and 96.2 % for k-NN. The SVM model demonstrated a precision of 0.98 and a recall of 0.95 for malignant cases. Feature selection revealed that mean radius, texture, and area were the most influential features, and SHapley Additive exPlanations (SHAP) analysis confirmed their clinical relevance in breast cancer diagnosis. A tumor-immune dynamic model also indicated that treatment efficacy (γ = 0.0500/day) was a critical parameter for tumor control. Statistical significance tests (p < 0.05) confirmed that the SVM classifier outperformed the other models. This study highlights the potential of combining machine learning with clinical insights to develop an effective framework for breast cancer prediction, offering high diagnostic accuracy and biological interpretability.",study aims enhance early breast cancer prediction accuracy utilizing machine learning classifiers feature selection techniques wisconsin diagnostic breast cancer wdbc dataset used train evaluate three popular machine learning classifiers support vector machine svm random forest nearest neighbors feature selection methods applied optimize model performance including recursive feature elimination rfe mutual information cross validation hyperparameter tuning conducted ensure robustness reliability models results showed svm classifier achieved highest performance accuracy compared svm model demonstrated precision recall malignant cases feature selection revealed mean radius texture area influential features shapley additive explanations shap analysis confirmed clinical relevance breast cancer diagnosis tumor immune dynamic model also indicated treatment efficacy day critical parameter tumor control statistical significance tests confirmed svm classifier outperformed models study highlights potential combining machine learning clinical insights develop effective framework breast cancer prediction offering high diagnostic accuracy biological interpretability
"Lycium ruthenicum Murr. (LR) has attracted significant attention for its potential in combating colorectal carcinoma (CRC). However, the identification of key bioactive metabolites and the elucidation of their molecular mechanisms remain elusive. The study aimed to identify the bioactive metabolites profile in vivo and elucidate the mechanisms underlying their anti-CRC effects. Initially, a UHPLC-Q-Exactive-MS technique was integrated with a feature-based molecular networking approach to annotate 26 prototype metabolites in rat serum. Subsequently, a preliminary evaluation of the anti-CRC effect of the serum was conducted. Thereafter, network pharmacology, molecular docking, and molecular dynamics were employed to investigate the potential mechanisms and to screen for bioactive metabolites. Following this, the signal pathways of the primary bioactive metabolites were validated through cell experiments. Our findings suggested that the candidate targets of those prototypes’ metabolites were implicated in the PI3K/AKT pathways. Furthermore, cell experiments demonstrated that LR-containing serum could inhibit the proliferation, apoptosis, and migration of HCT-116 cells. Molecular docking and dynamics analyses indicated significant binding affinity and stability between the core targets and bioactive metabolites, such as luteolin and esculin. The bioactive metabolites of LR are potential material bases for anti-CRC via the PI3K/AKT signalling pathway, offering new insights for the functional development and utilisation of LR.",lycium ruthenicum murr attracted significant attention potential combating colorectal carcinoma crc however identification key bioactive metabolites elucidation molecular mechanisms remain elusive study aimed identify bioactive metabolites profile vivo elucidate mechanisms underlying anti crc effects initially uhplc exactive technique integrated feature based molecular networking approach annotate prototype metabolites rat serum subsequently preliminary evaluation anti crc effect serum conducted thereafter network pharmacology molecular docking molecular dynamics employed investigate potential mechanisms screen bioactive metabolites following signal pathways primary bioactive metabolites validated cell experiments findings suggested candidate targets prototypes metabolites implicated akt pathways furthermore cell experiments demonstrated containing serum could inhibit proliferation apoptosis migration hct cells molecular docking dynamics analyses indicated significant binding affinity stability core targets bioactive metabolites luteolin esculin bioactive metabolites potential material bases anti crc via akt signalling pathway offering new insights functional development utilisation
"The TAM (Tyro3, AXL, and Mer) receptor tyrosine kinases play vital roles in immunity and various complex diseases, particularly cancer. In this family, AXL has stood out as a promising target for therapeutic development due to its significant role in cancer progression and resistance to therapies. AXL and its ligand GAS6 promote metastasis and therapeutic resistance in several human cancers. Dysregulated AXL signaling is implicated in a spectrum of diseases, notably metastatic cancer. Elevated AXL expression correlates with drug resistance and poor survival in multiple cancers such as lung, breast, pancreatic, ovarian, colon, and melanoma. In this study, we conducted a virtual screening of phytochemicals sourced from the IMPPAT 2.0 database of Indian medicinal plants to identify potential AXL inhibitors. Preliminary screening was performed based on the physicochemical properties of the phytochemicals, followed by their interaction studies in molecular docking with AXL. Of 17,908 phytochemicals initially screened, 11,676 drug-like compounds complied with Lipinski’s rule-of-five and were subjected to molecular docking and downstream analyses. Subsequent evaluation included ADMET analysis, PAINS examination, and PASS analysis to identify potent hits against AXL. From this screening, Neogitogenin and Solaspigenin emerged as promising candidates demonstrating favorable drug-like properties and significant binding potential with the AXL binding pocket. Neogitogenin and Solaspigenin showed binding affinities of −10.1 to −10.8 kcal/mol, favorable ADMET profiles, and with RMSD values ranging between 0.32 and 0.38 nm, indicating stable binding. Furthermore, molecular dynamics simulation over 200 ns revealed stable protein-ligand complexes with some minor conformational fluctuations. This study suggests that, after further experimentation, modulating AXL with natural compounds holds promise for combating human malignancies, potentially overcoming limitations of existing synthetic inhibitors such as R428.",tam tyro axl mer receptor tyrosine kinases play vital roles immunity various complex diseases particularly cancer family axl stood promising target therapeutic development due significant role cancer progression resistance therapies axl ligand gas promote metastasis therapeutic resistance several human cancers dysregulated axl signaling implicated spectrum diseases notably metastatic cancer elevated axl expression correlates drug resistance poor survival multiple cancers lung breast pancreatic ovarian colon melanoma study conducted virtual screening phytochemicals sourced imppat database indian medicinal plants identify potential axl inhibitors preliminary screening performed based physicochemical properties phytochemicals followed interaction studies molecular docking axl phytochemicals initially screened drug like compounds complied lipinski rule five subjected molecular docking downstream analyses subsequent evaluation included admet analysis pains examination pass analysis identify potent hits axl screening neogitogenin solaspigenin emerged promising candidates demonstrating favorable drug like properties significant binding potential axl binding pocket neogitogenin solaspigenin showed binding affinities kcal mol favorable admet profiles rmsd values ranging indicating stable binding furthermore molecular dynamics simulation revealed stable protein ligand complexes minor conformational fluctuations study suggests experimentation modulating axl natural compounds holds promise combating human malignancies potentially overcoming limitations existing synthetic inhibitors
"Sphingosine kinase (SphK1) is acrucial enzyme that aids in the processing of sphingolipids by adding a phosphate group to sphingosine, converting it into sphingosine-1-phosphate. A recent study has suggested that dysregulation of SphK1 is linked to tumor progression and metastasis in lung and bladder cancers,making SphK1 a promising therapeutic target for these diseases. In this study, we employedmachine learning-based virtual screening along with structure-based drug design to identify potential SphK1 inhibitors with diverse chemical scaffolds. A total of 16 machine learning models were generated using molecular fingerprints, and the most effective models were employed to conductvirtual screening of the Maybridge library. The screened compounds were then subjected to molecular docking to determine a suitable docked pose against the SphK1 protein. Upon visualization of the best docked compounds, we found that six compounds exhibited strong interactions with the SphK1 protein compared to the control (SQS). To further support our findings, we conducted 100 ns long molecular dynamics (MD) simulations of all six compounds to analyzeconformational changes and stability. Two compounds (SCR00139 and SCR00133) demonstratedpromising stability and fit well within the binding pocket of the SphK1 protein. Furthermore, MM-PBSA and MM-GBSA studies were carried out on these two compounds, providing favorable relative binding estimations. This study introduces an integrated pipeline of machine learning-based virtual screening for the identification of new scaffolds targeting cancer progression. However, in vitro evaluations are necessary to assess the efficacy of these compounds.",sphingosine kinase sphk acrucial enzyme aids processing sphingolipids adding phosphate group sphingosine converting sphingosine phosphate recent study suggested dysregulation sphk linked tumor progression metastasis lung bladder cancers making sphk promising therapeutic target diseases study employedmachine learning based virtual screening along structure based drug design identify potential sphk inhibitors diverse chemical scaffolds total machine learning models generated using molecular fingerprints effective models employed conductvirtual screening maybridge library screened compounds subjected molecular docking determine suitable docked pose sphk protein upon visualization best docked compounds found six compounds exhibited strong interactions sphk protein compared control sqs support findings conducted long molecular dynamics simulations six compounds analyzeconformational changes stability two compounds scr scr demonstratedpromising stability fit well within binding pocket sphk protein furthermore pbsa gbsa studies carried two compounds providing favorable relative binding estimations study introduces integrated pipeline machine learning based virtual screening identification new scaffolds targeting cancer progression however vitro evaluations necessary assess efficacy compounds
"This study details the synthesis, characterization, and in silico studies of two new molecules, 4-(3-(2-methyl-5-nitro-1H-imidazol-1-yl)propyl)phthalonitrile (5) and 1-(4-chloro-2-nitrophenoxy)-1-(1H-imidazol-1-yl)-3,3-dimethylbutan-2-one (7), which are derivatives of metronidazole and climbazole. The structural description of nitro-climbazole (7) was confirmed by single crystal X-ray analysis, which revealed the monoclinic crystal system with the space group P21/n. Complementary to the experimental findings, computational studies were carried out using Density Functional Theory (DFT) with Gaussian 09 software. These calculations, including geometry optimization and frequency analysis, were performed at the mPW1PW91/6-311G(d,p) level for molecule 5 and at the wb97xd/6-311G(d,p) level for molecule 7. These analyses yielded important data regarding the stability and electronic structures of the molecules. Hirshfeld surface analysis based on CIF data obtained from the crystal structure 7 revealed intermolecular H∙∙∙H, O∙∙∙H, and Cl∙∙∙H contacts, demonstrating the contribution of these interactions to the stability of the crystal structure. Energy framework analyses elucidated the internal interactions of the structure by visualizing the cohesive forces within the crystal lattice. Frontier Molecular Orbital (FMO) analysis of molecules 5 and 7 revealed the distribution of their HOMO and LUMO orbitals, providing insights into their electronic behavior and reactivity potential. Furthermore, surface Electrostatic Potential (ESP) mapping identified electron-rich and electron-poor regions within the molecules, providing complementary information regarding potential biological interaction sites. Furthermore, their antibacterial and antifungal activities were investigated using in silico methods. This study, which combines experimental and theoretical approaches, comprehensively reveals the structural and electronic properties of the synthesized molecules.",study details synthesis characterization silico studies two new molecules methyl nitro imidazol propyl phthalonitrile chloro nitrophenoxy imidazol dimethylbutan one derivatives metronidazole climbazole structural description nitro climbazole confirmed single crystal ray analysis revealed monoclinic crystal system space group complementary experimental findings computational studies carried using density functional theory dft gaussian software calculations including geometry optimization frequency analysis performed mpw level molecule level molecule analyses yielded important data regarding stability electronic structures molecules hirshfeld surface analysis based cif data obtained crystal structure revealed intermolecular contacts demonstrating contribution interactions stability crystal structure energy framework analyses elucidated internal interactions structure visualizing cohesive forces within crystal lattice frontier molecular orbital fmo analysis molecules revealed distribution homo lumo orbitals providing insights electronic behavior reactivity potential furthermore surface electrostatic potential esp mapping identified electron rich electron poor regions within molecules providing complementary information regarding potential biological interaction sites furthermore antibacterial antifungal activities investigated using silico methods study combines experimental theoretical approaches comprehensively reveals structural electronic properties synthesized molecules
"Ovarian cancer is a major health concern for women, contributing to substantial mortality and morbidity. Timely identification of ovarian cancer is crucial for enhancing patient health and survival rates. Current diagnostic practices involve the manual analysis of various clinical biomarkers to detect ovarian cancer. However, this approach can be subjective, time-consuming, and dependent on the expertise of the medical professional. To optimize workflow efficiency and improve diagnosis accuracy, we develop an automated deep learning model, called EA-ResMLP, which integrates a residual multilayer perceptron with squeeze-and-excitation attention block and explainable artificial intelligence. The integration of residual connections and attention mechanisms contributes to improved diagnostic accuracy by enabling deeper feature learning and emphasizing the most informative features through adaptive recalibration. The experimental results demonstrated that proposed method achieved an accuracy of 92.05%, indicating a 7.98% improvement over the conventional multilayer perceptron. Furthermore, the predictions of the EA-ResMLP model are analyzed using explainable artificial intelligence techniques such as local interpretable model-agnostic explanations, which generate feature contribution charts to highlight the impact of each input feature on the prediction. By integrating model predictions with feature contribution charts, the proposed model provides an explainable framework for ovarian cancer detection.",ovarian cancer major health concern women contributing substantial mortality morbidity timely identification ovarian cancer crucial enhancing patient health survival rates current diagnostic practices involve manual analysis various clinical biomarkers detect ovarian cancer however approach subjective time consuming dependent expertise medical professional optimize workflow efficiency improve diagnosis accuracy develop automated deep learning model called resmlp integrates residual multilayer perceptron squeeze excitation attention block explainable artificial intelligence integration residual connections attention mechanisms contributes improved diagnostic accuracy enabling deeper feature learning emphasizing informative features adaptive recalibration experimental results demonstrated proposed method achieved accuracy indicating improvement conventional multilayer perceptron furthermore predictions resmlp model analyzed using explainable artificial intelligence techniques local interpretable model agnostic explanations generate feature contribution charts highlight impact input feature prediction integrating model predictions feature contribution charts proposed model provides explainable framework ovarian cancer detection
"This review meticulously examines the development, design, and pharmacological assessment of both well known antiviral and antihypertensive medications all time employing new chemical techniques and structure-based drug design to design and synthesize vital therapeutic entities such as aliskiren (renin inhibitor), captopril (a2-ACE-Inhibitor), dorzolamide (inhibitor of carbonic anhydrase) the review demonstrates initial steps regarding the significance of stereoselective synthesis, metal chelating pharmacophores, and rational molecular properties. More importantly, protease inhibitors (i.e., saquinavir, ritonavir, indinavir, amprenavir, etc.) and more contemporary agents (i.e., oseltamivir, nirmatrelvir/ritonavir (Paxlovid), etc.) were identified in the review and form a basis of advancement in antiviral therapy. Using the above-mentioned computational applications (i.e., molecular dockings, structure–activity relationships (SAR)), quantitative structure-activity relationships (QSAR modeling), and ADMET profile information, selectivity, safety, and therapeutic reliability of compounds toward antiviral activity and antihypertensive activity have improved further as outcomes of drug discovery research. Through the combinatorial application of computational drug design and experimental chemistry, researchers have significantly optimized the drug discovery process, minimized off-target effects, and expedited the pathway to develop therapeutically useful medications. In conclusion, this review highlights the transformative power of interdisciplinary approaches, including structure-based design, computational modeling, and the emerging approaches of drug repurposing and virtual screening. This hybrid approach provides a greater opportunity to improve drug therapy for cardiovascular disease and viral infections, and represents the rewards of collaboration.",review meticulously examines development design pharmacological assessment well known antiviral antihypertensive medications time employing new chemical techniques structure based drug design design synthesize vital therapeutic entities aliskiren renin inhibitor captopril ace inhibitor dorzolamide inhibitor carbonic anhydrase review demonstrates initial steps regarding significance stereoselective synthesis metal chelating pharmacophores rational molecular properties importantly protease inhibitors saquinavir ritonavir indinavir amprenavir etc contemporary agents oseltamivir nirmatrelvir ritonavir paxlovid etc identified review form basis advancement antiviral therapy using mentioned computational applications molecular dockings structure activity relationships sar quantitative structure activity relationships qsar modeling admet profile information selectivity safety therapeutic reliability compounds toward antiviral activity antihypertensive activity improved outcomes drug discovery research combinatorial application computational drug design experimental chemistry researchers significantly optimized drug discovery process minimized target effects expedited pathway develop therapeutically useful medications conclusion review highlights transformative power interdisciplinary approaches including structure based design computational modeling emerging approaches drug repurposing virtual screening hybrid approach provides greater opportunity improve drug therapy cardiovascular disease viral infections represents rewards collaboration
"Three novel compounds, each featuring a tetra-dentate ligand known as 1-((2-oxoindolin-3-ylidene)amino)-2-((2-oxoindolin-3-ylidene)amino)anthracene-9,10-dione (BIA), have been successfully synthesized. These molecules exhibit the unique characteristic of forming complexes with Cu(II), Ru(III), and VO(II) metal ions, resulting in distinct metal-organic structures.Structural characterization was performed using elemental analysis, magnetic properties measurement, FT-IR spectroscopy, and electronic spectroscopy. Moreover, the stoichiometry in solution was determined through both continuous variation and molar ratio analysis. These analyses have shown that the copper and ruthenium complexes exhibit an octahedral geometric configuration. Conversely, the vanadyl (VO) complex demonstrates a distinct square pyramidal structure.Density functional theory (DFT) computations were employed to confirm the geometrical configurations of the prepared complexes. The synthesized BIA ligand and its corresponding metal complexes were assessed for their in vitro antimicrobial. The results indicated that the RuBIA complex emerged as the most efficacious agent against both bacterial and fungal growth, outperforming established medications like Ofloxacin and Fluconazole as standard drugs with sequenceBIA < VOBIA < CuBIA <RuBIAcomplex.Additionally, the study evaluated the in vitro cytotoxicity of the synthesized compounds against Hep-G2, MCF-7, and HCT-116 cancer cell lines. The results suggested that the RuBIA complex had the highest potency (IC50 =3.42–6.45 µg/µl), followed by CuBIA(IC50 =4.42–7.85 µg/µl), and VOBIA(IC50 =5.72–8.35 µg/µl), indicating their potential as promising anticancer agents. The DPPH radical scavenging activity was also assessed, and all complexes displayed greater efficacy than Ascorbic acid. Investigations employing molecular docking methodologies were undertaken to discern the interaction mechanisms of the aforementioned complexes. The findings revealed that the incorporation of metal ions substantially bolstered the molecular affinities, with the sequence of binding potency as follows: RuBIA> CuBIA > VOBIA complex>BIA ligand.",three novel compounds featuring tetra dentate ligand known oxoindolin ylidene amino oxoindolin ylidene amino anthracene dione bia successfully synthesized molecules exhibit unique characteristic forming complexes iii metal ions resulting distinct metal organic structures structural characterization performed using elemental analysis magnetic properties measurement spectroscopy electronic spectroscopy moreover stoichiometry solution determined continuous variation molar ratio analysis analyses shown copper ruthenium complexes exhibit octahedral geometric configuration conversely vanadyl complex demonstrates distinct square pyramidal structure density functional theory dft computations employed confirm geometrical configurations prepared complexes synthesized bia ligand corresponding metal complexes assessed vitro antimicrobial results indicated rubia complex emerged efficacious agent bacterial fungal growth outperforming established medications like ofloxacin fluconazole standard drugs sequencebia cubia vobia complex bia ligand
"Multiple studies have linked aging to a result of the inflammatory response. Thus, there is a recognized need for cosmeceuticals that modulate inflammation pathways to prevent and treat aging. In this sense, four bioactive compounds were selected for their documented anti-inflammatory/antioxidant properties. Therefore, this study aimed to evaluate whether the bioactive compounds astaxanthin, curcumin, quercetin, and resveratrol are effective in treating the effects of skin aging, using in silico analyses. Protein-protein interaction networks (PPINs) related to skin aging and the bioactive compounds astaxanthin, curcumin, quercetin, and resveratrol were generated using the Cytoscape plug-in to analyze the functional enrichment of recovered proteins. From these main networks, clusters and bottleneck networks were generated. Initially, 5 main PPINs were generated. From the clusters recovered from the main networks, 3 were selected from the general network and 11 from the specific networks. Through functional enrichment of the clusters, the biological process of response to oxidative stress was identified. Blood and blood-forming tissue, vascular, and immune system abnormality phenotypes were also observed, along with an increase in inflammatory response. Additionally, Reactome pathways related to interleukin signaling and detoxification of reactive oxygen species were noted. Finally, the key genes for each network were identified from the bottleneck networks: IL-6 (general and astaxanthin), TAB1 (curcumin), TNF-α (quercetin), and TP53 (resveratrol). Based on this research, the analyzed bioactive compounds suggest potential efficacy to be included in cosmetic products, as they are capable of reducing excessive oxidative stress and inflammatory processes, consequently preventing cellular aging.",multiple studies linked aging result inflammatory response thus recognized need cosmeceuticals modulate inflammation pathways prevent treat aging sense four bioactive compounds selected documented anti inflammatory antioxidant properties therefore study aimed evaluate whether bioactive compounds astaxanthin curcumin quercetin resveratrol effective treating effects skin aging using silico analyses protein protein interaction networks ppins related skin aging bioactive compounds astaxanthin curcumin quercetin resveratrol generated using cytoscape plug analyze functional enrichment recovered proteins main networks clusters bottleneck networks generated initially main ppins generated clusters recovered main networks selected general network specific networks functional enrichment clusters biological process response oxidative stress identified blood blood forming tissue vascular immune system abnormality phenotypes also observed along increase inflammatory response additionally reactome pathways related interleukin signaling detoxification reactive oxygen species noted finally key genes network identified bottleneck networks general astaxanthin tab curcumin tnf quercetin resveratrol based research analyzed bioactive compounds suggest potential efficacy included cosmetic products capable reducing excessive oxidative stress inflammatory processes consequently preventing cellular aging
"Modeling cellular perturbation responses is essential for understanding disease mechanisms and developing therapeutic strategies. However, current computational approaches face significant challenges when learning from sparse, noisy, high-dimensional, and size-limited scRNA-seq data. These limitations hinder their ability to capture complex cellular heterogeneity and perturbation dynamics, leading to poor generalization across diverse biological contexts. Recent advances in single-cell foundation models (scFMs) offer a promising solution by providing biologically meaningful representations. Inspired by the success of REPresentation Alignment (REPA) in generative diffusion models, we propose scREPA, a novel framework for single-cell perturbation prediction that aligns the internal representations of a variational autoencoder (VAE)-based model with high-quality external representations from pretrained scFMs. Specifically, scREPA aligns VAE latent embeddings from noisy gene expression profiles with biologically meaningful embeddings from scFMs. We also propose Cycle-Consistent Representation Alignment by aligning the re-encoded embeddings of VAE-generated gene expression profiles with both original scFM representations and initial VAE embeddings, enforcing dual consistency and further improving representation quality. During inference, scREPA applies optimal transport to align the distributions of unpaired control and perturbed data, enabling robust prediction of cellular responses by minimizing mismatch. Experiments on diverse datasets show that scREPA outperforms existing methods in predicting both top differentially expressed genes and whole-transcriptome responses while generalizing well to unseen conditions, cross-study settings, and maintaining strong performance under noisy or limited data.",modeling cellular perturbation responses essential understanding disease mechanisms developing therapeutic strategies however current computational approaches face significant challenges learning sparse noisy high dimensional size limited scrna seq data limitations hinder ability capture complex cellular heterogeneity perturbation dynamics leading poor generalization across diverse biological contexts recent advances single cell foundation models scfms offer promising solution providing biologically meaningful representations inspired success representation alignment repa generative diffusion models propose screpa novel framework single cell perturbation prediction aligns internal representations variational autoencoder vae based model high quality external representations pretrained scfms specifically screpa aligns vae latent embeddings noisy gene expression profiles biologically meaningful embeddings scfms also propose cycle consistent representation alignment aligning encoded embeddings vae generated gene expression profiles original scfm representations initial vae embeddings enforcing dual consistency improving representation quality inference screpa applies optimal transport align distributions unpaired control perturbed data enabling robust prediction cellular responses minimizing mismatch experiments diverse datasets show screpa outperforms existing methods predicting top differentially expressed genes whole transcriptome responses generalizing well unseen conditions cross study settings maintaining strong performance noisy limited data
"This study introduces an optimal fourth-order iterative method derived by combining two established methods, resulting in enhanced convergence when solving nonlinear equations. Through rigorous convergence analysis using both Taylor expansion and the Banach space framework, the fourth-order optimality condition is verified. We demonstrate the superior efficiency and stability of this new method compared to traditional alternatives. Numerical experiments confirm its effectiveness, showing a reduction in the average number of iterations and computational time. Visual analysis with polynomiographs confirms the method's robustness, focusing on convergence area index, iteration count, computational time, fractal dimension, and Wada measure of basins. These findings underscore the potential of this optimal method for tackling complex nonlinear problems in various scientific and engineering fields.",study introduces optimal fourth order iterative method derived combining two established methods resulting enhanced convergence solving nonlinear equations rigorous convergence analysis using taylor expansion banach space framework fourth order optimality condition verified demonstrate superior efficiency stability new method compared traditional alternatives numerical experiments confirm effectiveness showing reduction average number iterations computational time visual analysis polynomiographs confirms method robustness focusing convergence area index iteration count computational time fractal dimension wada measure basins findings underscore potential optimal method tackling complex nonlinear problems various scientific engineering fields
"Hepatocellular carcinoma (HCC) remains one of the leading causes of cancer-related deaths globally, with limited treatment options and a poor prognosis. The identification of reliable prognostic biomarkers and novel therapeutic candidates is essential for improving patient outcomes. This study employed an integrative bioinformatics and molecular simulation approach to uncover key gene signatures and explore potential plant-based therapeutics for HCC. Gene expression profiles from the GEO dataset GSE121248 were analyzed using R-based statistical tools to identify differentially expressed genes (DEGs). Key hub genes were selected through protein–protein interaction network analysis and functional enrichment. Three hub genesCOL1A1, NQO1, and FOS—were identified, and their expression was validated using the TCGA, GEPIA, and HPA databases. Notably, COL1A1 and NQO1 were upregulated in tumor tissues and associated with poor survival outcomes, while FOS was downregulated. To identify therapeutic candidates, bioactive compounds from Astragalus membranaceus were screened using ADMET profiling. Selected compounds were docked with the hub proteins, and molecular dynamics simulations, MM/GBSA binding energy calculations, and quantum chemical descriptors were employed to evaluate stability and reactivity. Among the candidates, isorhamnetin demonstrated strong and stable binding to all three hub targets, suggesting its potential as a multi-target inhibitor for HCC therapy. These findings provide new insights into HCC pathogenesis and propose isorhamnetin as a promising natural compound for further experimental validation.",hepatocellular carcinoma hcc remains one leading causes cancer related deaths globally limited treatment options poor prognosis identification reliable prognostic biomarkers novel therapeutic candidates essential improving patient outcomes study employed integrative bioinformatics molecular simulation approach uncover key gene signatures explore potential plant based therapeutics hcc gene expression profiles geo dataset gse analyzed using based statistical tools identify differentially expressed genes degs key hub genes selected protein protein interaction network analysis functional enrichment three hub genescol nqo fos identified expression validated using tcga gepia hpa databases notably col nqo upregulated tumor tissues associated poor survival outcomes fos downregulated identify therapeutic candidates bioactive compounds astragalus membranaceus screened using admet profiling selected compounds docked hub proteins molecular dynamics simulations gbsa binding energy calculations quantum chemical descriptors employed evaluate stability reactivity among candidates isorhamnetin demonstrated strong stable binding three hub targets suggesting potential multi target inhibitor hcc therapy findings provide new insights hcc pathogenesis propose isorhamnetin promising natural compound experimental validation
"The development of efficient separation methods is essential for the production of fine chemicals and materials. Among them, the aqueous two-phase extraction (ATPE) allows for the isolation of single-walled carbon nanotubes (SWCNTs) of specific structures and other substances. However, this easy-to-use method, in which an analyte is partitioned between two phases, still demands a better understanding of its mechanism to make its application more effective. Herein, we demonstrate how various biphasic systems can be formed according to the nature of the phase-forming components. Moreover, by employing polyethylene-block-poly(ethylene glycol) (PEPEG), previously unrecognized in this context, we reveal the versatility of nonionic polymers for ATPE, which can successfully act as phase-forming compounds, partitioning modulators, and dispersing agents. Interestingly, as proven by experiments and modelling, PEPEG exhibited chirality-sensitive preference toward SWCNTs, which can significantly facilitate the purification of SWCNTs using various approaches. Capitalizing on this finding, we report how the extraction environment may be tailored to promote the isolation of (8,3) SWCNTs and other chirality-enriched SWCNT fractions. The relationships noted, based on the examination of a model material (SWCNTs), provide substantial insight into the elusive mechanism of the ATPE purification approach, widely employed across a range of analytes, from cell organelles to nanostructures.",development efficient separation methods essential production fine chemicals materials among aqueous two phase extraction atpe allows isolation single walled carbon nanotubes swcnts specific structures substances however easy use method analyte partitioned two phases still demands better understanding mechanism make application effective herein demonstrate various biphasic systems formed according nature phase forming components moreover employing polyethylene block poly ethylene glycol pepeg previously unrecognized context reveal versatility nonionic polymers atpe successfully act phase forming compounds partitioning modulators dispersing agents interestingly proven experiments modelling pepeg exhibited chirality sensitive preference toward swcnts significantly facilitate purification swcnts using various approaches capitalizing finding report extraction environment may tailored promote isolation swcnts chirality enriched swcnt fractions relationships noted based examination model material swcnts provide substantial insight elusive mechanism atpe purification approach widely employed across range analytes cell organelles nanostructures
"This work considers the numerical computation of ground states of rotating Bose–Einstein condensates (BECs) which can exhibit a multiscale lattice of quantized vortices. This problem involves the minimization of an energy functional on a Riemannian manifold. For this we apply the framework of nonlinear conjugate gradient methods in combination with the paradigm of Sobolev gradients to investigate different metrics. Here we build on previous work that proposed to enhance the convergence of regular Riemannian gradients methods by an adaptively changing metric that is based on the current energy. In this work, we extend this approach to the branch of Riemannian conjugate gradient (CG) methods and investigate the arising schemes numerically. Special attention is given to the selection of the momentum parameter in search direction and how this affects the performance of the resulting schemes. As known from similar applications, we find that the choice of the momentum parameter plays a critical role, with certain parameters reducing the number of iterations required to achieve a specified tolerance by a significant factor. Besides the influence of the momentum parameters, we also investigate how the methods with adaptive metric compare to the corresponding realizations with a standard H 0 1 -metric. As one of our main findings, the results of the numerical experiments show that the Riemannian CG method with the proposed adaptive metric along with a Polak–Ribiére or Hestenes–Stiefel-type momentum parameter show the best performance and highest robustness compared to the other CG methods that were part of our numerical study.",work considers numerical computation ground states rotating bose einstein condensates becs exhibit multiscale lattice quantized vortices problem involves minimization energy functional riemannian manifold apply framework nonlinear conjugate gradient methods combination paradigm sobolev gradients investigate different metrics build previous work proposed enhance convergence regular riemannian gradients methods adaptively changing metric based current energy work extend approach branch riemannian conjugate gradient methods investigate arising schemes numerically special attention given selection momentum parameter search direction affects performance resulting schemes known similar applications find choice momentum parameter plays critical role certain parameters reducing number iterations required achieve specified tolerance significant factor besides influence momentum parameters also investigate methods adaptive metric compare corresponding realizations standard metric one main findings results numerical experiments show riemannian method proposed adaptive metric along polak ribiére hestenes stiefel type momentum parameter show best performance highest robustness compared methods part numerical study
"Abdominal aortic aneurysm (AAA) is a progressive and life-threatening vascular disorder characterized by abnormal dilation of the abdominal aorta and a high risk of rupture. Current pharmacological interventions remain limited in efficacy, highlighting the need for alternative therapeutic strategies. Si-Miao-Yong-An Decoction (SMYAD), a classical formula in traditional Chinese medicine, has demonstrated anti-inflammatory and vascular-protective effects, yet its underlying mechanisms in AAA treatment remain unclear. This study employed an integrative approach combining network pharmacology, machine learning, and molecular modeling to elucidate the pharmacological basis of SMYAD against AAA. A total of 106 bioactive compounds and 235 putative targets were identified from the Traditional Chinese Medicine Systems Pharmacology Database and Analysis Platform database. These were cross-referenced with disease-associated and differentially expressed genes from GEO datasets, identifying 15 targets potentially involved in AAA pathogenesis. Functional enrichment analyses revealed their involvement in the interleukin-17 and tumor necrosis factor signaling pathways. Integrated PPI network analysis and 3 machine learning algorithms jointly identified 6 hub genes (IL6, PTGS2, IL1B, FOS, MAOA, and COL1A1) as central to AAA pathology. Gene expression profiling and ROC curve analysis further supported the diagnostic relevance of these targets. Five key compounds—quercetin, luteolin, kaempferol, isorhamnetin, and stigmasterol—exhibited strong binding affinities with the identified hub targets. Molecular docking and dynamics simulations confirmed stable interactions between the selected compounds and their targets. Overall, this study provides mechanistic insights into the multi-target actions of SMYAD in AAA and offers theoretical support for its potential clinical application.",abdominal aortic aneurysm aaa progressive life threatening vascular disorder characterized abnormal dilation abdominal aorta high risk rupture current pharmacological interventions remain limited efficacy highlighting need alternative therapeutic strategies miao yong decoction smyad classical formula traditional chinese medicine demonstrated anti inflammatory vascular protective effects yet underlying mechanisms aaa treatment remain unclear study employed integrative approach combining network pharmacology machine learning molecular modeling elucidate pharmacological basis smyad aaa total bioactive compounds putative targets identified traditional chinese medicine systems pharmacology database analysis platform database cross referenced disease associated differentially expressed genes geo datasets identifying targets potentially involved aaa pathogenesis functional enrichment analyses revealed involvement interleukin tumor necrosis factor signaling pathways integrated ppi network analysis machine learning algorithms jointly identified hub genes ptgs fos maoa col central aaa pathology gene expression profiling roc curve analysis supported diagnostic relevance targets five key compounds quercetin luteolin kaempferol isorhamnetin stigmasterol exhibited strong binding affinities identified hub targets molecular docking dynamics simulations confirmed stable interactions selected compounds targets overall study provides mechanistic insights multi target actions smyad aaa offers theoretical support potential clinical application
"This paper presents an innovative iterative two-stage algorithm designed for estimating threshold boundary regression (TBR) models. By transforming the non-differentiable least-squares (LS) problem inherent in fitting TBR models into an optimization framework, our algorithm combines the optimization of a weighted classification error function for the threshold model with obtaining LS estimators for regression models. To improve the efficiency and flexibility of TBR model estimation, we integrate the weighted support vector machine (WSVM) as a surrogate method for solving the weighted classification problem. The TBR-WSVM algorithm offers several key advantages over recently developed methods: it eliminates pre-specification requirements for threshold parameters, accommodates flexible estimation of nonlinear threshold boundaries, and streamlines the estimation process. We conducted several simulation studies to illustrate the finite-sample performance of TBR-WSVM. Finally, we demonstrate the practical applicability of the TBR model through a real data analysis.",paper presents innovative iterative two stage algorithm designed estimating threshold boundary regression tbr models transforming non differentiable least squares problem inherent fitting tbr models optimization framework algorithm combines optimization weighted classification error function threshold model obtaining estimators regression models improve efficiency flexibility tbr model estimation integrate weighted support vector machine wsvm surrogate method solving weighted classification problem tbr wsvm algorithm offers several key advantages recently developed methods eliminates pre specification requirements threshold parameters accommodates flexible estimation nonlinear threshold boundaries streamlines estimation process conducted several simulation studies illustrate finite sample performance tbr wsvm finally demonstrate practical applicability tbr model real data analysis
"The traditional k-means clustering algorithm is sensitive to initial centroids selection, prone to local optima, and inefficient on large-scale datasets. To address these limitations, this article proposes an Adaptive K-Means algorithm (AK-means) that integrates a density-based initialization strategy and an Improved Quadratic Interpolation Optimization (IQIO) method. Specifically, a novel density-aware initialization approach is introduced to generate initial centroids by evaluating data point sparsity and distance distribution, effectively mitigating centroid deviation, and an enhanced IQIO mechanism. In particular, IQIO is improved through two key innovations, including a dynamic neighborhood search mechanism based on the absolute slope of interpolated points and the position of known optima, which significantly reduces computational complexity compared to traditional methods; and the incorporation of gradient descent to refine local exploitation, accelerating convergence to high-accuracy solutions. To verify the effectiveness of IQIO, experiments on the CEC2017 benchmark suite demonstrate its superiority over 10 state-of-the-art optimizers, with Wilcoxon rank-sum tests, confirming statistically significant improvements in 22 out of 29 test functions. Furthermore, evaluations on 18 UCI datasets reveal that AK-means achieves superior clustering performance compared to 4 baseline methods, achieving lower Mean Squared Error (MSE) and reduction in Davies-Bouldin Index (DB) compared to traditional k-means, particularly on large-scale data. These results highlight the algorithm's ability to balance cluster compactness and separation while maintaining scalability for high-dimensional data. The proposed hybrid optimization strategy provides a better program for real-world applications requiring efficient and precise clustering of large-scale datasets.",traditional means clustering algorithm sensitive initial centroids selection prone local optima inefficient large scale datasets address limitations article proposes adaptive means algorithm means integrates density based initialization strategy improved quadratic interpolation optimization iqio method specifically novel density aware initialization approach introduced generate initial centroids evaluating data point sparsity distance distribution effectively mitigating centroid deviation enhanced iqio mechanism particular iqio improved two key innovations including dynamic neighborhood search mechanism based absolute slope interpolated points position known optima significantly reduces computational complexity compared traditional methods incorporation gradient descent refine local exploitation accelerating convergence high accuracy solutions verify effectiveness iqio experiments cec benchmark suite demonstrate superiority state art optimizers wilcoxon rank sum tests confirming statistically significant improvements test functions furthermore evaluations uci datasets reveal means achieves superior clustering performance compared baseline methods achieving lower mean squared error mse reduction davies bouldin index compared traditional means particularly large scale data results highlight algorithm ability balance cluster compactness separation maintaining scalability high dimensional data proposed hybrid optimization strategy provides better program real world applications requiring efficient precise clustering large scale datasets
"Doxorubicin (DOX) is a potent chemotherapeutic agent whose dose-dependent cardiotoxicity is associated with oxidative stress, inflammation, and enzymatic dysfunction. This study evaluates the cardioprotective potential of esculetin, a natural coumarin derivative, against DOX-induced cardiac injury in rats. Forty-eight male Sprague-Dawley rats were divided into six groups, including control, DOX, esculetin (50 and 100 mg/kg), and combination treatments. DOX markedly altered the expression of oxidative stress-related genes (Ache, Ar, Sord upregulated; Pon1, Gst downregulated) and impaired enzyme activities, accompanied by increased malondialdehyde and depleted glutathione levels. Esculetin administration, particularly at 100 mg/kg, reversed these molecular and biochemical disturbances, restoring antioxidant defense and normalizing gene expression. Molecular docking revealed strong binding interactions of esculetin with the active sites of key enzymes including AR, SORD, AChE, GST, and PON1, supporting its regulatory role. These findings suggest that esculetin exerts multi-targeted protective effects and may serve as a promising candidate for mitigating chemotherapy-induced cardiotoxicity. Further research is warranted to explore its integration into clinical cardioprotective strategies.",doxorubicin dox potent chemotherapeutic agent whose dose dependent cardiotoxicity associated oxidative stress inflammation enzymatic dysfunction study evaluates cardioprotective potential esculetin natural coumarin derivative dox induced cardiac injury rats forty eight male sprague dawley rats divided six groups including control dox esculetin combination treatments dox markedly altered expression oxidative stress related genes ache sord upregulated pon gst downregulated impaired enzyme activities accompanied increased malondialdehyde depleted glutathione levels esculetin administration particularly reversed molecular biochemical disturbances restoring antioxidant defense normalizing gene expression molecular docking revealed strong binding interactions esculetin active sites key enzymes including sord ache gst pon supporting regulatory role findings suggest esculetin exerts multi targeted protective effects may serve promising candidate mitigating chemotherapy induced cardiotoxicity research warranted explore integration clinical cardioprotective strategies
"In this paper, we study an initial-value problem for a fourth-order neutral Volterra integro-differential equation. First, the properties of the exact solution are analysed. Next, the problem is solved numerically using the finite difference method containing the composite trapezoidal rule for the integral part of the equation. Error estimate for the approximate solution is carried out and second-order convergence is attained. In support of the idea, numerical examples are given.",paper study initial value problem fourth order neutral volterra integro differential equation first properties exact solution analysed next problem solved numerically using finite difference method containing composite trapezoidal rule integral part equation error estimate approximate solution carried second order convergence attained support idea numerical examples given
"Epilepsy, a neurological disorder affecting millions worldwide, has driven the development of various antiseizure medications (ASMs). Isoguvacine (IGV), a potent and selective agonist of the GABAA receptor (GABAAR), has shown potential in the treatment of epilepsy and other neurological disorders. However, its low blood-brain barrier permeability impairs its ability to act effectively within the central nervous system. To address this limitation, two novel ester derivatives of IGV, E7 and E14, were synthesized via Steglich esterification and evaluated through an integrated computational framework comprising density functional theory (DFT) calculations, molecular docking, molecular dynamics (MD) simulations, and in silico ADMET predictions. DFT analysis revealed that esterification significantly modified the electronic properties of IGV, with E14 exhibiting the highest polarizability (225.895 Å³) and smallest energy gap (–0.155 eV), indicative of enhanced reactivity. Molecular docking demonstrated that GABA (–8.46 kcal/mol) and IGV (–8.35 kcal/mol) exhibit similar binding affinity and complex stability with GABAAR, supporting the reliability of our computational approach. MD simulations further confirmed the stability of these complexes, where lower RMSD, RMSF, and Rg values indicated that binding of GABA and IGV did not induce significant conformational changes in the overall receptor structure. Moreover, the derivatives were projected to exhibit optimal intestinal absorption (>90%), oral bioavailability, as well as favorable safety profiles with minimal interaction risks and non-carcinogenic properties. Collectively, these in silico findings highlight the potential of ester prodrug design to overcome the central pharmacokinetic limitations of IGV, with E14 emerging as the most promising ASM candidate for further experimental development in epilepsy therapy. Beyond identifying therapeutic advantages of E14, this study also underscores the broader value of integrated computational approaches as powerful and predictive tools in early-stage drug discovery for neurological disorders.",epilepsy neurological disorder affecting millions worldwide driven development various antiseizure medications asms isoguvacine igv potent selective agonist gabaa receptor gabaar shown potential treatment epilepsy neurological disorders however low blood brain barrier permeability impairs ability act effectively within central nervous system address limitation two novel ester derivatives igv synthesized via steglich esterification evaluated integrated computational framework comprising density functional theory dft calculations molecular docking molecular dynamics simulations silico admet predictions dft analysis revealed esterification significantly modified electronic properties igv exhibiting highest polarizability smallest energy gap indicative enhanced reactivity molecular docking demonstrated gaba kcal mol igv kcal mol exhibit similar binding affinity complex stability gabaar supporting reliability computational approach simulations confirmed stability complexes lower rmsd rmsf values indicated binding gaba igv induce significant conformational changes overall receptor structure moreover derivatives projected exhibit optimal intestinal absorption oral bioavailability well favorable safety profiles minimal interaction risks non carcinogenic properties collectively silico findings highlight potential ester prodrug design overcome central pharmacokinetic limitations igv emerging promising asm candidate experimental development epilepsy therapy beyond identifying therapeutic advantages study also underscores broader value integrated computational approaches powerful predictive tools early stage drug discovery neurological disorders
"Alcoholic acute liver injury (AALI) represents a critical early stage in the progression of alcoholic liver disease, yet its early diagnosis faces significant challenges due to the lack of sensitive and reliable detection tools. Existing fluorescent probes are often limited by short emission wavelengths and small Stokes shifts, severely compromising detection accuracy. By integrating a donor-acceptor electronic structure design with computational chemistry-guided molecular optimization, we developed a new fluorescent probe DSMO, which features an impressively large Stokes shift of over 200 nm. The probe DSM-O was found to possess excellent resistance to interference and remarkable viscosity responsiveness. Importantly, it was effectively applied in the bioimaging of acute alcoholic liver injury, indicating its great promise as a diagnostic and therapeutic tool for early-stage liver damage.",alcoholic acute liver injury aali represents critical early stage progression alcoholic liver disease yet early diagnosis faces significant challenges due lack sensitive reliable detection tools existing fluorescent probes often limited short emission wavelengths small stokes shifts severely compromising detection accuracy integrating donor acceptor electronic structure design computational chemistry guided molecular optimization developed new fluorescent probe dsmo features impressively large stokes shift probe dsm found possess excellent resistance interference remarkable viscosity responsiveness importantly effectively applied bioimaging acute alcoholic liver injury indicating great promise diagnostic therapeutic tool early stage liver damage
"Lentinula edodes (shiitake mushroom) is a widely cultivated edible and medicinal fungus, valued for its bioactive compounds. While East Asian strains have been well studied, Indian populations remain under-characterized. This study explores the genetic and functional diversity of five Indian-origin L. edodes strains (DMRO-34, DMRO-35, DMRO-356, DMRO-388s, and DMRO-623) using multilocus sequence analysis targeting ITS, LSU, TEF1-α, and β-tubulin regions (660–1200 bp). Phylogenetic analyses (neighbor-joining, maximum likelihood, and Bayesian inference) revealed three well-supported clusters (bootstrap > 90 %; posterior probability > 0.95). AMOVA (p < 0.01) and diversity indices confirmed significant genetic differentiation. DMRO-356 and DMRO-388s showed close genetic relatedness and phenotypic consistency, while DMRO-623 was genetically distinct. Nonsynonymous mutations were identified in genes linked to stress response, metabolism, and β-glucan biosynthesis. Transcriptomic profiling showed higher expression of β-glucan synthesis genes (FKS1, GLS2) in DMRO-356 and DMRO-388s, indicating potential for nutraceutical use. DMRO-623 exhibited upregulation of stress-responsive genes (SOD, HSP70), suggesting adaptation to environmental stress and suitability for resilient cultivation. Compared to traditional markers (RAPD, SSR), the multilocus approach offered improved resolution for distinguishing closely related strains. Strong correlations (r = 0.80–0.88) between genetic variation and key traits (β-glucan content, antioxidant activity, yield) emphasize the functional significance of the observed diversity. This first multilocus-based study of Indian-origin L. edodes strains provides a genomic framework for marker-assisted breeding, trait optimization, and conservation, supporting their commercial and ecological value in diverse environments.",lentinula edodes shiitake mushroom widely cultivated edible medicinal fungus valued bioactive compounds east asian strains well studied indian populations remain characterized study explores genetic functional diversity five indian origin edodes strains dmro dmro dmro dmro dmro using multilocus sequence analysis targeting lsu tef tubulin regions phylogenetic analyses neighbor joining maximum likelihood bayesian inference revealed three well supported clusters bootstrap posterior probability amova diversity indices confirmed significant genetic differentiation dmro dmro showed close genetic relatedness phenotypic consistency dmro genetically distinct nonsynonymous mutations identified genes linked stress response metabolism glucan biosynthesis transcriptomic profiling showed higher expression glucan synthesis genes fks gls dmro dmro indicating potential nutraceutical use dmro exhibited upregulation stress responsive genes sod hsp suggesting adaptation environmental stress suitability resilient cultivation compared traditional markers rapd ssr multilocus approach offered improved resolution distinguishing closely related strains strong correlations genetic variation key traits glucan content antioxidant activity yield emphasize functional significance observed diversity first multilocus based study indian origin edodes strains provides genomic framework marker assisted breeding trait optimization conservation supporting commercial ecological value diverse environments
"Antigenic peptide (AP) prediction is one of the most important roles in improve vaccine design and interpreting immune responses. This paper develops a Multi-Level Pooling-based Transformer (MLPT) model, which improves the accuracy and efficiency of predicting T-cell epitopes (TCEs). The model has utilized peptide sequences from the Immune Epitope Database (IEDB) and utilized a refined Kolaskar & Tongaonkar algorithm for feature extraction as well as a Self-Improved Black-winged Kite optimization algorithm to optimize the scoring matrix. The MLPT architecture takes the input features from the Adaptive Depthwise Multi-Kernel Atrous Module (ADMAM) as inputs to the Swin Transformer, and the output of Swin block 1 is concatenated with the features extracted from the Kolaskar-Tongaonkar algorithm with the SA-BWK model. This hierarchical integration enhances feature representation and predictive capability. Advanced feature extraction, coupled with optimized feature selection for the MLPT model improves its performance over the conventional approach in the identification of reduced-complexity antigenic determinants.",antigenic peptide prediction one important roles improve vaccine design interpreting immune responses paper develops multi level pooling based transformer mlpt model improves accuracy efficiency predicting cell epitopes tces model utilized peptide sequences immune epitope database iedb utilized refined kolaskar tongaonkar algorithm feature extraction well self improved black winged kite optimization algorithm optimize scoring matrix mlpt architecture takes input features adaptive depthwise multi kernel atrous module admam inputs swin transformer output swin block concatenated features extracted kolaskar tongaonkar algorithm bwk model hierarchical integration enhances feature representation predictive capability advanced feature extraction coupled optimized feature selection mlpt model improves performance conventional approach identification reduced complexity antigenic determinants
"B-cell acute lymphoblastic leukemia (B-ALL) is an aggressive hematological malignancy that primarily affects children but can also occur in adults, progressing rapidly and requiring urgent clinical intervention. Late-stage diagnosis often results in reduced survival rates and typically depends on costly, time-intensive diagnostic procedures. Peripheral blood smear (PBS) imaging plays a central role in the preliminary screening of B-ALL and provides an accessible foundation for computer-assisted diagnosis. To support early and efficient classification, this study proposes a lightweight convolutional neural network (CNN) designed to classify B-ALL subtypes directly from PBS images without the need for pre-segmentation. The model is computationally efficient, comprising only 986,126 trainable parameters, and integrates Squeeze-and-Excitation (SE) modules within Inverted Residual Blocks to strengthen feature representation. Experimental results demonstrated excellent classification performance, achieving 100 % accuracy, precision, sensitivity, specificity, F1-score, and Matthews correlation coefficient (MCC). To further assess generalizability, cross-dataset validation was performed on the independent Blood Cells Cancer (ALL) dataset without retraining or fine-tuning, yielding a robust accuracy of 99.85 %. Model interpretability was performed using Gradient-weighted Class Activation Mapping (Grad-CAM) and Local Interpretable Model-agnostic Explanations (LIME), which provided visual explanations and highlighted key discriminative cellular features, respectively. Taken together, these results demonstrate that the proposed framework delivers a highly accurate, resource-efficient, and interpretable solution for B-ALL classification, underscoring its strong potential for integration into real-world clinical practice. Additionally, the implementation code for this study is publicly available at: https://github.com/awazabbas/Efficient-Lightweight-CNN-for-Automated-Classification-of-B-cell-Acute-Lymphoblastic-Leukemia-.",cell acute lymphoblastic leukemia aggressive hematological malignancy primarily affects children also occur adults progressing rapidly requiring urgent clinical intervention late stage diagnosis often results reduced survival rates typically depends costly time intensive diagnostic procedures peripheral blood smear pbs imaging plays central role preliminary screening provides accessible foundation computer assisted diagnosis support early efficient classification study proposes lightweight convolutional neural network cnn designed classify subtypes directly pbs images without need pre segmentation model computationally efficient comprising trainable parameters integrates squeeze excitation modules within inverted residual blocks strengthen feature representation experimental results demonstrated excellent classification performance achieving accuracy precision sensitivity specificity score matthews correlation coefficient mcc assess generalizability cross dataset validation performed independent blood cells cancer dataset without retraining fine tuning yielding robust accuracy model interpretability performed using gradient weighted class activation mapping grad cam local interpretable model agnostic explanations lime provided visual explanations highlighted key discriminative cellular features respectively taken together results demonstrate proposed framework delivers highly accurate resource efficient interpretable solution classification underscoring strong potential integration real world clinical practice additionally implementation code study publicly available https github com awazabbas efficient lightweight cnn automated classification cell acute lymphoblastic leukemia
"Inhibition of the hERG (human ether-a-go-go-related gene) channel by drug molecules can lead to severe cardiac toxicity, resulting in the withdrawal of many approved drugs from the market or halting their development in later stages. These findings highlight the pressing need to evaluate hERG blockade during drug development. We propose a novel framework for feature extraction and aggregation optimization (FEAOF), which primarily consists of a feature extraction module and an aggregation optimization module. The model integrates diverse ligand representations, including molecular fingerprints, descriptors, and graphs, as well as ligand–receptor interaction features. Based on this integration, we further optimize the algorithmic framework to achieve precise predictions of compounds cardiac toxicity. Two independent test sets exhibiting pronounced structural dissimilarity from the training data were constructed to rigorously assess the model’s generalization ability. The results demonstrate that the FEAOF model exhibits strong robustness compared to seven baseline models, achieving F1 score of 66.1 % and 68.1 %. Notably, when benchmarked against five existing models on two external test sets, FEAOF also achieved the highest or near-highest scores across all key evaluation metrics. Importantly, this model can be easily adapted for other drug-target interaction prediction tasks. It is made available as open source under the permissive MIT license at https://github.com/ConfusedAnt/FEAOF.",inhibition herg human ether related gene channel drug molecules lead severe cardiac toxicity resulting withdrawal many approved drugs market halting development later stages findings highlight pressing need evaluate herg blockade drug development propose novel framework feature extraction aggregation optimization feaof primarily consists feature extraction module aggregation optimization module model integrates diverse ligand representations including molecular fingerprints descriptors graphs well ligand receptor interaction features based integration optimize algorithmic framework achieve precise predictions compounds cardiac toxicity two independent test sets exhibiting pronounced structural dissimilarity training data constructed rigorously assess model generalization ability results demonstrate feaof model exhibits strong robustness compared seven baseline models achieving score notably benchmarked five existing models two external test sets feaof also achieved highest near highest scores across key evaluation metrics importantly model easily adapted drug target interaction prediction tasks made available open source permissive mit license https github com confusedant feaof
"Understanding the thermal behaviour of firebrand piles is essential for assessing their energy transfer characteristics and their impact on underlying materials. Due to experimental limitations in capturing spatial thermal distributions within and beneath the pile, as well as the thermal response of individual firebrands within the pile. This study employed a numerical approach to investigate the thermal performance of smouldering firebrand accumulations. The model examined the effects of varying wind speeds (0.9–2.7 m/s) and coverage densities (0.06 and 0.16 g/cm2) on firebrand surface temperature, released heat flux, and heat flux received by the substrate. Model accuracy was verified by comparing its output with the heat release rate per unit area (HRRPUA) derived from an experimental study, ensuring the reliability of the numerical data. Results showed that increased wind speeds and coverage densities significantly increase both average and localized thermal parameters. For smouldering firebrand accumulations, peak average total heat flux to the substrate ranged from 19 to 52 kW/m2, while localized regions within the pile reached up to 120 kW/m2. These findings demonstrate the importance of spatial thermal analysis in characterizing firebrand pile behaviour.",understanding thermal behaviour firebrand piles essential assessing energy transfer characteristics impact underlying materials due experimental limitations capturing spatial thermal distributions within beneath pile well thermal response individual firebrands within pile study employed numerical approach investigate thermal performance smouldering firebrand accumulations model examined effects varying wind speeds coverage densities firebrand surface temperature released heat flux heat flux received substrate model accuracy verified comparing output heat release rate per unit area hrrpua derived experimental study ensuring reliability numerical data results showed increased wind speeds coverage densities significantly increase average localized thermal parameters smouldering firebrand accumulations peak average total heat flux substrate ranged localized regions within pile reached findings demonstrate importance spatial thermal analysis characterizing firebrand pile behaviour
"There is a significant need for computer experiments to study and model complex physical systems. In both computer and physical experiments, constructing experimental designs with good space-filling and column-orthogonality properties is crucial. While maximin distance designs and uniform designs ensure space-filling in full-dimensional spaces, they lack guarantees for low-dimensional projections. Uniform projection designs address this gap by ensuring space-filling properties in low-dimensional subspaces. Orthogonal designs enable efficient factor screening in Gaussian processes and ensure uncorrelated estimates of main effects in linear models. However, constructing such optimal designs remains challenging. A design that combines these advantages would outperform individual approaches. This paper fills this gap by proposing seven novel theoretical techniques for constructing orthogonal maximin distance uniform projection designs. The proposed designs demonstrate superior performance as the number of factors increases, making them particularly well-suited for surrogate modeling and linear trend estimation in high-dimensional Gaussian processes. Comparative studies show that the proposed techniques outperform existing methods.",significant need computer experiments study model complex physical systems computer physical experiments constructing experimental designs good space filling column orthogonality properties crucial maximin distance designs uniform designs ensure space filling full dimensional spaces lack guarantees low dimensional projections uniform projection designs address gap ensuring space filling properties low dimensional subspaces orthogonal designs enable efficient factor screening gaussian processes ensure uncorrelated estimates main effects linear models however constructing optimal designs remains challenging design combines advantages would outperform individual approaches paper fills gap proposing seven novel theoretical techniques constructing orthogonal maximin distance uniform projection designs proposed designs demonstrate superior performance number factors increases making particularly well suited surrogate modeling linear trend estimation high dimensional gaussian processes comparative studies show proposed techniques outperform existing methods
"We introduce a novel method to handle the time dimension when Physics-Informed Neural Networks (PINN) are used to solve time-dependent differential equations; our proposal focuses on how time sampling and weighting strategies affect solution quality. While previous methods proposed heuristic time-weighting schemes, our approach is grounded in theoretical insights derived from the Lyapunov exponents, which quantify the sensitivity of solutions to perturbations over time. This principled methodology automatically adjusts weights based on the stability regime of the system — whether chaotic, periodic, or stable. Numerical experiments on challenging benchmarks, including the chaotic Lorenz system and the Burgers’ equation, demonstrate the effectiveness and robustness of the proposed method. Compared to existing techniques, our approach offers improved convergence and accuracy without requiring additional hyperparameter tuning. The findings underline the importance of incorporating causality and dynamical system behavior into PINN training strategies, providing a robust framework for solving time-dependent problems with enhanced reliability.",introduce novel method handle time dimension physics informed neural networks pinn used solve time dependent differential equations proposal focuses time sampling weighting strategies affect solution quality previous methods proposed heuristic time weighting schemes approach grounded theoretical insights derived lyapunov exponents quantify sensitivity solutions perturbations time principled methodology automatically adjusts weights based stability regime system whether chaotic periodic stable numerical experiments challenging benchmarks including chaotic lorenz system burgers equation demonstrate effectiveness robustness proposed method compared existing techniques approach offers improved convergence accuracy without requiring additional hyperparameter tuning findings underline importance incorporating causality dynamical system behavior pinn training strategies providing robust framework solving time dependent problems enhanced reliability
"This paper considers the weakly robust global optimization problem of max-plus linear systems with non-negative constraint sets. A characteristic of the weak robustness of global optimization is given, and the weakly robust perturbation bound is constructed to ensure that the parameter perturbations within the bound do not affect the original global minimum and one globally optimal solution. The maximal element of each row in a max-plus matrix is used to characterize the variable elements and invariable elements, and their maximum number is determined. The relatively maximal perturbation bounds are derived by using all variable and invariable elements, and a polynomial algorithm for finding these bounds is provided. The proposed weakly robust global optimization is applied to optimizing time parameters in data transmission systems. The effectiveness of the proposed method is demonstrated by examples.",paper considers weakly robust global optimization problem max plus linear systems non negative constraint sets characteristic weak robustness global optimization given weakly robust perturbation bound constructed ensure parameter perturbations within bound affect original global minimum one globally optimal solution maximal element row max plus matrix used characterize variable elements invariable elements maximum number determined relatively maximal perturbation bounds derived using variable invariable elements polynomial algorithm finding bounds provided proposed weakly robust global optimization applied optimizing time parameters data transmission systems effectiveness proposed method demonstrated examples
"This study investigates the synthesis, antimicrobial, anticancer, and in silico properties of novel quinoline-chalcone hybrids (nQCa-l), which were synthesized and characterized. Their antimicrobial activity revealed broad-spectrum efficacy, with compound 2QC-h demonstrating superior potency compared to several standard antibiotics and antifungals. The anticancer potential was assessed against gastrointestinal system cancer cell lines (AGS, HepG2, HCT116), where 2QC-h emerged as the most potent antiproliferative agent, often surpassing oxaliplatin in efficacy, particularly in AGS gastric cancer cells. Mechanistic studies have demonstrated that 2QC-h synergistically induces apoptosis and inhibits epithelial-mesenchymal transition (EMT) in AGS cells through the intrinsic mitochondrial pathway, thereby enhancing the anticancer effect of oxaliplatin. Crucially, 2QC-h exhibited selective cytotoxicity towards gastrointestinal system cancer cells (AGS cells: 4.85 ± 0.22 µg/mL and 2.66 ± 0.58 µg/mL, HCT116 cells: 6.61 ± 0.29 µg/mL and 2.39 ± 0.57 µg/mL, and HepG2 cells: 9.14 ± 0.49 µg/mL and 6.15 ± 0.27 µg/mL for 24 h and 48 h, respectively) and minimal morphological effects on healthy HUVEC cells. Computational studies, including DFT analysis, MEP, RDG, ELF, LOL, and ALIE, provided comprehensive insights into the electronic structure, reactivity, and non-covalent interactions, elucidating the structure-activity relationships (SAR). Molecular docking simulations identified VEGFR-2 and EGFR as the preferential targets for these derivatives, with nanomolar binding affinities, which correlated strongly with experimental cytotoxic potencies. ADME highlighted favorable drug-likeness properties while identifying areas for further optimization. Overall, this research establishes quinoline-chalcone hybrids as promising multi-target therapeutic agents with significant potential for developing novel antimicrobial and anticancer drugs.",study investigates synthesis antimicrobial anticancer silico properties novel quinoline chalcone hybrids nqca synthesized characterized antimicrobial activity revealed broad spectrum efficacy compound demonstrating superior potency compared several standard antibiotics antifungals anticancer potential assessed gastrointestinal system cancer cell lines ags hepg hct emerged potent antiproliferative agent often surpassing oxaliplatin efficacy particularly ags gastric cancer cells mechanistic studies demonstrated synergistically induces apoptosis inhibits epithelial mesenchymal transition emt ags cells intrinsic mitochondrial pathway thereby enhancing anticancer effect oxaliplatin crucially exhibited selective cytotoxicity towards gastrointestinal system cancer cells ags cells hct cells hepg cells respectively minimal morphological effects healthy huvec cells computational studies including dft analysis mep rdg elf lol alie provided comprehensive insights electronic structure reactivity non covalent interactions elucidating structure activity relationships sar molecular docking simulations identified vegfr egfr preferential targets derivatives nanomolar binding affinities correlated strongly experimental cytotoxic potencies adme highlighted favorable drug likeness properties identifying areas optimization overall research establishes quinoline chalcone hybrids promising multi target therapeutic agents significant potential developing novel antimicrobial anticancer drugs
"Direct Dark Matter detection and studies on the nature of neutrinos demand detector systems with extremely low background levels, including from radioactivity. Additive-free, electroformed copper, in addition to a set of advantages, exhibits exceptional radiopurity, making it the material of choice for mechanical and structural components for rare-event searches experiments. To satisfy the increasing demand for materials with superior mechanical strength, the development of copper–chromium alloys is pursued. Early investigations explored the synthesis of these alloys by electrodeposition and thermal processing. A materials-design approach is proposed to optimize the fabrication and thermal processing stages of manufacturing. It is assisted by materials modeling tools based on the thermodynamic and kinetic properties of alloy compositions, which enables faster development of novel materials by predicting properties and materials performance. This approach is demonstrated by comparing simulations with previously reported experimental investigations and proposing improved thermal processing.",direct dark matter detection studies nature neutrinos demand detector systems extremely low background levels including radioactivity additive free electroformed copper addition set advantages exhibits exceptional radiopurity making material choice mechanical structural components rare event searches experiments satisfy increasing demand materials superior mechanical strength development copper chromium alloys pursued early investigations explored synthesis alloys electrodeposition thermal processing materials design approach proposed optimize fabrication thermal processing stages manufacturing assisted materials modeling tools based thermodynamic kinetic properties alloy compositions enables faster development novel materials predicting properties materials performance approach demonstrated comparing simulations previously reported experimental investigations proposing improved thermal processing
"This study presents a comprehensive theoretical evaluation of the photon interaction behavior of selected radiomimetic compounds such as Bleomycin, Talazoparib, Neocarzinostatin (NCS), Chromophore (C-1027), and Calicheamicin (Cali) using advanced Monte Carlo-based simulation tools and radiation transport codes. The simulations were conducted using PHITS code in a water-based spherical phantom model to investigate energy deposition, collision parameters, and particle transport behavior. Complementary photon attenuation parameters, including mass attenuation coefficients, half-value layers, atomic cross-sections, and exposure buildup factors, were also calculated using Phy-X/PSD. The results show distinct differences in the physical interaction profiles of the compounds, with Calicheamicin demonstrating notably higher photon attenuation and energy deposition, likely due to the presence of high-Z elements in its structure. These findings are based on Monte Carlo-based simulation results and are intended to provide a foundational understanding of the physical interaction mechanisms of radiomimetic materials under photon irradiation. No biological or therapeutic interpretations are made, and the results are intended to inform future experimental or interdisciplinary research.",study presents comprehensive theoretical evaluation photon interaction behavior selected radiomimetic compounds bleomycin talazoparib neocarzinostatin ncs chromophore calicheamicin cali using advanced monte carlo based simulation tools radiation transport codes simulations conducted using phits code water based spherical phantom model investigate energy deposition collision parameters particle transport behavior complementary photon attenuation parameters including mass attenuation coefficients half value layers atomic cross sections exposure buildup factors also calculated using phy psd results show distinct differences physical interaction profiles compounds calicheamicin demonstrating notably higher photon attenuation energy deposition likely due presence high elements structure findings based monte carlo based simulation results intended provide foundational understanding physical interaction mechanisms radiomimetic materials photon irradiation biological therapeutic interpretations made results intended inform future experimental interdisciplinary research
"Dual-phase compositionally complex ultra-high temperature ceramics were formulated by incorporating different Groups V and VI metals such as V, Nb, Ta, Cr, Mo, or W into a base composition containing the Group IV elements, Hf, Ti, and Zr. Metal distribution was predicted using first-principles-based thermodynamics simulations and compared with experimental results. Moreover, phase stability, microstructure, and mechanical properties were evaluated for all of the ceramics. Compositions containing Cr, V, Nb, or Ta formed dual-phase ceramics containing only one boride and one carbide phase, while compositions containing Mo or W developed an additional third phase. The experimental metal distribution trends generally aligned with thermodynamic predictions, except for compositions containing V, which showed unexpected segregation behavior that was influenced by complex interactions of the coexistence of boride and carbide structures. From the dual-phase ceramics, the composition containing V exhibited the highest hardness (HV1 = 25.5 ± 0.6 GPa) combined with smaller grain sizes (0.99 ± 0.33 μm for the boride and 1.15 ± 0.31 μm for the carbide phases). Our findings provide insights into phase formation and elemental segregation and help the design of next-generation dual-phase UHTCs with tailored properties.",dual phase compositionally complex ultra high temperature ceramics formulated incorporating different groups metals base composition containing group elements metal distribution predicted using first principles based thermodynamics simulations compared experimental results moreover phase stability microstructure mechanical properties evaluated ceramics compositions containing formed dual phase ceramics containing one boride one carbide phase compositions containing developed additional third phase experimental metal distribution trends generally aligned thermodynamic predictions except compositions containing showed unexpected segregation behavior influenced complex interactions coexistence boride carbide structures dual phase ceramics composition containing exhibited highest hardness gpa combined smaller grain sizes boride carbide phases findings provide insights phase formation elemental segregation help design next generation dual phase uhtcs tailored properties
"Bioavailable testosterone (BAT), a critical factor for reproductive, metabolic, and psychological health, is primarily regulated by sex hormone-binding globulin (SHBG). However, the molecular mechanisms driving SHBG-mediated regulation of BAT remain unclear. Identifying key protein regulators offers promising therapeutic opportunities for testosterone-related disorders. We conducted a comprehensive proteome-wide Mendelian randomization (PWMR) and colocalization analysis using large-scale pQTL and GWAS datasets. Mediation MR assessed SHBG’s role in regulating BAT, and functional enrichment, protein interaction networks, phenome-wide association studies (PheWAS), and drug prediction were performed to explore therapeutic relevance and safety. We identified 36 proteins that influence BAT via SHBG mediation. Among them, five proteins (MAX, TXNL4B, GLRX2, F13B, SNUPN) showed strong or moderate colocalization with BAT, suggesting shared genetic regulation. MAX and TXNL4B increased BAT by reducing SHBG, while GLRX2, F13B, and SNUPN decreased BAT via elevated SHBG levels. PheWAS suggested potential risks for GLRX2 (depression) and TXNL4B (lipid disorders). Drug and compound prediction highlighted compounds, including cardiac glycosides, antioxidants, and endocrine-disrupting chemicals, targeting these proteins. Our findings reveal novel protein regulators of testosterone bioavailability through SHBG and provide a framework for developing targeted therapeutics. This integrative approach may support safer, more precise treatment strategies for testosterone-related metabolic and endocrine disorders.",bioavailable testosterone bat critical factor reproductive metabolic psychological health primarily regulated sex hormone binding globulin shbg however molecular mechanisms driving shbg mediated regulation bat remain unclear identifying key protein regulators offers promising therapeutic opportunities testosterone related disorders conducted comprehensive proteome wide mendelian randomization pwmr colocalization analysis using large scale pqtl gwas datasets mediation assessed shbg role regulating bat functional enrichment protein interaction networks phenome wide association studies phewas drug prediction performed explore therapeutic relevance safety identified proteins influence bat via shbg mediation among five proteins max txnl glrx snupn showed strong moderate colocalization bat suggesting shared genetic regulation max txnl increased bat reducing shbg glrx snupn decreased bat via elevated shbg levels phewas suggested potential risks glrx depression txnl lipid disorders drug compound prediction highlighted compounds including cardiac glycosides antioxidants endocrine disrupting chemicals targeting proteins findings reveal novel protein regulators testosterone bioavailability shbg provide framework developing targeted therapeutics integrative approach may support safer precise treatment strategies testosterone related metabolic endocrine disorders
"Accurate prediction of drug-drug interactions (DDIs) is paramount for preventing adverse drug events and ensuring patient safety. While existing computational methods show promise, they often struggle to effectively model long-range intramolecular dependencies and identify the most salient substructures for interaction. To address these limitations, we introduce TK-DDI, a novel deep learning framework based on molecular tokenization. TK-DDI first converts drug molecules into sequences of tokens, creating a unified representation that captures both 2D structural and 3D conformational information. A Transformer encoder is then employed to learn the contextual relationships between all token pairs, effectively modeling the influence of distant functional groups. To elucidate the interaction mechanism, TK-DDI incorporates a two-stage attention strategy: an intra-drug attention module to highlight key substructures within each molecule, followed by an inter-drug attention module to fuse the representations of the drug pair. Comprehensive experiments on benchmark datasets demonstrate that TK-DDI robustly outperforms state-of-the-art methods, establishing a new standard for DDI prediction.",accurate prediction drug drug interactions ddis paramount preventing adverse drug events ensuring patient safety existing computational methods show promise often struggle effectively model long range intramolecular dependencies identify salient substructures interaction address limitations introduce ddi novel deep learning framework based molecular tokenization ddi first converts drug molecules sequences tokens creating unified representation captures structural conformational information transformer encoder employed learn contextual relationships token pairs effectively modeling influence distant functional groups elucidate interaction mechanism ddi incorporates two stage attention strategy intra drug attention module highlight key substructures within molecule followed inter drug attention module fuse representations drug pair comprehensive experiments benchmark datasets demonstrate ddi robustly outperforms state art methods establishing new standard ddi prediction
"Nano cones have several benefits in treating chronic illnesses through targeted and site-specific drug delivery. Density functional theory (DFT) was used in this work to explore the therapeutic potential of carbon nanocone oxide (ONC) as a drug carrier for Favipiravir (FPV) against viral infection. The Frontier Molecular Orbitals (FMOs) defined the bandgap as a decline from 5.52 to 5.36 eV of Favipiravir when interacting with ONC. The molecular electrostatic potential (MEP) surfaces revealed that in the FPV-ONC complex, the Favipiravir acts as nucleophile and ONC as electrophile. The FMOs also calculated quantum chemical parameters in the gas and solvent phases to confirm the correlation between Favipiravir and ONC stabilization energy in complex formation. The density of state and charge deposition analysis showed the charge transfer phenomenon during complex formation. NBO, ELF, NCI, and iso-surfaces were used to identify the charge transfer between Favipiravir and ONC. The excited states were analyzed by plotting the TDM and spectral investigation (Raman, UV-Vis, IR, and NMR) of FPV, ONC, and FPV-ONC. The drug-carrier interaction generated fluorescence quenching in PET, which also gave a pictorial description of the unique excited states from 1 to 10. The results collectively indicate the therapeutic potential of the ONC carrier as a favipiravir carrier for managing viral disease.",nano cones several benefits treating chronic illnesses targeted site specific drug delivery density functional theory dft used work explore therapeutic potential carbon nanocone oxide onc drug carrier favipiravir fpv viral infection frontier molecular orbitals fmos defined bandgap decline favipiravir interacting onc molecular electrostatic potential mep surfaces revealed fpv onc complex favipiravir acts nucleophile onc electrophile fmos also calculated quantum chemical parameters gas solvent phases confirm correlation favipiravir onc stabilization energy complex formation density state charge deposition analysis showed charge transfer phenomenon complex formation nbo elf nci iso surfaces used identify charge transfer favipiravir onc excited states analyzed plotting tdm spectral investigation raman vis nmr fpv onc fpv onc drug carrier interaction generated fluorescence quenching pet also gave pictorial description unique excited states results collectively indicate therapeutic potential onc carrier favipiravir carrier managing viral disease
"The currently marketed antitubercular drugs have limited efficacy with the potential to cause organ toxicity. Thus, there is a need for new drug therapies to combat tuberculosis. Methyl 2-(7-hydroxy-3-methyloctyl)-1,3-dimethyl-4-oxocyclohex-2-enecarboxylate (PE14) and (E)-3,7,11,15-tetramethylhexadec-2-en-1-ol (EA8) are the natural antitubercular lead-like molecules isolated from petroleum ether and ethyl acetate leaf extracts of Ipomea sepiaria, respectively. Extensive research has demonstrated the wide range of health benefits associated with this plant. However, the antitubercular effects of phytocompounds isolated from this species have not been systematically investigated. To evaluate the antitubercular effect of the natural compound, in silico prediction of binding affinity against selected antitubercular target proteins was conducted, and this was compared with co-crystallized ligands as a standard. Additionally, the physicochemical properties, pharmacokinetics, and various toxicity-related parameters were also predicted. Two ligand docking complexes were selected for molecular dynamics simulations to calculate the binding free energy over 250 ns. Moreover, FMO and DFT were also investigated. PE14 complies with RO5 and exhibits suitable ADMET profiles. The molecular docking scores in kcal/mol showed comparatively more potency against antitubercular drug targets compared to the co-crystalized ligand of the target protein as well as EA8. Overall, the strength of interaction between the ligands with their selected target proteins from the molecular docking study, heat change that occurs during the ligand-target interactions from a molecular dynamic simulation study, the electronic reactivity trend was established as STD > PE14 > CIP > EA8 from FMO analysis and other multi-parametric druggability profiles of target proteins suggests that PE14 can be considered as a suitable antitubercular lead-like for the treatment of M. tuberculosis. The results of the current study were closely correlated with those of our previous study on Ipomea sepiaria in the LRP assay. However, necessary in vitro and in vivo studies on the synthesized pure compound must be carried out to participate in a clinical trial, where the in silico results would help expedite the process of drug development.",currently marketed antitubercular drugs limited efficacy potential cause organ toxicity thus need new drug therapies combat tuberculosis methyl hydroxy methyloctyl dimethyl oxocyclohex enecarboxylate tetramethylhexadec natural antitubercular lead like molecules isolated petroleum ether ethyl acetate leaf extracts ipomea sepiaria respectively extensive research demonstrated wide range health benefits associated plant however antitubercular effects phytocompounds isolated species systematically investigated evaluate antitubercular effect natural compound silico prediction binding affinity selected antitubercular target proteins conducted compared crystallized ligands standard additionally physicochemical properties pharmacokinetics various toxicity related parameters also predicted two ligand docking complexes selected molecular dynamics simulations calculate binding free energy moreover fmo dft also investigated complies exhibits suitable admet profiles molecular docking scores kcal mol showed comparatively potency antitubercular drug targets compared crystalized ligand target protein well overall strength interaction ligands selected target proteins molecular docking study heat change occurs ligand target interactions molecular dynamic simulation study electronic reactivity trend established std cip fmo analysis multi parametric druggability profiles target proteins suggests considered suitable antitubercular lead like treatment tuberculosis results current study closely correlated previous study ipomea sepiaria lrp assay however necessary vitro vivo studies synthesized pure compound must carried participate clinical trial silico results would help expedite process drug development
"Treatment resistance and leukemic stem cell survival are significant challenges in managing chronic myeloid leukemia (CML). While Breakpoint Cluster Region-Abelson (BCR::ABL1) fusion drives disease initiation, emerging evidence suggests that non-coding RNAs, particularly long non-coding RNAs (lncRNAs), regulate critical cellular processes that promote leukemic transformation and treatment failure. We employed a comprehensive in silico approach integrating multiple omics platforms to investigate CML-associated lncRNAs. Candidate lncRNAs were identified by cross-referencing LncRNA Disease and LncTarD repositories, followed by validation expression analysis of data in leukemia transcriptomic cohorts. StarBase and miRWalk algorithms predicted the interaction networks between lncRNAs and microRNAs, while transcription factor (TF) binding was determined through ChIP-seq analysis and TF databases to map the regulatory networks governing lncRNA and miRNA expression. Four critical long non-coding RNAs (lncRNAs), H19, HOTAIR, MEG3, and UCA1, were differentially expressed in CML and were involved in regulating proliferation, cell death, and therapeutic response pathways. Network analysis revealed extensive regulatory interactions with microRNAs, especially hsa-miR-18a-5p and hsa-miR-106b-5p, which influence major oncogenic pathways. TFs mapping identified pivotal hubs, including MYC and STAT5A, as key regulators of lncRNA and miRNA networks, which contribute to disrupted normal growth control, and apoptotic and survival pathways in leukemic cells. This study identified a non-coding RNA interactome in CML involving lncRNAs and microRNAs that may cooperatively contribute to CML progression. These interconnected circuits offer novel insights into treatment resistance and stem cell persistence, identifying promising therapeutic targets. Future validation of these interactions may guide the development of non-coding RNA–targeted therapies for resistant CML.",treatment resistance leukemic stem cell survival significant challenges managing chronic myeloid leukemia cml breakpoint cluster region abelson bcr abl fusion drives disease initiation emerging evidence suggests non coding rnas particularly long non coding rnas lncrnas regulate critical cellular processes promote leukemic transformation treatment failure employed comprehensive silico approach integrating multiple omics platforms investigate cml associated lncrnas candidate lncrnas identified cross referencing lncrna disease lnctard repositories followed validation expression analysis data leukemia transcriptomic cohorts starbase mirwalk algorithms predicted interaction networks lncrnas micrornas transcription factor binding determined chip seq analysis databases map regulatory networks governing lncrna mirna expression four critical long non coding rnas lncrnas hotair meg uca differentially expressed cml involved regulating proliferation cell death therapeutic response pathways network analysis revealed extensive regulatory interactions micrornas especially hsa mir hsa mir influence major oncogenic pathways tfs mapping identified pivotal hubs including myc stat key regulators lncrna mirna networks contribute disrupted normal growth control apoptotic survival pathways leukemic cells study identified non coding rna interactome cml involving lncrnas micrornas may cooperatively contribute cml progression interconnected circuits offer novel insights treatment resistance stem cell persistence identifying promising therapeutic targets future validation interactions may guide development non coding rna targeted therapies resistant cml
"The pervasive challenges in cancer management, ranging from accurate early diagnosis to effective personalised therapies and precise patient stratification, represent significant clinical unmet needs. Artificial intelligence (AI) is transforming cancer research by offering unprecedented capabilities in analysing complex genomic datasets. AI has completely transformed omics research by simplifying the integration of multi-omics data, offering more profound insights into cancer heterogeneity, and enhancing predictive models for patient treatment responses. In the past decade, numerous studies have highlighted how AI has revolutionised omics research. AI models are instrumental in enhancing oncology by addressing these unmet needs through improved clinical trial matching, refined risk assessment, and precise treatment selection. They contribute to more personalised and effective cancer care by classifying cancer types and subtypes, identifying biomarkers, predicting drug responses, stratifying patients, and analysing tumour evolution and heterogeneity. This comprehensive review specifically focuses on the development and validation of AI-powered multi-omics language models for cancer genomics. We posit that the integration of diverse omics data types provides synergistic insights beyond single-omics approaches, which are critical for unravelling cancer heterogeneity and addressing the complex challenges within cancer genomics. The review highlights recent advances, current difficulties, and potential paths forward for these integrated AI approaches. We also detail the main elements of these models, such as their architectures, training plans, evaluation techniques, and data preprocessing. All things considered, multi-omics language models powered by AI hold immense promise for deriving biological insights from intricate cancer genomic data and converting them into actionable information for clinical settings.",pervasive challenges cancer management ranging accurate early diagnosis effective personalised therapies precise patient stratification represent significant clinical unmet needs artificial intelligence transforming cancer research offering unprecedented capabilities analysing complex genomic datasets completely transformed omics research simplifying integration multi omics data offering profound insights cancer heterogeneity enhancing predictive models patient treatment responses past decade numerous studies highlighted revolutionised omics research models instrumental enhancing oncology addressing unmet needs improved clinical trial matching refined risk assessment precise treatment selection contribute personalised effective cancer care classifying cancer types subtypes identifying biomarkers predicting drug responses stratifying patients analysing tumour evolution heterogeneity comprehensive review specifically focuses development validation powered multi omics language models cancer genomics posit integration diverse omics data types provides synergistic insights beyond single omics approaches critical unravelling cancer heterogeneity addressing complex challenges within cancer genomics review highlights recent advances current difficulties potential paths forward integrated approaches also detail main elements models architectures training plans evaluation techniques data preprocessing things considered multi omics language models powered hold immense promise deriving biological insights intricate cancer genomic data converting actionable information clinical settings
"Lysophosphatidic acid (LPA) is an important bioactive signaling molecule that activates six distinct G protein-coupled receptors (GPCRs), among which the LPA1 subtype possesses high therapeutic target potential due to its critical roles in malignant tumors, pulmonary fibrosis, inflammation, and neuropathic pain. Recent studies have shown that the small molecule ONO-0740556 can effectively disrupt the overall structure of LPA1 in solution; however, its effects in a native membrane environment remain unclear. According to the Traditional Chinese Medicine Systems Pharmacology Database (TCMS), Dan Shen(Salvia miltiorrhiza) exhibits anti-inflammatory and anti-pulmonary fibrosis properties, and its natural compound cryptoxanthin may serve as a novel LPA1 inhibitor. Here, we employed microsecond-scale all-atom molecular dynamics simulations to systematically compare the structural modulation of LPA1 by ONO-0740556 and cryptoxanthin in a membrane environment. The results indicate that both ligands enlarge the entrance of the LPA1 ligand-binding channel, weaken the interactions between transmembrane helix 7 (TM7) and other structural communities, and reduce hydrophobic interactions between LPA1 and the membrane, thereby inducing membrane structural perturbations. Notably, cryptoxanthin exerts a more pronounced effect in widening the binding channel and selectively attenuating TM7–community interactions. These findings provide atomic-level insights into how small molecules modulate LPA1 structure in its physiologically relevant membrane environment. Such mechanistic understanding not only offers a theoretical basis for the rational design of LPA1-targeted therapeutics but also supports the discovery of natural product–derived inhibitors as viable drug candidates. Moreover, our results highlight potential allosteric sites and dynamic features of LPA1 that could be exploited to develop highly selective modulators, paving the way for more precise therapeutic interventions against LPA1-related diseases.",lysophosphatidic acid lpa important bioactive signaling molecule activates six distinct protein coupled receptors gpcrs among lpa subtype possesses high therapeutic target potential due critical roles malignant tumors pulmonary fibrosis inflammation neuropathic pain recent studies shown small molecule ono effectively disrupt overall structure lpa solution however effects native membrane environment remain unclear according traditional chinese medicine systems pharmacology database tcms dan shen salvia miltiorrhiza exhibits anti inflammatory anti pulmonary fibrosis properties natural compound cryptoxanthin may serve novel lpa inhibitor employed microsecond scale atom molecular dynamics simulations systematically compare structural modulation lpa ono cryptoxanthin membrane environment results indicate ligands enlarge entrance lpa ligand binding channel weaken interactions transmembrane helix structural communities reduce hydrophobic interactions lpa membrane thereby inducing membrane structural perturbations notably cryptoxanthin exerts pronounced effect widening binding channel selectively attenuating community interactions findings provide atomic level insights small molecules modulate lpa structure physiologically relevant membrane environment mechanistic understanding offers theoretical basis rational design lpa targeted therapeutics also supports discovery natural product derived inhibitors viable drug candidates moreover results highlight potential allosteric sites dynamic features lpa could exploited develop highly selective modulators paving way precise therapeutic interventions lpa related diseases
"Cancer presents a significant challenge in the field of public health due to its high incidence, mortality rate, and inherent heterogeneity. Integrating multi-omics biological data offers a comprehensive and intricate understanding of biological processes, disease mechanisms, and cancer subtyping, rendering it an influential tool for scientific research. Nevertheless, current approaches to integrating multi-omics data often fail to consider the scale of data in feature information and overlook the analysis of the individual and shared feature expressions in multi-omics data. The proposed study introduces a Multiview-Cooperated graph neural network (MCgnn), an end-to-end cancer subtype classifier that effectively integrates and analyzes complex multi-omics data. Firstly, MCgnn innovatively constructs a similarity network using Mahalanobis distance and density methods. Then, by employing stacked graph convolution layers, MCgnn effectively captures potential local structural features. Subsequently, MCgnn extracts and fuses complementary information from different omics data through the attention mechanism among views to achieve effective integration of multiple views and attain higher classification performance. Finally, MCgnn performs multi-task learning across omics data using a cross-omics tensor to seamlessly integrate the feature learning component with the classification component. Experiments on four publicly available The Cancer Genome Atlas (TCGA) datasets were conducted to demonstrate that MCgnn outperforms most comparison classification algorithms in effectively addressing the cancer subtype classification problem and exhibits remarkable robustness and generalization capabilities. Additionally, MCgnn was employed to identify pivotal biomarkers in cancer, providing a valuable reference for precision medicine.",cancer presents significant challenge field public health due high incidence mortality rate inherent heterogeneity integrating multi omics biological data offers comprehensive intricate understanding biological processes disease mechanisms cancer subtyping rendering influential tool scientific research nevertheless current approaches integrating multi omics data often fail consider scale data feature information overlook analysis individual shared feature expressions multi omics data proposed study introduces multiview cooperated graph neural network mcgnn end end cancer subtype classifier effectively integrates analyzes complex multi omics data firstly mcgnn innovatively constructs similarity network using mahalanobis distance density methods employing stacked graph convolution layers mcgnn effectively captures potential local structural features subsequently mcgnn extracts fuses complementary information different omics data attention mechanism among views achieve effective integration multiple views attain higher classification performance finally mcgnn performs multi task learning across omics data using cross omics tensor seamlessly integrate feature learning component classification component experiments four publicly available cancer genome atlas tcga datasets conducted demonstrate mcgnn outperforms comparison classification algorithms effectively addressing cancer subtype classification problem exhibits remarkable robustness generalization capabilities additionally mcgnn employed identify pivotal biomarkers cancer providing valuable reference precision medicine
"The goal of this paper is to analyze the pointwise controllability properties of a one-dimensional degenerate/singular parabolic equation. We prove the conditions that characterize approximate and null controllability. Besides, a numerical simulation based on B-splines is provided, in which both the state and the control are represented in terms of B-spline basis functions. The numerical results obtained match the theoretical ones.",goal paper analyze pointwise controllability properties one dimensional degenerate singular parabolic equation prove conditions characterize approximate null controllability besides numerical simulation based splines provided state control represented terms spline basis functions numerical results obtained match theoretical ones
"The interaction between cancer and the immune system plays a vital role in tumor development, progression, and treatment, as well as in the advancement of effective cancer therapies. This paper explores the dynamic interaction between immune cells and cancerous cells, highlighting the critical role of the immune system in cancer prevention and treatment. A fractional-order model is developed to examine the influence of T-helper cells, cytotoxic T cells, B cells, and antibodies on cancerous cells. The model is thoroughly analyzed for feasibility and solution positivity. Additionally, the existence and uniqueness criteria, along with possible equilibrium points, are established. Conditions for both local and global asymptotic stability of equilibrium points are derived. Finally, numerical simulations validate the theoretical findings and emphasize key parameters affecting cancerous cell dynamics. The findings demonstrate that cytotoxic T cells and antibodies are vital in targeting and eliminating cancer cells, thereby strengthening the immune response. Additionally, the memory effect inherent in the fractional-order derivative profoundly shapes the system’s dynamics.",interaction cancer immune system plays vital role tumor development progression treatment well advancement effective cancer therapies paper explores dynamic interaction immune cells cancerous cells highlighting critical role immune system cancer prevention treatment fractional order model developed examine influence helper cells cytotoxic cells cells antibodies cancerous cells model thoroughly analyzed feasibility solution positivity additionally existence uniqueness criteria along possible equilibrium points established conditions local global asymptotic stability equilibrium points derived finally numerical simulations validate theoretical findings emphasize key parameters affecting cancerous cell dynamics findings demonstrate cytotoxic cells antibodies vital targeting eliminating cancer cells thereby strengthening immune response additionally memory effect inherent fractional order derivative profoundly shapes system dynamics
"Alcohol abuse and dependence induce neurobiological and behavioral changes in individuals with alcohol use disorder (AUD). Few studies have explored AUD-induced changes in proteins to repurpose drugs that interfere with alcohol withdrawal symptoms and relapse. This multi-level analysis identified the most prevalent changes in protein expression in the prefrontal cortex (PFC) of individuals with AUD and identified candidate drugs with potential benefits for its treatment. We searched databases for differentially expressed proteins (DEPs) in the PFC of individuals with AUD and matched controls, to identify hub and bottleneck genes through a protein-protein interaction network and topological analysis. DrugBank was used to provide candidate drugs targeting the key genes obtained. Metascape was used for gene annotations and functional enrichment analysis. Most DEPs identified in the PFC of individuals with AUD were downregulated (68 %) and were associated with metabolic processes, including aerobic respiration, vesicle-mediated transport, and cellular responses to stress. We also identified key genes through survival analysis, particularly GAPDH (Glyceraldehyde 3-phosphate dehydrogenase) and ACTB (Actin Beta). The DrugBank database identified that artenimol and quercetin are potential candidate drugs for interacting with key genes in the PFC of individuals with AUD. Thus, reduced PFC metabolism may alter executive functions, decision-making, and behaviors in individuals with AUD. Proteomic and multi-level analysis are valuable tools for identifying brain dysfunction and new therapeutic targets in AUD.",alcohol abuse dependence induce neurobiological behavioral changes individuals alcohol use disorder aud studies explored aud induced changes proteins repurpose drugs interfere alcohol withdrawal symptoms relapse multi level analysis identified prevalent changes protein expression prefrontal cortex pfc individuals aud identified candidate drugs potential benefits treatment searched databases differentially expressed proteins deps pfc individuals aud matched controls identify hub bottleneck genes protein protein interaction network topological analysis drugbank used provide candidate drugs targeting key genes obtained metascape used gene annotations functional enrichment analysis deps identified pfc individuals aud downregulated associated metabolic processes including aerobic respiration vesicle mediated transport cellular responses stress also identified key genes survival analysis particularly gapdh glyceraldehyde phosphate dehydrogenase actb actin beta drugbank database identified artenimol quercetin potential candidate drugs interacting key genes pfc individuals aud thus reduced pfc metabolism may alter executive functions decision making behaviors individuals aud proteomic multi level analysis valuable tools identifying brain dysfunction new therapeutic targets aud
"Robust quantification of uncertainty regarding the number of change-points presents a significant challenge in data analysis, particularly when employing false discovery rate (FDR) control techniques. Emphasizing the detection of genuine signals while controlling false positives is crucial, especially for identifying shifts in location parameters within flexible distributions. Traditional parametric methods often exhibit sensitivity to outliers and heavy-tailed data. Addressing this limitation, a robust method accommodating diverse data structures is proposed. The approach constructs component-wise sign-based statistics. Leveraging the global symmetry inherent in these statistics enables the derivation of data-driven thresholds suitable for multiple testing scenarios. Method development occurs within the framework of U-statistics, which naturally encompasses existing cumulative sum-based procedures. Theoretical guarantees establish FDR control for the component-wise sign-based method under mild assumptions. Demonstrations of effectiveness utilize simulations with synthetic data and analyses of real data.",robust quantification uncertainty regarding number change points presents significant challenge data analysis particularly employing false discovery rate fdr control techniques emphasizing detection genuine signals controlling false positives crucial especially identifying shifts location parameters within flexible distributions traditional parametric methods often exhibit sensitivity outliers heavy tailed data addressing limitation robust method accommodating diverse data structures proposed approach constructs component wise sign based statistics leveraging global symmetry inherent statistics enables derivation data driven thresholds suitable multiple testing scenarios method development occurs within framework statistics naturally encompasses existing cumulative sum based procedures theoretical guarantees establish fdr control component wise sign based method mild assumptions demonstrations effectiveness utilize simulations synthetic data analyses real data
"This study investigates potential inhibitors of nitric oxide (NO) production, α-glucosidase, and protein tyrosine phosphatase 1B (PTP1B) from Amanita abrupta and Amanita pantherina. Sixteen compounds (1–16) were isolated by chromatographic techniques and high-performance liquid chromatography (HPLC), and their structures were elucidated by nuclear magnetic resonance (NMR) and literature comparison. Biological activities were evaluated via in vitro assays: the Griess assay for NO inhibition, the p-nitrophenol assay for α-glucosidase, and enzymatic assays for PTP1B. Molecular docking was employed to investigate protein–ligand interactions. The isolates included alanine (1), threonine (2), 2-amino-4-pentynoic acid (3), 4-hydroxy-2-pyrrolidinone (4), ibotenic acid (5), serotonin (6), uracil (7), hypoxanthine (8), bufotenine (9), β-sitosterol (10), stigmasterol (11), cycloeucalenol (12), spinasterol (13), daucosterol (14), stigmasterol-3-O-β-D-glucoside (15), and betulin (16). Compounds 1 and 2 exhibited weak NO inhibition (IC50 = 68.76 and 52.47 μM). Compound 14 displayed the strongest α-glucosidase inhibition (IC50 = 10.56 μM), while compounds 10–13 and 16 showed moderate effects (IC50 = 19.52–32.14 μM). Serotonin (6) also had moderate activity (IC50 = 78.23 μM). For PTP1B inhibition, betulin (16) was most potent (IC50 = 16.80 μM), followed by compounds 10, 11, and 13 with weaker activities (IC50 = 36.28–48.17 μM). Overall, selected compounds (1, 2, 10–14, and 16) exhibited promising bioactivities, warranting further docking studies against inducible nitric oxide synthase (iNOS), PTP1B, and 3AJ7. ADMET (absorption, distribution, metabolism, excretion, and toxicity) predictions supported favorable pharmacokinetic properties and potential drug-likeness, highlighting these molecules as leads for anti-inflammatory and anti-diabetic drug discovery.",study investigates potential inhibitors nitric oxide production glucosidase protein tyrosine phosphatase ptp amanita abrupta amanita pantherina sixteen compounds isolated chromatographic techniques high performance liquid chromatography hplc structures elucidated nuclear magnetic resonance nmr literature comparison biological activities evaluated via vitro assays griess assay inhibition nitrophenol assay glucosidase enzymatic assays ptp molecular docking employed investigate protein ligand interactions isolates included alanine threonine amino pentynoic acid hydroxy pyrrolidinone ibotenic acid serotonin uracil hypoxanthine bufotenine sitosterol stigmasterol cycloeucalenol spinasterol daucosterol stigmasterol glucoside betulin compounds exhibited weak inhibition compound displayed strongest glucosidase inhibition compounds showed moderate effects serotonin also moderate activity ptp inhibition betulin potent followed compounds weaker activities overall selected compounds exhibited promising bioactivities warranting docking studies inducible nitric oxide synthase inos ptp admet absorption distribution metabolism excretion toxicity predictions supported favorable pharmacokinetic properties potential drug likeness highlighting molecules leads anti inflammatory anti diabetic drug discovery
"Dementia represents a rapidly rising global health challenge as a progressive neurodegenerative disease with few options for disease-modifyingtreatments. The present studyaimed to explore the leading phytochemicals from Crocus sativus (saffron) and Matricaria chamomilla (chamomile) and apply AI fragmentation on lead phytochemicals to target the aryl hydrocarbon receptor (AHR), an expertized target for dementia therapy. Bioactive compounds were screened from ISO 3632–2–2010 (E) specified for saffron and GC-MS specified for chamomile. Protein Network mapping, Density Functional Theory, Molecular docking, and molecular dynamics simulations were performed to determine thebinding affinity and interactions stability of key phytochemicals with AHR, such as safranal and bisabolone oxide A. In-silico ADMET predictions of pharmacokinetics and toxicity showed good properties for these molecules. In addition, their structuraland pharmacological properties were optimized to enhance drug-like features by using artificial intelligence (AI) generative model. Collectively, our findings highlight these AI-enhanced phytochemicals as promising AHR modulators with potentially therapeutic activities in pathological pathways that lead toneuroinflammation and oxidative stress involved in the pathogenesis of dementia. They offer an avenue for additional experimental validation and encourage further investigation of these leads as sources of new therapeutic modalities to treat neurodegenerativediseases.",dementia represents rapidly rising global health challenge progressive neurodegenerative disease options disease modifyingtreatments present studyaimed explore leading phytochemicals crocus sativus saffron matricaria chamomilla chamomile apply fragmentation lead phytochemicals target aryl hydrocarbon receptor ahr expertized target dementia therapy bioactive compounds screened iso specified saffron specified chamomile protein network mapping density functional theory molecular docking molecular dynamics simulations performed determine thebinding affinity interactions stability key phytochemicals ahr safranal bisabolone oxide silico admet predictions pharmacokinetics toxicity showed good properties molecules addition structuraland pharmacological properties optimized enhance drug like features using artificial intelligence generative model collectively findings highlight enhanced phytochemicals promising ahr modulators potentially therapeutic activities pathological pathways lead toneuroinflammation oxidative stress involved pathogenesis dementia offer avenue additional experimental validation encourage investigation leads sources new therapeutic modalities treat neurodegenerativediseases
"Elevated temperatures during gear grinding readily induce workpiece surface quality degradation, diminished machining accuracy, and premature tool wear, consequently constraining processing efficiency and escalating manufacturing costs. Ultrasonic vibration-assisted grinding (UVAG) technology, as a powerful thermal mitigation strategy, demonstrates considerable promise for enhancing grinding process stability and improving workpiece surface quality. However, a systematic understanding of the influence mechanism of UVAG on the grinding temperature field and its efficacy optimization in gear machining remains inadequately supported by current theoretical and experimental investigations. To address this gap, this study establishes a theoretical model for the temperature field in gear ultrasonic vibration-assisted form grinding (G-UVAG). The model combines a triangular distribution of heat flux density with the unique characteristics of UVAG. The model incorporates the modulation mechanism of ultrasonic vibration on heat flux distribution, thereby enabling a more precise characterization of the transient heat field in the grinding zone. The findings reveal excellent agreement between the flank temperature predicted by the model and experimentally measured values. In contrast to conventional grinding (CG) methods, the G-UVAG technique achieves a reduction of up to 40 % in the maximum temperature observed within the grinding zone. The deviation among simulation and experimental results is constrained within 9.8 % for CG and 12 % for G-UVAG. Furthermore, G-UVAG effectively diminishes the heat-affected zone depth, shortens the duration of the temperature peak within the tooth flank contact zone, and accelerates both the heating and cooling rates. Analysis indicates that ultrasonic vibration not only promotes a more homogeneous distribution of the grinding heat source but also effectively mitigates the risks of thermally induced deformation and surface burn by suppressing localized temperature spikes. This study establishes an essential theoretical basis for precision gear grinding while broadening the application avenues of UVAG technology to support the realization of efficient and high-quality gear production.",elevated temperatures gear grinding readily induce workpiece surface quality degradation diminished machining accuracy premature tool wear consequently constraining processing efficiency escalating manufacturing costs ultrasonic vibration assisted grinding uvag technology powerful thermal mitigation strategy demonstrates considerable promise enhancing grinding process stability improving workpiece surface quality however systematic understanding influence mechanism uvag grinding temperature field efficacy optimization gear machining remains inadequately supported current theoretical experimental investigations address gap study establishes theoretical model temperature field gear ultrasonic vibration assisted form grinding uvag model combines triangular distribution heat flux density unique characteristics uvag model incorporates modulation mechanism ultrasonic vibration heat flux distribution thereby enabling precise characterization transient heat field grinding zone findings reveal excellent agreement flank temperature predicted model experimentally measured values contrast conventional grinding methods uvag technique achieves reduction maximum temperature observed within grinding zone deviation among simulation experimental results constrained within uvag furthermore uvag effectively diminishes heat affected zone depth shortens duration temperature peak within tooth flank contact zone accelerates heating cooling rates analysis indicates ultrasonic vibration promotes homogeneous distribution grinding heat source also effectively mitigates risks thermally induced deformation surface burn suppressing localized temperature spikes study establishes essential theoretical basis precision gear grinding broadening application avenues uvag technology support realization efficient high quality gear production
"The motor control board has various defects such as inconsistent color differences, incorrect plug-in positions, solder short circuits, and more. These defects directly affect the performance and stability of the motor control board, thereby having a negative impact on product quality. Therefore, studying the defect detection technology of the motor control board is an important means to improve the quality control level of the motor control board. Firstly, the processing methods of digital images about the motor control board were studied, and the noise suppression methods that affect image feature extraction were analyzed. Secondly, a specific model for defect feature extraction and color difference recognition of the tested motor control board was established, and qualified or defective products were determined based on feature thresholds. Thirdly, the search algorithm for defective images was optimized. Finally, comparative experiments were conducted on the typical motor control board, and the experimental results demonstrate that the accuracy of the motor control board defect detection model-based on image processing established in this paper reached over 99 %. It is suitable for timely image processing of large quantities of motor control boards on the production line, and achieved efficient defect detection. The defect detection method can not only be used for online detection of the motor control board defects, but also provide solutions for the integrated circuit board defect processing for the industry.",motor control board various defects inconsistent color differences incorrect plug positions solder short circuits defects directly affect performance stability motor control board thereby negative impact product quality therefore studying defect detection technology motor control board important means improve quality control level motor control board firstly processing methods digital images motor control board studied noise suppression methods affect image feature extraction analyzed secondly specific model defect feature extraction color difference recognition tested motor control board established qualified defective products determined based feature thresholds thirdly search algorithm defective images optimized finally comparative experiments conducted typical motor control board experimental results demonstrate accuracy motor control board defect detection model based image processing established paper reached suitable timely image processing large quantities motor control boards production line achieved efficient defect detection defect detection method used online detection motor control board defects also provide solutions integrated circuit board defect processing industry
"In this paper, we propose an efficient temporal splitting scheme coupled with explicit-implicit-null (EIN) method for nonlinear time fractional multiscale diffusion equations with generalized kernel functions. To avoid the iterative method for nonlinear problems at each time step, the idea of EIN is to add and subtract a linear term to the original equation and apply implicit and explicit time marching scheme to the linear damping term and other terms, respectively. To handle the multiscale feature in the linear part, we further introduce a partially explicit temporal splitting scheme and construct two multiscale subspaces by means of the nonlocal multi-continua method to speed up the computation. The splitting schemes treat implicitly the subspace containing the high contrast and explicitly the other. We perform a rigorous stability analysis of the proposed algorithm and give the stability condition. Several numerical experiments are presented to confirm our theoretical results and demonstrate that the proposed algorithm achieves high accuracy while reducing computational cost for high contrast problems.",paper propose efficient temporal splitting scheme coupled explicit implicit null ein method nonlinear time fractional multiscale diffusion equations generalized kernel functions avoid iterative method nonlinear problems time step idea ein add subtract linear term original equation apply implicit explicit time marching scheme linear damping term terms respectively handle multiscale feature linear part introduce partially explicit temporal splitting scheme construct two multiscale subspaces means nonlocal multi continua method speed computation splitting schemes treat implicitly subspace containing high contrast explicitly perform rigorous stability analysis proposed algorithm give stability condition several numerical experiments presented confirm theoretical results demonstrate proposed algorithm achieves high accuracy reducing computational cost high contrast problems
"The accurate identification and classification of medicinal plants are crucial for botanical research, pharmacology, and traditional medicine, where wrong identification or categorization of the plant species may lead to worse medical effects. In this research, MediFlora-Net, a novel Deep Learning (DL) model is created and ideal for the accurate identification of medicinal plants. The proposed MediFlora-Net uses multi-modal DL methodologies, quantum-assisted feature extraction and hybrid ensembling methodologies in constructing the plant recognition model. Besides, the methodology uses Vision Transformer (ViT), Convolutional Neural Networks (CNNs) and Proposed Med-Plant-Generative Adversarial Networks (GANs). This makes the framework to be capable of handling multiple imaging modalities such as RGB, and Hyperspectral Botanical Imagery. Also, a new quantum-inspired feature extraction technique is integrated into the model in which quantum probabilistic feature mapping and entanglement-based representation are utilized to extract higher-order botanical features. The framework also includes separate ‘feature fusion’, fine-tuned attention, and probabilistic decision-making. The proposed MediFlora-Net advances medicinal plant identification to greater precision and flexibility for practical use in the conservation of biological diversity, ethnobotanical studies, and pharmacology. This work effectively exploits DL techniques and quantum-inspired approaches to tackle the inherent issues of botanical identification to enable the design of better-advanced systems of plant identification. The implementation and source code are available at https://github.com/kvuma02-svg/MEDICINAL-PLANT-IDENTIFICATION.",accurate identification classification medicinal plants crucial botanical research pharmacology traditional medicine wrong identification categorization plant species may lead worse medical effects research mediflora net novel deep learning model created ideal accurate identification medicinal plants proposed mediflora net uses multi modal methodologies quantum assisted feature extraction hybrid ensembling methodologies constructing plant recognition model besides methodology uses vision transformer vit convolutional neural networks cnns proposed med plant generative adversarial networks gans makes framework capable handling multiple imaging modalities rgb hyperspectral botanical imagery also new quantum inspired feature extraction technique integrated model quantum probabilistic feature mapping entanglement based representation utilized extract higher order botanical features framework also includes separate feature fusion fine tuned attention probabilistic decision making proposed mediflora net advances medicinal plant identification greater precision flexibility practical use conservation biological diversity ethnobotanical studies pharmacology work effectively exploits techniques quantum inspired approaches tackle inherent issues botanical identification enable design better advanced systems plant identification implementation source code available https github com kvuma svg medicinal plant identification
"Breast cancer remains a leading cause of mortality among women, necessitating the development of non-invasive diagnostic methods. Formaldehyde (FA) has emerged as a potential biomarker for early detection of breast cancer in urine. This study explores the efficacy of boron-nitrogen-doped C60 heterofullerenes (BN(5,6)C58 and BN(6,6)C58) as highly sensitive and selective biosensors for FA detection using density functional theory (DFT). A comprehensive set of electronic, thermodynamic, and quantum chemical descriptors was employed to evaluate the sensing potential. Key computed parameters (including a significantly reduced energy gap (HLG = 0.49 eV), a high adsorption energy (Eads = −12.55 kcal/mol), a favorable Gibbs free energy change (ΔG = −12.73 kcal/mol), an enhanced dipole moment (μ = 7.425 D), increased polarizability (α = 525.640), and non-covalent interaction (NCI) analysis) collectively confirmed that BN doping significantly enhances the interaction strength with FA, with BN(6,6)C58 exhibiting the highest sensitivity (1.9 ×1017). Electronic property analyses demonstrated a reduced energy gap and enhanced charge transfer in BN(6,6)C58@FA, corroborated by molecular electrostatic potential and NCI analyses. The sensor's rapid recovery time (1.65 ×10−3 s) and high electrical conductivity (16 A.m−2) further underscore its potential for real-time breath analysis. These findings highlight BN(6,6)C58 as a promising candidate for non-invasive breast cancer diagnostics, paving the way for developing advanced electrochemical biosensors.",breast cancer remains leading cause mortality among women necessitating development non invasive diagnostic methods formaldehyde emerged potential biomarker early detection breast cancer urine study explores efficacy boron nitrogen doped heterofullerenes highly sensitive selective biosensors detection using density functional theory dft comprehensive set electronic thermodynamic quantum chemical descriptors employed evaluate sensing potential key computed parameters including significantly reduced energy gap hlg high adsorption energy eads kcal mol favorable gibbs free energy change kcal mol enhanced dipole moment increased polarizability non covalent interaction nci analysis collectively confirmed doping significantly enhances interaction strength exhibiting highest sensitivity electronic property analyses demonstrated reduced energy gap enhanced charge transfer corroborated molecular electrostatic potential nci analyses sensor rapid recovery time high electrical conductivity underscore potential real time breath analysis findings highlight promising candidate non invasive breast cancer diagnostics paving way developing advanced electrochemical biosensors
"Antimicrobial resistance (AMR) poses a growing global threat, with antibiotic-resistant infections becoming a leading cause of death worldwide. The present study explores natural cyanobacterial compounds as possible inhibitors of Escherichia coli DNA gyrase B (GyrB) which is a verified antibacterial target that is not present in higher eukaryotes. Because of the urgent need for novel antibacterial drugs, we identified nine drug-like candidates using lipinski's rule of five and ADMET profiling. Molecular docking revealed that Biselyngbyaside B and Smenamide A exhibited greater binding affinities in comparison to the co-crystallized inhibitor EOF, with a binding energy of −9.03 kcal/mol. Further molecular dynamics simulations revealed that the Biselyngbyaside B-DNA gyrase B complex surpassed both EOF and Smenamide A in terms of structural stability, compactness, and strong hydrogen bonding. Umbrella sampling was employed to estimate the binding free energy from thirty sampling simulations, and Biselyngbyaside B exhibited a significantly favourable ΔG bind of −91.66 kJ/mol, outperforming EOF (-68.93 kJ/mol) and Smenamide A (-36.4 kJ/mol). These findings clearly indicate a stronger and more stable interaction between Biselyngbyaside B and GyrB. Biselyngbyaside B continuously showed better pharmacokinetic characteristics, non-hepatotoxicity, and a greater binding affinity than previously documented DNA gyrase B inhibitors. This study emphasizes the integration of molecular dockings, molecular dynamics simulation, umbrella sampling, and ADMET analysis provided crucial quantitative insights into the identification of potent drug-like candidates for further validation. Overall, the Biselyngbyaside B was found to be the most promising lead compound for novel antibacterial drug development targeting DNA gyrase B.",antimicrobial resistance amr poses growing global threat antibiotic resistant infections becoming leading cause death worldwide present study explores natural cyanobacterial compounds possible inhibitors escherichia coli dna gyrase gyrb verified antibacterial target present higher eukaryotes urgent need novel antibacterial drugs identified nine drug like candidates using lipinski rule five admet profiling molecular docking revealed biselyngbyaside smenamide exhibited greater binding affinities comparison crystallized inhibitor eof binding energy kcal mol molecular dynamics simulations revealed biselyngbyaside dna gyrase complex surpassed eof smenamide terms structural stability compactness strong hydrogen bonding umbrella sampling employed estimate binding free energy thirty sampling simulations biselyngbyaside exhibited significantly favourable bind mol outperforming eof mol smenamide mol findings clearly indicate stronger stable interaction biselyngbyaside gyrb biselyngbyaside continuously showed better pharmacokinetic characteristics non hepatotoxicity greater binding affinity previously documented dna gyrase inhibitors study emphasizes integration molecular dockings molecular dynamics simulation umbrella sampling admet analysis provided crucial quantitative insights identification potent drug like candidates validation overall biselyngbyaside found promising lead compound novel antibacterial drug development targeting dna gyrase
"Parents of children with ASD face significantly greater parenting challenges than those raising typically developing children due to prolonged exposure to their children’s developmental disorders, emotional distress, and atypical behaviors, underscoring the urgency of addressing their mental health concerns. This study examined the relationship between fear of negative evaluation (FNE) and social anxiety in parents of children with ASD, with a focus on the mediating roles of perceived social support and coping styles. A cross-sectional survey was conducted among 585 parents of children with ASD using validated instruments: the Brief Fear of Negative Evaluation Scale (BFNE), the Social Anxiety Scale, the Perceived Social Support Scale, and the Simple Coping Style Questionnaire. Data were analyzed through SPSS 29.0 and AMOS 25.0 for structural equation modeling. Key findings revealed: (1) FNE directly predicted social anxiety, with significant standardized direct effects of 0.179 and 0.159 across tested models (p < 0.001); (2) Perceived social support, positive coping styles, and negative coping styles each partially mediated the FNE–social anxiety relationship, with indirect effects of 0.035 (95 % CI [0.025, 0.066]), 0.096 (95 % CI [0.181, 0.242]), and 0.110 (95 % CI [0.115, 0.126]), respectively; (3) Significant chain mediation pathways emerged through perceived social support → positive coping styles (effect = 0.008, 95 % CI [0.012, 0.016]) and perceived social support → negative coping styles (effect = 0.014, 95 % CI [0.031, 0.033]). These findings elucidate the mechanisms linking FNE to social anxiety in this population and provide theoretical foundations for targeted interventions to mitigate parental distress.",parents children asd face significantly greater parenting challenges raising typically developing children due prolonged exposure children developmental disorders emotional distress atypical behaviors underscoring urgency addressing mental health concerns study examined relationship fear negative evaluation fne social anxiety parents children asd focus mediating roles perceived social support coping styles cross sectional survey conducted among parents children asd using validated instruments brief fear negative evaluation scale bfne social anxiety scale perceived social support scale simple coping style questionnaire data analyzed spss amos structural equation modeling key findings revealed fne directly predicted social anxiety significant standardized direct effects across tested models perceived social support positive coping styles negative coping styles partially mediated fne social anxiety relationship indirect effects respectively significant chain mediation pathways emerged perceived social support positive coping styles effect perceived social support negative coping styles effect findings elucidate mechanisms linking fne social anxiety population provide theoretical foundations targeted interventions mitigate parental distress
"With the rapid development of the digital animation industry, the role of dynamic light and shadow effects in improving the visual expressiveness of animation works has become increasingly prominent. However, traditional light and shadow design methods often rely on manual adjustment, which is inefficient and difficult to achieve complex dynamic effects. Therefore, this study proposes an animation effect design model based on a dynamic light and shadow algorithm, aiming at achieving efficient and realistic light and shadow effects through algorithm-driven. Firstly, the limitations of existing light and shadow algorithms in animation applications are analyzed, and then a dynamic light and shadow algorithm which combines a physical illumination model and artistic style is designed according to the characteristics of animation art. The algorithm calculates the position, intensity, and surface properties of the light source in real-time and generates light and shadow effects that conform to the animation style. In order to verify the effectiveness of the model, an experimental platform was constructed in this study, and various animation scenes were tested. The experimental results show that compared with the traditional manual design, the model improves the efficiency of light and shadow effect generation by about 60%, and at the same time, it won more than 80% of the audience’s praise in the visual effect satisfaction survey. In addition, the research also optimizes the performance of the model. By introducing parallel computing and caching mechanisms, the light and shadow generation time is further shortened, and the resource consumption is reduced. This study not only provides a new technical path for animation light and shadow design but also provides a useful reference for the research in related fields.",rapid development digital animation industry role dynamic light shadow effects improving visual expressiveness animation works become increasingly prominent however traditional light shadow design methods often rely manual adjustment inefficient difficult achieve complex dynamic effects therefore study proposes animation effect design model based dynamic light shadow algorithm aiming achieving efficient realistic light shadow effects algorithm driven firstly limitations existing light shadow algorithms animation applications analyzed dynamic light shadow algorithm combines physical illumination model artistic style designed according characteristics animation art algorithm calculates position intensity surface properties light source real time generates light shadow effects conform animation style order verify effectiveness model experimental platform constructed study various animation scenes tested experimental results show compared traditional manual design model improves efficiency light shadow effect generation time audience praise visual effect satisfaction survey addition research also optimizes performance model introducing parallel computing caching mechanisms light shadow generation time shortened resource consumption reduced study provides new technical path animation light shadow design also provides useful reference research related fields
"With the rapid development of natural language processing technology, the importance of English word sense comparison classification has become increasingly prominent in areas such as semantic understanding and machine translation. The research results in this field play a crucial role in enhancing the performance of language processing systems. However, most existing English word sense comparison classification methods currently have significant shortcomings when dealing with complex semantic relationships. They struggle to accurately capture long-distance dependencies between words and exhibit weak discrimination capabilities when handling polysemy and synonyms, posing challenges in building high-performance comparison classification systems. The limitations of existing methods are mainly reflected in two aspects: on one hand, traditional models find it difficult to effectively extract semantic features when processing long sentences and complex semantic structures; on the other hand, a single model cannot simultaneously account for sequence information retention and long-distance dependency capture, leading to limited classification accuracy. This research focuses on these issues by proposing the construction of an English word sense comparison classification system using Transformer and LSTM. The self-attention mechanism of the Transformer can effectively capture long-distance dependencies between words, enhancing the ability to extract semantic features; the memory functionality of LSTM preserves sequence information, and the combination of the two optimizes classification performance. In our experiments, we constructed an English word sense comparison dataset containing 10,000-word pairs and used cross-validation for evaluation. The results show that, compared to the classification accuracy of the single Transformer model at 80.2% and the LSTM model at 78.9%, the Transformer LSTM model improved the classification accuracy to 85.6%, achieving an average percentage improvement of 5.4% and 6.7%. In the comparison tasks dealing with polysemy and synonyms, the model reached accuracies of 88.3% and 86.1%, respectively, demonstrating excellent discrimination capabilities and stable performance in handling complex semantic relationships and long sentences, significantly outperforming traditional models.",rapid development natural language processing technology importance english word sense comparison classification become increasingly prominent areas semantic understanding machine translation research results field play crucial role enhancing performance language processing systems however existing english word sense comparison classification methods currently significant shortcomings dealing complex semantic relationships struggle accurately capture long distance dependencies words exhibit weak discrimination capabilities handling polysemy synonyms posing challenges building high performance comparison classification systems limitations existing methods mainly reflected two aspects one hand traditional models find difficult effectively extract semantic features processing long sentences complex semantic structures hand single model simultaneously account sequence information retention long distance dependency capture leading limited classification accuracy research focuses issues proposing construction english word sense comparison classification system using transformer lstm self attention mechanism transformer effectively capture long distance dependencies words enhancing ability extract semantic features memory functionality lstm preserves sequence information combination two optimizes classification performance experiments constructed english word sense comparison dataset containing word pairs used cross validation evaluation results show compared classification accuracy single transformer model lstm model transformer lstm model improved classification accuracy achieving average percentage improvement comparison tasks dealing polysemy synonyms model reached accuracies respectively demonstrating excellent discrimination capabilities stable performance handling complex semantic relationships long sentences significantly outperforming traditional models
"To investigate differential effects of multicultural interactions (MIs) and exposures (MEs) and to test Maddux et al.’s new structural model of multicultural experience, this research examines the impact of MIs and MEs on intrapersonal versus interpersonal creative thinking. Study 1 tests the associations of MIs and MEs with creative thinking in idea generation and conceptual conflict resolution. Study 2 examines MIs’ and MEs’ relationship with creative thinking in applied conflict resolution (i.e., international business negotiation), with closed-mindedness as a mediator. Results show that MIs (but not MEs) were positively associated with self-reported (intrapersonal) measures of creative thinking in conflict resolution (CTCR), whereas MEs (but not MIs) were positively associated with creative thinking (interpersonal) outcomes in realistic, business conflict resolution contexts. Closed-mindedness fully mediated the former and partially mediated the latter association.",investigate differential effects multicultural interactions exposures mes test maddux new structural model multicultural experience research examines impact mes intrapersonal versus interpersonal creative thinking study tests associations mes creative thinking idea generation conceptual conflict resolution study examines mes relationship creative thinking applied conflict resolution international business negotiation closed mindedness mediator results show mes positively associated self reported intrapersonal measures creative thinking conflict resolution ctcr whereas mes positively associated creative thinking interpersonal outcomes realistic business conflict resolution contexts closed mindedness fully mediated former partially mediated latter association
"Video data protection is a prominent area of research. Researchers have proposed several coding techniques to ensure the secure and efficient transmission of video data. Compressed code stream video encryption is an advanced method designed to enhance security and protection by encrypting video data at the compressed code stream level. Leveraging the advantages of compressed video streams, this approach maximizes encryption efficiency without compromising playback speed or video quality. In this research, we first compress the raw video data into a compressed code stream using the H.264/AVC technique. A common method encodes multiple independent slices into a video. We utilize this functionality to implement the ChaCha20 stream cipher for encryption. The cryptographic key derivation function (KDF) generates and updates the key dynamically. To enhance the security of video encryption, we propose a novel multi-key verifiable homomorphic lightweight cryptosystem (MKVHLC) to guarantee data privacy. In our experiments, we evaluated the proposed method using various reference video sequences featuring motion, texture, and objects.",video data protection prominent area research researchers proposed several coding techniques ensure secure efficient transmission video data compressed code stream video encryption advanced method designed enhance security protection encrypting video data compressed code stream level leveraging advantages compressed video streams approach maximizes encryption efficiency without compromising playback speed video quality research first compress raw video data compressed code stream using avc technique common method encodes multiple independent slices video utilize functionality implement chacha stream cipher encryption cryptographic key derivation function kdf generates updates key dynamically enhance security video encryption propose novel multi key verifiable homomorphic lightweight cryptosystem mkvhlc guarantee data privacy experiments evaluated proposed method using various reference video sequences featuring motion texture objects
"Ventilation plays a crucial role in influencing airflow and particle dispersion, highlighting the importance of effective systems to mitigate the spread of viruses in indoor spaces. While most research has concentrated on non-residential buildings, less attention has been given to optimizing ventilation in residential settings. This study examines a typical ventilation system in a detached house in British Columbia, Canada, using Computational Fluid Dynamics simulations to evaluate its effectiveness in reducing airborne infection risks. The study investigates eight different scenarios, varying diffuser locations (near the ceiling and floor), two types of ventilation rates based on standards, and the positioning of the infected individual while considering flow, heat, and particle dynamics. The concentration of injected and distributed particles is used to assess infection probabilities as an additional risk indicator. In addition, the Wells-Riley model is applied to quantify the infection probability. The results indicate that the location of the infected person and diffuser placement significantly impact particle dispersion and infection risk, with near-ceiling outlets generally reducing concentrations more effectively than near-floor outlets. Higher ventilation rates decrease particle concentration when diffusers are near the ceiling but can increase concentrations with near-floor diffusers due to turbulence. Optimizing diffuser placement and ventilation rates is crucial for minimizing airborne transmission in residential settings.",ventilation plays crucial role influencing airflow particle dispersion highlighting importance effective systems mitigate spread viruses indoor spaces research concentrated non residential buildings less attention given optimizing ventilation residential settings study examines typical ventilation system detached house british columbia canada using computational fluid dynamics simulations evaluate effectiveness reducing airborne infection risks study investigates eight different scenarios varying diffuser locations near ceiling floor two types ventilation rates based standards positioning infected individual considering flow heat particle dynamics concentration injected distributed particles used assess infection probabilities additional risk indicator addition wells riley model applied quantify infection probability results indicate location infected person diffuser placement significantly impact particle dispersion infection risk near ceiling outlets generally reducing concentrations effectively near floor outlets higher ventilation rates decrease particle concentration diffusers near ceiling increase concentrations near floor diffusers due turbulence optimizing diffuser placement ventilation rates crucial minimizing airborne transmission residential settings
"Teaching and learning of fluid mechanics can be improved by the use of computational fluid dynamics (CFD). Implementation of CFD in term projects is increasingly gaining traction, especially for mechanical engineering students. This paper aims to study the teaching enhancement method through project-based learning of CFD as an effective educational and learning method. This study also aims to examine the ability of students to comprehend the fundamental learning concepts of fluid mechanics. The investigation method used in this study is carried out by examining student performance in the achievement of the project learning concepts (PLCs) and its impact on the course learning outcomes (CLOs). In addition, the student participation in the class increased by 15% compared to previous semesters. One of the most important improvement achieved when using CFD in course teaching is the strong comprehension and improved students’ performance related to the fluid movements, frictional effect, pressure, and velocity variation. The use of simulation and animations in the studied fluid flow case has proven to be effective in enhancing student education and learning of fluid mechanics.",teaching learning fluid mechanics improved use computational fluid dynamics cfd implementation cfd term projects increasingly gaining traction especially mechanical engineering students paper aims study teaching enhancement method project based learning cfd effective educational learning method study also aims examine ability students comprehend fundamental learning concepts fluid mechanics investigation method used study carried examining student performance achievement project learning concepts plcs impact course learning outcomes clos addition student participation class increased compared previous semesters one important improvement achieved using cfd course teaching strong comprehension improved students performance related fluid movements frictional effect pressure velocity variation use simulation animations studied fluid flow case proven effective enhancing student education learning fluid mechanics
"With the rapid use of electric vehicles (EVs), the need for intelligent and real-time fault diagnosis systems for lithium-ion batteries (LIBs) has become critical to ensure safety, efficiency, and extended operational life. Numerous existing fault diagnosis approaches often require complex computations and fail to adapt to nonlinear, dynamic EV conditions. In addition, data-driven models lack real-time capability, exhibit high computational overhead, and are rarely optimized for embedded deployment. This research presents a hybrid DL-based diagnostic framework named Optimized Elman Spotted Hyena Tuned NeuroNet (OESH-Net) for accurate State of Charge (SOC) estimation and fault detection in EV battery systems. The proposed model integrates a Thevenin-equivalent circuit-based LIB model simulated in MATLAB with a recurrent Elman Neural Network (ENN) optimized using the Spotted Hyena Optimizer (SHO) in TensorFlow/Keras. High-frequency sensor data, including voltage, current, temperature, motor speed, and hall signals, are collected and preprocessed using low-pass filtering, sliding window segmentation, and min-max normalization, to create structured feature vectors. Then, the preprocessed data are fed into the OESH-Net for SOC prediction and health classification into Normal, Warning, and Fault states based on residual SOC error thresholds. Experimental results shows that OESH-Net outperforms models’ sin both charging and discharging phases, achieving 98.86% accuracy, 99.22% F1-score, 0.00078 MSE, and 4.85 ms inference time. It maintains robust performance with a mean absolute SOC estimation error of 2.06%. The proposed OESH-Net framework enables predictive maintenance and early fault detection, offering a scalable, efficient, and real-time solution for smart EV battery management systems.",rapid use electric vehicles evs need intelligent real time fault diagnosis systems lithium ion batteries libs become critical ensure safety efficiency extended operational life numerous existing fault diagnosis approaches often require complex computations fail adapt nonlinear dynamic conditions addition data driven models lack real time capability exhibit high computational overhead rarely optimized embedded deployment research presents hybrid based diagnostic framework named optimized elman spotted hyena tuned neuronet oesh net accurate state charge soc estimation fault detection battery systems proposed model integrates thevenin equivalent circuit based lib model simulated matlab recurrent elman neural network enn optimized using spotted hyena optimizer sho tensorflow keras high frequency sensor data including voltage current temperature motor speed hall signals collected preprocessed using low pass filtering sliding window segmentation min max normalization create structured feature vectors preprocessed data fed oesh net soc prediction health classification normal warning fault states based residual soc error thresholds experimental results shows oesh net outperforms models charging discharging phases achieving accuracy score mse inference time maintains robust performance mean absolute soc estimation error proposed oesh net framework enables predictive maintenance early fault detection offering scalable efficient real time solution smart battery management systems
"In the evolving landscape of vocational education, enhancing quality requires accurate, timely, and data-driven evaluation mechanisms that transcend traditional assessment methods. This research presents an AI-powered Feedback Analytics Engine designed to optimize the quality assessment of vocational education by systematically collecting, processing, and analyzing feedback from diverse educational sources. The system aggregates data from student surveys, instructor evaluations, learning management system logs, and classroom observations, enabling a holistic view of the teaching and learning environment. Textual data is preprocessed using natural language techniques (NLP), such as cleaning and tokenization. To address data imbalance challenges, the Synthetic Minority Over-sampling Technique (SMOTE) is employed, ensuring equal representation across various performance levels and enhancing model generalizability. Feature extraction is performed using Word2Vec, which captures semantic relationships within the feedback to generate rich vector representations. A Modified Migrating Birds Optimizer-driven Attention-based Recurrent neural network (MMB-Att-RNN) model is used to evaluate vocational education quality, which enables collaborative, distributed training across multiple institutions while preserving data privacy. An integrated attention mechanism helps the model focus on the most relevant features within the feedback, thereby improving prediction accuracy and interpretability. The proposed engine demonstrates superior performance, significantly outperforming traditional models with an accuracy of 98.42%. The results appear in the form of an interactive dashboard that enables educators and administrators to track the effectiveness of their instruction, areas of improvement, and evidence-based decision making. The AI-powered framework converts raw feedback into relevant intelligence and prepares a new standard of constant quality improvement in vocational education.",evolving landscape vocational education enhancing quality requires accurate timely data driven evaluation mechanisms transcend traditional assessment methods research presents powered feedback analytics engine designed optimize quality assessment vocational education systematically collecting processing analyzing feedback diverse educational sources system aggregates data student surveys instructor evaluations learning management system logs classroom observations enabling holistic view teaching learning environment textual data preprocessed using natural language techniques nlp cleaning tokenization address data imbalance challenges synthetic minority sampling technique smote employed ensuring equal representation across various performance levels enhancing model generalizability feature extraction performed using word vec captures semantic relationships within feedback generate rich vector representations modified migrating birds optimizer driven attention based recurrent neural network mmb att rnn model used evaluate vocational education quality enables collaborative distributed training across multiple institutions preserving data privacy integrated attention mechanism helps model focus relevant features within feedback thereby improving prediction accuracy interpretability proposed engine demonstrates superior performance significantly outperforming traditional models accuracy results appear form interactive dashboard enables educators administrators track effectiveness instruction areas improvement evidence based decision making powered framework converts raw feedback relevant intelligence prepares new standard constant quality improvement vocational education
"Chinese figure painting, with its rich history and diverse styles, represents a significant cultural heritage that captures the essence of Chinese art and aesthetics. However, many traditional figure painting styles, particularly rare ones, remain underrepresented and face considerable challenges in modern-day recognition, categorization, and preservation. This issue is especially prominent in the context of few-shot learning, where models must classify and identify rare categories with only minimal training data. To address this challenge, the research proposes a novel meta-learning-optimized deep feature fusion strategy for the few-shot classification of rare Chinese figure painting styles. The research utilizes a Chinese art styles dataset of Chinese figure paintings, with a specific focus on rare and fine-grained styles. Preprocessing steps such as image resizing and noise removal were employed to enhance data quality. The ResNet-50 model was utilized to extract low-level characteristics from the input images, such as edges, textures, and colors. The features extracted from different layers were then fused using deep feature fusion techniques to create a comprehensive and discriminative representation of the painting styles. This feature fusion process was further optimized through a meta-learning approach based on the Elephant Clan Optimized Attention-based Variational Autoencoder (ECO-Att-VAE) method, which fine-tunes feature weighting based on the support set in the few-shot classification task. Experiments show that the suggested strategy works noticeably better than current methods in the categorization of unusual art styles. Results show that the model achieves a notable improvement in classification accuracy, providing a robust and effective solution for few-shot classification in the domain of rare Chinese figure painting styles.",chinese figure painting rich history diverse styles represents significant cultural heritage captures essence chinese art aesthetics however many traditional figure painting styles particularly rare ones remain underrepresented face considerable challenges modern day recognition categorization preservation issue especially prominent context shot learning models must classify identify rare categories minimal training data address challenge research proposes novel meta learning optimized deep feature fusion strategy shot classification rare chinese figure painting styles research utilizes chinese art styles dataset chinese figure paintings specific focus rare fine grained styles preprocessing steps image resizing noise removal employed enhance data quality resnet model utilized extract low level characteristics input images edges textures colors features extracted different layers fused using deep feature fusion techniques create comprehensive discriminative representation painting styles feature fusion process optimized meta learning approach based elephant clan optimized attention based variational autoencoder eco att vae method fine tunes feature weighting based support set shot classification task experiments show suggested strategy works noticeably better current methods categorization unusual art styles results show model achieves notable improvement classification accuracy providing robust effective solution shot classification domain rare chinese figure painting styles
"This study proposes a new biased extended memory polynomial model and a new clustering enhanced recursive least squares algorithm, which jointly innovate the digital pre-distortion algorithm. Compared with the conventional digital pre-distortion algorithm, it has higher accuracy without increasing complexity. The proposed biased extended memory polynomial model introduces a constant term to explain the DC offset and system error, and shows special effectiveness under low amplitude excitation. At the same time, the clustering enhanced recursive least squares algorithm uses K-means clustering to partition the nonlinear feature space, thereby realizing coefficient propagation between clusters, realizing local parameter updates, and significantly accelerating the convergence speed, improving convergence stability and accuracy.",study proposes new biased extended memory polynomial model new clustering enhanced recursive least squares algorithm jointly innovate digital pre distortion algorithm compared conventional digital pre distortion algorithm higher accuracy without increasing complexity proposed biased extended memory polynomial model introduces constant term explain offset system error shows special effectiveness low amplitude excitation time clustering enhanced recursive least squares algorithm uses means clustering partition nonlinear feature space thereby realizing coefficient propagation clusters realizing local parameter updates significantly accelerating convergence speed improving convergence stability accuracy
"To promote the high-quality development of regional tourism and implement the strategy of building a strong tourism province, tourism efficiency across 16 prefecture-level cities in Anhui Province from 2011 to 2022 was evaluated using the Data Envelopment Analysis (DEA)-Malmquist index. Influencing factors of tourism efficiency were further examined through a Tobit model. The results indicate that, from a static efficiency perspective, only Chizhou was strongly DEA efficient, while the remaining 15 cities were DEA inefficient. From a dynamic efficiency perspective, in the temporaldimension, the total factor productivity (TFP) index of Anhui Province’s tourism industry demonstrated an overall upward trend, with an average annual growth rate of 11.6% in tourism efficiency in the prefecture-level city dimension, an overall increase in tourism efficiency was observed, although variations in growth magnitude were identified. In the regional dimension, the Central Anhui region exhibited the highest tourism efficiency, followed by Southern Anhui, with Northern Anhui displaying the lowest efficiency levels. Economic development and transportation accessibility were found to promote tourism efficiency, while the degree of openness and tourism resource endowment exerted a suppressive effect. Industrial structure was not found to have a statistically significant impact on tourism efficiency.",promote high quality development regional tourism implement strategy building strong tourism province tourism efficiency across prefecture level cities anhui province evaluated using data envelopment analysis dea malmquist index influencing factors tourism efficiency examined tobit model results indicate static efficiency perspective chizhou strongly dea efficient remaining cities dea inefficient dynamic efficiency perspective temporaldimension total factor productivity tfp index anhui province tourism industry demonstrated overall upward trend average annual growth rate tourism efficiency prefecture level city dimension overall increase tourism efficiency observed although variations growth magnitude identified regional dimension central anhui region exhibited highest tourism efficiency followed southern anhui northern anhui displaying lowest efficiency levels economic development transportation accessibility found promote tourism efficiency degree openness tourism resource endowment exerted suppressive effect industrial structure found statistically significant impact tourism efficiency
"Classical Chinese literary texts, such as poetry and prose, are rich in emotional and cultural expression but pose significant challenges for computational analysis due to their archaic language, symbolic structures, and lack of punctuation. Traditional sentiment analysis models often fail to capture the depth and nuance of these texts, limiting their effectiveness in literary interpretation and digital humanities research. This research aims to develop a deep learning (DL)-based RoCoSenti-Classical framework tailored to classical Chinese literature, capable of accurately identifying and classifying sentiments despite the language’s complexity and historical nature. A comprehensive corpus of classical Chinese literary texts was gathered from reputable digital sources. The text data underwent pre-processing steps, including text normalization, tokenization, and stopword removal. Domain-specific language representations were created by fine-tuning RoBERTa embeddings on the classical corpus. These embeddings were then input into a ConvLSTM model, which combines convolutional layers for local feature extraction with LSTM layers for progressive sentiment modeling. Implemented in Python, the findings show that the RoCoSenti-Classical framework performs better than multimodal baseline architectures, achieving superior results with accuracy, F1-score, recall, and precision ranging from 90% to 97%. The suggested RoCoSenti-Classical framework effectively captures complex emotional cues in classical texts, demonstrating robustness across different literary genres and significantly improving sentiment analysis performance for classical Chinese literature. These findings support the broader integration of Artificial Intelligence (AI) in digital humanities and open new avenues for the computational interpretation of ancient texts.",classical chinese literary texts poetry prose rich emotional cultural expression pose significant challenges computational analysis due archaic language symbolic structures lack punctuation traditional sentiment analysis models often fail capture depth nuance texts limiting effectiveness literary interpretation digital humanities research research aims develop deep learning based rocosenti classical framework tailored classical chinese literature capable accurately identifying classifying sentiments despite language complexity historical nature comprehensive corpus classical chinese literary texts gathered reputable digital sources text data underwent pre processing steps including text normalization tokenization stopword removal domain specific language representations created fine tuning roberta embeddings classical corpus embeddings input convlstm model combines convolutional layers local feature extraction lstm layers progressive sentiment modeling implemented python findings show rocosenti classical framework performs better multimodal baseline architectures achieving superior results accuracy score recall precision ranging suggested rocosenti classical framework effectively captures complex emotional cues classical texts demonstrating robustness across different literary genres significantly improving sentiment analysis performance classical chinese literature findings support broader integration artificial intelligence digital humanities open new avenues computational interpretation ancient texts
"With the acceleration of urbanization, intelligent building design has become the key to improving urban comfort. This study aims to explore the influence of humanized spatial layout on urban comfort in intelligent building design. Through field investigation and simulation experiments, it is found that humanized spatial layout plays a significant role in intelligent building design. In a commercial complex, an intelligent guidance system and space streamline design reduce customer stay time by 15% and increase satisfaction by 25%. In the intelligent transformation of residential quarters, intelligent lighting, temperature control, and greening layout optimization have improved residents’ comfort scores from 3.2 to 4.5 and reduced energy consumption by 20%. In addition, in the urban park renovation project, intelligent seats, interactive landscape and navigation systems have increased the number of tourists by 30% and satisfaction by 40%. The results show that the humanized spatial layout in intelligent building design not only improves space use efficiency and user experience but also realizes resource conservation and environmental protection, which has a far-reaching impact on improving urban comfort.",acceleration urbanization intelligent building design become key improving urban comfort study aims explore influence humanized spatial layout urban comfort intelligent building design field investigation simulation experiments found humanized spatial layout plays significant role intelligent building design commercial complex intelligent guidance system space streamline design reduce customer stay time increase satisfaction intelligent transformation residential quarters intelligent lighting temperature control greening layout optimization improved residents comfort scores reduced energy consumption addition urban park renovation project intelligent seats interactive landscape navigation systems increased number tourists satisfaction results show humanized spatial layout intelligent building design improves space use efficiency user experience also realizes resource conservation environmental protection far reaching impact improving urban comfort
"Aiming at the limitation that the laboratory safety education knowledge graph (KG) lacks the ability to mine symbolic rules and is difficult to reason about the implicit relationships in the KG, this paper first represents the entities and relations in the laboratory safety education KG, and then designs neural network encoding and symbolic rule encoding to extract neural-symbolic features, respectively. Contrastive learning technology is used to project the neural network knowledge embedding and symbolic rule knowledge embedding of the same sample into the same vector space, and to bring the same samples closer and push the different samples further apart in the same vector space, achieving further fusion of neural-symbolic features. Experimental results show that the MRR of the proposed model is 0.956, which is better than the comparative model, proving that the model has excellent performance.",aiming limitation laboratory safety education knowledge graph lacks ability mine symbolic rules difficult reason implicit relationships paper first represents entities relations laboratory safety education designs neural network encoding symbolic rule encoding extract neural symbolic features respectively contrastive learning technology used project neural network knowledge embedding symbolic rule knowledge embedding sample vector space bring samples closer push different samples apart vector space achieving fusion neural symbolic features experimental results show mrr proposed model better comparative model proving model excellent performance
"Aiming at the problems of low recovery rate of tailings resources and poor adaptability to dynamic working conditions, this paper proposes a reinforcement learning (RL) optimization method driven by Industrial Internet of Things (IIoT). By deploying a multi-source sensor network for real-time sensing of ore grade, flow rate and equipment status, a digital twin collaborative framework is constructed; a multi-intelligent body depth deterministic policy gradient (MADDPG) algorithm is innovatively designed to realize dynamic decision-making for agent addition and equipment control. Based on the open data set validation, the system recovery rate is increased to 92.3%, and the energy consumption per ton of ore is reduced by 18.7%, which significantly optimizes the dynamic resource recovery efficiency. The method provides key technical support for the green transformation of the mining industry.",aiming problems low recovery rate tailings resources poor adaptability dynamic working conditions paper proposes reinforcement learning optimization method driven industrial internet things iiot deploying multi source sensor network real time sensing ore grade flow rate equipment status digital twin collaborative framework constructed multi intelligent body depth deterministic policy gradient maddpg algorithm innovatively designed realize dynamic decision making agent addition equipment control based open data set validation system recovery rate increased energy consumption per ton ore reduced significantly optimizes dynamic resource recovery efficiency method provides key technical support green transformation mining industry
"To address the issue of quality of experience (QoE) degradation caused by incorrect bitrate selection in film and television media, this paper proposes a dynamic optimization method for film and television media parameters based on reinforcement learning (RL) and causal reasoning. First, based on causal reasoning for RL algorithm optimization (CRPPO), by learning the causal structure of the environment, further construct a causal mask acting on the action space to help reduce the decision space of RL. Then, predict the bandwidth in the next period based on network information, enhance the accuracy of bandwidth prediction to assist the CRPPO algorithm in decision-making. According to the cross-layer bandwidth prediction value, buffer status information, and QoE metrics, flexibly generate bitrate selection strategies to achieve global QoE optimization. Experimental outcome indicates that the comprehensive QoE of the proposed method is at least improved by 14.6%, proving the effectiveness of the proposed method.",address issue quality experience qoe degradation caused incorrect bitrate selection film television media paper proposes dynamic optimization method film television media parameters based reinforcement learning causal reasoning first based causal reasoning algorithm optimization crppo learning causal structure environment construct causal mask acting action space help reduce decision space predict bandwidth next period based network information enhance accuracy bandwidth prediction assist crppo algorithm decision making according cross layer bandwidth prediction value buffer status information qoe metrics flexibly generate bitrate selection strategies achieve global qoe optimization experimental outcome indicates comprehensive qoe proposed method least improved proving effectiveness proposed method
"One persistent issue for the field of Human Factors Engineering (HFE) is that it is often confused with closely related disciplines such as User Experience (UX), Customer Experience (CX), Human Centered Design (HCD), and Cognitive Engineering (CE). The proliferation of related disciplines, techniques, and skills sends mixed messages to stakeholders and program managers, resulting in the following impacts: (a) misalignment with job requirements; (b) Lack of clarity on user-centered requirements and relevant standards; (c) Difficulty in communicating HFE and HFE-related Return on Investment (ROI); and d) Confusion on the techniques/analyses/artifacts needed to approach human-centered problems. Using design thinking techniques, this interactive activity provides the HFES community with a way to share their lessons learned and best practices. We will identify terms, their meaning, definitions, and where similarities lie in hopes of providing a starting point of mutual understanding to continue discussion within the Human Factors and adjacent communities.",one persistent issue field human factors engineering hfe often confused closely related disciplines user experience customer experience human centered design hcd cognitive engineering proliferation related disciplines techniques skills sends mixed messages stakeholders program managers resulting following impacts misalignment job requirements lack clarity user centered requirements relevant standards difficulty communicating hfe hfe related return investment roi confusion techniques analyses artifacts needed approach human centered problems using design thinking techniques interactive activity provides hfes community way share lessons learned best practices identify terms meaning definitions similarities lie hopes providing starting point mutual understanding continue discussion within human factors adjacent communities
"This study addresses critical gaps in cultural education by establishing a comprehensive framework that operationalizes situated learning theory through culturally-constrained multi-sensor data fusion algorithms to create immersive Chinese cultural learning environments. The research employed a pre-post control group experimental design with 120 university participants (60 experimental, 60 control) over 12-week learning periods. The cultural-constraint multi-sensor fusion algorithm integrated traditional Chinese aesthetic principles into mathematical optimization processes, while virtual environments incorporated authentic cultural practices including calligraphy, tea ceremony, and guqin performance through high-resolution RGB-D cameras, haptic feedback systems, and natural language processing capabilities. Experimental groups demonstrated 26.2% superior learning outcomes compared to traditional instruction methods while maintaining 94.2% cultural authenticity preservation. The system achieved 99.3% operational stability with 94.7 fps rendering performance. Longitudinal assessments revealed 85.1% enhanced cultural identity formation and sustained learning motivation throughout six-month follow-up periods, with virtual cultural communities facilitating 4.7 times more peer interactions than conventional classroom settings. The integration validates that educational theory-driven computational innovations can transcend traditional classroom limitations by creating persistent communities of practice that preserve cultural transmission integrity while leveraging contemporary technological capabilities. This framework provides robust foundations for advancing cultural education digitization, establishing precedents for intelligent educational algorithms guided by learning theory principles and virtual learning community architectures optimized through educational science insights.",study addresses critical gaps cultural education establishing comprehensive framework operationalizes situated learning theory culturally constrained multi sensor data fusion algorithms create immersive chinese cultural learning environments research employed pre post control group experimental design university participants experimental control week learning periods cultural constraint multi sensor fusion algorithm integrated traditional chinese aesthetic principles mathematical optimization processes virtual environments incorporated authentic cultural practices including calligraphy tea ceremony guqin performance high resolution rgb cameras haptic feedback systems natural language processing capabilities experimental groups demonstrated superior learning outcomes compared traditional instruction methods maintaining cultural authenticity preservation system achieved operational stability fps rendering performance longitudinal assessments revealed enhanced cultural identity formation sustained learning motivation throughout six month follow periods virtual cultural communities facilitating times peer interactions conventional classroom settings integration validates educational theory driven computational innovations transcend traditional classroom limitations creating persistent communities practice preserve cultural transmission integrity leveraging contemporary technological capabilities framework provides robust foundations advancing cultural education digitization establishing precedents intelligent educational algorithms guided learning theory principles virtual learning community architectures optimized educational science insights
"This paper proposes a Topological Optimization Homotopy Framework (TOHF) and provide the first rigorous proof of its global convergence to solve the non-convex optimization challenges in high-dimensional tensor decomposition. TOHF transforms the tensor algebraic structure into topological invariants to construct two-layer manifold optimization paths with global guidance. By integrating homotopy parameter scheduling and curvature correction, it overcomes the saddle-dominated energy landscape. Theoretically, TOHF achieves global convergence under the Wasserstein topological distance. Experiments in recommender systems, medical imaging, and neural network compression show that TOHF provides significant improvements in accuracy and convergence efficiency compared to state-of-the-art methods, effectively mitigating local minima traps, accelerating objective function descent, and enhancing solution set stability. This study establishes a topology-based convergence analysis paradigm for high-dimensional nonconvex optimization problems, which is of theoretical guidance for large-scale data processing applications.",paper proposes topological optimization homotopy framework tohf provide first rigorous proof global convergence solve non convex optimization challenges high dimensional tensor decomposition tohf transforms tensor algebraic structure topological invariants construct two layer manifold optimization paths global guidance integrating homotopy parameter scheduling curvature correction overcomes saddle dominated energy landscape theoretically tohf achieves global convergence wasserstein topological distance experiments recommender systems medical imaging neural network compression show tohf provides significant improvements accuracy convergence efficiency compared state art methods effectively mitigating local minima traps accelerating objective function descent enhancing solution set stability study establishes topology based convergence analysis paradigm high dimensional nonconvex optimization problems theoretical guidance large scale data processing applications
"With the acceleration of globalization, intangible cultural heritage has become an important resource for protecting national culture and promoting social development. However, research on its geographical distribution and potential mechanisms is still insufficient, especially in the use of spatial statistics and geographic information systems for analysis and management. This study uses GIS technology and spatial statistical methods, including spatial autocorrelation, hotspot analysis, and geographically weighted regression (GWR), to explore the spatial distribution of China’s intangible cultural heritage. This study analyzed data from national intangible cultural heritage projects to reveal regional characteristics and distribution patterns. The research results show that intangible cultural heritage has significant regional agglomeration, with high-density areas concentrated in the eastern coastal areas and around major cities, while the density is lower in the western and remote areas. In addition, factors such as geographical location, economic development, and population density have a significant impact on distribution. About 55% of intangible cultural heritage projects in China are concentrated in Jiangsu, Zhejiang, and Guangdong provinces. This study provides valuable insights into the spatial pattern of intangible cultural heritage and scientific basis for effective cultural protection and policy formulation. These findings can guide local governments in allocating resources, formulating strategies for cultural heritage protection and inheritance, especially in balancing the urban-rural gap.",acceleration globalization intangible cultural heritage become important resource protecting national culture promoting social development however research geographical distribution potential mechanisms still insufficient especially use spatial statistics geographic information systems analysis management study uses gis technology spatial statistical methods including spatial autocorrelation hotspot analysis geographically weighted regression gwr explore spatial distribution china intangible cultural heritage study analyzed data national intangible cultural heritage projects reveal regional characteristics distribution patterns research results show intangible cultural heritage significant regional agglomeration high density areas concentrated eastern coastal areas around major cities density lower western remote areas addition factors geographical location economic development population density significant impact distribution intangible cultural heritage projects china concentrated jiangsu zhejiang guangdong provinces study provides valuable insights spatial pattern intangible cultural heritage scientific basis effective cultural protection policy formulation findings guide local governments allocating resources formulating strategies cultural heritage protection inheritance especially balancing urban rural gap
"Under the vigorous development of the global digital economy, college students’ entrepreneurship is gradually becoming a new kinetic energy for innovation-driven economic growth. Faced with resource constraints and technical challenges, how to effectively improve the technical competitiveness and market adaptability of entrepreneurial projects is a difficulty that contemporary college entrepreneurs must overcome. This study proposes an innovative idea, integrating the ant colony algorithm and EfficientNet deep network to optimize the critical links of entrepreneurial projects. Experimental data shows that the fusion scheme of this study has achieved remarkable results. In the image recognition task, the model accuracy rate jumped from 89.5% to 93.2%, an increase of 3.7 percentage points, and the model training time was reduced by about 25%. Computing resources were efficiently utilized. Cost–benefit analysis shows that compared with traditional methods, the cost per thousand model tunes is reduced by 18%, providing an essential financial buffer for entrepreneurial teams with tight budgets. The data of this study fully proves the potential of technology integration in improving performance and saving costs and provides a feasible way to optimize college students’ entrepreneurial projects. This study not only enriches the theoretical system of the intersection of deep learning and bionic algorithms but also provides a good idea for optimizing youth entrepreneurial projects.",vigorous development global digital economy college students entrepreneurship gradually becoming new kinetic energy innovation driven economic growth faced resource constraints technical challenges effectively improve technical competitiveness market adaptability entrepreneurial projects difficulty contemporary college entrepreneurs must overcome study proposes innovative idea integrating ant colony algorithm efficientnet deep network optimize critical links entrepreneurial projects experimental data shows fusion scheme study achieved remarkable results image recognition task model accuracy rate jumped increase percentage points model training time reduced computing resources efficiently utilized cost benefit analysis shows compared traditional methods cost per thousand model tunes reduced providing essential financial buffer entrepreneurial teams tight budgets data study fully proves potential technology integration improving performance saving costs provides feasible way optimize college students entrepreneurial projects study enriches theoretical system intersection deep learning bionic algorithms also provides good idea optimizing youth entrepreneurial projects
"The operation platform of the power system contains a large amount of multisource information data. Efficient and accurate retrieval of this data information is not an easy task. To address this issue, this paper first studies data mining technologies based on term frequency-inverse document frequency (TF-IDF) and Word2Vec methods, aimed at extracting keywords and features from the operational data of the power grid system. Then, the paper proposes an improved decision tree (IDT) algorithm based on mutual information and parallel computing, and constructs a decision tree (DT) model on this basis. Finally, by setting up simulation experiments using various system databases as examples, the effectiveness and advantages of the proposed IDT model are validated. The experimental results demonstrate that the IDT algorithm achieves higher mining accuracy compared to the traditional ID3 algorithm, with accuracies of up to 99.72% across different datasets. Additionally, the model shows significant improvements in retrieval efficiency, effectively handling large-scale data with reduced processing time. The paper also introduces a database for the power grid supervision and management (SM) system to verify the effectiveness of data mining technology and the advantages of the proposed IDT algorithm in retrieving key information.",operation platform power system contains large amount multisource information data efficient accurate retrieval data information easy task address issue paper first studies data mining technologies based term frequency inverse document frequency idf word vec methods aimed extracting keywords features operational data power grid system paper proposes improved decision tree idt algorithm based mutual information parallel computing constructs decision tree model basis finally setting simulation experiments using various system databases examples effectiveness advantages proposed idt model validated experimental results demonstrate idt algorithm achieves higher mining accuracy compared traditional algorithm accuracies across different datasets additionally model shows significant improvements retrieval efficiency effectively handling large scale data reduced processing time paper also introduces database power grid supervision management system verify effectiveness data mining technology advantages proposed idt algorithm retrieving key information
"With the progress of globalization, English learning has become an important topic in education. However, the traditional teaching mode makes it difficult to meet individualized learning needs, resulting in uneven learning effects. Therefore, this study aims to build a personalized English learning path optimization system based on MVO-CNN (multiverse optimization-convolutional neural networks) to improve learning efficiency and effectiveness. There are some problems in English learning, such as scattered resources and single methods, which are difficult to adapt to the characteristics and needs of different learners. As an advanced artificial intelligence technology, MVO-CNN can effectively process complex learning data and provide strong support for personalized learning. In the system construction process, we first collected English learning data, including learners’ basic information, study habits, and achievement records. Then, MVO-CNN is used to mine and analyze the data deeply to identify the key factors affecting the learning effect. Based on these factors, the system generates personalized learning paths for each learner, including recommended learning resources, learning plans and learning methods. We conducted a 3-month experiment with 500 learners participating to verify the system’s effectiveness. The experimental results show that learners who use personalized learning paths have an average improvement of 15% in English scores, a 20% reduction in learning time and a 30% increase in learning satisfaction. In addition, the system’s recommendation accuracy rate reaches 85%, better than the 60% of the traditional teaching mode. The personalized English learning path optimization system based on MVO-CNN can improve learners’ English level and learning efficiency and has broad application value.",progress globalization english learning become important topic education however traditional teaching mode makes difficult meet individualized learning needs resulting uneven learning effects therefore study aims build personalized english learning path optimization system based mvo cnn multiverse optimization convolutional neural networks improve learning efficiency effectiveness problems english learning scattered resources single methods difficult adapt characteristics needs different learners advanced artificial intelligence technology mvo cnn effectively process complex learning data provide strong support personalized learning system construction process first collected english learning data including learners basic information study habits achievement records mvo cnn used mine analyze data deeply identify key factors affecting learning effect based factors system generates personalized learning paths learner including recommended learning resources learning plans learning methods conducted month experiment learners participating verify system effectiveness experimental results show learners use personalized learning paths average improvement english scores reduction learning time increase learning satisfaction addition system recommendation accuracy rate reaches better traditional teaching mode personalized english learning path optimization system based mvo cnn improve learners english level learning efficiency broad application value
"Accurate quantification of illegal additives in flour is critical for ensuring food safety. Conventional detection methods, however, suffer from low efficiency and are inadequate for rapid quantitative analysis. Herein, we presented a near-infrared spectroscopy-based quantitative detection approach for the simultaneous determination of Azodicarbonamide (ADA), Talcum Powder (TalcP), and Gypsum Powder (GypsumP) in flour. Three wavelength selection algorithms were respectively combined with three multivariate regression methodologies to construct rapid detection model of three illegal additives in flour. The genetic simulated annealing algorithm was applied for modeling parameter optimization in the nonlinear modeling process. Results showed that the combination of competitive adaptive reweighted sampling and kernel extreme learning machine achieved the best detection performance for ADA, TalcP, and GypsumP. The prediction set’s coefficients of determination were 0.9855, 0.9917, and 0.9875; the root mean square errors were 0.0008, 0.1889, and 0.2354; and the residual prediction deviations were 8.2928, 11.0041, and 8.9388, respectively. The detection accuracy meets the requirements of practical applications. This approach provides a scientific basis for flour quality detection and strong technical support for ensuring food safety and consumer health.",accurate quantification illegal additives flour critical ensuring food safety conventional detection methods however suffer low efficiency inadequate rapid quantitative analysis herein presented near infrared spectroscopy based quantitative detection approach simultaneous determination azodicarbonamide ada talcum powder talcp gypsum powder gypsump flour three wavelength selection algorithms respectively combined three multivariate regression methodologies construct rapid detection model three illegal additives flour genetic simulated annealing algorithm applied modeling parameter optimization nonlinear modeling process results showed combination competitive adaptive reweighted sampling kernel extreme learning machine achieved best detection performance ada talcp gypsump prediction set coefficients determination root mean square errors residual prediction deviations respectively detection accuracy meets requirements practical applications approach provides scientific basis flour quality detection strong technical support ensuring food safety consumer health
"With the rapid development of the digital music market, users’ demand for personalized music recommendations is increasing daily. Traditional recommendation algorithms often face problems such as insufficient feature extraction and low recommendation accuracy when dealing with large-scale and high-dimensional music data. Therefore, this study proposes an intelligent music personalized recommendation algorithm based on efficient feature extraction of MLP-Mixer. The algorithm uses MLP-Mixer’s multi-layer perceptron network to effectively capture deep-seated features in music data, thereby improving the accuracy and diversity of the recommendation system. In the experimental session, we constructed a dataset containing 100,000 songs and 1 million user ratings, using mean square error (MSE) and Accuracy as evaluation indicators. The experimental results show that compared with the traditional recommendation algorithm, the proposed algorithm reduces the MSE index by 15% and improves the Accuracy index by 8%. In addition, the user satisfaction survey shows that the user satisfaction of the recommendation system using the new algorithm has increased by 12%, significantly enhancing the user experience. This study not only verifies the effectiveness of MLP-Mixer in music feature extraction but also provides a new technical path for intelligent music personalized recommendation.",rapid development digital music market users demand personalized music recommendations increasing daily traditional recommendation algorithms often face problems insufficient feature extraction low recommendation accuracy dealing large scale high dimensional music data therefore study proposes intelligent music personalized recommendation algorithm based efficient feature extraction mlp mixer algorithm uses mlp mixer multi layer perceptron network effectively capture deep seated features music data thereby improving accuracy diversity recommendation system experimental session constructed dataset containing songs million user ratings using mean square error mse accuracy evaluation indicators experimental results show compared traditional recommendation algorithm proposed algorithm reduces mse index improves accuracy index addition user satisfaction survey shows user satisfaction recommendation system using new algorithm increased significantly enhancing user experience study verifies effectiveness mlp mixer music feature extraction also provides new technical path intelligent music personalized recommendation
"With the rapid development of information technology, the demand for real-time data processing and audit optimization in financial management has become increasingly urgent. This study aims to explore real-time data processing and audit optimization methods for financial management based on IoT and edge computing. Realize real-time collection of financial data through Internet of Things technology to ensure the universality and authenticity of data sources; Use edge computing for preliminary data processing and analysis to improve data processing efficiency. The experimental results show that this method improves the data processing speed by about 35% and significantly shortens the period from data acquisition to usability. At the same time, the audit accuracy is improved by about 25%, effectively reducing errors and delays in data transmission. In addition, the system response speed is accelerated by about 40%, enhancing the emergency response capability of financial management. This study provides new ideas and methods for the digital and intelligent transformation of financial management and has a wide application prospect and practical value.",rapid development information technology demand real time data processing audit optimization financial management become increasingly urgent study aims explore real time data processing audit optimization methods financial management based iot edge computing realize real time collection financial data internet things technology ensure universality authenticity data sources use edge computing preliminary data processing analysis improve data processing efficiency experimental results show method improves data processing speed significantly shortens period data acquisition usability time audit accuracy improved effectively reducing errors delays data transmission addition system response speed accelerated enhancing emergency response capability financial management study provides new ideas methods digital intelligent transformation financial management wide application prospect practical value
"As businesses get more complicated, figuring out how to best optimize their organizational structure and improve the efficiency of work scheduling and resource allocation has become a major concern for managers. We suggest a model called NetOrg-Opt for improving the structure of a company, and we apply the Louvain algorithm to find communities. The results of tests to improve organizational structure demonstrate that the NetOrg-Opt model works the best of all the algorithms, with a modularity value of 0.91. The task assignment and resource optimization tests demonstrate that NetOrg-Opt can finish a task in 120 h and use 92% of its resources. This shows that the NetOrg-Opt model has a lot of potential for improving the structure of a business. The NetOrg-Opt approach has a lot of potential for improving the structure of businesses in both experiments.",businesses get complicated figuring best optimize organizational structure improve efficiency work scheduling resource allocation become major concern managers suggest model called netorg opt improving structure company apply louvain algorithm find communities results tests improve organizational structure demonstrate netorg opt model works best algorithms modularity value task assignment resource optimization tests demonstrate netorg opt finish task use resources shows netorg opt model lot potential improving structure business netorg opt approach lot potential improving structure businesses experiments
"To address the issues of semantic singularity and poor generation effects in existing government new media crisis discourse generation models, this paper suggests a novel Generative Adversarial Network model (WOA-GAN). WOA-GAN first uses an improved BERT model to encode the text of the discourse and uses a residual network to encode the image. Then, a cross-attention mechanism is designed to generate a mask map, effectively integrating the text semantic information and visual characteristics in a multimodal way. Eventually, a semantic space-aware attention mixing module is designed to integrate the text characteristics with the mask map, and guide the multimodal discourse features to enhance details, thus generating high-quality crisis discourse. Experimental results demonstrate that the Bilingual Evaluation Understudy (BLEU) of WOA-GAN has increased by 8.2%–19.5%, which can significantly improve the generation speed while maintaining the generation quality.",address issues semantic singularity poor generation effects existing government new media crisis discourse generation models paper suggests novel generative adversarial network model woa gan woa gan first uses improved bert model encode text discourse uses residual network encode image cross attention mechanism designed generate mask map effectively integrating text semantic information visual characteristics multimodal way eventually semantic space aware attention mixing module designed integrate text characteristics mask map guide multimodal discourse features enhance details thus generating high quality crisis discourse experimental results demonstrate bilingual evaluation understudy bleu woa gan increased significantly improve generation speed maintaining generation quality
"Wearable sensors show great potential for exercise energy management. This paper addresses the issues of insufficient sample data and low estimation accuracy in existing studies. First, an exercise training monitoring system with real-time capabilities is implemented utilizing multi-modal wearable sensors, and the monitoring data are preprocessed to remove redundant features by the regression coefficient method. The preprocessed data are augmented in light of an enhanced generative adversarial network to expand the sample data. Then an improved residual neural network model is proposed to reduce the computational complexity by decomposition convolution and group regularization, and finally the motion energy estimation results are obtained by softmax. Simulation results indicate that the measurement error of the proposed method is within 8% for each physiological index, and the accuracy of energy consumption estimation is 92.71%, which significantly improves the management effect of exercise energy consumption.",wearable sensors show great potential exercise energy management paper addresses issues insufficient sample data low estimation accuracy existing studies first exercise training monitoring system real time capabilities implemented utilizing multi modal wearable sensors monitoring data preprocessed remove redundant features regression coefficient method preprocessed data augmented light enhanced generative adversarial network expand sample data improved residual neural network model proposed reduce computational complexity decomposition convolution group regularization finally motion energy estimation results obtained softmax simulation results indicate measurement error proposed method within physiological index accuracy energy consumption estimation significantly improves management effect exercise energy consumption
"Addressing the challenges of spatio-temporal continuity and ergonomic optimization in dynamic action generation for theatrical performance roles, this study introduces a novel generative adversarial network (GAN)-driven framework. We construct a spatio-temporal constraint generator utilizing the Archive of Motion Capture as Surface Shapes (AMASS) motion capture dataset, integrated with a biomechanical feedback mechanism to optimize joint load distribution. Experimental validation demonstrates that our method significantly enhances action naturalness in choreographed fight and prop-handling scenarios, achieving an Fréchet Inception Distance (FID) of 29.7 (16.6% lower than the optimal baseline PhysGAN). Furthermore, it reduces peak lumbar spine loads by 24.2% and optimizes rapid entire body assessment (REBA) scores by 21.5% compared to PhysGAN (from 6.5 to 5.1). The proposed framework delivers a high-fidelity, low-risk action generation solution for automated performer-technology collaboration systems, enhancing performance realism and actor safety with demonstrable theatrical/cinematic application value.",addressing challenges spatio temporal continuity ergonomic optimization dynamic action generation theatrical performance roles study introduces novel generative adversarial network gan driven framework construct spatio temporal constraint generator utilizing archive motion capture surface shapes amass motion capture dataset integrated biomechanical feedback mechanism optimize joint load distribution experimental validation demonstrates method significantly enhances action naturalness choreographed fight prop handling scenarios achieving fréchet inception distance fid lower optimal baseline physgan furthermore reduces peak lumbar spine loads optimizes rapid entire body assessment reba scores compared physgan proposed framework delivers high fidelity low risk action generation solution automated performer technology collaboration systems enhancing performance realism actor safety demonstrable theatrical cinematic application value
"The aim of this article is to examine educational technologies that positively influence the development of critical and creative thinking among students in higher education institutions. The study explores the potential of using modern digital tools to enhance students’ cognitive skills and foster a new approach to learning focused on practical tasks and innovative methods. The methodology involved an experiment conducted with 60 fourth-year students at Kyrgyz State University named after I. Arabaev. The experiment employed pedagogical technologies, including the flipped classroom model, the Case method and artificial intelligence (AI) tools. These approaches enabled students to actively engage with the material, develop cognitive abilities, and deepen their understanding of the educational process. The study’s primary findings indicate that the use of educational robots significantly enhances students’ levels of critical and creative thinking. During the experiment, students who worked with robots demonstrated superior performance in areas such as solving complex problems, improving programming skills and developing algorithmic thinking. The increased effectiveness of the educational process is attributed to the interactive and practice-oriented nature of robotics, which enables students to not only acquire theoretical knowledge but also apply it in practical contexts. Furthermore, participation in such processes fosters the development of additional skills, including teamwork, rapid decision-making and adaptability to changing conditions. A notable feature of using digital technologies in education is the substantial motivation they inspire in students. This is evidenced by their heightened activity, greater engagement with the subject, and improved performance in task completion, which collectively contribute to better assimilation of the material. The results of the study confirm that integrating educational robots into the learning process can significantly enhance the development of critical and creative thinking among students.",aim article examine educational technologies positively influence development critical creative thinking among students higher education institutions study explores potential using modern digital tools enhance students cognitive skills foster new approach learning focused practical tasks innovative methods methodology involved experiment conducted fourth year students kyrgyz state university named arabaev experiment employed pedagogical technologies including flipped classroom model case method artificial intelligence tools approaches enabled students actively engage material develop cognitive abilities deepen understanding educational process study primary findings indicate use educational robots significantly enhances students levels critical creative thinking experiment students worked robots demonstrated superior performance areas solving complex problems improving programming skills developing algorithmic thinking increased effectiveness educational process attributed interactive practice oriented nature robotics enables students acquire theoretical knowledge also apply practical contexts furthermore participation processes fosters development additional skills including teamwork rapid decision making adaptability changing conditions notable feature using digital technologies education substantial motivation inspire students evidenced heightened activity greater engagement subject improved performance task completion collectively contribute better assimilation material results study confirm integrating educational robots learning process significantly enhance development critical creative thinking among students
"With the rise of health tourism, tourists’ requirements for tourism experience are increasing daily, and sentiment analysis has become an important means to optimize the experience. Based on deep learning and natural language processing (NLP) technology, this study constructs a big data sentiment analysis model of health tourism, aiming to provide data support for experience optimization by analyzing tourists’ online reviews, mining emotional tendencies, and providing data support for experience optimization. The study first collected millions of reviews related to health tourism from major travel platforms. After data cleaning and preprocessing, deep learning models such as Word2Vec and BERT were used for feature extraction, and emotion dictionaries and machine learning algorithms were combined for emotion classification. The experimental results show that the emotion classification accuracy of the proposed model reaches 92.3%, which is nearly five percentage points higher than that of the traditional method. Furthermore, through the results of sentiment analysis, the research identifies the key factors that affect tourists’ experience, such as service quality, environmental facilities, and health effects, and puts forward targeted optimization suggestions. In addition, the study also found that by implementing optimization measures, tourists’ satisfaction increased by 8.7%, and their willingness to revisit increased by 15.2%. This study provides an effective sentiment analysis tool for the health and wellness tourism industry and a scientific basis for improving the tourist experience and promoting the development of health and wellness tourism.",rise health tourism tourists requirements tourism experience increasing daily sentiment analysis become important means optimize experience based deep learning natural language processing nlp technology study constructs big data sentiment analysis model health tourism aiming provide data support experience optimization analyzing tourists online reviews mining emotional tendencies providing data support experience optimization study first collected millions reviews related health tourism major travel platforms data cleaning preprocessing deep learning models word vec bert used feature extraction emotion dictionaries machine learning algorithms combined emotion classification experimental results show emotion classification accuracy proposed model reaches nearly five percentage points higher traditional method furthermore results sentiment analysis research identifies key factors affect tourists experience service quality environmental facilities health effects puts forward targeted optimization suggestions addition study also found implementing optimization measures tourists satisfaction increased willingness revisit increased study provides effective sentiment analysis tool health wellness tourism industry scientific basis improving tourist experience promoting development health wellness tourism
"As the green sharing economy grows quickly, figuring out how to efficiently and dynamically deploy resources to deal with changing demands and environmental changes has become a major concern for its long-term growth. This study suggests an intelligent resource dynamic provisioning system based on input from many different sources. The goal is to make better use of resources and make scheduling responses faster. The study creates two simulation datasets. The results show that the priority scheduling strategy has the best resource utilization rate (0.92 and 0.91), while the time window scheduling strategy has the best response time (0.76 and 0.70). The findings of the experiment reveal that the proposed resource dynamic scheduling method has several benefits and is a good way to manage resources intelligently in the green sharing economy.",green sharing economy grows quickly figuring efficiently dynamically deploy resources deal changing demands environmental changes become major concern long term growth study suggests intelligent resource dynamic provisioning system based input many different sources goal make better use resources make scheduling responses faster study creates two simulation datasets results show priority scheduling strategy best resource utilization rate time window scheduling strategy best response time findings experiment reveal proposed resource dynamic scheduling method several benefits good way manage resources intelligently green sharing economy
"Trust Energy Corporation (TEC) is a 75-year-old Indian Public Sector Unit operating in the Oil and Gas industry which is characterised by uncertainty and volatility. Since after liberalisation of the sector, TEC has had to face competition from global giants. It implemented several HR initiatives in an attempt to retain talent and improve employee engagement levels. However, contrary to expectations, the initiatives had an adverse effect on employee engagement levels. The present case examines this counterintuitive phenomenon from a systems perspective and highlights the importance of systems thinking and organisational agility in a Volatile, Uncertain, Complex and Ambiguous (VUCA) world. The researchers who worked on a consultancy project with TEC gathered primary data using open interviews and focus group discussions; secondary data was collected from TEC’s policy documents, working documents and internal communications. The researchers applied systems thinking approach and arrived at six major challenges faced by TEC. The findings indicate that the root cause of all the challenges is the lack of an overarching structure or strategy that could enable organisational agility. The case integrates the theory of change management with systems thinking approach and has practical utility to OD practitioners and HR managers.",trust energy corporation tec year old indian public sector unit operating oil gas industry characterised uncertainty volatility since liberalisation sector tec face competition global giants implemented several initiatives attempt retain talent improve employee engagement levels however contrary expectations initiatives adverse effect employee engagement levels present case examines counterintuitive phenomenon systems perspective highlights importance systems thinking organisational agility volatile uncertain complex ambiguous vuca world researchers worked consultancy project tec gathered primary data using open interviews focus group discussions secondary data collected tec policy documents working documents internal communications researchers applied systems thinking approach arrived six major challenges faced tec findings indicate root cause challenges lack overarching structure strategy could enable organisational agility case integrates theory change management systems thinking approach practical utility practitioners managers
"Industrial electrical automation fault detection is a guarantee for the stable operation of industrial systems. Focusing on issues of low detection accuracy and insufficient depth perception ability in the current research, this paper establishes a multimetric stereo vision measurement model of electrical equipment, and stereo-corrects the images of electrical equipment. Then a two-channel convolutional neural network (CNN) is designed to capture global and local characteristics of the electrical equipment image. Finally, the YOLOv8 approach was adopted as the baseline model to optimize the design of the feature fusion part of the network. The recognition head network was superseded by Dyhead dynamic recognition head. The loss function part is redesigned using Focaler-IoU to solve the fitting problem caused by sample imbalance. Experimental outcome indicates that the proposed technique boosts the mean Average Precision (mAP) by no less than 5.42%, which significantly improves the detection efficiency and reliability.",industrial electrical automation fault detection guarantee stable operation industrial systems focusing issues low detection accuracy insufficient depth perception ability current research paper establishes multimetric stereo vision measurement model electrical equipment stereo corrects images electrical equipment two channel convolutional neural network cnn designed capture global local characteristics electrical equipment image finally yolov approach adopted baseline model optimize design feature fusion part network recognition head network superseded dyhead dynamic recognition head loss function part redesigned using focaler iou solve fitting problem caused sample imbalance experimental outcome indicates proposed technique boosts mean average precision map less significantly improves detection efficiency reliability
"The importance of User Experience Measurement allows understanding how experiences can be optimized to meet functional and emotional needs, complemented with Design-Based Research to scale the redesign of educational platforms, four online educational platforms presented are powered by Artificial Intelligence and educational data mining to offer a tailored learning experience, combining computational thinking with Sustainable Development Goals. We present the analysis of responses of 1,573 users, which yield elements of improvement that will be implemented in the redesign. To measure User Experience of the platform, a nonexperimental quantitative analysis was carried out, using a Feedback survey based on a Likert-scale instrument, measuring perceptions: Emotional impact, Usability, and Satisfaction. The transcendence of this proposal demonstrates good practice in documenting experiences with the redesign of educational platforms that incorporate Artificial Intelligence and data mining, thereby opening a gap in Research on the quality of assessments and scaling of this approach.",importance user experience measurement allows understanding experiences optimized meet functional emotional needs complemented design based research scale redesign educational platforms four online educational platforms presented powered artificial intelligence educational data mining offer tailored learning experience combining computational thinking sustainable development goals present analysis responses users yield elements improvement implemented redesign measure user experience platform nonexperimental quantitative analysis carried using feedback survey based likert scale instrument measuring perceptions emotional impact usability satisfaction transcendence proposal demonstrates good practice documenting experiences redesign educational platforms incorporate artificial intelligence data mining thereby opening gap research quality assessments scaling approach
"With the rapid development of artificial intelligence (AI) in the field of education, its application in English teaching has gradually become an important research field. The purpose of this study is to explore the application effect and potential of AI in English teaching. Through questionnaires, experimental design, and comparative research, as well as the construction and evaluation of AI teaching models, this study reveals the advantages of AI teaching methods in improving learning efficiency, personalized teaching, enhancing learning motivation, and enhancing learner satisfaction. The results show that the AI teaching model has shown remarkable results in practical applications, especially in promoting personalized learning and improving teaching efficiency. However, the study also found limitations to the use of AI in teaching, such as technical, ethical, and resource challenges. Future research needs to focus on the adaptability of AI technology in different teaching Settings and populations, as well as addressing related ethical and privacy issues. This study provides valuable insights for understanding and optimizing the application of AI in English teaching, and points the way for future research and practice in related fields.",rapid development artificial intelligence field education application english teaching gradually become important research field purpose study explore application effect potential english teaching questionnaires experimental design comparative research well construction evaluation teaching models study reveals advantages teaching methods improving learning efficiency personalized teaching enhancing learning motivation enhancing learner satisfaction results show teaching model shown remarkable results practical applications especially promoting personalized learning improving teaching efficiency however study also found limitations use teaching technical ethical resource challenges future research needs focus adaptability technology different teaching settings populations well addressing related ethical privacy issues study provides valuable insights understanding optimizing application english teaching points way future research practice related fields
"With the rapid expansion of digital music content, efficient and accurate methods for music feature extraction and style generation have become critical research areas in music information processing. Traditional manual labeling methods are insufficient to manage large-scale music data due to their inefficiency and subjectivity. Deep learning technologies, particularly Long Short-Term Memory (LSTM) networks, have shown significant promise in capturing temporal and stylistic characteristics of musical sequences. This paper introduces an integrated framework that combines spectral and cepstral analyses—applying short-time Fourier transform on overlapping 20 ms frames with 10 ms hops and extracting log-power cepstrum plus Mel-frequency cepstral coefficients (MFCCs)—to systematically derive pitch, resonance peaks, and timbre features. These handcrafted vectors are concatenated with learned embeddings and fed into a bidirectional LSTM, enabling the model to leverage both explicit frequency-domain cues and long-range temporal dependencies. By processing each sequence in forward and backward directions, the bidirectional LSTM outperforms unidirectional variants in genre classification accuracy and produces more coherent musical transitions. In tests on five genres (Jazz, Classical, Rock, Country, and Disco), our framework achieved an average classification accuracy of 81.2%, an F1-score of 0.79, and a Mean Opinion Score of 4.1/5 for stylistic coherence in a blind listening study. Additionally, quantitative evaluation of harmonic progression consistency (85% retention of original chord transitions) and dynamic contour reproduction (Pearson correlation of 0.82 with source note velocities) demonstrates the model’s ability to generalize to non-genre-specific musical elements. Experimental results confirm that the proposed method generates high-quality, stylistically faithful music across diverse genres, offering an automated, efficient solution for both music analysis and creative generation.",rapid expansion digital music content efficient accurate methods music feature extraction style generation become critical research areas music information processing traditional manual labeling methods insufficient manage large scale music data due inefficiency subjectivity deep learning technologies particularly long short term memory lstm networks shown significant promise capturing temporal stylistic characteristics musical sequences paper introduces integrated framework combines spectral cepstral analyses applying short time fourier transform overlapping frames hops extracting log power cepstrum plus mel frequency cepstral coefficients mfccs systematically derive pitch resonance peaks timbre features handcrafted vectors concatenated learned embeddings fed bidirectional lstm enabling model leverage explicit frequency domain cues long range temporal dependencies processing sequence forward backward directions bidirectional lstm outperforms unidirectional variants genre classification accuracy produces coherent musical transitions tests five genres jazz classical rock country disco framework achieved average classification accuracy score mean opinion score stylistic coherence blind listening study additionally quantitative evaluation harmonic progression consistency retention original chord transitions dynamic contour reproduction pearson correlation source note velocities demonstrates model ability generalize non genre specific musical elements experimental results confirm proposed method generates high quality stylistically faithful music across diverse genres offering automated efficient solution music analysis creative generation
"In the context of rapid urbanization, the emergence of urban problems has drawn increasing attention to the construction of healthy cities. A comprehensive analysis of research on healthy cities can provide valuable insights into the current state of the field and guide future development directions. This study utilizes the CiteSpace literature analysis tool and examines literature from the Web of Science and CNKI databases over the past three decades to identify relevant research papers. Employing visual analytics techniques, it explores research hotspots, authors, and institutions to extract key findings and identify areas for further improvement in the research of healthy cities. The analysis reveals several important observations: Firstly, international research primarily focuses on individual health from a biomedical perspective, while Chinese studies place greater emphasis on the impact of the built environment and urban planning, yet less consideration is given to the psychological consideration of human health. Secondly, the major of professional international authors in the field of urban health are concentrated in Europe and Australia, where influential Chinese authors are mainly affiliated with institutions in Shanghai and Beijing, indicating the regional characteristics of China’s health city research and its concentrated development in these cities. This statistical result reveals the differences in contributions of different regions in the field of health city research, while reflecting the geographical distribution bias of research resources and forces. Furthermore, universities are the dominant research institutions in both Chinese and international contexts. Lastly, this study synthesizes the theoretical foundations, empirical results, construction evaluation, and governance measures of healthy cities, and provides four constructive suggestions for future development: refining the theoretical framework of healthy cities, fostering interdisciplinary collaborations, expanding the assessment system and methods, and implementing comprehensive governance strategies that encompass various dimensions of health.",context rapid urbanization emergence urban problems drawn increasing attention construction healthy cities comprehensive analysis research healthy cities provide valuable insights current state field guide future development directions study utilizes citespace literature analysis tool examines literature web science cnki databases past three decades identify relevant research papers employing visual analytics techniques explores research hotspots authors institutions extract key findings identify areas improvement research healthy cities analysis reveals several important observations firstly international research primarily focuses individual health biomedical perspective chinese studies place greater emphasis impact built environment urban planning yet less consideration given psychological consideration human health secondly major professional international authors field urban health concentrated europe australia influential chinese authors mainly affiliated institutions shanghai beijing indicating regional characteristics china health city research concentrated development cities statistical result reveals differences contributions different regions field health city research reflecting geographical distribution bias research resources forces furthermore universities dominant research institutions chinese international contexts lastly study synthesizes theoretical foundations empirical results construction evaluation governance measures healthy cities provides four constructive suggestions future development refining theoretical framework healthy cities fostering interdisciplinary collaborations expanding assessment system methods implementing comprehensive governance strategies encompass various dimensions health
"Predicting crack propagation in a composite airfield pavement is a computationally challenging task. This study presents the development of a fracture-based modeling approach to capture crack propagation in an asphalt concrete (AC) overlay on jointed Portland cement concrete (PCC) pavement structure. A four-stage numerical framework was developed to predict thermal induced joint reflective cracking. The framework leverages a combination of finite difference methods, finite element (FE) thermo-mechanical modeling, and the Generalized Finite Element Method (GFEM) coupled with the elastic-viscoelastic correspondence principle (EVCP). FE thermo-mechanical and GFEM fracture simulations were solved in 3-D. Performing the simulations in 3-D domain shows the non-uniformity of PCC joint movement through the depth and width of the concrete slabs. The results show that this non-uniformity is mainly influenced by AC stiffness and thermal expansion/contraction. Simplified design models were prepared for joint opening under different cooling cycles. GFEM s adaptive meshing and global-local analysis enable accurate calculation of stress intensity factors from the elastic solution. Application of EVCP provided the ability to address the critical challenges of incorporating 3-D viscoelastic analysis within the framework. Using EVCP viscoelastic ERR can be calculated for various AC mixtures and cooling cycles using a limited set of elastic solutions. The framework was validated using FAA outdoor test section for joint reflective cracking at the National Airport Pavement Test Facility. Thermal reflective cracking performance for different overlay scenarios was simulated in four different climatic regions, highlighting the framework s ability of capturing the effect of pavement structure and climate on overlay fatigue life.",predicting crack propagation composite airfield pavement computationally challenging task study presents development fracture based modeling approach capture crack propagation asphalt concrete overlay jointed portland cement concrete pcc pavement structure four stage numerical framework developed predict thermal induced joint reflective cracking framework leverages combination finite difference methods finite element thermo mechanical modeling generalized finite element method gfem coupled elastic viscoelastic correspondence principle evcp thermo mechanical gfem fracture simulations solved performing simulations domain shows non uniformity pcc joint movement depth width concrete slabs results show non uniformity mainly influenced stiffness thermal expansion contraction simplified design models prepared joint opening different cooling cycles gfem adaptive meshing global local analysis enable accurate calculation stress intensity factors elastic solution application evcp provided ability address critical challenges incorporating viscoelastic analysis within framework using evcp viscoelastic err calculated various mixtures cooling cycles using limited set elastic solutions framework validated using faa outdoor test section joint reflective cracking national airport pavement test facility thermal reflective cracking performance different overlay scenarios simulated four different climatic regions highlighting framework ability capturing effect pavement structure climate overlay fatigue life
"Children with Autism Spectrum Disorder (ASD) face significant difficulties in emotional expression and recognition, and traditional manual observation methods struggle to capture their weak and transient micro-expression features. To address this issue, this paper proposes an autism emotion recognition model that integrates Vision Transformer with multimodal features. The model first employs the TVL1 optical flow algorithm to extract facial motion features and utilizes Vision Transformer to model long-range dependencies between different facial regions. Subsequently, it introduces a feature selection fusion module (FSFM) to filter key image patches, a cross-attention fusion module (CAFM) to integrate horizontal and vertical optical flow information, and designs a spatial consistency attention module (SCAM) to ensure feature distribution consistency. Finally, it incorporates Maximally Collapsing Metric Learning (MCML) to optimize the feature space structure. On standard micro-expression databases including MMEW, CASMEII, and SAMM, this method achieves recognition accuracies significantly superior to existing approaches (73.0%, 76.4%, and 70.5%, respectively). Furthermore, the proposed method demonstrates good generalization capability and real-time performance, showing promise as an intelligent assistive tool for special education to enhance teachers’ understanding of emotional states in children with autism and improve intervention efficiency, thereby promoting personalized education development.",children autism spectrum disorder asd face significant difficulties emotional expression recognition traditional manual observation methods struggle capture weak transient micro expression features address issue paper proposes autism emotion recognition model integrates vision transformer multimodal features model first employs tvl optical flow algorithm extract facial motion features utilizes vision transformer model long range dependencies different facial regions subsequently introduces feature selection fusion module fsfm filter key image patches cross attention fusion module cafm integrate horizontal vertical optical flow information designs spatial consistency attention module scam ensure feature distribution consistency finally incorporates maximally collapsing metric learning mcml optimize feature space structure standard micro expression databases including mmew casmeii samm method achieves recognition accuracies significantly superior existing approaches respectively furthermore proposed method demonstrates good generalization capability real time performance showing promise intelligent assistive tool special education enhance teachers understanding emotional states children autism improve intervention efficiency thereby promoting personalized education development
"The paper presents a Distinctive Functional Analytics Model (DFAM) for managing big data in Cyber-Physical Systems (CPS) that tries to make analytics on data more effective and accurate. CPS integrates computational, communication, and hardware systems to couple physical and internet platforms for real-time data processing. DFAM addresses processing large, composite data through self-trained smart learning to handle data integrity and normality with high-precision outputs and error minimization through stochastic computing. The model is reusable and flexible in real-world applications and guarantees output without interrupting computation. The results verify that DFAM outperforms other models by 7.42% more accuracy, 18.39% less latency, and 25.85% and 9.76% less complexity and error, respectively. Results verify the usability of the model to process actual-time large amounts of data on CPS platforms with an established solution for data-oriented applications and services.",paper presents distinctive functional analytics model dfam managing big data cyber physical systems cps tries make analytics data effective accurate cps integrates computational communication hardware systems couple physical internet platforms real time data processing dfam addresses processing large composite data self trained smart learning handle data integrity normality high precision outputs error minimization stochastic computing model reusable flexible real world applications guarantees output without interrupting computation results verify dfam outperforms models accuracy less latency less complexity error respectively results verify usability model process actual time large amounts data cps platforms established solution data oriented applications services
"Digitization of paper documents is crucial for the management of modern power grid enterprise. Table structure recognition, which identifies table cells, presents challenges due to diverse table formats. This paper introduces a novel table structure recognition method based on dual-branch encoder-decoder segmentation. The proposed approach converts table structure extraction into row and column segmentation sub-problems, which utilizes a single encoder for feature extraction and two independent decoder branches for segment prediction. In this framework, a Conv-Res-CBAM unit is proposed to enhance feature extraction and transmission. Additionally, the Tesseract OCR engine is incorporated for character recognition. Extensive experiments on two public datasets and a self-collected dataset demonstrate the superiority of our method.",digitization paper documents crucial management modern power grid enterprise table structure recognition identifies table cells presents challenges due diverse table formats paper introduces novel table structure recognition method based dual branch encoder decoder segmentation proposed approach converts table structure extraction row column segmentation sub problems utilizes single encoder feature extraction two independent decoder branches segment prediction framework conv res cbam unit proposed enhance feature extraction transmission additionally tesseract ocr engine incorporated character recognition extensive experiments two public datasets self collected dataset demonstrate superiority method
"As technology advances, the demands for precise industrial product manufacturing increase. Consequently, measuring instruments have also improved in accuracy. To ensure that the measurement accuracy of the three-coordinate measuring instrument meets modern industrial manufacturing’s precision measurement requirements, the quantity traceability method is proposed to improve the measurement accuracy of angle error. And the geometric error compensation method is used to improve the measurement accuracy of the coordinate measuring instrument. By establishing a complete, accurate, and reliable traceability chain, the coordinate measuring instrument has traceability and calibration functions. Error compensation improves the geometric error in the coordinate measuring instrument through operation command correction. The results showed that the phase relationship of the measurement error of the self-calibrating encoder was basically consistent with the design value. Compared to the R1 reading head, the deviation value of the R6 reading head was the smallest, at 0.1°. The deviation value of the R4 and R5 reading heads was larger, at 0.4° compared to the R1 reading head. The deviation values of the R2 and R3 reading heads compared to the R1 reading head were 0.3° and 0.2°, respectively. The precision error compensation model had a higher degree of error identification and accuracy. In measuring errors in the three axes, the outcomes of the traditional error model show greater variability as the displacement increases. The method of error compensation in the measurement system effectively improves accuracy by reducing errors. This method not only proves the feasibility of accuracy improvement but also provides guidance for further enhancements of the measurement system’s accuracy.",technology advances demands precise industrial product manufacturing increase consequently measuring instruments also improved accuracy ensure measurement accuracy three coordinate measuring instrument meets modern industrial manufacturing precision measurement requirements quantity traceability method proposed improve measurement accuracy angle error geometric error compensation method used improve measurement accuracy coordinate measuring instrument establishing complete accurate reliable traceability chain coordinate measuring instrument traceability calibration functions error compensation improves geometric error coordinate measuring instrument operation command correction results showed phase relationship measurement error self calibrating encoder basically consistent design value compared reading head deviation value reading head smallest deviation value reading heads larger compared reading head deviation values reading heads compared reading head respectively precision error compensation model higher degree error identification accuracy measuring errors three axes outcomes traditional error model show greater variability displacement increases method error compensation measurement system effectively improves accuracy reducing errors method proves feasibility accuracy improvement also provides guidance enhancements measurement system accuracy
"This paper presents a collaborative optimization control framework using a Stackelberg game (leader-follower game) approach for microgrid source-grid-load-storage coordination. In this framework, the microgrid system acts as the leader, whose objective is to minimize the total operational cost. Distributed energy resources including renewable energy and energy storage as well as flexible loads serve as followers. Specifically, renewable energy followers seek to minimize costs by considering penalties for curtailment. Energy storage followers optimize regulation performance while considering overcharging/over-discharging risks. Load followers focus on improving power quality while maintaining demand response capabilities. Results demonstrate that all participants under the proposed game framework can reach a Stackelberg equilibrium, effectively harmonizing the interests of various participants and enhancing energy storage safety and regulation capabilities. The microgrid scheduling plan ensures the secure and stable operation of the system.",paper presents collaborative optimization control framework using stackelberg game leader follower game approach microgrid source grid load storage coordination framework microgrid system acts leader whose objective minimize total operational cost distributed energy resources including renewable energy energy storage well flexible loads serve followers specifically renewable energy followers seek minimize costs considering penalties curtailment energy storage followers optimize regulation performance considering overcharging discharging risks load followers focus improving power quality maintaining demand response capabilities results demonstrate participants proposed game framework reach stackelberg equilibrium effectively harmonizing interests various participants enhancing energy storage safety regulation capabilities microgrid scheduling plan ensures secure stable operation system
"The construction of subway tunnels has always been a key link in urban infrastructure construction. However, it is also accompanied by various construction risks and safety challenges. A safety management technology for subway tunnel construction is proposed to address the safety management issues of subway tunnels. By deploying real-time monitoring systems and integrating historical data, descriptive and inferential statistical analysis of safety performance is conducted. Failure mode impact analysis is used to identify potential risks, and the analytic hierarchy process is applied to comprehensively evaluate safety management factors. The technical logic of the research emphasizes data-driven, systematic analysis, and predictive prevention, and improves the adaptability of management measures through dynamic adjustment mechanisms, providing scientific basis and practical guidance for tunnel safety management. In the calculation time test, the calculation time of the research method was maintained at 296 ms for 100 engineering projects. When calculating the expected implementation cost, the highest implementation cost of the research method was only 2468K pesos when it reached 320 days. In the project progress analysis of safety management, the maximum completion time of the research method was only 435 days. This indicates that the research method can effectively manage the safety of subway tunnels. It has better implementation results and lower negative impacts on the construction period.",construction subway tunnels always key link urban infrastructure construction however also accompanied various construction risks safety challenges safety management technology subway tunnel construction proposed address safety management issues subway tunnels deploying real time monitoring systems integrating historical data descriptive inferential statistical analysis safety performance conducted failure mode impact analysis used identify potential risks analytic hierarchy process applied comprehensively evaluate safety management factors technical logic research emphasizes data driven systematic analysis predictive prevention improves adaptability management measures dynamic adjustment mechanisms providing scientific basis practical guidance tunnel safety management calculation time test calculation time research method maintained engineering projects calculating expected implementation cost highest implementation cost research method pesos reached days project progress analysis safety management maximum completion time research method days indicates research method effectively manage safety subway tunnels better implementation results lower negative impacts construction period
"In order to intelligently and efficiently identify the standardization of marketing on-site operations, an attention mechanism based method for identifying standardized small targets in marketing on-site operation images is proposed. In this method, a standardized small target recognition method based on attention mechanism for marketing on-site operation images is proposed to address the characteristics of image occlusion, diverse targets, and complex backgrounds. In the target detection network, attention mechanism has been added, detection head module has been added, and loss function has been optimized to improve the accuracy of model recognition. The proposed method achieves a 5.56% improvement in AP and a 5.35% improvement in mAP. We have collected and produced a large number of proprietary industry datasets for marketing on-site scenarios. The experimental results show that this method has high accuracy and robustness in identifying the standardization of marketing on-site operation images. At the same time, this method can provide effective technical means for the management and supervision of marketing on-site operations, and has high practical and promotional value.",order intelligently efficiently identify standardization marketing site operations attention mechanism based method identifying standardized small targets marketing site operation images proposed method standardized small target recognition method based attention mechanism marketing site operation images proposed address characteristics image occlusion diverse targets complex backgrounds target detection network attention mechanism added detection head module added loss function optimized improve accuracy model recognition proposed method achieves improvement improvement map collected produced large number proprietary industry datasets marketing site scenarios experimental results show method high accuracy robustness identifying standardization marketing site operation images time method provide effective technical means management supervision marketing site operations high practical promotional value
"Effectively capturing both temporal and spatial features of human actions is fundamental to designing robust action recognition classifiers. In this study, we introduce an end-to-end dual-stream approach for human action recognition that leverages global and local feature representations in conjunction with conditional random fields. The proposed framework adopts a dual-stream network design, where spatial and temporal cues from video frames are initially extracted using the ViBe algorithm (enhanced with a flicker coefficient) and the unsupervised TV-Net, respectively. These features are separately fed into the corresponding spatial and temporal branches of the network for pre-training and subsequent feature extraction. A parallel fusion mechanism is then applied to integrate the outputs from both streams, thereby enriching the descriptive power of the learned features. For the final stage, an improved anisotropic Markov random field model is employed for network training and result refinement. Comprehensive experiments conducted on widely used datasets—UCF101, HMDB51—as well as a proprietary Fujian electric power measurement action dataset, demonstrate that the proposed method achieves superior robustness and high recognition accuracy compared to state-of-the-art techniques.",effectively capturing temporal spatial features human actions fundamental designing robust action recognition classifiers study introduce end end dual stream approach human action recognition leverages global local feature representations conjunction conditional random fields proposed framework adopts dual stream network design spatial temporal cues video frames initially extracted using vibe algorithm enhanced flicker coefficient unsupervised net respectively features separately fed corresponding spatial temporal branches network pre training subsequent feature extraction parallel fusion mechanism applied integrate outputs streams thereby enriching descriptive power learned features final stage improved anisotropic markov random field model employed network training result refinement comprehensive experiments conducted widely used datasets ucf hmdb well proprietary fujian electric power measurement action dataset demonstrate proposed method achieves superior robustness high recognition accuracy compared state art techniques
"Scaphotrapeziotrapezoid arthrodesis is a controversial surgical procedure for wrist disorders and its biomechanical effect remains unclear. This study investigated scaphotrapeziotrapezoid fusion based on a previously validated whole-wrist finite element model to simulate arthrodesis by creating a unified bone complex from the three bones (scaphoid, trapezium and trapezoid) in the joint. The model was analysed under physiological grasping loads to examine axial load distributions and articular contact pressures at the radioscaphoid and radiolunate interfaces. The fused complex becomes a major load-bearing structure, while radiocarpal contact pressures at both the radioscaphoid and radiolunate interfaces show reductions compared with the intact model. This altered load distribution pattern suggests significant biomechanical adaptations after the procedure and supports its use for scapholunate instability and the treatment of Kienböck’s disease.",scaphotrapeziotrapezoid arthrodesis controversial surgical procedure wrist disorders biomechanical effect remains unclear study investigated scaphotrapeziotrapezoid fusion based previously validated whole wrist finite element model simulate arthrodesis creating unified bone complex three bones scaphoid trapezium trapezoid joint model analysed physiological grasping loads examine axial load distributions articular contact pressures radioscaphoid radiolunate interfaces fused complex becomes major load bearing structure radiocarpal contact pressures radioscaphoid radiolunate interfaces show reductions compared intact model altered load distribution pattern suggests significant biomechanical adaptations procedure supports use scapholunate instability treatment kienb disease
"In traditional overhead transmission line fault monitoring, traveling wave signals are not captured in time and the processing accuracy is insufficient, which seriously restricts the accuracy of fault location and the efficiency of power grid operation and maintenance. To address this problem, this paper constructs a real-time traveling wave monitoring system based on the Internet of Things (IoT) and cloud computing architecture and builds an efficient data processing link through the end-edge-cloud collaborative mechanism. The perception layer uses low-power IoT terminals to collect high-frequency electrical disturbance data. The edge layer deploys lightweight AI (Artificial Intelligence) models to extract signal features and trigger anomalies, significantly reducing the cloud load through localized preprocessing. The cloud relies on a distributed timing analysis engine to achieve deep fusion and precise positioning of multi-source data. Experimental results show that the accuracy of the designed system in 5G environment is 98.6% in traveling wave recognition; the fault location error is 110 m; the real-time response delay is only 28 milliseconds. It meets the monitoring requirements of high-precision and high real-time performance. The end-edge-cloud collaborative mechanism proposed in this paper provides a stable and efficient technical path for transmission line status perception and intelligent fault handling.",traditional overhead transmission line fault monitoring traveling wave signals captured time processing accuracy insufficient seriously restricts accuracy fault location efficiency power grid operation maintenance address problem paper constructs real time traveling wave monitoring system based internet things iot cloud computing architecture builds efficient data processing link end edge cloud collaborative mechanism perception layer uses low power iot terminals collect high frequency electrical disturbance data edge layer deploys lightweight artificial intelligence models extract signal features trigger anomalies significantly reducing cloud load localized preprocessing cloud relies distributed timing analysis engine achieve deep fusion precise positioning multi source data experimental results show accuracy designed system environment traveling wave recognition fault location error real time response delay milliseconds meets monitoring requirements high precision high real time performance end edge cloud collaborative mechanism proposed paper provides stable efficient technical path transmission line status perception intelligent fault handling
"This study focuses on the application and impact of mixed reality (MR) technology in English communicative competence training. Through an empirical study of MR technology, it explores how it can simulate real English communicative situations in order to promote the enhancement of listening, reading, writing, and speaking skills. The results of the study show that MR technology shows significant advantages in improving students’ listening comprehension and oral expression, and its personalized learning path and adaptive scenario design provide strong support for meeting students’ individual needs and development. In the process of implementation, the application architecture of MR technology includes advanced hardware equipment, software platform, stable and efficient network environment, and rich content resource library, which work together to build a highly immersive learning environment, realize personalized learning path setting, dynamic scene adaptation, analysis of in-depth learning data, and personalized feedback loops, so as to create an innovative English learning practice platform. Through well-designed experimental evaluations, the study confirms that the effectiveness of English communicative competence training using MR technology is superior to that of traditional teaching methods. MR technology shows obvious advantages in multiple dimensions such as listening comprehension, oral expression, use of communicative strategies, and the cultivation of cross-cultural communicative awareness. This research reveals the innovative role of MR technology in the reform of English education, and indicates that it has a broad application prospect and potential value in the future education research, which will lead the education field to develop in the direction of higher efficiency, stronger personalization and more fun.",study focuses application impact mixed reality technology english communicative competence training empirical study technology explores simulate real english communicative situations order promote enhancement listening reading writing speaking skills results study show technology shows significant advantages improving students listening comprehension oral expression personalized learning path adaptive scenario design provide strong support meeting students individual needs development process implementation application architecture technology includes advanced hardware equipment software platform stable efficient network environment rich content resource library work together build highly immersive learning environment realize personalized learning path setting dynamic scene adaptation analysis depth learning data personalized feedback loops create innovative english learning practice platform well designed experimental evaluations study confirms effectiveness english communicative competence training using technology superior traditional teaching methods technology shows obvious advantages multiple dimensions listening comprehension oral expression use communicative strategies cultivation cross cultural communicative awareness research reveals innovative role technology reform english education indicates broad application prospect potential value future education research lead education field develop direction higher efficiency stronger personalization fun
"Traditional villages are favored by many tourism enthusiasts due to their unique historical heritage and cultural attributes. However, with the rapid increase in regional traffic flow, serious traffic congestion problems may arise in the area. In response to the above issues, the study adopts a dual level planning model to plan the transportation of traditional village contiguous development areas. The study first analyzes regional transportation issues and establishes a model of upper level influencing factors based on the current situation. Next, the study considers the balanced configuration of lower level transportation and constructs a lower level planning model. Finally, the study introduces linear constraint method and improved genetic algorithm to solve the two-layer model. In genetic algorithm optimization training, the search performance reaches its optimal level when the crossover probability is 0.9 and the population size is 40. When the weight of the upper model is (0.5, 0.5), the saturation of each section of the transportation network is within the expected range, and road planning proceeds smoothly. Comparing the actual planning effects of different traffic planning models, the study found that in the traffic planning of congested sections 6 and 10, the research model has good planning effects and is superior to other models. In the comparison of planning costs, the research model has lower costs in 1 km road planning and adding one lane planning.",traditional villages favored many tourism enthusiasts due unique historical heritage cultural attributes however rapid increase regional traffic flow serious traffic congestion problems may arise area response issues study adopts dual level planning model plan transportation traditional village contiguous development areas study first analyzes regional transportation issues establishes model upper level influencing factors based current situation next study considers balanced configuration lower level transportation constructs lower level planning model finally study introduces linear constraint method improved genetic algorithm solve two layer model genetic algorithm optimization training search performance reaches optimal level crossover probability population size weight upper model saturation section transportation network within expected range road planning proceeds smoothly comparing actual planning effects different traffic planning models study found traffic planning congested sections research model good planning effects superior models comparison planning costs research model lower costs road planning adding one lane planning
"This paper discusses the innovative design and implementation of intelligent music visual teaching system, aiming at innovating music education through scientific and technological means, and coping with the challenges of uneven educational resources and single teaching methods. The system combines cutting-edge visualization technology and artificial intelligence algorithms to provide learners with an intuitive, efficient and personalized music learning experience. Based on the literature review, the advantages and disadvantages of the existing system are identified, and the detailed system requirements analysis is carried out. On this basis, a comprehensive intelligent teaching system is designed, which includes audio feature extraction and analysis, innovative visualization technology, machine learning model construction, and interaction mechanism. Experimental evaluation shows that the system is significantly better than traditional teaching methods in improving music theory knowledge, performance skills, music creation ability and user experience, while technical performance is stable, ensuring the security of user data. There was also a significant increase in user emotional engagement and motivation, demonstrating the system’s great potential to promote equity and quality in education. The research results of this paper provide strong support for the modernization transformation of music education, and put forward the direction for the continuous optimization and development of music teaching system in the future.",paper discusses innovative design implementation intelligent music visual teaching system aiming innovating music education scientific technological means coping challenges uneven educational resources single teaching methods system combines cutting edge visualization technology artificial intelligence algorithms provide learners intuitive efficient personalized music learning experience based literature review advantages disadvantages existing system identified detailed system requirements analysis carried basis comprehensive intelligent teaching system designed includes audio feature extraction analysis innovative visualization technology machine learning model construction interaction mechanism experimental evaluation shows system significantly better traditional teaching methods improving music theory knowledge performance skills music creation ability user experience technical performance stable ensuring security user data also significant increase user emotional engagement motivation demonstrating system great potential promote equity quality education research results paper provide strong support modernization transformation music education put forward direction continuous optimization development music teaching system future
"With the in-depth development of globalization, semantic alignment technology of cross-border English texts plays an increasingly important role in multilingual information processing. However, traditional semantic alignment methods are often limited by model size and computational efficiency, and it isn’t easy to meet the dual requirements of real-time and accuracy. This study aims to improve the performance of cross-border English text semantic alignment through BERT-INT8 quantitative model. We first perform INT8 quantization processing on the BERT model, significantly reducing the model parameters and computational complexity. The experimental results show that the semantic alignment accuracy of the optimized model has improved by 5.2% to 92.7%. The processing speed has been increased by 1.8 times, reaching the speed of processing 3000-word pairs per second. The average alignment accuracy on different types of text reached 90.5%, which was 4.3 percentage points higher than that of the unoptimized model. These results show that the BERT-INT8 quantization model significantly improves the processing efficiency while maintaining high alignment accuracy, with strong generalization ability and robustness. This study provides a new technical path for fast and accurate alignment of cross-border English texts, which can play an important role in practical applications.",depth development globalization semantic alignment technology cross border english texts plays increasingly important role multilingual information processing however traditional semantic alignment methods often limited model size computational efficiency easy meet dual requirements real time accuracy study aims improve performance cross border english text semantic alignment bert int quantitative model first perform int quantization processing bert model significantly reducing model parameters computational complexity experimental results show semantic alignment accuracy optimized model improved processing speed increased times reaching speed processing word pairs per second average alignment accuracy different types text reached percentage points higher unoptimized model results show bert int quantization model significantly improves processing efficiency maintaining high alignment accuracy strong generalization ability robustness study provides new technical path fast accurate alignment cross border english texts play important role practical applications
"With the acceleration of urbanization, the design and optimization of urban public space have become the key to improving urban quality and residents’ quality of life. This study explores the optimal urban public space design strategy based on GIS (geographic information system) and spatial behavior analysis. The research background lies in the problems of low efficiency, single function and traffic congestion in urban public spaces. Through GIS technology, the spatial analysis of urban public space is carried out. The residents’ usage habits are understood by combining spatial behavior analysis, and the optimization design strategy is put forward. The experimental results show that after optimizing the spatial layout, the use efficiency of public space increased by 25%; the functional configuration increased in leisure and entertainment areas, and the average daily flow of people in public spaces is increased by 40%. Improved traffic organization, reduced congestion points, and improved the accessibility of public space by 30%. This study confirms the effectiveness of GIS and spatial behavior analysis in the optimal public space design. It provides the scientific basis and practical guidance for future urban public space design and transformation.",acceleration urbanization design optimization urban public space become key improving urban quality residents quality life study explores optimal urban public space design strategy based gis geographic information system spatial behavior analysis research background lies problems low efficiency single function traffic congestion urban public spaces gis technology spatial analysis urban public space carried residents usage habits understood combining spatial behavior analysis optimization design strategy put forward experimental results show optimizing spatial layout use efficiency public space increased functional configuration increased leisure entertainment areas average daily flow people public spaces increased improved traffic organization reduced congestion points improved accessibility public space study confirms effectiveness gis spatial behavior analysis optimal public space design provides scientific basis practical guidance future urban public space design transformation
"With the increasingly refined and intelligent requirements for lighting and sound in dance performances, the traditional synchronous control methods of lighting and sound have made it difficult to meet the complex needs of stage performances. This study proposes a lighting and sound synchronization control method based on an adaptive algorithm, which improves the system’s response speed and synchronization accuracy in a dynamic stage environment through technologies such as deep learning, reinforcement learning and model-free adaptive control. In the initial test of the synchronization control of light and sound in dance performance, it was found that the fit between the light brightness change and the sound rhythm was only 79.4%. In the process of stage performance, there is still much room for optimization in the synchronous control of lighting and sound. Especially in complex dance passages, there are significant delay and synchronization errors. In a dance paragraph with a duration of 18.53 s, the average delay time of light color conversion is 35.39 ms, while the delay of sound volume adjustment is 29.09 ms. With the change of stage performance rhythm, this delay problem may lead to the out-of-synchronization of the audience’s visual and auditory experience, thus affecting the expressiveness of the performance. In a specific stage scene, the movement deviation of the lighting position reaches 34.46 cm, while the spatial positioning deviation of the sound effect is 56.64 cm. In the initial test of the synchronization control of light and sound in dance performance, it was found that the fit between the light brightness change and the sound rhythm was only 79.4%. In the process of stage performance, there is still much room for optimization in the synchronous control of lighting and sound. Especially in complex dance passages—defined as segments with movement velocity exceeding 1.5 m/s, direction changes more than 3 times per second, or simultaneous variations in at least three signal modalities (e.g., dance movements, sound frequency, and light intensity)—there are significant delay and synchronization errors.",increasingly refined intelligent requirements lighting sound dance performances traditional synchronous control methods lighting sound made difficult meet complex needs stage performances study proposes lighting sound synchronization control method based adaptive algorithm improves system response speed synchronization accuracy dynamic stage environment technologies deep learning reinforcement learning model free adaptive control initial test synchronization control light sound dance performance found fit light brightness change sound rhythm process stage performance still much room optimization synchronous control lighting sound especially complex dance passages significant delay synchronization errors dance paragraph duration average delay time light color conversion delay sound volume adjustment change stage performance rhythm delay problem may lead synchronization audience visual auditory experience thus affecting expressiveness performance specific stage scene movement deviation lighting position reaches spatial positioning deviation sound effect initial test synchronization control light sound dance performance found fit light brightness change sound rhythm process stage performance still much room optimization synchronous control lighting sound especially complex dance passages defined segments movement velocity exceeding direction changes times per second simultaneous variations least three signal modalities dance movements sound frequency light intensity significant delay synchronization errors
"This study proposes a fusion feature extraction and transfer learning-based framework for emotion recognition in art and design. We extract low-level visual features (color and texture) and semantic features, and integrate them using a multi-branch convolutional network. Transfer learning techniques are employed to enhance recognition performance on small-scale artistic datasets. Experimental evaluations on the Flickr dataset demonstrate that our method improves recognition efficiency by 8% and increases accuracy by 5% compared to traditional approaches. The proposed approach shows strong generalization across diverse artistic styles, offering a promising solution for emotion analysis in creative industries.",study proposes fusion feature extraction transfer learning based framework emotion recognition art design extract low level visual features color texture semantic features integrate using multi branch convolutional network transfer learning techniques employed enhance recognition performance small scale artistic datasets experimental evaluations flickr dataset demonstrate method improves recognition efficiency increases accuracy compared traditional approaches proposed approach shows strong generalization across diverse artistic styles offering promising solution emotion analysis creative industries
"Modern power systems, serving as the lifeline of societal operations, face challenges to their stability due to equipment damage and service interruptions caused by grid faults. Existing learning-based methods of-ten rely solely on structured data as features, where random fluctuations in the data can lead to overly sensitive fault predictions. To address this issue, this study innovatively integrates multi-dimensional in-formation from both structured and unstructured data, proposing the ALLA prediction framework based on the collaborative reasoning of Attention-LSTM and large language models. By dynamically filtering key features from structured data such as current and voltage through the attention mechanism, and leveraging the temporal reasoning capabilities of large language models on unstructured texts like fault re-ports and maintenance logs, a dual-channel feature enhancement model is constructed. Experimental results demonstrate that this method achieves an average F1 score of 0.9818 across five power grid datasets, representing a 2.4% improvement over current state-of-the-art methods. This framework provides a new paradigm for resolving the sensitivity-accuracy trade-off in fault prediction under complex environments, significantly enhancing the accuracy of converter valve fault predictions.",modern power systems serving lifeline societal operations face challenges stability due equipment damage service interruptions caused grid faults existing learning based methods ten rely solely structured data features random fluctuations data lead overly sensitive fault predictions address issue study innovatively integrates multi dimensional formation structured unstructured data proposing alla prediction framework based collaborative reasoning attention lstm large language models dynamically filtering key features structured data current voltage attention mechanism leveraging temporal reasoning capabilities large language models unstructured texts like fault ports maintenance logs dual channel feature enhancement model constructed experimental results demonstrate method achieves average score across five power grid datasets representing improvement current state art methods framework provides new paradigm resolving sensitivity accuracy trade fault prediction complex environments significantly enhancing accuracy converter valve fault predictions
"This research introduces an innovative computational framework that automatically extracts multidimensional personalized feature labels from student course reflection reports. Unlike traditional learner profiling methods that rely on questionnaires and standardized assessments, our approach leverages natural language processing, sentiment analysis, and unsupervised learning to mine naturally occurring student-generated text. The key innovation lies in our integration of computational linguistics with educational theory to identify five distinct learner profiles (Academic Analytical, Innovative Thinker, Collaborative, Optimistic Active, and Expressive Rich Learner) along with specific trait labels that capture unique cognitive, emotional, and social dimensions of learning. We demonstrate the framework’s effectiveness through experimental validation with both student self-assessment (78.2% agreement) and instructor evaluation, confirming that algorithmically-determined classifications strongly align with human perceptions. Our methodology establishes formal mapping relationships between extracted personalized labels and specific instructional strategies, enabling educators to tailor teaching approaches to individual learning profiles. By analyzing unstructured text that students naturally produce during their educational journey, we achieve non-intrusive learner profiling that provides insight into student learning characteristics. The methodology we’ve developed establishes a foundation that could potentially be extended to capture learning development over time in future longitudinal studies with larger samples. This work contributes to the advancement of adaptive education by bridging the gap between computational data analysis and practical pedagogical interventions, ultimately supporting the development of more responsive and personalized learning environments.",research introduces innovative computational framework automatically extracts multidimensional personalized feature labels student course reflection reports unlike traditional learner profiling methods rely questionnaires standardized assessments approach leverages natural language processing sentiment analysis unsupervised learning mine naturally occurring student generated text key innovation lies integration computational linguistics educational theory identify five distinct learner profiles academic analytical innovative thinker collaborative optimistic active expressive rich learner along specific trait labels capture unique cognitive emotional social dimensions learning demonstrate framework effectiveness experimental validation student self assessment agreement instructor evaluation confirming algorithmically determined classifications strongly align human perceptions methodology establishes formal mapping relationships extracted personalized labels specific instructional strategies enabling educators tailor teaching approaches individual learning profiles analyzing unstructured text students naturally produce educational journey achieve non intrusive learner profiling provides insight student learning characteristics methodology developed establishes foundation could potentially extended capture learning development time future longitudinal studies larger samples work contributes advancement adaptive education bridging gap computational data analysis practical pedagogical interventions ultimately supporting development responsive personalized learning environments
"This paper introduces a learning innovation that merges science fiction with entrepreneurship education to encourage bold, futures-oriented thinking and entrepreneurial action today. Developed for master’s level students at a public research-oriented university, this learning innovation promotes the exploration of distant futures, leveraging established tools from futures studies within an entrepreneurial context. Through a structured five-step process, students shift their focus from cognitively proximate futures to those that radically diverge from the present, enabling them to conceive of innovative ventures. The learning innovation thus fosters performative futures thinking, engaging in speculative science fiction authoring, and utilizing backcasting to translate visionary futures into immediate entrepreneurial actions. Over three yearly iterations, the learning innovation has demonstrated its effectiveness in broadening students’ imaginative capabilities and improving their capacity to act upon fictional expectations. This paper reflects on the outcomes, student feedback, and potential applications of the learning innovation in various educational and corporate settings.",paper introduces learning innovation merges science fiction entrepreneurship education encourage bold futures oriented thinking entrepreneurial action today developed master level students public research oriented university learning innovation promotes exploration distant futures leveraging established tools futures studies within entrepreneurial context structured five step process students shift focus cognitively proximate futures radically diverge present enabling conceive innovative ventures learning innovation thus fosters performative futures thinking engaging speculative science fiction authoring utilizing backcasting translate visionary futures immediate entrepreneurial actions three yearly iterations learning innovation demonstrated effectiveness broadening students imaginative capabilities improving capacity act upon fictional expectations paper reflects outcomes student feedback potential applications learning innovation various educational corporate settings
"With the continuous advancement of artificial intelligence technology, intelligent digital art design gradually integrates multi-modal data, such as images, text, and audio, in the creative process, improving the creativity and efficiency of design. Traditional art design platforms have problems such as insufficient information fusion and low creative efficiency when dealing with multi-modal data. In order to solve these challenges, this paper proposes an intelligent digital art design fusion platform based on multi-modal Transformer, which effectively fuses data of different modalities through the multi-modal Transformer architecture to improve creative efficiency and work quality. The proposed multi-modal Transformer framework is a novel approach to digital art creation, overcoming traditional limitations of single-modal platforms by integrating image, text, and audio. This multi-modal fusion significantly enhances creative efficiency by 33.3% and creativity by improving both the diversity and expressiveness of the generated artworks, primarily within a unified image resolution framework. However, the current platform is optimized for fixed resolution image generation. Handling mixed resolutions or modified images (in pixel or grid formats) presents challenges, particularly in maintaining output integrity. This limitation is recognized and will be addressed in future work through adaptive resolution techniques. The innovation lies in effectively leveraging the self-attention mechanism to balance the computational load while enriching creative outputs, addressing both artistic and technological challenges. Specific data analysis shows that the average time of three-modal fusion creation design is 80 min, while that of single-modal creation is 120 min, which proves the significant advantages of multi-modal fusion in accelerating design creation. In addition, the platform has also achieved good results in the quality of creation, and the creative score has increased by about 25% compared with the traditional platform.",continuous advancement artificial intelligence technology intelligent digital art design gradually integrates multi modal data images text audio creative process improving creativity efficiency design traditional art design platforms problems insufficient information fusion low creative efficiency dealing multi modal data order solve challenges paper proposes intelligent digital art design fusion platform based multi modal transformer effectively fuses data different modalities multi modal transformer architecture improve creative efficiency work quality proposed multi modal transformer framework novel approach digital art creation overcoming traditional limitations single modal platforms integrating image text audio multi modal fusion significantly enhances creative efficiency creativity improving diversity expressiveness generated artworks primarily within unified image resolution framework however current platform optimized fixed resolution image generation handling mixed resolutions modified images pixel grid formats presents challenges particularly maintaining output integrity limitation recognized addressed future work adaptive resolution techniques innovation lies effectively leveraging self attention mechanism balance computational load enriching creative outputs addressing artistic technological challenges specific data analysis shows average time three modal fusion creation design min single modal creation min proves significant advantages multi modal fusion accelerating design creation addition platform also achieved good results quality creation creative score increased compared traditional platform
"This paper explores the application framework of big data in the evaluation of teaching quality in higher education, focusing on data collection and integration, personalized learning path design, innovation of teaching content and methods, and support for teachers’ professional development. By analyzing students’ learning behavior data, teachers’ teaching activity data, and course resource data, this study designed and implemented a comprehensive evaluation system, including a learning ability evaluation model, a personalized recommendation system, a teaching effect prediction model, a teacher ability evaluation model, and a development indicator quantification model. Specifically, the experimental results show that each model performs well in terms of accuracy, explanatory power, and prediction efficiency: the AUC of the learning ability evaluation model reaches 0.892, the accuracy of the personalized recommendation system reaches 0.765, and the determination coefficient of the teaching effect prediction model is 0.789. These key findings not only reveal the main influencing factors of teaching quality, but also put forward suggestions for the optimization of teaching evaluation indicators, thereby providing a scientific basis for educational decision-making.",paper explores application framework big data evaluation teaching quality higher education focusing data collection integration personalized learning path design innovation teaching content methods support teachers professional development analyzing students learning behavior data teachers teaching activity data course resource data study designed implemented comprehensive evaluation system including learning ability evaluation model personalized recommendation system teaching effect prediction model teacher ability evaluation model development indicator quantification model specifically experimental results show model performs well terms accuracy explanatory power prediction efficiency auc learning ability evaluation model reaches accuracy personalized recommendation system reaches determination coefficient teaching effect prediction model key findings reveal main influencing factors teaching quality also put forward suggestions optimization teaching evaluation indicators thereby providing scientific basis educational decision making
"With the rapid development of Internet technology, the digital contract is a vital medium of information dissemination. The rapid spread of rumors in Internet news communication misleads public cognition and may cause social panic and instability. This study aims to explore a deep reinforcement learning (DRL) model for rumor detection in news communication that integrates an attention mechanism. By enriching and advancing the theoretical framework and technical methods in the field of rumor detection, it seeks to provide more accurate and reliable information support for Internet news communication. This study first analyzes the public opinion transmission cycle of Internet news. It makes it clear that the word vector model is the key link in the natural language processing task of news rumor detection. In the task of rumor detection, a Deep Q-Network (DQN) detection model is further proposed, which takes text information as input, extracts features, and learns optimal strategies through the deep neural network, to achieve accurate recognition of rumors. The attention mechanism is integrated into the model to help it better identify the key information in the text, such as keywords, key sentences, etc., thereby improving the accuracy of detection. On the Pheme Twitter and Weibo datasets, the DQN + Attention model proposed in this study has improved accuracy by 11.09% and 10.1% compared to the Long Short-Term Memory model. Compared with the Relay Diffusion Model, the proposed model’s accuracy on the two datasets is increased by 2.5% and 0.2%, respectively. This finding demonstrates that the model can still achieve or even surpass the effectiveness of complex models that combine multiple types of information even when using only textual information. The research results have vital practical value for the field of rumor detection. They not only assist in accurately and timely identifying rumors but also provide effective rumor prevention and control measures for social media platforms.",rapid development internet technology digital contract vital medium information dissemination rapid spread rumors internet news communication misleads public cognition may cause social panic instability study aims explore deep reinforcement learning drl model rumor detection news communication integrates attention mechanism enriching advancing theoretical framework technical methods field rumor detection seeks provide accurate reliable information support internet news communication study first analyzes public opinion transmission cycle internet news makes clear word vector model key link natural language processing task news rumor detection task rumor detection deep network dqn detection model proposed takes text information input extracts features learns optimal strategies deep neural network achieve accurate recognition rumors attention mechanism integrated model help better identify key information text keywords key sentences etc thereby improving accuracy detection pheme twitter weibo datasets dqn attention model proposed study improved accuracy compared long short term memory model compared relay diffusion model proposed model accuracy two datasets increased respectively finding demonstrates model still achieve even surpass effectiveness complex models combine multiple types information even using textual information research results vital practical value field rumor detection assist accurately timely identifying rumors also provide effective rumor prevention control measures social media platforms
"The grounding grid is an important component of the substation, and its corrosion will directly influence the safe operation of the power system. In order to overcome the limitations of traditional corrosion diagnosis methods, the corresponding corrosion diagnosis equations model was constructed by using electric network theory. Considering the limited number of measurable nodes in the actual substation, which leads to the characteristics of high-dimensional underdetermination of the equations, an improved RMSProp-Lookahead hybrid optimization algorithm was proposed to solve the problem. The method combined the Gradient normalization mechanism and the Lookahead dual update strategy, optimized the hyperparameter configuration through grid search, and improved the convergence speed and global search capability of the optimization process. The simulation and experimental results showed that the method can effectively locate the corrosion branch, accurately judge the corrosion degree, and significantly improved the diagnostic efficiency and accuracy, which has important practical significance for the safety maintenance of the grounding network.",grounding grid important component substation corrosion directly influence safe operation power system order overcome limitations traditional corrosion diagnosis methods corresponding corrosion diagnosis equations model constructed using electric network theory considering limited number measurable nodes actual substation leads characteristics high dimensional underdetermination equations improved rmsprop lookahead hybrid optimization algorithm proposed solve problem method combined gradient normalization mechanism lookahead dual update strategy optimized hyperparameter configuration grid search improved convergence speed global search capability optimization process simulation experimental results showed method effectively locate corrosion branch accurately judge corrosion degree significantly improved diagnostic efficiency accuracy important practical significance safety maintenance grounding network
"In the field of machine design and control systems for carbon fiber production, achieving precise control of filament conveying speed is crucial as it directly affects product quality and production efficiency. The existing approximate methods for this problem include traditional PID control systems and manual control mechanisms. These methods have some drawbacks, such as poor handling of nonlinear dynamics, long response times, and inability to optimize production in real time. To partially overcome these drawbacks, this manuscript proposes an implementation of a method based on deep learning algorithms, particularly convolutional neural networks and long-term short-term memory networks. This method improves the performance of carbon fiber shaping machines by predicting and adjusting the filament conveying speed with higher accuracy and responsiveness. Compared with traditional PID control systems, the positive results of this method include a 38.2% improvement in control accuracy, a 22.3% reduction in response time, and a 10.5% increase in production efficiency. However, there are still some limitations, such as the challenge of integrating the system into existing manufacturing infrastructure and the need for further improvements in noise reduction technology. Future work will focus on addressing long-term system performance and maintenance, including periodic retraining of models to adapt to evolving production conditions. Regular maintenance of the system infrastructure, such as sensor calibration and data quality checks, will be necessary to ensure stable and efficient operation over extended periods.",field machine design control systems carbon fiber production achieving precise control filament conveying speed crucial directly affects product quality production efficiency existing approximate methods problem include traditional pid control systems manual control mechanisms methods drawbacks poor handling nonlinear dynamics long response times inability optimize production real time partially overcome drawbacks manuscript proposes implementation method based deep learning algorithms particularly convolutional neural networks long term short term memory networks method improves performance carbon fiber shaping machines predicting adjusting filament conveying speed higher accuracy responsiveness compared traditional pid control systems positive results method include improvement control accuracy reduction response time increase production efficiency however still limitations challenge integrating system existing manufacturing infrastructure need improvements noise reduction technology future work focus addressing long term system performance maintenance including periodic retraining models adapt evolving production conditions regular maintenance system infrastructure sensor calibration data quality checks necessary ensure stable efficient operation extended periods
"This paper introduces the development status and prospects of cross-border e-commerce platforms and the impact of language barriers on their service quality and user trust, and analyzes the needs and problems of cross-border e-commerce platforms in language translation. In this paper, an innovative deep learning-driven language translation solution is proposed to realize real-time multi-language and multimodal conversion by integrating neural machine translation, multimodal translation and domain-adaptive translation to improve the accuracy, fluency, adaptability, coverage, robustness, consistency, and evaluability of translation. This paper conducts experiments on several datasets and language pairs, compares with several other machine translation models, evaluates the translation quality and efficiency of this paper’s model on different language pairs and domains, and analyzes the strengths and limitations of this paper’s model when dealing with cross-border e-commerce related texts.",paper introduces development status prospects cross border commerce platforms impact language barriers service quality user trust analyzes needs problems cross border commerce platforms language translation paper innovative deep learning driven language translation solution proposed realize real time multi language multimodal conversion integrating neural machine translation multimodal translation domain adaptive translation improve accuracy fluency adaptability coverage robustness consistency evaluability translation paper conducts experiments several datasets language pairs compares several machine translation models evaluates translation quality efficiency paper model different language pairs domains analyzes strengths limitations paper model dealing cross border commerce related texts
"With the increasing pressure of study, university students generally have a series of psychological problems, and these psychological problems affect the emotional regulation ability of university students. How to make university students have better emotion regulation ability so that they can better cope with various psychological problems is a point of great concern in current college education. In this article, a series of researches are conducted on the psychological health curriculum and university students’ emotion regulation ability. In view of the shortcomings of the original college psychological courses, a new course quality assessment index system is constructed and an assessment model is built using (Genetic Algorithm-Back Propagation Neural Network, GA-BPNN) GA-BPNN taking into account the influence of various emotional factors. The experimental results show that the average testing error of the proposed GA-BPNN model is 3.14, which does not generate extreme data and is lower than the traditional BPNN’s 5.15. The average scores for expectations, attitudes of others, and the influence of environment and others are 3.72, 3.21, and 3.56, respectively. The average scores for positive and negative emotions are 0.96 and 0.79, respectively. The experimental results have demonstrated the performance of the proposed GA-BPNN model in evaluating the quality of mental health courses, and pointed out that the improvement of mental health education courses has a certain impact on the emotional regulation ability of college students. The research results are helpful in promoting the improvement of mental health education courses, thereby improving the mental health status of college students.",increasing pressure study university students generally series psychological problems psychological problems affect emotional regulation ability university students make university students better emotion regulation ability better cope various psychological problems point great concern current college education article series researches conducted psychological health curriculum university students emotion regulation ability view shortcomings original college psychological courses new course quality assessment index system constructed assessment model built using genetic algorithm back propagation neural network bpnn bpnn taking account influence various emotional factors experimental results show average testing error proposed bpnn model generate extreme data lower traditional bpnn average scores expectations attitudes others influence environment others respectively average scores positive negative emotions respectively experimental results demonstrated performance proposed bpnn model evaluating quality mental health courses pointed improvement mental health education courses certain impact emotional regulation ability college students research results helpful promoting improvement mental health education courses thereby improving mental health status college students
"The calculation of compaction degree for earth-rock dams based on non-embedded methods can assess compaction levels. However, due to the variability in the source, filling processes, and environmental conditions of earth-rock dams, the spatial distribution of fill materials is often uneven, particularly in mixed fill, leading to discrepancies in compaction degree and complicating assessment efforts. To enhance the accuracy of compaction degree evaluation, the latest deep learning Evo-Learn model is utilized to correct compaction parameters. This method employs a weight optimization strategy that combines Genetic Algorithm (GA) with Backpropagation to optimize neural network weights, thereby improving model robustness and performance. Through selection, crossover, mutation, and other operations, the mixed error between the sample set and the test set is optimized, so as to improve the generalization performance of the compression prediction model and reduce overfitting. The experimental results show that the new algorithm proposed in this paper can effectively improve the accuracy of predicting the compaction degree of earth-rock dams with intelligent sensing technology.",calculation compaction degree earth rock dams based non embedded methods assess compaction levels however due variability source filling processes environmental conditions earth rock dams spatial distribution fill materials often uneven particularly mixed fill leading discrepancies compaction degree complicating assessment efforts enhance accuracy compaction degree evaluation latest deep learning evo learn model utilized correct compaction parameters method employs weight optimization strategy combines genetic algorithm backpropagation optimize neural network weights thereby improving model robustness performance selection crossover mutation operations mixed error sample set test set optimized improve generalization performance compression prediction model reduce overfitting experimental results show new algorithm proposed paper effectively improve accuracy predicting compaction degree earth rock dams intelligent sensing technology
"What do we learn when we invite others to make and create? How can drawing, cutting and pasting, repurposing objects, and photography enable us to explore complex, or hard-to-talk about experiences? What do we miss when we ask only with words, and not with action? This article explores these questions, demonstrating how to enact different arts-based research methods in practices of inquiry to open the process of thought and care in research related to public health. With reference to one line of inquiry as an exemplar – namely, how to promote care – this article reveals the complementary value of several arts-based research methods – these include: found objects; body mapping; collective collage making; and photography. This article: presents an overview of arts-based research methods, explaining what they are (and are not) and their purpose; demonstrates how arts-based research might be used to promote care; clarifies the benefits, limitations, and ethical considerations associated with arts-based research; and invites readers to consider how they might incorporate arts-based research in their scholarship, highlighting particular questions that warrant consideration.",learn invite others make create drawing cutting pasting repurposing objects photography enable explore complex hard talk experiences miss ask words action article explores questions demonstrating enact different arts based research methods practices inquiry open process thought care research related public health reference one line inquiry exemplar namely promote care article reveals complementary value several arts based research methods include found objects body mapping collective collage making photography article presents overview arts based research methods explaining purpose demonstrates arts based research might used promote care clarifies benefits limitations ethical considerations associated arts based research invites readers consider might incorporate arts based research scholarship highlighting particular questions warrant consideration
"The present study offers an exploratory analysis of the creative processes and products at the heart of song lyric writing. Drawing on insights from creative metacognition research and work that integrates individual and collective aspects of creativity, we focus on instances where individuals generate artistic outcomes both for themselves and for others. Sixty-three participants (35 musical experts and 28 novices) were invited to complete a series of questionnaires and craft new lyrics for two distinct musical excerpts. They were instructed to write for either themselves (envisioning they would sing the final product) or for another individual (considering someone else would perform). The creativity of the lyrics was assessed using three complementary approaches: (i) self-ratings by participants, (ii) qualitative evaluations by six expert raters, and (iii) algorithmic analysis through distributional semantic modeling. Although we did not find significant differences in creativity between lyrics created for oneself and lyrics created for others, our analysis revealed that participants’ behaviors and states are more predictive of self-assessment and computational ratings of creativity than expert ratings. Furthermore, we identified a robust correlation between expert ratings and computational evaluations of the lyrics’ creativity.",present study offers exploratory analysis creative processes products heart song lyric writing drawing insights creative metacognition research work integrates individual collective aspects creativity focus instances individuals generate artistic outcomes others sixty three participants musical experts novices invited complete series questionnaires craft new lyrics two distinct musical excerpts instructed write either envisioning would sing final product another individual considering someone else would perform creativity lyrics assessed using three complementary approaches self ratings participants qualitative evaluations six expert raters iii algorithmic analysis distributional semantic modeling although find significant differences creativity lyrics created oneself lyrics created others analysis revealed participants behaviors states predictive self assessment computational ratings creativity expert ratings furthermore identified robust correlation expert ratings computational evaluations lyrics creativity
"With the increasingly prominent mental health issues among college students, how to identify students with psychological abnormalities through behavioral data has become an urgent research problem to be solved. A Hybrid Model for Psychological Abnormal Student Behavior Identification (HMPABI) based on educational big data is proposed. By combining multidimensional behavioral characteristics and psychological assessment data of students during their school years, an efficient and accurate identification model for students with psychological abnormalities is constructed through clustering, oversampling techniques, and a mixed classification strategy of logistic regression and support vector machine. The study takes the Urumqi University Student Campus Behavior Dataset and the Adolescent Mental Health and Behavior Dataset for experimental verification. On the UUS-CBD dataset, the average absolute error of the HMPABI model was lower than that of other comparison models in all testing time windows, exhibiting higher prediction accuracy and stability, with a maximum error value of 0.28. In contrast, the errors of the other two models reached 0.45 and 0.42 at the maximum time window (45 min), respectively. The HMPBAI model can fully explore the potential information of students’ behavioral characteristics. By integrating different types of data, it can more accurately predict students with psychological abnormalities. This study provides a new technological path for monitoring and intervening in the mental health of college students and has high practical application value.",increasingly prominent mental health issues among college students identify students psychological abnormalities behavioral data become urgent research problem solved hybrid model psychological abnormal student behavior identification hmpabi based educational big data proposed combining multidimensional behavioral characteristics psychological assessment data students school years efficient accurate identification model students psychological abnormalities constructed clustering oversampling techniques mixed classification strategy logistic regression support vector machine study takes urumqi university student campus behavior dataset adolescent mental health behavior dataset experimental verification uus cbd dataset average absolute error hmpabi model lower comparison models testing time windows exhibiting higher prediction accuracy stability maximum error value contrast errors two models reached maximum time window min respectively hmpbai model fully explore potential information students behavioral characteristics integrating different types data accurately predict students psychological abnormalities study provides new technological path monitoring intervening mental health college students high practical application value
"This study aims to explore the application of deep learning in a sports teaching model based on the flipped classroom concept to enhance student engagement and teaching effectiveness. It constructs a personalized recommendation system based on the self-attention mechanism, integrating deep learning algorithms to provide customized learning paths and resources for sports teaching. Implemented in a flipped classroom, the system collects and analyzes student learning data to optimize the personalized learning model. The study is conducted in a university sports course, dividing students into an experimental group and a control group. The experimental group uses the flipped classroom model supported by the personalized recommendation system, while the control group follows the traditional teaching model. The results show that the prediction accuracy of the personalized recommendation system reaches 95.15%, significantly higher than other models (such as Convolutional Neural Network (CNN)). The surveys indicate that the experimental group outperforms the control group in classroom participation, learning satisfaction, and problem-solving abilities, with statistically significant differences (P &lt; 0.05). This demonstrates that the system significantly improves learning outcomes. The study concludes that the personalized recommendation system based on the self-attention mechanism can effectively overcome the limitations of traditional teaching models. This study offers valuable practical solutions for innovation in sports teaching and promotes the deep integration of educational technology and sports instruction.",study aims explore application deep learning sports teaching model based flipped classroom concept enhance student engagement teaching effectiveness constructs personalized recommendation system based self attention mechanism integrating deep learning algorithms provide customized learning paths resources sports teaching implemented flipped classroom system collects analyzes student learning data optimize personalized learning model study conducted university sports course dividing students experimental group control group experimental group uses flipped classroom model supported personalized recommendation system control group follows traditional teaching model results show prediction accuracy personalized recommendation system reaches significantly higher models convolutional neural network cnn surveys indicate experimental group outperforms control group classroom participation learning satisfaction problem solving abilities statistically significant differences demonstrates system significantly improves learning outcomes study concludes personalized recommendation system based self attention mechanism effectively overcome limitations traditional teaching models study offers valuable practical solutions innovation sports teaching promotes deep integration educational technology sports instruction
"Economic globalization makes the technological innovation of enterprises become the key to win or lose in market competition. To improve the innovation ability of enterprises and enable them to achieve sustainable development, many enterprises have begun to carry out digital transformation (DT). Considering the financial risks existing in the DT of enterprises, a prediction model was first established based on convolution neural network (CNN) and entropy method, aiming at providing data reference for enterprises. To further study the relationship between DT and enterprise innovation (EI) development, three research hypotheses were given through theoretical analysis. The correlation was calculated by multiple linear regression. According to the results of financial risk simulation experiment, when the iterations was 1500, the prediction accuracy of GRA-Entropy-CNN was 93%. This was 2% and 8% higher than TOPSIS-Entropy-CNN and FCE-Entropy-CNN, respectively. The accuracy of GRA-Entropy-SOM-CNN was 92%, which was 12% higher than that of GRA-CRITIC-SOM-CNN. This shows that the combination of entropy method and grey correlation method with CNN can improve the prediction accuracy. The empirical analysis results show that the development of innovation will be positively affected by the transformation of digital technology, and its regression coefficient is 0.556, and the significance level is 0.001; the innovation development will be positively affected by the knowledge innovation ability, and its regression coefficient is 0.188, and the significance level is 0.001. The above results demonstrate that the combination of convolutional neural networks and entropy method can effectively improve the accuracy of financial risk prediction, and also prove that digital technology transformation can promote innovative development of enterprises, providing a theoretical basis for the formulation of strategies for enterprises in digital transformation.",economic globalization makes technological innovation enterprises become key win lose market competition improve innovation ability enterprises enable achieve sustainable development many enterprises begun carry digital transformation considering financial risks existing enterprises prediction model first established based convolution neural network cnn entropy method aiming providing data reference enterprises study relationship enterprise innovation development three research hypotheses given theoretical analysis correlation calculated multiple linear regression according results financial risk simulation experiment iterations prediction accuracy gra entropy cnn higher topsis entropy cnn fce entropy cnn respectively accuracy gra entropy som cnn higher gra critic som cnn shows combination entropy method grey correlation method cnn improve prediction accuracy empirical analysis results show development innovation positively affected transformation digital technology regression coefficient significance level innovation development positively affected knowledge innovation ability regression coefficient significance level results demonstrate combination convolutional neural networks entropy method effectively improve accuracy financial risk prediction also prove digital technology transformation promote innovative development enterprises providing theoretical basis formulation strategies enterprises digital transformation
"With the rapid development of vocational education informatization, how to accurately evaluate curriculum quality and learning effect has become an important topic in the field of education. This paper proposes an intelligent evaluation model based on the combination of Diffusion reinforcement learning and multi-scale residual network, aiming at improving the evaluation accuracy and intelligence level of vocational education courses. The model is designed to scale efficiently for large datasets and diverse educational platforms, ensuring its applicability across different regions with varying curricula and student engagement. By designing a comprehensive model, this model can monitor the learning process in real time, analyze students’ learning behaviors, and ensure the security of data based on network security requirements. Based on the Diffusion reinforcement learning method, this paper uses the state, action, and reward mechanism in the reinforcement learning framework to dynamically evaluate the course content. The training process of reinforcement learning obtains real-time feedback through interaction with students, forming an accurate curriculum evaluation model. Experimental data show that the model based on Diffusion reinforcement learning improves the accuracy of course evaluation by about 15%, and the evaluation error of the model is reduced by 20% compared with traditional evaluation methods. Combined with multi-scale residual network, the curriculum data is deeply analyzed, and the accuracy and robustness of the evaluation system are enhanced by multi-level feature extraction. The introduction of MSRN enables the model to extract features at different scales, further improving the depth and accuracy of understanding of course content. The experimental verification shows that the evaluation accuracy of the model after using MSRN is improved by 18%, and it performs well in the adaptability of different course contents, especially in complex data sets. Considering the network security, this paper specially designs the data transmission and storage mode based on encryption technology to ensure the privacy and security of students’ data. By adopting SSL encryption protocol and firewall technology, the system can effectively resist external attacks and ensure the integrity of evaluation data. Experiments show that under the premise of network security, the evaluation results of the model are not affected by malicious attacks, and the stability of the model is improved by 25%. The intelligent evaluation model proposed in this research has significant advantages in improving the accuracy and robustness of vocational education curriculum evaluation, especially in the context of complex data and network security, and shows a good practical application prospect.",rapid development vocational education informatization accurately evaluate curriculum quality learning effect become important topic field education paper proposes intelligent evaluation model based combination diffusion reinforcement learning multi scale residual network aiming improving evaluation accuracy intelligence level vocational education courses model designed scale efficiently large datasets diverse educational platforms ensuring applicability across different regions varying curricula student engagement designing comprehensive model model monitor learning process real time analyze students learning behaviors ensure security data based network security requirements based diffusion reinforcement learning method paper uses state action reward mechanism reinforcement learning framework dynamically evaluate course content training process reinforcement learning obtains real time feedback interaction students forming accurate curriculum evaluation model experimental data show model based diffusion reinforcement learning improves accuracy course evaluation evaluation error model reduced compared traditional evaluation methods combined multi scale residual network curriculum data deeply analyzed accuracy robustness evaluation system enhanced multi level feature extraction introduction msrn enables model extract features different scales improving depth accuracy understanding course content experimental verification shows evaluation accuracy model using msrn improved performs well adaptability different course contents especially complex data sets considering network security paper specially designs data transmission storage mode based encryption technology ensure privacy security students data adopting ssl encryption protocol firewall technology system effectively resist external attacks ensure integrity evaluation data experiments show premise network security evaluation results model affected malicious attacks stability model improved intelligent evaluation model proposed research significant advantages improving accuracy robustness vocational education curriculum evaluation especially context complex data network security shows good practical application prospect
"As biofouling on ship hulls can reach several millimeters in thickness, accurately predicting frictional resistance requires consideration of roughness effects beyond the scope of traditional empirical formulas developed for microscale surfaces. To address this, the complex and irregular surface geometry of biofouling was simplified by decomposing it into a series of single-wave models. Large Eddy Simulations (LES) were conducted to simulate turbulent channel flow over wavy walls, generating a comprehensive dataset of frictional resistance components. The frictional force acting on the wavy surface was defined as the effective shear stress, accounting for both viscous shear and pressure-induced forces. The distribution of friction velocity exhibited a strong correlation with the pressure component acting on the surface. The friction coefficient was found to be highest in regions characterized by large wave amplitudes and short wavelengths.",biofouling ship hulls reach several millimeters thickness accurately predicting frictional resistance requires consideration roughness effects beyond scope traditional empirical formulas developed microscale surfaces address complex irregular surface geometry biofouling simplified decomposing series single wave models large eddy simulations conducted simulate turbulent channel flow wavy walls generating comprehensive dataset frictional resistance components frictional force acting wavy surface defined effective shear stress accounting viscous shear pressure induced forces distribution friction velocity exhibited strong correlation pressure component acting surface friction coefficient found highest regions characterized large wave amplitudes short wavelengths
"Prefactual thinking, a form of prospective mental simulation, significantly impacts individuals’ decision-making. To understand the influence of media consumption on public opinion regarding emerging technologies, this study, in the context of generative artificial intelligence (AI), examined prefactual thinking as a cognitive process of media consumption, extending the cognitive mediation model. Using quota sampling, we conducted an online survey of 1,129 Hong Kong adult residents. The findings revealed that elaborative processing was positively associated with prefactual thinking, which, in turn, influenced emotional responses. Prefactual thinking and emotional responses shaped benefit-risk perceptions, which influenced individuals’ generative AI opinion. Theoretical and practical implications were discussed.",prefactual thinking form prospective mental simulation significantly impacts individuals decision making understand influence media consumption public opinion regarding emerging technologies study context generative artificial intelligence examined prefactual thinking cognitive process media consumption extending cognitive mediation model using quota sampling conducted online survey hong kong adult residents findings revealed elaborative processing positively associated prefactual thinking turn influenced emotional responses prefactual thinking emotional responses shaped benefit risk perceptions influenced individuals generative opinion theoretical practical implications discussed
"Modeling the complex interrelationships among students, learning resources, and knowledge points remains a significant challenge in intelligent educational systems. To address this issue, this study proposes an improved Heterogeneous Graph Neural Network (HetGNN) framework for comprehensive learning behavior modeling and personalized educational recommendation. We define three distinct node types—students, resources, and knowledge points—and construct a multi-relational heterogeneous graph by incorporating diverse edge types derived from behavioral interactions (e.g., clicks, completions), knowledge dependencies (prerequisite relationships), and content associations (label-based matching). A student–resource–knowledge point meta-path is designed to capture composite relational patterns, and a Relational Graph Convolutional Network (R-GCN) is employed to aggregate neighborhood information while preserving semantic distinctions across relation types. To model temporal learning sequences, a Transformer encoder is integrated to generate dynamic attention weights that reflect evolving student engagement. A gated fusion mechanism is introduced to effectively combine dynamic sequential features with static structural representations, ensuring feature complementarity and minimizing interference. The model further incorporates two jointly optimized output branches—learning status prediction (including dropout risk and performance forecasting) and personalized resource recommendation—through shared parameter learning. Experimental results on real-world educational datasets demonstrate the superiority of the proposed approach: the F1-score for dropout risk prediction reaches 0.91, grade prediction achieves a stable RMSE of approximately 0.32, and resource recommendation attains an NDCG (Normalized Discounted Cumulative Gain) @10 of 0.93 in standard scenarios and 0.88 in knowledge gap scenarios. Moreover, the coverage of long-tail resources improves to 0.56, with a reduced recommendation bias coefficient of 0.29. The results validate the model’s effectiveness in capturing intricate student–resource–knowledge dynamics, offering a robust solution for learning analytics and adaptive educational systems.",modeling complex interrelationships among students learning resources knowledge points remains significant challenge intelligent educational systems address issue study proposes improved heterogeneous graph neural network hetgnn framework comprehensive learning behavior modeling personalized educational recommendation define three distinct node types students resources knowledge points construct multi relational heterogeneous graph incorporating diverse edge types derived behavioral interactions clicks completions knowledge dependencies prerequisite relationships content associations label based matching student resource knowledge point meta path designed capture composite relational patterns relational graph convolutional network gcn employed aggregate neighborhood information preserving semantic distinctions across relation types model temporal learning sequences transformer encoder integrated generate dynamic attention weights reflect evolving student engagement gated fusion mechanism introduced effectively combine dynamic sequential features static structural representations ensuring feature complementarity minimizing interference model incorporates two jointly optimized output branches learning status prediction including dropout risk performance forecasting personalized resource recommendation shared parameter learning experimental results real world educational datasets demonstrate superiority proposed approach score dropout risk prediction reaches grade prediction achieves stable rmse approximately resource recommendation attains ndcg normalized discounted cumulative gain standard scenarios knowledge gap scenarios moreover coverage long tail resources improves reduced recommendation bias coefficient results validate model effectiveness capturing intricate student resource knowledge dynamics offering robust solution learning analytics adaptive educational systems
"To improve students’ learning efficiency in online martial arts teaching and correct their erroneous movements, a Kinect device is selected to collect relevant data and construct a three-dimensional skeleton skin model of the character. The Dual Quaternion Linear Blending (DLB) algorithm is employed to enhance the realism of the model’s motion. The improved median-mean filtering algorithm and the improved Exponential Weighted Moving Average (EWMA) algorithm are utilized to detect and recognize human motion. These techniques are integrated into a martial arts learning assistance system. The results showed that compared with the standard median-mean filtering algorithm, the improved median-mean filtering algorithm had better filtering effect and more sensitive processing nodes in both martial arts motion forms. Its stability rate and real-time rate were 95.53% and 97.05%, respectively, with an improvement of 8.49% and 26.54%, indicating that the improved algorithm has better performance. In comparison to the standard EWMA algorithm, the enhanced EWMA algorithm demonstrated favorable performance in both scenarios involving angle alteration. This resulted in a reduction in computational error and delay, as well as an improvement in both the smoothing rate and real-time rate, with an increase of 53.02% and 70.42%, respectively. In the two-player mode, the CPU resource usage was reduced by 34.5% and the GPU resource usage was increased by 29.5% by updating the vertices and rendering through GPU. The online teaching method of martial arts adopted in this study can effectively help students learn martial arts and lay a solid foundation for technological innovation in the field of physical education teaching.",improve students learning efficiency online martial arts teaching correct erroneous movements kinect device selected collect relevant data construct three dimensional skeleton skin model character dual quaternion linear blending dlb algorithm employed enhance realism model motion improved median mean filtering algorithm improved exponential weighted moving average ewma algorithm utilized detect recognize human motion techniques integrated martial arts learning assistance system results showed compared standard median mean filtering algorithm improved median mean filtering algorithm better filtering effect sensitive processing nodes martial arts motion forms stability rate real time rate respectively improvement indicating improved algorithm better performance comparison standard ewma algorithm enhanced ewma algorithm demonstrated favorable performance scenarios involving angle alteration resulted reduction computational error delay well improvement smoothing rate real time rate increase respectively two player mode cpu resource usage reduced gpu resource usage increased updating vertices rendering gpu online teaching method martial arts adopted study effectively help students learn martial arts lay solid foundation technological innovation field physical education teaching
"With the rapid development of Shenzhen’s economy, there is an urgent need for a large number of urban core industrial parks that meet modern needs. However, many core industrial parks have low utilization rates due to the aging facilities and irrational functional layout. Therefore, to enhance the utilization rate of industrial parks, the study proposes a design quality research method for urban renewal projects of Shenzhen industrial park that integrates building information modeling and virtual reality technology. The study integrates the data in building information modeling into an update project engineering schedule by introducing an improved genetic algorithm, and constructs an evaluation quality index for industrial park update project design to improve the overall design quality. The experimental results revealed that the genetic ant colony algorithm reduced the number of iterations by 36% and 25%, the iteration time by 25.6% and the accuracy by 97.4% compared to the genetic algorithm and the ant colony algorithm. The improved genetic ant colony algorithm optimized the building information modeling data by 16%, and the load indexes decreased by 0.096 and 0.184 relative to the other two algorithms. The project has a quality management score of 88.97, with an overall rating of excellent. As a result, the proposed model is able to generate the updated project schedule and enhance the efficiency of project management, which can help to promote the upgrading of the industrial parks in Shenzhen and the sustainable development of the city. The suggested strategy can create a timetable for renewal projects and increase project management effectiveness.",rapid development shenzhen economy urgent need large number urban core industrial parks meet modern needs however many core industrial parks low utilization rates due aging facilities irrational functional layout therefore enhance utilization rate industrial parks study proposes design quality research method urban renewal projects shenzhen industrial park integrates building information modeling virtual reality technology study integrates data building information modeling update project engineering schedule introducing improved genetic algorithm constructs evaluation quality index industrial park update project design improve overall design quality experimental results revealed genetic ant colony algorithm reduced number iterations iteration time accuracy compared genetic algorithm ant colony algorithm improved genetic ant colony algorithm optimized building information modeling data load indexes decreased relative two algorithms project quality management score overall rating excellent result proposed model able generate updated project schedule enhance efficiency project management help promote upgrading industrial parks shenzhen sustainable development city suggested strategy create timetable renewal projects increase project management effectiveness
"Due to its specific audience, traditional preschool art courses are relatively limited. To enhance preschool teachers’ art teaching abilities, this paper proposes combining virtual reality technology and digital media art. Utilizing computers and modern network technology, users’ information is collected in 12 different teaching environments under the backdrop of digital media virtual technology. The score of the user’s information receiving effect is obtained by the ratio of the number of scenes in the user’s subjective memory to the total number of scenes. SMOTE data enhancement algorithm and Few-shot learning algorithm are used to process the received information to get the user’s receiving effect. The score range of the user’s information receiving effect is 0–1, the larger the number, the better the information receiving effect. The significance of this research lies in its demonstration of how integrating offline laboratory hardware with online virtual reality technology to create an immersive teaching environment can significantly enhance students’ learning interest. By effectively combining physical and virtual resources, this innovative approach highlights the potential for advanced technologies to foster greater engagement and improve educational outcomes. The findings underscore the importance of developing hybrid teaching methods to maximize student motivation and participation, paving the way for more effective and interactive learning experiences.",due specific audience traditional preschool art courses relatively limited enhance preschool teachers art teaching abilities paper proposes combining virtual reality technology digital media art utilizing computers modern network technology users information collected different teaching environments backdrop digital media virtual technology score user information receiving effect obtained ratio number scenes user subjective memory total number scenes smote data enhancement algorithm shot learning algorithm used process received information get user receiving effect score range user information receiving effect larger number better information receiving effect significance research lies demonstration integrating offline laboratory hardware online virtual reality technology create immersive teaching environment significantly enhance students learning interest effectively combining physical virtual resources innovative approach highlights potential advanced technologies foster greater engagement improve educational outcomes findings underscore importance developing hybrid teaching methods maximize student motivation participation paving way effective interactive learning experiences
"This article extends discussions on decolonizing language policy and planning in African contexts to include African and Africana philosophical concepts. Drawing on Kwasi Wiredu’s project of conceptual decolonization and Molefi Asante’s concept of Afrocentricity, the article argues that the challenges facing Ghana and other African contexts in terms of educational language planning are primarily ideological which means “language-focused” concepts (e.g. mother–tongue bilingual education) alone might not be able to describe the complexities of the situation, instead theories that emphasize emancipation of the African mind might help us better understand the genesis of this problem and how it could be re-envisioned.",article extends discussions decolonizing language policy planning african contexts include african africana philosophical concepts drawing kwasi wiredu project conceptual decolonization molefi asante concept afrocentricity article argues challenges facing ghana african contexts terms educational language planning primarily ideological means language focused concepts mother tongue bilingual education alone might able describe complexities situation instead theories emphasize emancipation african mind might help better understand genesis problem could envisioned
"Traditional landscape design is mainly based on the personal experience and actual evaluation of designers, and cannot effectively and quickly complete design updates and iterations. To study and propose a new digital landscape design method that optimizes park path system design by combining spatial theory and landscape elements. The new method uses a convex space model to construct a model of the park path system, treating specific roads as convex spaces and the grasslands, fences, and shrubs on both sides of the roads as convex space boundaries. Using a two-step spontaneous geolocation data platform to visualize park path system data. Visualize the path system of the park landscape using digital technology. Introduce five characteristic elements of park landscape space and determine the relationship between the main landscape elements and spatial theory. Analyze the qualitative relationship between these elements and spontaneous geolocation data through linear regression analysis and correlation testing, and conduct simulation experiments. In the simulation experiment, the corresponding selectivity of the flower show area was 0.97 at a distance of 40 m. The path selectivity corresponding to the parent-child area was second only to the flower show area, with a specific value of 0.89. The path selectivity for these three areas of bamboo pavilion, badminton, and basketball was not significantly different, all located around 0.75. As the distance increased, the selectivity value gradually decreased, and the changing magnitude gradually decreased. This research method has significant effectiveness and feasibility in the important design parameters of parks. This can provide certain reference value for landscape architecture design.",traditional landscape design mainly based personal experience actual evaluation designers effectively quickly complete design updates iterations study propose new digital landscape design method optimizes park path system design combining spatial theory landscape elements new method uses convex space model construct model park path system treating specific roads convex spaces grasslands fences shrubs sides roads convex space boundaries using two step spontaneous geolocation data platform visualize park path system data visualize path system park landscape using digital technology introduce five characteristic elements park landscape space determine relationship main landscape elements spatial theory analyze qualitative relationship elements spontaneous geolocation data linear regression analysis correlation testing conduct simulation experiments simulation experiment corresponding selectivity flower show area distance path selectivity corresponding parent child area second flower show area specific value path selectivity three areas bamboo pavilion badminton basketball significantly different located around distance increased selectivity value gradually decreased changing magnitude gradually decreased research method significant effectiveness feasibility important design parameters parks provide certain reference value landscape architecture design
"The taxpaying credit system is an indispensable link in the construction of the national credit system and an important practice of innovating social governance and improving governance efficiency. Based on the taxpaying credit rating data obtained from the website of the State Taxation Administration, this paper empirically tested the relationship between the degree of corporate tax avoidance and the taxpaying credit rating. The study found that the more aggressive the tax avoidance behavior of private enterprises was, the higher the probability of being rated as low taxpaying credit would become. Fiscal pressures and political connections could weaken the relationship. Further research found that media attention and analysts’ concern could effectively monitor the tax avoidance behavior of politically connected companies. The research conclusions of this paper have certain reference value for improving the tax governance capability.",taxpaying credit system indispensable link construction national credit system important practice innovating social governance improving governance efficiency based taxpaying credit rating data obtained website state taxation administration paper empirically tested relationship degree corporate tax avoidance taxpaying credit rating study found aggressive tax avoidance behavior private enterprises higher probability rated low taxpaying credit would become fiscal pressures political connections could weaken relationship research found media attention analysts concern could effectively monitor tax avoidance behavior politically connected companies research conclusions paper certain reference value improving tax governance capability
"This study aims to develop machine learning (ML)-based homogenization models to efficiently predict the effective elastoplastic properties of short fiber-reinforced composites (SFRCs), reducing the reliance on computationally expensive micromechanical simulations. Sobol sampling is employed to generate training data by varying constitutive and microstructural parameters of representative volume element. Several ML models including artificial neural networks (ANN), support vector regression (SVR), random forest (RF), and extreme gradient boosting (XGB) are trained to predict homogenized stress-strain responses. A new approach is introduced that decomposes the stress-strain response into elastic and plastic components, allowing the ML models to learn these components separately and effectively. Additionally, the Taguchi method (L27 orthogonal array) is employed to minimize simulation runs and evaluate parameter sensitivity through ANOVA. The best-performing ML model is implemented in a finite element analysis (FEA) of an automotive component. Among all models, ANN demonstrated the highest accuracy in predicting the macroscopic elastoplastic response across a wide range of input parameters. Finally, the ANN-based elastoplastic material model is validated on a real-world macroscopic structure by implementing it into the finite element analysis of an automotive component. The results demonstrate that ML-based homogenization closely matches the traditional homogenization methods and also highlights its ability to effectively capture nonlinear material behavior.",study aims develop machine learning based homogenization models efficiently predict effective elastoplastic properties short fiber reinforced composites sfrcs reducing reliance computationally expensive micromechanical simulations sobol sampling employed generate training data varying constitutive microstructural parameters representative volume element several models including artificial neural networks ann support vector regression svr random forest extreme gradient boosting xgb trained predict homogenized stress strain responses new approach introduced decomposes stress strain response elastic plastic components allowing models learn components separately effectively additionally taguchi method orthogonal array employed minimize simulation runs evaluate parameter sensitivity anova best performing model implemented finite element analysis fea automotive component among models ann demonstrated highest accuracy predicting macroscopic elastoplastic response across wide range input parameters finally ann based elastoplastic material model validated real world macroscopic structure implementing finite element analysis automotive component results demonstrate based homogenization closely matches traditional homogenization methods also highlights ability effectively capture nonlinear material behavior
"With the rapid change of science and technology, the deep integration of artificial intelligence, big data, and Internet of Things has given rise to many innovative applications in various fields. In the field of education, this phenomenon of technological integration provides unprecedented opportunities for the reform and upgrade of the teaching mode. Especially in the field of English teaching, how to effectively integrate these three cutting-edge technologies into the teaching process in order to improve the quality of teaching, optimize the learning experience and realize personalized education has become an important topic that needs to be studied urgently. The purpose of this thesis is to explore the integration and utilization strategies of artificial intelligence, big data, and Internet of Things in English teaching, and to conceptualize a corresponding teaching model framework. Based on the results of theoretical analysis and empirical research, this study innovatively design an English teaching model framework integrating AI, Big Data, and IoT technologies, which covers multiple dimensions such as teaching goal setting, teaching content planning, teaching method selection, teaching evaluation system establishment, and teaching management strategies, etc., and provide detailed explanations and demonstrations in combination with actual teaching cases. Moreover, through experimental research, this study compare and analyze the differences between this new English teaching model and the traditional teaching model in terms of teaching effect enhancement and learning experience optimization. The experimental results show that this new English teaching mode can significantly improve learners’ English proficiency, learning efficiency, learning enthusiasm, and learning satisfaction, which is better than the traditional teaching mode, situational teaching mode, and flipped classroom mode. The results of the content analysis show that learners’ views and suggestions on this new English teaching model mainly focus on personalization, intelligence, high flexibility, fun, challenging, helpful, and sense of accomplishment, reflecting the advantages and limitations of this new English teaching model. The fusion of AI, big data, and IoT in English education marks a pivotal leap in tech-driven pedagogy, revolutionizing language learning. Our study targets the shortcomings of conventional methods, aiming to cultivate a dynamic, tailored, and efficacious learning milieu for students. The model’s novelty stems from its holistic integration of AI analytics, data-informed choices, and IoT-optimized spaces, enriching the dialogue on educational modernization and amplifying teaching efficacy.",rapid change science technology deep integration artificial intelligence big data internet things given rise many innovative applications various fields field education phenomenon technological integration provides unprecedented opportunities reform upgrade teaching mode especially field english teaching effectively integrate three cutting edge technologies teaching process order improve quality teaching optimize learning experience realize personalized education become important topic needs studied urgently purpose thesis explore integration utilization strategies artificial intelligence big data internet things english teaching conceptualize corresponding teaching model framework based results theoretical analysis empirical research study innovatively design english teaching model framework integrating big data iot technologies covers multiple dimensions teaching goal setting teaching content planning teaching method selection teaching evaluation system establishment teaching management strategies etc provide detailed explanations demonstrations combination actual teaching cases moreover experimental research study compare analyze differences new english teaching model traditional teaching model terms teaching effect enhancement learning experience optimization experimental results show new english teaching mode significantly improve learners english proficiency learning efficiency learning enthusiasm learning satisfaction better traditional teaching mode situational teaching mode flipped classroom mode results content analysis show learners views suggestions new english teaching model mainly focus personalization intelligence high flexibility fun challenging helpful sense accomplishment reflecting advantages limitations new english teaching model fusion big data iot english education marks pivotal leap tech driven pedagogy revolutionizing language learning study targets shortcomings conventional methods aiming cultivate dynamic tailored efficacious learning milieu students model novelty stems holistic integration analytics data informed choices iot optimized spaces enriching dialogue educational modernization amplifying teaching efficacy
"In order to improve the effect of interior and exterior decoration design of modern building, this paper combines artificial intelligence technology to build a modern interior and exterior decoration design system of modern building. In this paper, the method of combining model-driven and data-driven is adopted, combines the advantages of model-driven method with obvious object details and edges to construct an initial model based on point cloud, which improves the work efficiency, the degree of automation and the accuracy of the initial value of the model, and is conducive to the guarantee of 3D reconstruction of the model. Through the experimental research, it can be seen that the interior and exterior decoration design system of modern building based on artificial intelligence proposed in this paper can effectively improve the design effect of building decoration.",order improve effect interior exterior decoration design modern building paper combines artificial intelligence technology build modern interior exterior decoration design system modern building paper method combining model driven data driven adopted combines advantages model driven method obvious object details edges construct initial model based point cloud improves work efficiency degree automation accuracy initial value model conducive guarantee reconstruction model experimental research seen interior exterior decoration design system modern building based artificial intelligence proposed paper effectively improve design effect building decoration
"With the development of Internet technology and the progress of social economy, tourism has become the first choice for most people to take a vacation. However, the complicated information retrieval content presented by the Internet search engine has increased the difficulty of travel information search. To improve the targeting and accuracy of tourism information search behavior, data mining methods are used for analysis. Based on the characteristics of tourism text data, an improved clustering algorithm is introduced to classify the data, and a search method based on dynamic themes and search intentions is proposed. The results showed that the algorithm proposed in the study had good convergence and stability. The sum of squared errors curve results indicated that its error results in the three dimensions of traffic, catering, and accommodation were relatively small. The normalized discounted cumulative gain value of tourist attraction information search showed that the algorithm proposed in the study was significantly better than other comparative algorithms. Its maximum value approached 0.80 when the theme dimension was 5, and its accuracy in classifying information under both target search intention and analytical search intention exceeded 80%. The maximum time spent did not exceed 3 s. The main contribution of this study is to propose an innovative tourism information search method, which not only optimizes the processing and analysis process of tourism data, but also provides scientific basis for the improvement of tourism data recommendation systems. In addition, this method is of great significance for improving user experience and assisting users in making more informed travel decisions, promoting the advancement of tourism information search technology to a higher level.",development internet technology progress social economy tourism become first choice people take vacation however complicated information retrieval content presented internet search engine increased difficulty travel information search improve targeting accuracy tourism information search behavior data mining methods used analysis based characteristics tourism text data improved clustering algorithm introduced classify data search method based dynamic themes search intentions proposed results showed algorithm proposed study good convergence stability sum squared errors curve results indicated error results three dimensions traffic catering accommodation relatively small normalized discounted cumulative gain value tourist attraction information search showed algorithm proposed study significantly better comparative algorithms maximum value approached theme dimension accuracy classifying information target search intention analytical search intention exceeded maximum time spent exceed main contribution study propose innovative tourism information search method optimizes processing analysis process tourism data also provides scientific basis improvement tourism data recommendation systems addition method great significance improving user experience assisting users making informed travel decisions promoting advancement tourism information search technology higher level
"This study investigates the driving effects of Intelligent MICE tourism on regional economic development, with Guizhou Province as the research focus. Using tourism economic data from 2009 to 2019, combined with geographic information analysis and an input-output model, we examine the spatiotemporal evolution of Guizhou’s tourism industry and its impact on regional economic growth. The results indicate that after 2016, Guizhou’s total tourism revenue and visitor numbers entered a phase of rapid expansion, with annual growth rates averaging 36.96% and 32.15%, respectively. Spatial analysis reveals that tourism revenue distribution across county-level administrative divisions follows a diffusion pattern, expanding from core cities such as Guiyang and Zunyi to surrounding areas. Furthermore, an analysis of industrial structure indicates that Guizhou’s economy is primarily driven by the secondary and tertiary sectors, with the tourism industry’s backward linkages within the industrial chain strengthening over time, positioning it as a key catalyst for growth in related industries. Model validation results demonstrate that both the influence and sensitivity coefficients of the tourism industry significantly exceed the national economic average, underscoring its substantial stimulative effect on regional economic expansion. Geographic detector analysis further reveals that policy regulation, advancements in information technology, and regional industrial structures play a critical role in the coupled and coordinated development of the tourism economy and the broader regional economy. Based on these findings, this study proposes policy recommendations to optimize Guizhou’s tourism industry structure, enhance tourism service infrastructure, and promote integrated regional development. By integrating spatiotemporal pattern analysis with industrial chain effect evaluation, this study highlights tourism’s core role in regional economic development and quantitatively assesses its contribution from a geographical perspective.",study investigates driving effects intelligent mice tourism regional economic development guizhou province research focus using tourism economic data combined geographic information analysis input output model examine spatiotemporal evolution guizhou tourism industry impact regional economic growth results indicate guizhou total tourism revenue visitor numbers entered phase rapid expansion annual growth rates averaging respectively spatial analysis reveals tourism revenue distribution across county level administrative divisions follows diffusion pattern expanding core cities guiyang zunyi surrounding areas furthermore analysis industrial structure indicates guizhou economy primarily driven secondary tertiary sectors tourism industry backward linkages within industrial chain strengthening time positioning key catalyst growth related industries model validation results demonstrate influence sensitivity coefficients tourism industry significantly exceed national economic average underscoring substantial stimulative effect regional economic expansion geographic detector analysis reveals policy regulation advancements information technology regional industrial structures play critical role coupled coordinated development tourism economy broader regional economy based findings study proposes policy recommendations optimize guizhou tourism industry structure enhance tourism service infrastructure promote integrated regional development integrating spatiotemporal pattern analysis industrial chain effect evaluation study highlights tourism core role regional economic development quantitatively assesses contribution geographical perspective
"With the rapid development of Location-Based Services (LBSs), research on Indoor Positioning Systems (IPS) has gained extensive attention. Although existing channel state information (CSI)-based positioning algorithms have achieved good positioning accuracy, they still face significant challenges in practical applications due to their high computational complexity and cost. To address this, this paper proposes an indoor positioning method based on the frequency fading characteristics of broadband signals, named the ACCS algorithm. This method introduces the innovative use of the autocorrelation spectrum of broadband signals to transform the indoor positioning problem into an image classification task. After that, to achieve lightweight model design, a convolutional autoencoder (CAE), which is an unsupervised learning model based on convolutional neural networks, is employed to reduce the dimension and compress the input images. Finally, a deep residual network combined with a CS-Attention module, which is composed of a channel feature extraction module and a spatial feature extraction module, is used to efficiently capture image features for more accurate position estimation. The performance of the proposed method was tested using data collected in a conference room. Experimental results show that the autocorrelation processing and attention module in the proposed algorithm significantly improve positioning accuracy, achieving a positioning error of 0.245 m in complex indoor environments. Additionally, comparisons with common positioning algorithms (AOA, W-KNN) demonstrate that our method exhibits significant advantages in positioning accuracy and robustness.",rapid development location based services lbss research indoor positioning systems ips gained extensive attention although existing channel state information csi based positioning algorithms achieved good positioning accuracy still face significant challenges practical applications due high computational complexity cost address paper proposes indoor positioning method based frequency fading characteristics broadband signals named accs algorithm method introduces innovative use autocorrelation spectrum broadband signals transform indoor positioning problem image classification task achieve lightweight model design convolutional autoencoder cae unsupervised learning model based convolutional neural networks employed reduce dimension compress input images finally deep residual network combined attention module composed channel feature extraction module spatial feature extraction module used efficiently capture image features accurate position estimation performance proposed method tested using data collected conference room experimental results show autocorrelation processing attention module proposed algorithm significantly improve positioning accuracy achieving positioning error complex indoor environments additionally comparisons common positioning algorithms aoa knn demonstrate method exhibits significant advantages positioning accuracy robustness
"The 2016 election of Donald Trump heightened the disproportionate risks from toxicants faced by minoritized communities from cuts in the US Environmental Protection Agency (EPA) funding, reworking environmental health regulations, and obfuscating related information. In response, the Environmental Data and Governance Initiative (EDGI)—a US-based network of social scientists, technologists, and activists—was formed to develop an environmental data justice framework to address the ongoing political forces that influence how environmental data is collected, shared, and rendered flawed, incomplete, or vulnerable. By collaborating with communities and non-profit organizations, EDGI's Environmental Enforcement Watch (EEW) project designed computational notebooks that critically situate, question, and analyze EPA datasets on industry compliance and state enforcement of environmental regulations beyond the Trump political moment. These civic partnerships expose gaps, biases, and silences in environmental data that sustain a harmful permission-to-pollute system. Using a multi-sited auto-technography, we reflect on the development and use of EEW notebooks, tracing their application across workshops, collectively designed reports, and data stewardship practices. We show how feminist care practices informed the creation of computational notebooks as an effective resource for environmental justice groups to negotiate environmental datafication, supporting the co-production of critical knowledge about environmental governance.",election donald trump heightened disproportionate risks toxicants faced minoritized communities cuts environmental protection agency epa funding reworking environmental health regulations obfuscating related information response environmental data governance initiative edgi based network social scientists technologists activists formed develop environmental data justice framework address ongoing political forces influence environmental data collected shared rendered flawed incomplete vulnerable collaborating communities non profit organizations edgi environmental enforcement watch eew project designed computational notebooks critically situate question analyze epa datasets industry compliance state enforcement environmental regulations beyond trump political moment civic partnerships expose gaps biases silences environmental data sustain harmful permission pollute system using multi sited auto technography reflect development use eew notebooks tracing application across workshops collectively designed reports data stewardship practices show feminist care practices informed creation computational notebooks effective resource environmental justice groups negotiate environmental datafication supporting production critical knowledge environmental governance
"This paper focus on the recent progress regarding nitrogen-containing heterocyclic quaternary ammonium salt corrosion inhibitors in high-temperature, high-pressure (HTHP) oilfield produced fluid environments. Under these extreme conditions, the synergistic effects of elevated temperature and aggressive media exacerbate multi-mechanism corrosion in oil and gas pipelines, posing high failure risks for conventional steel protection strategies. Nitrogen-containing heterocyclic quaternary ammonium salts form protective films primarily through coordination bonding and electrostatic adsorption, with structural optimization significantly enhancing their performance. Composite inhibitor formulations demonstrate superior efficacy in field applications. However, current studies exhibit critical gaps, including validated failure models under multi-field coupling conditions and high-temperature adsorption kinetics data. Future research should prioritize molecular design optimization, enhanced environmental compatibility, and advanced multi-scale simulations. Integrating in-situ characterization techniques with machine learning approaches will enable mechanism-driven, precision design of high-temperature corrosion inhibitors, thereby offering robust theoretical frameworks and practical engineering solutions for long-term corrosion mitigation in extreme environments.",paper focus recent progress regarding nitrogen containing heterocyclic quaternary ammonium salt corrosion inhibitors high temperature high pressure hthp oilfield produced fluid environments extreme conditions synergistic effects elevated temperature aggressive media exacerbate multi mechanism corrosion oil gas pipelines posing high failure risks conventional steel protection strategies nitrogen containing heterocyclic quaternary ammonium salts form protective films primarily coordination bonding electrostatic adsorption structural optimization significantly enhancing performance composite inhibitor formulations demonstrate superior efficacy field applications however current studies exhibit critical gaps including validated failure models multi field coupling conditions high temperature adsorption kinetics data future research prioritize molecular design optimization enhanced environmental compatibility advanced multi scale simulations integrating situ characterization techniques machine learning approaches enable mechanism driven precision design high temperature corrosion inhibitors thereby offering robust theoretical frameworks practical engineering solutions long term corrosion mitigation extreme environments
"In a variety of foundation treatment projects, the compaction degree of filler is one of the main quality control indicators. However, limited by the sparsity of measurement points and the requirements for implantable sensors, the existing test methods are difficult to achieve full-site visualization and rapid measurement. Therefore, this project takes the artificial vibration drum as the research object, takes the artificial neural network as the foundation, establishes a set of artificial neural network model which takes the artificial vibration as the foundation, and takes this as the foundation, establish a set of artificial neural network model based on artificial neural network. A new linear regression analysis method of drum vibration spectrum and amplitude characteristics based on multivariate characteristics is proposed, and a new packing compaction detection is realized. Through the measurement of the vibration frequency and amplitude of the rotary drum, and according to the physical properties and compaction characteristics of the soil, the accurate prediction is carried out. And its application prospect is discussed.",variety foundation treatment projects compaction degree filler one main quality control indicators however limited sparsity measurement points requirements implantable sensors existing test methods difficult achieve full site visualization rapid measurement therefore project takes artificial vibration drum research object takes artificial neural network foundation establishes set artificial neural network model takes artificial vibration foundation takes foundation establish set artificial neural network model based artificial neural network new linear regression analysis method drum vibration spectrum amplitude characteristics based multivariate characteristics proposed new packing compaction detection realized measurement vibration frequency amplitude rotary drum according physical properties compaction characteristics soil accurate prediction carried application prospect discussed
"Cultural tourism destination is a tourist place with distinctive cultural characteristics and rich cultural resources, which is the core element of cultural tourism and the main selection object of tourists. This paper proposes a cultural tourism destination selection and evaluation method based on fuzzy hesitation model, which can comprehensively consider the uncertain preference of tourists and the weight of evaluation indexes, so as to realize the scientific selection and evaluation of cultural tourism destinations. This paper constructs a cultural tourism destination evaluation index system containing objective attributes and subjective preferences, which reflects the diversity, complexity, dynamics, and subjectivity of cultural tourism destinations, and proposes a cultural tourism destination selection and evaluation method based on the fuzzy hesitation model, which is capable of describing the fuzzy cognition and hesitant emotions of tourists towards destinations, and utilizes the fuzzy hesitation weighted average operator to conduct a comprehensive evaluation of each destination. This paper collects and analyzes the tourists’ review data about four famous cultural tourism destinations, City A, City B, City C, and City D, on Baidu Tourism website by using web crawler technology and text mining technology, and verifies the validity and practicability of the method of selecting and evaluating cultural tourism destinations.",cultural tourism destination tourist place distinctive cultural characteristics rich cultural resources core element cultural tourism main selection object tourists paper proposes cultural tourism destination selection evaluation method based fuzzy hesitation model comprehensively consider uncertain preference tourists weight evaluation indexes realize scientific selection evaluation cultural tourism destinations paper constructs cultural tourism destination evaluation index system containing objective attributes subjective preferences reflects diversity complexity dynamics subjectivity cultural tourism destinations proposes cultural tourism destination selection evaluation method based fuzzy hesitation model capable describing fuzzy cognition hesitant emotions tourists towards destinations utilizes fuzzy hesitation weighted average operator conduct comprehensive evaluation destination paper collects analyzes tourists review data four famous cultural tourism destinations city city city city baidu tourism website using web crawler technology text mining technology verifies validity practicability method selecting evaluating cultural tourism destinations
"This study aims to explore the construction and effectiveness of college English wisdom classrooms in the context of big data. Through comprehensive analysis, this paper examines students’ learning behavior data in smart classrooms and evaluates the impact of different teaching strategies. The results indicate that students’ login frequency and study time are positively correlated with their academic performance. Cluster analysis effectively groups students based on their learning behaviors, providing a foundation for customized teaching. The use of big data technology significantly enhances the teaching quality of smart classrooms and improves students’ learning outcomes, underscoring its importance for the advancement of educational practice and theory.",study aims explore construction effectiveness college english wisdom classrooms context big data comprehensive analysis paper examines students learning behavior data smart classrooms evaluates impact different teaching strategies results indicate students login frequency study time positively correlated academic performance cluster analysis effectively groups students based learning behaviors providing foundation customized teaching use big data technology significantly enhances teaching quality smart classrooms improves students learning outcomes underscoring importance advancement educational practice theory
"Based on the background of the concrete face crack repair project for the Garden Aqueduct in Anhui Province of China, mechanical tests were conducted to explore the effects of different amounts of styrene-acrylic mortar, water-cement ratio, curing humidity, and curing temperature on the mechanical properties and crack resistance of the concrete face. The test results show that increasing the styrene-acrylic mortar content and reducing the water-cement ratio can significantly improve the crack resistance of the styrene-acrylic mortar, manifested as an increase in flexural strength, a decrease in the compressive-flexural ratio, and an improvement in toughness and bonding strength. However, there was no significant improvement in compressive strength. It was confirmed that the styrene-acrylic mortar with 15% styrene-acrylic mortar content and a water-cement ratio of 30% exhibited optimal mechanical and crack resistance properties and is suitable for repairing cracks in the concrete face of the aqueduct. Additionally, higher curing humidity and temperature significantly improved the dry cracking resistance of the styrene-acrylic mortar. Under various curing conditions, the shrinkage rate of the styrene-acrylic mortar increased rapidly within the first 14 days and stabilized by day 28. Based on the test results, construction process optimization measures for the crack repair of the aqueduct concrete face were proposed, along with directions for future research.",based background concrete face crack repair project garden aqueduct anhui province china mechanical tests conducted explore effects different amounts styrene acrylic mortar water cement ratio curing humidity curing temperature mechanical properties crack resistance concrete face test results show increasing styrene acrylic mortar content reducing water cement ratio significantly improve crack resistance styrene acrylic mortar manifested increase flexural strength decrease compressive flexural ratio improvement toughness bonding strength however significant improvement compressive strength confirmed styrene acrylic mortar styrene acrylic mortar content water cement ratio exhibited optimal mechanical crack resistance properties suitable repairing cracks concrete face aqueduct additionally higher curing humidity temperature significantly improved dry cracking resistance styrene acrylic mortar various curing conditions shrinkage rate styrene acrylic mortar increased rapidly within first days stabilized day based test results construction process optimization measures crack repair aqueduct concrete face proposed along directions future research
"This study fabricated ancient brick column specimens with two different mortar joint spalling depths based on the common material deterioration mode of ancient masonry. Specifically, modified composite sticky rice-lime mortar and natural hemp fiber rope were used as the raw materials, and horizontal mortar joint grouting and fiber rope reinforcement methods were adopted to prepare performance-enhanced ancient brick column specimens with six different strengthening conditions. Through uniaxial compression tests, the compression failure morphology and mechanical properties of the specimens under different strengthening conditions were studied, and the variation patterns of mechanical performance indicators with the strengthening conditions were analyzed. The calculation formula for the compressive strength of performance-enhanced ancient masonry was proposed, and the full stress-strain relationship model under compression and the crack length development model were established. The results show that the non-strengthened brick columns exhibit brittle failure morphology under compression, and the compressive strength and deformation modulus decrease with the increasing mortar joint spalling depth. The crack development process of reinforced brick columns under compression presents ductile characteristics. The compressive strength and crack density increase with an increasing grouting depth and mortar joint grouting-fiber rope reinforcement significantly enhances the elastoplastic deformation capacity of brick columns under compression. Under the 25% grouting depth ratio, the characteristic stress and corresponding deformation modulus of the stress-strain curve increase first and then decrease with the increasing fiber rope reinforcement ratio. The compressive strength and crack density of ancient brick column with the 12.5% grouting depth ratio and 0.25% reinforcement ratio increase by 57.1%, 24.7%, respectively. The established formula and models can clearly describe the evolution of mechanical properties of ancient masonry, offering good engineering applicability.",study fabricated ancient brick column specimens two different mortar joint spalling depths based common material deterioration mode ancient masonry specifically modified composite sticky rice lime mortar natural hemp fiber rope used raw materials horizontal mortar joint grouting fiber rope reinforcement methods adopted prepare performance enhanced ancient brick column specimens six different strengthening conditions uniaxial compression tests compression failure morphology mechanical properties specimens different strengthening conditions studied variation patterns mechanical performance indicators strengthening conditions analyzed calculation formula compressive strength performance enhanced ancient masonry proposed full stress strain relationship model compression crack length development model established results show non strengthened brick columns exhibit brittle failure morphology compression compressive strength deformation modulus decrease increasing mortar joint spalling depth crack development process reinforced brick columns compression presents ductile characteristics compressive strength crack density increase increasing grouting depth mortar joint grouting fiber rope reinforcement significantly enhances elastoplastic deformation capacity brick columns compression grouting depth ratio characteristic stress corresponding deformation modulus stress strain curve increase first decrease increasing fiber rope reinforcement ratio compressive strength crack density ancient brick column grouting depth ratio reinforcement ratio increase respectively established formula models clearly describe evolution mechanical properties ancient masonry offering good engineering applicability
"The droplet contact liquid cooling method can effectively control the battery temperature and temperature difference. However, the cooling performance may be affected by dynamic conditions such as acceleration and road slope. Taking the 18650 lithium-ion battery as the research object, the study focuses on designing a drop contact liquid cooling system targeting power batteries. Additionally, an in-depth study is conducted on the impact of vehicle driving conditions on the heat dissipation performance of the cooling system. Research results indicate that as the drop position offset increases, the heat dissipation performance of the cooling system gradually decreases. Specifically, when the coolant flow rate is 10 mL/min and the offset reaches 9 mm, the maximum temperature difference of the battery module increases to 3.7°C. Meanwhile, as the slope increases, the heat dissipation performance of the cooling system gradually decreases. When the slope reaches 20% and the coolant flow rate is 20 mL/min, the maximum temperature difference of the battery module is 4.8°C.",droplet contact liquid cooling method effectively control battery temperature temperature difference however cooling performance may affected dynamic conditions acceleration road slope taking lithium ion battery research object study focuses designing drop contact liquid cooling system targeting power batteries additionally depth study conducted impact vehicle driving conditions heat dissipation performance cooling system research results indicate drop position offset increases heat dissipation performance cooling system gradually decreases specifically coolant flow rate min offset reaches maximum temperature difference battery module increases meanwhile slope increases heat dissipation performance cooling system gradually decreases slope reaches coolant flow rate min maximum temperature difference battery module
"Online education is rapidly developing and the user base is constantly increasing. Therefore, the workload of security maintenance in the teaching system is gradually increasing. In order to make the security maintenance process of the English teaching system more intelligent and convenient, the research innovatively uses Struts + Spring + Hibernate framework to build a maintenance model that can be operated in real time and handle alarms on the monitoring, alarms, configuration, and other issues involved in the system, and applies it to the optimisation process of the teaching system. In the test results, when the maintenance cases reached 200, the maintenance failures for each functional module in the system were below 60, and the successful maintenance was between 150 and 190. The maintenance success rate of each module for transactions was about 92%. When testing the security monitoring function of the system, with 500 concurrent users, the transactions executed per second and the clicks per second of each monitoring module reached consistency. This proved that no response failure occurred, with a response rate of 100%. Therefore, the proposed new maintenance model improves the security performance of this English teaching system’s service quality. This system makes the security monitoring process simpler and more effective, which is conducive to promoting the further application and development of teaching systems.",online education rapidly developing user base constantly increasing therefore workload security maintenance teaching system gradually increasing order make security maintenance process english teaching system intelligent convenient research innovatively uses struts spring hibernate framework build maintenance model operated real time handle alarms monitoring alarms configuration issues involved system applies optimisation process teaching system test results maintenance cases reached maintenance failures functional module system successful maintenance maintenance success rate module transactions testing security monitoring function system concurrent users transactions executed per second clicks per second monitoring module reached consistency proved response failure occurred response rate therefore proposed new maintenance model improves security performance english teaching system service quality system makes security monitoring process simpler effective conducive promoting application development teaching systems
"As one of the world’s three major ecosystems, wetlands play an important role in maintaining ecological balance. However, in recent years, wetland resources have suffered from encroachment and frequent water environment and soil pollution problems, causing serious damage to China’s wetland ecosystem. The study proposes to conduct a performance audit (PA) of the wetland park project (WPP) using financial and accounting tools to assess the effectiveness of wetland restoration. The study first constructed a PA evaluation index system for WPP using the pressure-state-response (PSR) model to ensure the comprehensiveness of the evaluation indexes. Subsequently, grey relational analysis combined with analytic hierarchy process (GREY-AHP) was introduced to convert the scores into mutual inverse judgment matrices and assign weights to the indicators to improve the accuracy and scientificity of the construction system. Finally, the PA evaluation results are derived through comprehensive evaluation. The innovation of the study is to apply the PSR model to WPP’s PA, which significantly improves the accuracy and comprehensive performance of the evaluation compared with the traditional method. The experimental results show that the maximum F1 values of the research-proposed model on the test set and validation set are 95.2 and 97.6, respectively, which are higher than those of other algorithms, showing the advantage in comprehensive evaluation performance. The results of the study are of great significance for improving the performance level of WPP and effectively protecting and restoring wetlands.",one world three major ecosystems wetlands play important role maintaining ecological balance however recent years wetland resources suffered encroachment frequent water environment soil pollution problems causing serious damage china wetland ecosystem study proposes conduct performance audit wetland park project wpp using financial accounting tools assess effectiveness wetland restoration study first constructed evaluation index system wpp using pressure state response psr model ensure comprehensiveness evaluation indexes subsequently grey relational analysis combined analytic hierarchy process grey ahp introduced convert scores mutual inverse judgment matrices assign weights indicators improve accuracy scientificity construction system finally evaluation results derived comprehensive evaluation innovation study apply psr model wpp significantly improves accuracy comprehensive performance evaluation compared traditional method experimental results show maximum values research proposed model test set validation set respectively higher algorithms showing advantage comprehensive evaluation performance results study great significance improving performance level wpp effectively protecting restoring wetlands
"With the rapid development of information technology, digital village construction has become essential for promoting agriculture and rural modernization. This study thoroughly analyzes the construction and implementation of an agricultural information service system within the context of a digital countryside. By integrating theoretical research and empirical data, an efficient service system model was designed and validated, leading to significant improvements in agricultural production efficiency, product quality, and market competitiveness. The study identifies key challenges in the implementation process, including technological adaptability, user acceptance, and economic and horticulturalist factors, and proposes corresponding solutions. This study provides theoretical and practical guidance for agricultural formalization in the digital countryside, offering clear directions for relevant policy formulation and future research. The findings aim to enhance the effectiveness of digital technologies in rural areas, supporting sustainable agricultural development and rural revitalization.",rapid development information technology digital village construction become essential promoting agriculture rural modernization study thoroughly analyzes construction implementation agricultural information service system within context digital countryside integrating theoretical research empirical data efficient service system model designed validated leading significant improvements agricultural production efficiency product quality market competitiveness study identifies key challenges implementation process including technological adaptability user acceptance economic horticulturalist factors proposes corresponding solutions study provides theoretical practical guidance agricultural formalization digital countryside offering clear directions relevant policy formulation future research findings aim enhance effectiveness digital technologies rural areas supporting sustainable agricultural development rural revitalization
"The development of social networking services (SNS) has eliminated spatial constraints, blurring the boundaries of social relations between physical and digital spaces. As a result, the content and information on social media are exploding. Users mainly upload, watch and share videos on YouTube. As a global social media platform, it has users from more than 80 countries and regions, and the number of its active users exceeds two billion. Users can effectively carry out brand promotion, cultural communication, and other communication activities on YouTube platform. This is the introductory text. This paper describes the current situation of sentiment analysis on mainstream social media. It explores the implementation of multilingual text sentiment analysis based on deep learning. Taking the cross-cultural dissemination of Chinese Hanfu culture on YouTube as an example, this study adopts. NMT and CNN_Text_Word2vec model to extract emotional features, combined with likes weights to perform sentiment analysis of social media texts in multiple contexts. As suggested by the experimental results, users’ emotions toward Chinese Hanfu cultural videos on YouTube exhibit a bimodal distribution trend, with strong positive and strong negative emotional feedback far exceeding weak positive and weak negative emotional feedback. In this paper, combined with the weight of likes on YouTube, users’ emotions toward a certain topic are investigated from multiple perspectives, which is supposed to reflect users’ true emotions and provide decision-makers with valuable decision-making suggestions.",development social networking services sns eliminated spatial constraints blurring boundaries social relations physical digital spaces result content information social media exploding users mainly upload watch share videos youtube global social media platform users countries regions number active users exceeds two billion users effectively carry brand promotion cultural communication communication activities youtube platform introductory text paper describes current situation sentiment analysis mainstream social media explores implementation multilingual text sentiment analysis based deep learning taking cross cultural dissemination chinese hanfu culture youtube example study adopts nmt cnn text word vec model extract emotional features combined likes weights perform sentiment analysis social media texts multiple contexts suggested experimental results users emotions toward chinese hanfu cultural videos youtube exhibit bimodal distribution trend strong positive strong negative emotional feedback far exceeding weak positive weak negative emotional feedback paper combined weight likes youtube users emotions toward certain topic investigated multiple perspectives supposed reflect users true emotions provide decision makers valuable decision making suggestions
"The vigilance decrement is a central topic in human factors research but has not yet given way to a clear theoretical understanding. Resource depletion theory, the most popular account of the effect, provides at best a qualitative and vague model of vigilance performance. More precise predictions are necessary for strong theory testing. The current study developed a computational account of the vigilance decrement, building on a quantitative, dynamic cognitive model of signal detection. The model reproduces supposed hallmark patterns of resource depletion, without incorporating a depletion process. Results suggest that a stronger emphasis on computational modeling can provide more concrete theories of the vigilance decrement and help resolve lingering points of disagreement in theories of sustained attention.",vigilance decrement central topic human factors research yet given way clear theoretical understanding resource depletion theory popular account effect provides best qualitative vague model vigilance performance precise predictions necessary strong theory testing current study developed computational account vigilance decrement building quantitative dynamic cognitive model signal detection model reproduces supposed hallmark patterns resource depletion without incorporating depletion process results suggest stronger emphasis computational modeling provide concrete theories vigilance decrement help resolve lingering points disagreement theories sustained attention
"As one of the cores of economic development in the new era, ecological civilization and public health construction are mainly reflected by environmental improvement and green and healthy development, which can only be put into practice through more actions and labor practices. As an important part of the combination of the theory and practice of ecological civilization construction, labor education needs to be objectively evaluated through more intelligent methods. This paper aims at providing an objective indicator for labor education in colleges and universities. First, RNN network completes the state identification of the theoretical learning stage; then combined with the offline practical action acquisition technology based on wearable devices, the KNN method was used to complete the action recognition in the practice process. Finally, SVR technology is used to complete the evaluation regression analysis based on online and offline learning features. The proposed model performs well in RMSE and related coefficient indicators, providing technical support for the objective evaluation of labor education in colleges and universities in the future.",one cores economic development new ecological civilization public health construction mainly reflected environmental improvement green healthy development put practice actions labor practices important part combination theory practice ecological civilization construction labor education needs objectively evaluated intelligent methods paper aims providing objective indicator labor education colleges universities first rnn network completes state identification theoretical learning stage combined offline practical action acquisition technology based wearable devices knn method used complete action recognition practice process finally svr technology used complete evaluation regression analysis based online offline learning features proposed model performs well rmse related coefficient indicators providing technical support objective evaluation labor education colleges universities future
"With rapid development of artificial intelligence technology, personalized recommendation system of the traditional village landscape has become a research hotspot. This study aims to construct a personalized recommendation system based on Transformer and YOLO image recognition models to improve the recognition and recommendation effect of traditional village landscapes. In terms of background, as a cultural heritage, the unique landscape of traditional villages attracts many tourists, but the existing recommendation system often lacks personalization and accuracy. Firstly, the YOLO model is used to quickly and accurately identify traditional village images and extract key landscape features. Then, combined with the powerful sequence processing capabilities of the Transformer model, the user behavior data is deeply mined to build a personalized recommendation algorithm. Experimental results show that image recognition accuracy of the proposed system reaches 95.6%, which is 8.2% higher than that of the traditional method. Regarding recommendation satisfaction, user satisfaction was as high as 90.3%, which was significantly higher than 78.5% of control group. In addition, system response time is shortened to 1.2 s, effectively improving the user experience. This study provides new ideas for personalized recommendations of traditional village landscapes and technical support for protecting and disseminating cultural heritage.",rapid development artificial intelligence technology personalized recommendation system traditional village landscape become research hotspot study aims construct personalized recommendation system based transformer yolo image recognition models improve recognition recommendation effect traditional village landscapes terms background cultural heritage unique landscape traditional villages attracts many tourists existing recommendation system often lacks personalization accuracy firstly yolo model used quickly accurately identify traditional village images extract key landscape features combined powerful sequence processing capabilities transformer model user behavior data deeply mined build personalized recommendation algorithm experimental results show image recognition accuracy proposed system reaches higher traditional method regarding recommendation satisfaction user satisfaction high significantly higher control group addition system response time shortened effectively improving user experience study provides new ideas personalized recommendations traditional village landscapes technical support protecting disseminating cultural heritage
"In the rapid development of higher education management, informatization has become an indispensable driving force. With the popularity of the campus card and the accumulation of various business system data, the campus big data environment has initially taken shape. However, there are many shortcomings in the current student information management platform, especially in the accuracy of data mining. To meet this challenge, we use a decision tree, naive Bayes algorithm, and regression analysis to construct a comprehensive analysis model. On this basis, we use Hadoop technology to build a set of behavior analysis and intelligent management system frameworks for higher vocational college students. The system can comprehensively collect and organize students’ study, life and consumption data, and accurately present the overall situation of students on campus. Through in-depth correlation analysis, we can reveal the inner relationship between students’ learning results, life patterns, and psychological changes. To verify the effectiveness of the system, we conducted extensive testing. In the comparison of prediction accuracy of combination algorithms, we used the average relative error as an evaluation index to analyze and predict the behavior of vocational college students. The test data covers a sample of students ranging from 1000 to 6000. The results show that compared with traditional single algorithms, the student behavior prediction model proposed in this study based on multiple algorithm combinations exhibits higher prediction accuracy, with an average relative error of less than 5%. This result indicates that the hybrid algorithm model we constructed has superior predictive performance.",rapid development higher education management informatization become indispensable driving force popularity campus card accumulation various business system data campus big data environment initially taken shape however many shortcomings current student information management platform especially accuracy data mining meet challenge use decision tree naive bayes algorithm regression analysis construct comprehensive analysis model basis use hadoop technology build set behavior analysis intelligent management system frameworks higher vocational college students system comprehensively collect organize students study life consumption data accurately present overall situation students campus depth correlation analysis reveal inner relationship students learning results life patterns psychological changes verify effectiveness system conducted extensive testing comparison prediction accuracy combination algorithms used average relative error evaluation index analyze predict behavior vocational college students test data covers sample students ranging results show compared traditional single algorithms student behavior prediction model proposed study based multiple algorithm combinations exhibits higher prediction accuracy average relative error less result indicates hybrid algorithm model constructed superior predictive performance
"In the rapidly developing information age, the application of artificial intelligence technology in the field of education is more and more extensive, especially in labor education shows great potential and value. This study aims to develop and apply an artificial intelligence-based labor education innovation ability training system to improve the teaching efficiency and learning effectiveness of labor education by integrating advanced artificial intelligence technology. The system adopts multi-layer architecture design, including front-end user interface, back-end logic processing, database management, and artificial intelligence analysis four main parts, using Python, Java, VueJS, TensorFlow, and MySQL technology to achieve. The system includes educational content management, learning progress tracking, interactive learning tool development, data analysis and feedback mechanisms to provide personalized learning experiences for students, and real-time feedback and suggestions for teachers to optimize teaching resources. Future research directions include addressing technical challenges such as ensuring data security and privacy, improving the accuracy of AI-driven recommendations, and integrating emerging technologies like virtual and augmented reality to enhance interactive learning. Additionally, exploring the impact of AI-based systems on educational policies is crucial, as these technologies can influence curriculum design, teaching methodologies, and resource allocation. Understanding these impacts will help in creating supportive policies that facilitate the adoption and effective use of AI in education, ensuring that all students benefit from these advancements.",rapidly developing information age application artificial intelligence technology field education extensive especially labor education shows great potential value study aims develop apply artificial intelligence based labor education innovation ability training system improve teaching efficiency learning effectiveness labor education integrating advanced artificial intelligence technology system adopts multi layer architecture design including front end user interface back end logic processing database management artificial intelligence analysis four main parts using python java vuejs tensorflow mysql technology achieve system includes educational content management learning progress tracking interactive learning tool development data analysis feedback mechanisms provide personalized learning experiences students real time feedback suggestions teachers optimize teaching resources future research directions include addressing technical challenges ensuring data security privacy improving accuracy driven recommendations integrating emerging technologies like virtual augmented reality enhance interactive learning additionally exploring impact based systems educational policies crucial technologies influence curriculum design teaching methodologies resource allocation understanding impacts help creating supportive policies facilitate adoption effective use education ensuring students benefit advancements
"In the age of AI, English teacher transformation has also become a hot topic. English role transformation and adaptation is also a key path for teachers to improve their self-qualification. Currently, related studies mainly focus on the impact of teachers’ role change on students’ learning, while the research on teachers’ role change on their own development is relatively lacking. From the perspective of teachers’ own development, this paper discusses the necessity, strategies and effects of English teachers’ role change, as well as the difficulties and solutions encountered in the process of teachers’ role change, aiming to provide useful insights for English teachers’ professional development. Specifically, this paper analyzes the model and path of English teachers’ professional development from the process of needs analysis, curriculum design, task-driven, results materialization, feedback evaluation and three-self management. And, the effect of this model was studied through experimental design. The results of the study show that the professionalism of teachers in the experimental group improved significantly, their English proficiency progressed significantly, and all the indicators and overall scores were much higher than those of the control group, reflecting that the teachers in the experimental group possessed strong teaching reflection ability and awareness, and the students in the experimental group showed a high degree of satisfaction with the teachers’ teaching and their own learning, which shows that the model is effective and has certain theoretical and practical significance.",age english teacher transformation also become hot topic english role transformation adaptation also key path teachers improve self qualification currently related studies mainly focus impact teachers role change students learning research teachers role change development relatively lacking perspective teachers development paper discusses necessity strategies effects english teachers role change well difficulties solutions encountered process teachers role change aiming provide useful insights english teachers professional development specifically paper analyzes model path english teachers professional development process needs analysis curriculum design task driven results materialization feedback evaluation three self management effect model studied experimental design results study show professionalism teachers experimental group improved significantly english proficiency progressed significantly indicators overall scores much higher control group reflecting teachers experimental group possessed strong teaching reflection ability awareness students experimental group showed high degree satisfaction teachers teaching learning shows model effective certain theoretical practical significance
"In the context of globalization and digitalization, the translation of museum promotional texts has become a vital bridge for cultural exchange. This study examines the application of computer-aided lexical translation in museum promotional texts. Utilizing a comprehensive methodology that includes questionnaires, data collection, model building and evaluation, and data analysis, this research thoroughly assesses the technology’s performance in enhancing translation accuracy, efficiency, and user satisfaction. The results demonstrate that computer-aided lexical translation technology significantly improves the efficiency and accuracy of translating museum promotional texts and has been widely appreciated by users. However, challenges remain in terms of technical adaptability and user acceptance. This study proposes solutions to these challenges and explores the potential applications of this technology in cultural exchange and international cooperation.",context globalization digitalization translation museum promotional texts become vital bridge cultural exchange study examines application computer aided lexical translation museum promotional texts utilizing comprehensive methodology includes questionnaires data collection model building evaluation data analysis research thoroughly assesses technology performance enhancing translation accuracy efficiency user satisfaction results demonstrate computer aided lexical translation technology significantly improves efficiency accuracy translating museum promotional texts widely appreciated users however challenges remain terms technical adaptability user acceptance study proposes solutions challenges explores potential applications technology cultural exchange international cooperation
"In modern engineering, the importance of risk management in decision-making has increased proportionally with the growing scale and complexity of projects. Construction engineering projects are frequently faced with various risks, including insufficient financial support, human resource allocation difficulties, supply chain disruptions, etc., which may cause project delays, cost overruns, or even failure. Therefore, effective evaluation and mitigation of risks in different construction phases are crucial to ensuring project smooth progress. This study focuses on the challenge of intelligent risk assessment in construction engineering and proposes a novel evaluation model integrating deep neural network (DNN) modeling with intelligent decision calculus. The framework first uses the Analytic Hierarchy Process (AHP) to quantitatively measure phase-specific evaluation indicators, then performs feature extraction through Temporal Convolutional Networks (TCN). Reinforcement learning (via Deep Q-Networks, DQN) is incorporated to enhance the model’s interactive decision-making ability, realizing dynamic risk identification. Experimental results show that the model achieves an identification accuracy of over 80% in distinguishing low, medium, and high-risk scenarios, demonstrating an innovative approach to construction risk assessment that significantly improves decision-making efficiency.",modern engineering importance risk management decision making increased proportionally growing scale complexity projects construction engineering projects frequently faced various risks including insufficient financial support human resource allocation difficulties supply chain disruptions etc may cause project delays cost overruns even failure therefore effective evaluation mitigation risks different construction phases crucial ensuring project smooth progress study focuses challenge intelligent risk assessment construction engineering proposes novel evaluation model integrating deep neural network dnn modeling intelligent decision calculus framework first uses analytic hierarchy process ahp quantitatively measure phase specific evaluation indicators performs feature extraction temporal convolutional networks tcn reinforcement learning via deep networks dqn incorporated enhance model interactive decision making ability realizing dynamic risk identification experimental results show model achieves identification accuracy distinguishing low medium high risk scenarios demonstrating innovative approach construction risk assessment significantly improves decision making efficiency
"As science and technology advance, artificial intelligence is increasingly influencing the rural economy, offering innovative pathways for revitalization. This study examines the role and future prospects of artificial intelligence in rural economic development, highlighting its application across agriculture, education, healthcare, and infrastructure. The research identifies significant challenges, including the complexity of technology adoption, substantial capital investment requirements, and inadequate infrastructure in rural areas. However, by enhancing technology promotion, implementing supportive policies, and upgrading infrastructure, these challenges can be addressed, paving the way for successful rural revitalization. This study provides a comprehensive analysis and offers theoretical foundations and practical insights for policymakers and researchers focused on leveraging artificial intelligence to boost rural economic development.",science technology advance artificial intelligence increasingly influencing rural economy offering innovative pathways revitalization study examines role future prospects artificial intelligence rural economic development highlighting application across agriculture education healthcare infrastructure research identifies significant challenges including complexity technology adoption substantial capital investment requirements inadequate infrastructure rural areas however enhancing technology promotion implementing supportive policies upgrading infrastructure challenges addressed paving way successful rural revitalization study provides comprehensive analysis offers theoretical foundations practical insights policymakers researchers focused leveraging artificial intelligence boost rural economic development
"Taking the cloud computing-based English–Chinese interpreting collaborative teaching platform as the research object, this paper systematically evaluates and analyzes it from four aspects, namely, technological performance, teaching function, teaching effect, and teaching features and compares it with the traditional model and the model based on AI platform. The experimental results show that the English–Chinese Interpreting Collaborative Teaching Platform based on Cloud Computing shows obvious advantages in all aspects and is able to provide more advanced, efficient and effective teaching support for interpreting teaching, promote the improvement of teaching quality and teaching satisfaction of interpreting teachers and students, and promote the reform and innovation of interpreting teaching. This paper provides an in-depth analysis of the development principle, functional design, technical realization path, and its practical application effect of the English–Chinese Interpreting Collaborative Teaching Platform based on cloud computing, which provides a new perspective and effective strategy for the modern reform and development of English–Chinese interpreting teaching.",taking cloud computing based english chinese interpreting collaborative teaching platform research object paper systematically evaluates analyzes four aspects namely technological performance teaching function teaching effect teaching features compares traditional model model based platform experimental results show english chinese interpreting collaborative teaching platform based cloud computing shows obvious advantages aspects able provide advanced efficient effective teaching support interpreting teaching promote improvement teaching quality teaching satisfaction interpreting teachers students promote reform innovation interpreting teaching paper provides depth analysis development principle functional design technical realization path practical application effect english chinese interpreting collaborative teaching platform based cloud computing provides new perspective effective strategy modern reform development english chinese interpreting teaching
"The research will analyze and understand the current situation of the financial accounting management system in colleges and universities and the main challenges it faces, and conduct in-depth research on the fuzzy reasoning model, take the actual university financial management problem as an example, design and implement the decision support module based on fuzzy reasoning, test and evaluate its performance. The function and effect of the system are verified, and the optimization and improvement strategies are proposed. This research will provide theoretical and practical support for the development and application of financial accounting management system in colleges and universities, and promote the modernization and intelligence of financial management in colleges and universities.",research analyze understand current situation financial accounting management system colleges universities main challenges faces conduct depth research fuzzy reasoning model take actual university financial management problem example design implement decision support module based fuzzy reasoning test evaluate performance function effect system verified optimization improvement strategies proposed research provide theoretical practical support development application financial accounting management system colleges universities promote modernization intelligence financial management colleges universities
"Although supply chain finance is essential for modern business, it is also prone to fraud, mistakes, and inefficiencies. This paper presents a blockchain-based smart contract audit system to improve supply chain finance’s security, openness, and efficiency. The system uses distributed architecture, smart contracts and blockchain technology to automate and audit financial transactions. In terms of scalability, convergence speed, and accuracy, experimental results reveal quite excellent performance of the proposed system. The proposed method achieved in auditing financial transactions an accuracy of 95.2%. Showing a convergence speed of 41.2 s for 30 nodes, the system confirmed its ability to manage large-scale datasets. With 30 nodes, the system shown scalability proving it could control difficult supply chain financial circumstances. Promising supply chain finance solution the blockchain-based smart contract audit system shown a clear decrease in audit time and cost.",although supply chain finance essential modern business also prone fraud mistakes inefficiencies paper presents blockchain based smart contract audit system improve supply chain finance security openness efficiency system uses distributed architecture smart contracts blockchain technology automate audit financial transactions terms scalability convergence speed accuracy experimental results reveal quite excellent performance proposed system proposed method achieved auditing financial transactions accuracy showing convergence speed nodes system confirmed ability manage large scale datasets nodes system shown scalability proving could control difficult supply chain financial circumstances promising supply chain finance solution blockchain based smart contract audit system shown clear decrease audit time cost
"With the rapid development of science and technology, virtual reality (VR) technology has increasingly become an important tool for global cultural communication. The purpose of this paper is to discuss the application and influence of VR technology in the communication of Chinese and foreign culture. This paper first introduces the basic principle of VR technology and the theoretical framework of cultural communication, and then analyzes the application mode and effect of VR in different cultural backgrounds. By collecting and analyzing usage data from users with different cultural backgrounds, this study established an evaluation model aimed at quantifying the effects of VR technology in cultural communication. The results show that VR technology can significantly improve users’ cultural experience and interactive participation, so as to effectively promote cultural communication and exchange. The study also found that cultural differences and technology acceptance have a significant impact on the communication effectiveness of VR. The findings of this paper provide a new perspective for understanding and optimizing the application of VR technology in global cultural communication, while making suggestions for future development in related fields.",rapid development science technology virtual reality technology increasingly become important tool global cultural communication purpose paper discuss application influence technology communication chinese foreign culture paper first introduces basic principle technology theoretical framework cultural communication analyzes application mode effect different cultural backgrounds collecting analyzing usage data users different cultural backgrounds study established evaluation model aimed quantifying effects technology cultural communication results show technology significantly improve users cultural experience interactive participation effectively promote cultural communication exchange study also found cultural differences technology acceptance significant impact communication effectiveness findings paper provide new perspective understanding optimizing application technology global cultural communication making suggestions future development related fields
"With the rapid proliferation of intelligent applications in edge computing, the efficient scheduling and lightweight deployment of deep learning models have emerged as critical challenges. This study presents a unified framework that integrates adaptive deep neural network compression with priority-aware task scheduling, specifically tailored for edge computing environments. A hierarchical joint compression strategy is proposed, combining sparsity and quantization through learnable parameters to achieve substantial model size reduction while preserving predictive accuracy. Concurrently, a two-stage load redistribution scheduling algorithm is developed, guided by task latency tolerance and completion time deviation, to enhance resource utilization and achieve balanced server load distribution. Experimental evaluations conducted on benchmark deep learning models and real-world power IoT scenarios demonstrate that the proposed method attains a 143× compression ratio with only a 1.3% loss in accuracy. Furthermore, it improves task latency and load balancing efficiency by over 18% and 21%, respectively, when compared to conventional methods. These results substantiate the effectiveness of the proposed framework in facilitating lightweight, responsive, and resource-efficient edge intelligence.",rapid proliferation intelligent applications edge computing efficient scheduling lightweight deployment deep learning models emerged critical challenges study presents unified framework integrates adaptive deep neural network compression priority aware task scheduling specifically tailored edge computing environments hierarchical joint compression strategy proposed combining sparsity quantization learnable parameters achieve substantial model size reduction preserving predictive accuracy concurrently two stage load redistribution scheduling algorithm developed guided task latency tolerance completion time deviation enhance resource utilization achieve balanced server load distribution experimental evaluations conducted benchmark deep learning models real world power iot scenarios demonstrate proposed method attains compression ratio loss accuracy furthermore improves task latency load balancing efficiency respectively compared conventional methods results substantiate effectiveness proposed framework facilitating lightweight responsive resource efficient edge intelligence
"As the representative of high-density energy and clean energy, nuclear power has prominent advantages. Using nuclear power to generate electricity can not only alleviate the problem of energy tension, but also solve the problem of pollution faced by other energy sources. Therefore, nuclear power has become one of the major energy sources vigorously developed by countries around the world. With the development of the nuclear power industry, the nuclear industry has begun to research and develop various types of new reactors with a view to the future in order to further improve the safety and economy of nuclear power. In recent years, small modular reactors have gained wide attention due to their low initial cost, short construction period, flexible power plant layout, and wide use. At the same time, the small modular reactor of nuclear power plant also has great disadvantages, that is, the effectiveness of the safety system of the small modular reactor of nuclear power plant is difficult to be guaranteed under complex conditions, especially in the problem elimination after the accident, which has been the focus and difficulty of nuclear energy research in all countries. Therefore, taking the Westinghouse small modular reactor designed by Westinghouse as an example, this paper studies the actual elimination effects and measures for serious accidents of small modular reactors in nuclear power plants, and then verifies the safety of small modular reactors. After research, after a serious accident occurred in a small modular reactor, timely elimination of the residual heat of the small modular reactor is the key and difficult work to eliminate the small modular reactor; the passive core makeup tank designed in the small modular reactor has a rapid and excellent mitigation effect after a serious accident.",representative high density energy clean energy nuclear power prominent advantages using nuclear power generate electricity alleviate problem energy tension also solve problem pollution faced energy sources therefore nuclear power become one major energy sources vigorously developed countries around world development nuclear power industry nuclear industry begun research develop various types new reactors view future order improve safety economy nuclear power recent years small modular reactors gained wide attention due low initial cost short construction period flexible power plant layout wide use time small modular reactor nuclear power plant also great disadvantages effectiveness safety system small modular reactor nuclear power plant difficult guaranteed complex conditions especially problem elimination accident focus difficulty nuclear energy research countries therefore taking westinghouse small modular reactor designed westinghouse example paper studies actual elimination effects measures serious accidents small modular reactors nuclear power plants verifies safety small modular reactors research serious accident occurred small modular reactor timely elimination residual heat small modular reactor key difficult work eliminate small modular reactor passive core makeup tank designed small modular reactor rapid excellent mitigation effect serious accident
"This study aims to explore the characteristics and cultivation strategies of non-common language students’ foreign language digital narrative ability based on big data mining. Through the design and distribution of 800 questionnaires, 706 valid data were recovered and analyzed. Statistical methods such as Pearson correlation coefficient, Spearman grade correlation coefficient and regression analysis were adopted to deeply analyze the relationship between students’ use frequency of digital narrative tools, self-assessment of foreign language digital narrative ability and learning motivation. The results show that the use frequency of interactive learning tools and collaborative platforms has a significant positive correlation with students’ speaking and writing ability. Students’ learning motivation also showed a positive correlation with their frequency of using digital tools and their foreign language learning effectiveness. Based on these findings, this study suggests that educators should more widely integrate digital narrative tools into foreign language teaching in non-common languages, and design teaching strategies that stimulate students’ intrinsic learning motivation. This study not only provides empirical research support for the foreign language education of non-common language students but also puts forward useful strategies and suggestions on how to effectively use digital tools to promote students’ foreign language learning. Future research can further explore other influencing factors to provide more comprehensive teaching guidance.",study aims explore characteristics cultivation strategies non common language students foreign language digital narrative ability based big data mining design distribution questionnaires valid data recovered analyzed statistical methods pearson correlation coefficient spearman grade correlation coefficient regression analysis adopted deeply analyze relationship students use frequency digital narrative tools self assessment foreign language digital narrative ability learning motivation results show use frequency interactive learning tools collaborative platforms significant positive correlation students speaking writing ability students learning motivation also showed positive correlation frequency using digital tools foreign language learning effectiveness based findings study suggests educators widely integrate digital narrative tools foreign language teaching non common languages design teaching strategies stimulate students intrinsic learning motivation study provides empirical research support foreign language education non common language students also puts forward useful strategies suggestions effectively use digital tools promote students foreign language learning future research explore influencing factors provide comprehensive teaching guidance
"Table tennis is a highly technical and fast sport, which requires athletes to have good technical level and physical fitness. In order to achieve more efficient and intelligent table tennis assisted training, a knowledge-based general sports-assisted training framework (KGSTF) was introduced, and the functions of each module were analyzed. At the same time, an adaptive contact judgment threshold generation algorithm was proposed for the implementation of this framework in specific table tennis training. On this basis, a pose correction method based on contact position constraints and an optical motion capture data preprocessing method were designed, and an automated program was developed to handle data stitching, clutter removal, and labeling. The results showed that in the validation experiment based on the adaptive contact judgment threshold generation algorithm, by incorporating the optimization results of the previous frame into the objective function, the optimized actions can be made smoother. Meanwhile, the landing error of the advanced subjects was only 1.539 cm when receiving a slight topspin serve, 0.947 cm when receiving a downspin serve, and 5.294 cm when receiving a strong topspin serve, which illustrates that a smaller serve speed has more stability. In addition, the return success rate of advanced subjects in the slight topspin mode was the best, with a maximum value of 95.12% and an average of 91.38%. The average return success rate of intermediate subjects in the slight topspin mode was as high as 75.36%. Compared with the strong topspin and downspin service modes, the return success rate was increased by 13.72% and 26.26%, respectively. Meanwhile, the return success rate of advanced subjects was significantly higher than that of other subjects. The results show that the sports auxiliary framework designed by this study can be effectively applied to the sports evaluation of table tennis auxiliary training, which is helpful to provide personalized training guidance and improvement suggestions for table tennis players, and promotes the scientific and professional development of table tennis.",table tennis highly technical fast sport requires athletes good technical level physical fitness order achieve efficient intelligent table tennis assisted training knowledge based general sports assisted training framework kgstf introduced functions module analyzed time adaptive contact judgment threshold generation algorithm proposed implementation framework specific table tennis training basis pose correction method based contact position constraints optical motion capture data preprocessing method designed automated program developed handle data stitching clutter removal labeling results showed validation experiment based adaptive contact judgment threshold generation algorithm incorporating optimization results previous frame objective function optimized actions made smoother meanwhile landing error advanced subjects receiving slight topspin serve receiving downspin serve receiving strong topspin serve illustrates smaller serve speed stability addition return success rate advanced subjects slight topspin mode best maximum value average average return success rate intermediate subjects slight topspin mode high compared strong topspin downspin service modes return success rate increased respectively meanwhile return success rate advanced subjects significantly higher subjects results show sports auxiliary framework designed study effectively applied sports evaluation table tennis auxiliary training helpful provide personalized training guidance improvement suggestions table tennis players promotes scientific professional development table tennis
"Aiming at services such as nearby consumption and flexible interaction of new loads, it is necessary to achieve higher precision clock synchronization of local equipment, and the public network transmission delay fluctuates randomly, which cannot meet the corresponding accuracy requirements. In order to get rid of the limitations brought by public network transmission, distributed clock synchronization replaces the centralized timing of the master station through information exchange between nodes to achieve higher precision clock synchronization. However, in the process of distributed synchronization, a large amount of information will be exchanged between key nodes of the link and their neighbors, but the concurrent transmission capacity of key nodes such as dedicated power gateways and centralized coordinators (CCOs) is limited, which is difficult to meet the corresponding communication requirements. In order to solve the above problems, this paper proposes a time-triggered distributed synchronization scheme based on node priority, which can meet the requirements of high-precision synchronization and reduce the transmission burden of link aggregation nodes such as power dedicated gateways and CCOs.",aiming services nearby consumption flexible interaction new loads necessary achieve higher precision clock synchronization local equipment public network transmission delay fluctuates randomly meet corresponding accuracy requirements order get rid limitations brought public network transmission distributed clock synchronization replaces centralized timing master station information exchange nodes achieve higher precision clock synchronization however process distributed synchronization large amount information exchanged key nodes link neighbors concurrent transmission capacity key nodes dedicated power gateways centralized coordinators ccos limited difficult meet corresponding communication requirements order solve problems paper proposes time triggered distributed synchronization scheme based node priority meet requirements high precision synchronization reduce transmission burden link aggregation nodes power dedicated gateways ccos
"At present, various new technologies are applied to the reform of college English courses. Cases are also increasing. Big data technology has become the focus of most industries in the era of social media with data explosion. Its powerful data analysis and prediction capabilities provide convenience for its application. Applying big data technology to the difficulties encountered in curriculum reform will be able to efficiently and accurately solve related problems. The rapid coverage of IoT applications such as the Internet of Vehicles, smart homes, and smart cities has provided precedents for the use of IoT technology in various industries. With the rapid development of the needs of the Metaverse, it has become a trend to apply IoT technology to social and educational fields. Internet of Things technology can use various miniature portable devices to link into a body area network. The network can record the status of students when they learn English, which supports the subsequent analysis of students’ learning needs and learning effects. The analysis results can also provide more convenience for college English teachers to prepare lessons and teach. In order to investigate the current predicament of college English curriculum reform, this paper distributed 900 questionnaires to managers, teachers, and students in 20 colleges and universities, mainly through the questionnaire star online platform and offline campus distribution. Using classification and statistical algorithms to process and analyze the data, it is found that the problems existing in the current college English curriculum reform include the poor English ability of students, the heavy teaching burden of English teachers, and the lack of an effective comprehensive processing platform for English teaching information. Then, combined with big data and Internet of Things technology, we designed solutions for teachers and students’ dilemma in curriculum reform from the perspectives of platform establishment, intelligent interconnection, and data analysis.",present various new technologies applied reform college english courses cases also increasing big data technology become focus industries social media data explosion powerful data analysis prediction capabilities provide convenience application applying big data technology difficulties encountered curriculum reform able efficiently accurately solve related problems rapid coverage iot applications internet vehicles smart homes smart cities provided precedents use iot technology various industries rapid development needs metaverse become trend apply iot technology social educational fields internet things technology use various miniature portable devices link body area network network record status students learn english supports subsequent analysis students learning needs learning effects analysis results also provide convenience college english teachers prepare lessons teach order investigate current predicament college english curriculum reform paper distributed questionnaires managers teachers students colleges universities mainly questionnaire star online platform offline campus distribution using classification statistical algorithms process analyze data found problems existing current college english curriculum reform include poor english ability students heavy teaching burden english teachers lack effective comprehensive processing platform english teaching information combined big data internet things technology designed solutions teachers students dilemma curriculum reform perspectives platform establishment intelligent interconnection data analysis
"This study addresses the problem of Chinese pronunciation errors made by native Uyghur speakers by proposing a phoneme confusion modeling framework that integrates cross-linguistic phonological comparison and data-driven methods. Based on speech transfer theory, a corpus of 52,510 standard and non-standard pronunciations (total duration 167.2 hours) was constructed. The dynamic time warping (DTW) algorithm was employed to align phoneme sequences, and Apriori association rule mining was used to uncover high-frequency confusion patterns. The study found that differences in sound, like the lack of retroflex consonants and the simplification of compound vowels, caused systematic errors. For example, the high-frequency substitution of affricates “ch→q” (62.5%) and compound vowels “uo→o/u” (76.1%). Data-driven methods further validated the phonological hypotheses and identified context-dependent phenomena (such as “ian→an” medial weakening). The confusion rule base achieved a robustness of F1-score 0.87. The quantitative model based on the confusion matrix can be integrated into intelligent speech assessment systems to provide real-time corrective feedback (e.g., “‘uo’ detected as misread as ‘o’, probability 76.1%”). This study provides an interpretable rule base and dynamic alignment technical foundation for cross-language pronunciation error correction algorithms, which has theoretical and practical significance for optimizing computer-assisted language teaching systems.",study addresses problem chinese pronunciation errors made native uyghur speakers proposing phoneme confusion modeling framework integrates cross linguistic phonological comparison data driven methods based speech transfer theory corpus standard non standard pronunciations total duration hours constructed dynamic time warping dtw algorithm employed align phoneme sequences apriori association rule mining used uncover high frequency confusion patterns study found differences sound like lack retroflex consonants simplification compound vowels caused systematic errors example high frequency substitution affricates compound vowels data driven methods validated phonological hypotheses identified context dependent phenomena ian medial weakening confusion rule base achieved robustness score quantitative model based confusion matrix integrated intelligent speech assessment systems provide real time corrective feedback detected misread probability study provides interpretable rule base dynamic alignment technical foundation cross language pronunciation error correction algorithms theoretical practical significance optimizing computer assisted language teaching systems
"In the digital age, the traditional human resource management system cannot satisfy the enterprises’ requirements for human resource management. Currently, more development opportunities have been provided for the traditional human resource management system. However, the traditional human resource management has the problem of information capture and analysis classification, so this research builds a neural network algorithm model under the SSH framework. The model uses the improved long short-term memory neural network to process and classify the information of human resource management, and realizes the processing of human resource management information. The minimum root-mean-square error of the newly constructed algorithm model is 0.18, which is 0.15 smaller than that of the convolutional neural network and 0.06 smaller than that of the genetic neural network. The fitness of the new model is 6.2% higher than that of the convolutional neural network and 2.8% higher than that of the genetic algorithm. The whole model is superior to the traditional neural network algorithm in fit and accuracy, which can solve the problem of human resource management information processing under SSH framework. It provides new perspectives and tools for understanding and solving complex human resource problems, and expands the application of artificial intelligence and deep learning in the field of human resource management.",digital age traditional human resource management system satisfy enterprises requirements human resource management currently development opportunities provided traditional human resource management system however traditional human resource management problem information capture analysis classification research builds neural network algorithm model ssh framework model uses improved long short term memory neural network process classify information human resource management realizes processing human resource management information minimum root mean square error newly constructed algorithm model smaller convolutional neural network smaller genetic neural network fitness new model higher convolutional neural network higher genetic algorithm whole model superior traditional neural network algorithm fit accuracy solve problem human resource management information processing ssh framework provides new perspectives tools understanding solving complex human resource problems expands application artificial intelligence deep learning field human resource management
"Academic emotions and learning engagement are important factors that influence students’ learning outcomes and academic performance. However, few studies have explored the mechanism of college students’ learning engagement in blended learning from the perspective of fostering positive academic emotions. Based on a questionnaire survey of 375 college students in a blended learning environment, the data were analyzed and processed using SPSS24.0 and AMOS27.0 software. Structural equation modeling was established to explore the mediating effect of academic self-efficacy in moderating the relationship between academic engagement and positive academic mood. The findings suggest that: (1) in the blended learning environment, college students demonstrate more favorable academic performance and moderate learning engagement. (2) There is a significant positive correlation between active hyperarousal and active hypo-arousal with learning engagement. (3) Positive hyperarousal and positive hypo-arousal not only directly enhance learning engagement but also indirectly promote academic self-efficacy. In teaching practice, under a blended learning environment, educators should further improve the quality of students’ blended learning input. They need to help students foster positive academic emotions of enjoyment and pride and promote their self-efficacy at the same time. Furthermore, they should explore various teaching methods to improve students’ enthusiasm for learning and concentration.",academic emotions learning engagement important factors influence students learning outcomes academic performance however studies explored mechanism college students learning engagement blended learning perspective fostering positive academic emotions based questionnaire survey college students blended learning environment data analyzed processed using spss amos software structural equation modeling established explore mediating effect academic self efficacy moderating relationship academic engagement positive academic mood findings suggest blended learning environment college students demonstrate favorable academic performance moderate learning engagement significant positive correlation active hyperarousal active hypo arousal learning engagement positive hyperarousal positive hypo arousal directly enhance learning engagement also indirectly promote academic self efficacy teaching practice blended learning environment educators improve quality students blended learning input need help students foster positive academic emotions enjoyment pride promote self efficacy time furthermore explore various teaching methods improve students enthusiasm learning concentration
"This study focuses on the construction and application of smart campuses that integrate student management and educational innovation, and deeply analyzes how smart campuses can optimize student management processes and innovate education and teaching methods through advanced information technology. Through detailed data analysis, we found that the construction of smart campuses has achieved remarkable results in improving management efficiency. In comparison to the traditional student management model, smart campus leverages advanced data analysis technology to achieve precise tracking and evaluation of student behavior, learning status, and other pertinent information. This technological advancement has led to a noteworthy enhancement in management efficiency, with an approximate 30% increase. Furthermore, smart campus facilitates student management departments in promptly identifying and resolving issues faced by students through in-depth data analysis, ultimately leading to a significant improvement in student satisfaction. In terms of educational innovation, smart campus uses big data analysis to accurately locate students’ learning needs and points of interest, providing strong support for personalized teaching. According to statistics, in classes that adopt the smart campus teaching model, the average score of students has increased by 15%, and the degree of learning participation has also increased significantly. In addition, this study also focuses on data security and privacy protection issues in the construction of smart campuses, and proposes corresponding solutions. The construction and application of smart campuses that integrate student management and educational innovation not only improves management efficiency, but also promotes the innovation and development of education and teaching. In the future, we will continue to deepen research and promote the continuous optimization and upgrading of smart campus construction. The 15% improvement came from the one semester comparative experiment of the Taihu Lake University in Wuxi. 142 students in the experimental group used smart campus teaching, while 136 students in the control group used traditional teaching, covering computer, business administration, engineering and other majors. Statistical analysis confirms that the improvement in grades is significant, verifying the positive impact of smart campuses on learning outcomes.",study focuses construction application smart campuses integrate student management educational innovation deeply analyzes smart campuses optimize student management processes innovate education teaching methods advanced information technology detailed data analysis found construction smart campuses achieved remarkable results improving management efficiency comparison traditional student management model smart campus leverages advanced data analysis technology achieve precise tracking evaluation student behavior learning status pertinent information technological advancement led noteworthy enhancement management efficiency approximate increase furthermore smart campus facilitates student management departments promptly identifying resolving issues faced students depth data analysis ultimately leading significant improvement student satisfaction terms educational innovation smart campus uses big data analysis accurately locate students learning needs points interest providing strong support personalized teaching according statistics classes adopt smart campus teaching model average score students increased degree learning participation also increased significantly addition study also focuses data security privacy protection issues construction smart campuses proposes corresponding solutions construction application smart campuses integrate student management educational innovation improves management efficiency also promotes innovation development education teaching future continue deepen research promote continuous optimization upgrading smart campus construction improvement came one semester comparative experiment taihu lake university wuxi students experimental group used smart campus teaching students control group used traditional teaching covering computer business administration engineering majors statistical analysis confirms improvement grades significant verifying positive impact smart campuses learning outcomes
"V2G technology enables bidirectional energy transfer from EVs to the grid. It is possible to use the grid to charge EVs and to use EVs to charge the grid during peak grid hours, thus alleviating the problem of grid load fluctuations and improving grid efficiency. Many traditional authentication protocols have been proposed to protect V2G. However, the rapid development of high-speed Internet and the race to organize the development of quantum computers are a great threat to the existing authentication schemes. Therefore, traditional authentication protocols are vulnerable to attacks in the era of quantum computing. In this paper, I propose an extended chaotic mapping-based authentication and quantum key distribution scheme to defend against quantum attacks and prevent known traditional security attacks. The proposed extended chaotic mapping-based authentication protocol is secure under the real or random (ROR) model. Also, experimental results demonstrate that our protocol is lightweight compared to existing authentication protocols, as listed in the performance analysis section. The comparative analysis shows that the protocol is suitable for practical implementation in a quantum environment.",technology enables bidirectional energy transfer evs grid possible use grid charge evs use evs charge grid peak grid hours thus alleviating problem grid load fluctuations improving grid efficiency many traditional authentication protocols proposed protect however rapid development high speed internet race organize development quantum computers great threat existing authentication schemes therefore traditional authentication protocols vulnerable attacks quantum computing paper propose extended chaotic mapping based authentication quantum key distribution scheme defend quantum attacks prevent known traditional security attacks proposed extended chaotic mapping based authentication protocol secure real random ror model also experimental results demonstrate protocol lightweight compared existing authentication protocols listed performance analysis section comparative analysis shows protocol suitable practical implementation quantum environment
"In modern digital grid systems, ensuring the stability of power operations and the accuracy of energy metering is critically dependent on effective asset identification, reconfiguration, and automated maintenance strategies. Conventional grid monitoring mechanisms like manual inspections and fixed-sensor networks also experience many difficulties, such as high costs in operation, scalability problems, and inability to present real-time, all-around information. To address these challenges, this research introduces an advanced methodology that leverages deep learning models for asset identification and automation in digital grid systems. Specifically, the research utilizes Roach Infestation Optimized Attention-based Bidirectional Long Short-Term Memory (RI-Att-BiLSTM) networks for intelligent asset identification and topology recognition in low-voltage distribution network substations. Time-series data is collected from smart meters and sensors distributed across the low-voltage distribution network. These sensors continuously capture data on electricity consumption from substations and connected consumers. The collected data is preprocessed to remove noise, outliers, and gaps, followed by normalization to scale the data for effective modeling. The RI-Att-BiLSTM-based technique reduces the dimensionality of the electricity consumption data matrix, transforming the complex topology identification task into a solvable convex optimization problem. Experimental results demonstrate that, compared to traditional methods, the proposed model achieve a superior balance between accuracy (98.43%) and higher recall, precision, and F1-score. The enhanced grid resilience provided by this methodology facilitates precise asset identification, real-time monitoring, and fault-tolerant operations, thereby contributing to more efficient and reliable digital grid systems.",modern digital grid systems ensuring stability power operations accuracy energy metering critically dependent effective asset identification reconfiguration automated maintenance strategies conventional grid monitoring mechanisms like manual inspections fixed sensor networks also experience many difficulties high costs operation scalability problems inability present real time around information address challenges research introduces advanced methodology leverages deep learning models asset identification automation digital grid systems specifically research utilizes roach infestation optimized attention based bidirectional long short term memory att bilstm networks intelligent asset identification topology recognition low voltage distribution network substations time series data collected smart meters sensors distributed across low voltage distribution network sensors continuously capture data electricity consumption substations connected consumers collected data preprocessed remove noise outliers gaps followed normalization scale data effective modeling att bilstm based technique reduces dimensionality electricity consumption data matrix transforming complex topology identification task solvable convex optimization problem experimental results demonstrate compared traditional methods proposed model achieve superior balance accuracy higher recall precision score enhanced grid resilience provided methodology facilitates precise asset identification real time monitoring fault tolerant operations thereby contributing efficient reliable digital grid systems
"Traditional fault monitoring methods for converter valves in high-voltage direct current (HVDC) systems lack a unified standard for converter valve data, making it difficult to exchange and integrate information between different systems, and unable to timely and accurately identify potential problems in the operation of converter valves. This article utilized the Common Information Model (CIM) to achieve standardized representation and interactive operation of converter valve monitoring data, and improved the performance of fault detection for converter valves through ensemble learning. It collected data from various entities in the converter valve (such as multi-source sensor data, electrical information, and so on) and uses CIM for standardized representation. The attributes of each entity in the converter valve can be corresponding to the CIM model. This article preprocessed the collected converter valve data to ensure the quality and availability of the data. It used the Adaboost ensemble learning model to combine Support Vector Machine (SVM), Decision Tree (DT), K-Nearest Neighbors (KNN), and Naive Bayes. This article combined the CIM with ensemble learning Adaboost. The experimental results showed that CIM-Adaboost can accurately classify six common converter valve fault types. After mixing normal and faulty samples in this article, CIM-Adaboost still maintains high stability, with an accuracy rate of 94.6% for fault classification. The combination of CIM and Adaboost can effectively improve the effectiveness of converter valve fault diagnosis.",traditional fault monitoring methods converter valves high voltage direct current hvdc systems lack unified standard converter valve data making difficult exchange integrate information different systems unable timely accurately identify potential problems operation converter valves article utilized common information model cim achieve standardized representation interactive operation converter valve monitoring data improved performance fault detection converter valves ensemble learning collected data various entities converter valve multi source sensor data electrical information uses cim standardized representation attributes entity converter valve corresponding cim model article preprocessed collected converter valve data ensure quality availability data used adaboost ensemble learning model combine support vector machine svm decision tree nearest neighbors knn naive bayes article combined cim ensemble learning adaboost experimental results showed cim adaboost accurately classify six common converter valve fault types mixing normal faulty samples article cim adaboost still maintains high stability accuracy rate fault classification combination cim adaboost effectively improve effectiveness converter valve fault diagnosis
"This article focuses on the research of enterprise accounting information recognition system based on data mining, pointing out that traditional accounting information systems and management accounting methods in the era of big data rely too much on internal historical financial data, making it difficult to effectively process key internal and external unstructured data, resulting in insufficient depth of strategic decision analysis. Data mining technology provides a new path for integrating internal and external data and building a strategic management accounting system. A five layer strategic management accounting system framework was developed, which includes a basic theoretical layer and a data storage layer. The framework integrates theories such as Porter’s Five Forces model and regression analysis techniques. Taking e-commerce company M as a case study, the data mining application in scenarios such as intelligent supply chain replenishment was analyzed. Logistic regression model was used to identify the authenticity of financial reports, and LSTM neural network was used to predict sales data. The results showed that after applying the framework, M company’s market share reached 46% and growth rate was 36% in the 12th month. The logistics unit cost supported order volume increased by 400%. This study achieved interdisciplinary integration of data mining and strategic management accounting, providing enterprises with information recognition and strategic decision-making tools to help them optimize management and maintain core competitiveness through data-driven optimization in complex environments.",article focuses research enterprise accounting information recognition system based data mining pointing traditional accounting information systems management accounting methods big data rely much internal historical financial data making difficult effectively process key internal external unstructured data resulting insufficient depth strategic decision analysis data mining technology provides new path integrating internal external data building strategic management accounting system five layer strategic management accounting system framework developed includes basic theoretical layer data storage layer framework integrates theories porter five forces model regression analysis techniques taking commerce company case study data mining application scenarios intelligent supply chain replenishment analyzed logistic regression model used identify authenticity financial reports lstm neural network used predict sales data results showed applying framework company market share reached growth rate month logistics unit cost supported order volume increased study achieved interdisciplinary integration data mining strategic management accounting providing enterprises information recognition strategic decision making tools help optimize management maintain core competitiveness data driven optimization complex environments
"At this stage, the problem of population aging is becoming increasingly prominent, and the problem of predicting the elderly is gradually attracting social attention. It is just an urgent problem to ensure the physical and mental health of the elderly, meet their spiritual and cultural needs, promote the sustainability of elderly care services, and make their lives stable. Take into account this, this paper discussed the application of the ecological elderly care control system based on the Internet of things (IoT) in the Integrated Body-Care Model (IBCM) and analyzed the sustainable development strategy of the IBCM. This paper also studied the role of threshold analysis in the recognition of falls of the elderly and conducted an experimental study on the sustainability strategy of the ecological elderly care control system and the integrated elderly care model. The research showed that the average of the 3 month health index of the elderly in Group I was 1.83% higher than that of the elderly in Group T; the average mental well-being index of the elderly in Group I was 3.48% higher than that of the elderly in group T; the average value of the feasibility score of the experts on the sustainability strategy of the sports and nutrition integration model was 90.84, and the average value of the experts on the effectiveness score of the sustainability strategy of the sports and nutrition integration model was 92.98. The ecological elderly care control system can play an important role in the aging care service practice. The sustainability strategy of the integration model of body and breeding has certain feasibility and effectiveness.",stage problem population aging becoming increasingly prominent problem predicting elderly gradually attracting social attention urgent problem ensure physical mental health elderly meet spiritual cultural needs promote sustainability elderly care services make lives stable take account paper discussed application ecological elderly care control system based internet things iot integrated body care model ibcm analyzed sustainable development strategy ibcm paper also studied role threshold analysis recognition falls elderly conducted experimental study sustainability strategy ecological elderly care control system integrated elderly care model research showed average month health index elderly group higher elderly group average mental well index elderly group higher elderly group average value feasibility score experts sustainability strategy sports nutrition integration model average value experts effectiveness score sustainability strategy sports nutrition integration model ecological elderly care control system play important role aging care service practice sustainability strategy integration model body breeding certain feasibility effectiveness
"To address the challenge of low accuracy in gene mutation prediction for non-small cell lung cancer, we propose the LGA-Net model to integrate a lightweight Transformer with a convolutional neural network (CNN) module. The model predicts Epidermal Growth Factor Receptor (EGFR) and Kirsten Rat Sarcoma Virus Oncogene (KRAS) gene mutations in CT images using the Reinforced Local Perception Module (RLPM) and incorporates an Attention-based lightweight multi-head mechanism transformer (ATLM). Experimental results indicate that the Local-Global Attention Network (LGA-Net) model achieves accuracy rates of 89.56% and 88.29% for EGFR and KRAS mutation predictions, respectively. Ablation experiments validate the effectiveness of each module. Our experiments show that the model significantly enhances prediction performance and presents a promising approach for the non-invasive detection of gene mutations in non-small cell lung cancer.",address challenge low accuracy gene mutation prediction non small cell lung cancer propose lga net model integrate lightweight transformer convolutional neural network cnn module model predicts epidermal growth factor receptor egfr kirsten rat sarcoma virus oncogene kras gene mutations images using reinforced local perception module rlpm incorporates attention based lightweight multi head mechanism transformer atlm experimental results indicate local global attention network lga net model achieves accuracy rates egfr kras mutation predictions respectively ablation experiments validate effectiveness module experiments show model significantly enhances prediction performance presents promising approach non invasive detection gene mutations non small cell lung cancer
"Dopamine (DA), a catecholamine neurotransmitter, acts through five G protein-coupled receptors categorized into D1-like (D1, D5) and D2-like (D2, D3, D4) subfamilies. Emerging evidence suggests dopamine receptors influence pancreatic islet function and glucose homeostasis, yet the specific role of dopamine D2 receptors (D2Rs) in type 1 diabetes mellitus (T1DM) pathogenesis remains unclear. This study investigated D2R’s impact on T1DM progression and its underlying signaling mechanisms using both in vitro and in vivo approaches. MIN6 pancreatic beta cells were treated with D2R agonist quinpirole (50 μM), antagonist haloperidol (10 μM), or their combination, followed by 24-h streptozotocin (STZ) exposure. Cell viability was assessed using CCK-8 assays, apoptosis was evaluated through TUNEL staining and caspase-3/9 expression analysis, insulin secretion was measured by ELISA, and AKT pathway activation was examined via Western blot. Additionally, DRD2 knockout (DRD2-/-) mice were subjected to STZ-induced T1DM to validate in vitro findings. Results demonstrated that quinpirole significantly reduced MIN6 cell viability, enhanced apoptosis markers (cleaved caspase-3/9), decreased insulin secretion, and suppressed phosphorylated AKT (p-AKT) expression in STZ-treated cells. These detrimental effects were partially mitigated by haloperidol co-treatment. Importantly, DRD2-/-mice showed significantly higher insulin levels in both peripheral blood and pancreatic tissue compared to wild-type T1DM mice, suggesting D2R deficiency confers protection against STZ-induced beta-cell dysfunction. These findings collectively indicate that D2R activation negatively regulates pancreatic beta-cell survival and insulin secretion through AKT pathway suppression, positioning D2R as a potential therapeutic target for T1DM intervention.",dopamine catecholamine neurotransmitter acts five protein coupled receptors categorized like like subfamilies emerging evidence suggests dopamine receptors influence pancreatic islet function glucose homeostasis yet specific role dopamine receptors type diabetes mellitus pathogenesis remains unclear study investigated impact progression underlying signaling mechanisms using vitro vivo approaches min pancreatic beta cells treated agonist quinpirole antagonist haloperidol combination followed streptozotocin stz exposure cell viability assessed using cck assays apoptosis evaluated tunel staining caspase expression analysis insulin secretion measured elisa akt pathway activation examined via western blot additionally drd knockout drd mice subjected stz induced validate vitro findings results demonstrated quinpirole significantly reduced min cell viability enhanced apoptosis markers cleaved caspase decreased insulin secretion suppressed phosphorylated akt akt expression stz treated cells detrimental effects partially mitigated haloperidol treatment importantly drd mice showed significantly higher insulin levels peripheral blood pancreatic tissue compared wild type mice suggesting deficiency confers protection stz induced beta cell dysfunction findings collectively indicate activation negatively regulates pancreatic beta cell survival insulin secretion akt pathway suppression positioning potential therapeutic target intervention
"With the rapid development of information technology, the traditional way of art display is facing the severe test of not being able to effectively meet the growing demand of diversified, three-dimensional and immersive experience of the modern audience due to its inherent two-dimensionality and planar limitations in today’s society. As a subversive solution, holographic projection technology, by virtue of its ability to reconstruct three-dimensional images and its unique interactive characteristics, has broken the boundaries of the original artistic expression and greatly enhanced the presentation dimension of the art works and the audience’s depth of participation. In this study, we not only analyze how holographic projection technology can be skillfully combined with big data to realize the highly personalized and dynamic updating of artwork display, but also discuss in detail how it can drive the innovative integration of intelligent layout and immersive experience in the design of public art space. Meanwhile, in the field of ancient relics restoration display, holographic projection technology also shows its great potential in improving restoration accuracy and restoring vivid scenes, which makes historical and cultural heritage vividly presented in front of the audience. Through a series of detailed experiments and practical case studies, we have confirmed that the holographic projection art design has achieved remarkable results and achievements in practical applications, whether in enhancing the visual impact and infectious force of art display, or in optimizing the audience’s viewing experience and interactive participation, all of which have demonstrated their value that cannot be ignored.",rapid development information technology traditional way art display facing severe test able effectively meet growing demand diversified three dimensional immersive experience modern audience due inherent two dimensionality planar limitations today society subversive solution holographic projection technology virtue ability reconstruct three dimensional images unique interactive characteristics broken boundaries original artistic expression greatly enhanced presentation dimension art works audience depth participation study analyze holographic projection technology skillfully combined big data realize highly personalized dynamic updating artwork display also discuss detail drive innovative integration intelligent layout immersive experience design public art space meanwhile field ancient relics restoration display holographic projection technology also shows great potential improving restoration accuracy restoring vivid scenes makes historical cultural heritage vividly presented front audience series detailed experiments practical case studies confirmed holographic projection art design achieved remarkable results achievements practical applications whether enhancing visual impact infectious force art display optimizing audience viewing experience interactive participation demonstrated value ignored
"Against the backdrop of digital transformation in education, the integration of Mixed Reality (MR) and Artificial Intelligence (AI) has fostered a new “virtual-physical symbiotic” learning environment. However, current educational tools often fail to capture learners’ dynamic needs and the characteristics of 3D immersive scenarios, resulting in a disconnect between virtual character interactions and the teaching process. Existing approaches face various limitations: rule-based models struggle with the dynamics of three-dimensional spaces; collaborative filtering algorithms overlook spatial contextual features; and deep learning models lack joint modeling of emotion and context. These shortcomings highlight insufficient multi-source data integration and weak situational awareness. To address these challenges, this study proposes a virtual character recommendation method tailored to immersive learning environments. The model is composed of a transformation layer, a self-adversarial data generation layer, an embedding representation layer, and a virtual character prediction layer. It achieves dynamic matching between virtual characters and 3D interactive scenarios by incorporating multi-source data standardization, intelligent agent-based game simulation for scenario data generation, semantic vectorization of character features, and Long Short-Term Memory (LSTM)-attention mechanism fusion. This research marks the first application of MR spatial computation and self-adversarial learning in educational role recommendation, offering a technical framework to tackle adaptation challenges in immersive scene recommendations. The proposed approach contributes both theoretical innovations and practical guidance for the advancement of intelligent education.",backdrop digital transformation education integration mixed reality artificial intelligence fostered new virtual physical symbiotic learning environment however current educational tools often fail capture learners dynamic needs characteristics immersive scenarios resulting disconnect virtual character interactions teaching process existing approaches face various limitations rule based models struggle dynamics three dimensional spaces collaborative filtering algorithms overlook spatial contextual features deep learning models lack joint modeling emotion context shortcomings highlight insufficient multi source data integration weak situational awareness address challenges study proposes virtual character recommendation method tailored immersive learning environments model composed transformation layer self adversarial data generation layer embedding representation layer virtual character prediction layer achieves dynamic matching virtual characters interactive scenarios incorporating multi source data standardization intelligent agent based game simulation scenario data generation semantic vectorization character features long short term memory lstm attention mechanism fusion research marks first application spatial computation self adversarial learning educational role recommendation offering technical framework tackle adaptation challenges immersive scene recommendations proposed approach contributes theoretical innovations practical guidance advancement intelligent education
"Earthquakes can cause fatal damage to concrete bridges. To enhance the seismic resistance of reinforced concrete bridge structures, a finite element model of a continuous reinforced concrete beam bridge is designed using Building Information Modeling (BIM) technology, and a calculation method for the overall equivalent damping ratio of the bridge is designed based on this bridge type. In the case analysis, the designed calculation method has a smaller relative displacement error at the pier top under seismic loads compared to traditional calculation methods. As the longitudinal reinforcement ratio of the pier column section increases, the bearing capacity of the section significantly increases, while the corresponding ultimate curvature decreases. The greater the yield strength of the longitudinal reinforcement in the pier column section, the greater the bearing capacity of the pier column section, but the displacement ductility coefficient of the section shows a trend of increasing first and then decreasing. For the reinforcement ratio of the pier column section, increasing the reinforcement ratio does not significantly improve the bearing capacity of the pier column section, but the displacement ductility of the section is significantly improved. The yield strength of the hoop reinforcement is increased from 250 MPa to 550 MPa, and the maximum bending moment of the pier column section and the maximum bending moment in the elastic stage are increased by 3.84% and 1.85%, respectively. It can be seen that this method has a small error under seismic loads and has good application advantages in bridge seismic design. The research content will provide effective technical references for the design of concrete structures for building bridges, as well as seismic construction and maintenance of bridges.",earthquakes cause fatal damage concrete bridges enhance seismic resistance reinforced concrete bridge structures finite element model continuous reinforced concrete beam bridge designed using building information modeling bim technology calculation method overall equivalent damping ratio bridge designed based bridge type case analysis designed calculation method smaller relative displacement error pier top seismic loads compared traditional calculation methods longitudinal reinforcement ratio pier column section increases bearing capacity section significantly increases corresponding ultimate curvature decreases greater yield strength longitudinal reinforcement pier column section greater bearing capacity pier column section displacement ductility coefficient section shows trend increasing first decreasing reinforcement ratio pier column section increasing reinforcement ratio significantly improve bearing capacity pier column section displacement ductility section significantly improved yield strength hoop reinforcement increased mpa mpa maximum bending moment pier column section maximum bending moment elastic stage increased respectively seen method small error seismic loads good application advantages bridge seismic design research content provide effective technical references design concrete structures building bridges well seismic construction maintenance bridges
"The global cyberspace faces many cybersecurity challenges, including illegal changes to contract terms and loopholes in the smart contract framework itself. This study adopts the automatic execution and intelligence of digital contracts, uses public key infrastructure technology digital certificates to establish trust relationships, encrypts data, and improves network security; the distributed ledger adopts the Byzantine fault-tolerant algorithm to prevent data from being tampered with, solving the problems of low efficiency and low security of traditional cyberspace manual governance. The study shows that after 28 companies applied digital contracts in 2023, the average authenticity of the data was 50.95% higher than the average authenticity of the data in 2022, the average integrity of the data was 36.44% higher than the average integrity of the data in 2022, and the average security of the data was 110.22% higher than the average security of the data in 2022. The findings highlight the critical role of digital contract implementation in enhancing the security and operational efficiency of global cyberspace governance, offering an effective solution to address the complex challenges inherent in managing today’s interconnected digital environment.",global cyberspace faces many cybersecurity challenges including illegal changes contract terms loopholes smart contract framework study adopts automatic execution intelligence digital contracts uses public key infrastructure technology digital certificates establish trust relationships encrypts data improves network security distributed ledger adopts byzantine fault tolerant algorithm prevent data tampered solving problems low efficiency low security traditional cyberspace manual governance study shows companies applied digital contracts average authenticity data higher average authenticity data average integrity data higher average integrity data average security data higher average security data findings highlight critical role digital contract implementation enhancing security operational efficiency global cyberspace governance offering effective solution address complex challenges inherent managing today interconnected digital environment
"This article proposes an innovative embedded dynamic decision-making framework (KD-DRL) to address the key issues of insufficient interpretability and low training efficiency faced by intelligent decision-making systems in industry academia integration platforms. This framework achieves collaborative optimization of data-driven methods and domain knowledge through the organic integration of deep reinforcement learning and explicit knowledge reasoning, providing a new technological path for intelligent decision-making in complex educational scenarios. In terms of method design, KD-DRL innovatively constructs a dual module knowledge system: the heuristic acceleration knowledge module guides agents to explore efficiently in the early stages of training through intelligent intervention mechanisms, significantly reducing model convergence time; The evasive security knowledge module is based on formal rules to monitor the decision-making process in real time, effectively preventing potential risks. This hierarchical knowledge architecture not only retains the ability of deep reinforcement learning to handle high-dimensional state spaces, but also ensures the reliability and security of decisions through interpretable rule constraints. Experimental verification shows that in the multimodal data environment of real industry academia platforms, KD-DRL exhibits significant advantages: in terms of training efficiency, the model convergence speed is improved by more than 40% compared to traditional deep reinforcement learning methods; In terms of safety, the incidence of teaching accidents has been reduced to below 0.3%, while maintaining a reasonable intervention frequency; In terms of interpretability, the decision-making process supports rule tracing, and the average time for experts to verify a single decision logic is controlled within 5 s.",article proposes innovative embedded dynamic decision making framework drl address key issues insufficient interpretability low training efficiency faced intelligent decision making systems industry academia integration platforms framework achieves collaborative optimization data driven methods domain knowledge organic integration deep reinforcement learning explicit knowledge reasoning providing new technological path intelligent decision making complex educational scenarios terms method design drl innovatively constructs dual module knowledge system heuristic acceleration knowledge module guides agents explore efficiently early stages training intelligent intervention mechanisms significantly reducing model convergence time evasive security knowledge module based formal rules monitor decision making process real time effectively preventing potential risks hierarchical knowledge architecture retains ability deep reinforcement learning handle high dimensional state spaces also ensures reliability security decisions interpretable rule constraints experimental verification shows multimodal data environment real industry academia platforms drl exhibits significant advantages terms training efficiency model convergence speed improved compared traditional deep reinforcement learning methods terms safety incidence teaching accidents reduced maintaining reasonable intervention frequency terms interpretability decision making process supports rule tracing average time experts verify single decision logic controlled within
"With English as the dominant global language, demand for precise grammar processing grows rapidly. Traditional methods struggle with complex grammatical structures, especially for non-native speakers. To address this, we propose a deep neural network (DNN) and transfer learning-based system for automated English grammar parsing and correction. Our approach: (1) Uses Transformer-based DNNs to capture intricate grammar patterns from large corpora; (2) Applies transfer learning to adapt generic linguistic knowledge to specialized error correction; (3) Achieves 10% higher accuracy and 8% better recall than traditional tools. Critically, it handles complex errors like subject-verb disagreement and tense misuse with high robustness.",english dominant global language demand precise grammar processing grows rapidly traditional methods struggle complex grammatical structures especially non native speakers address propose deep neural network dnn transfer learning based system automated english grammar parsing correction approach uses transformer based dnns capture intricate grammar patterns large corpora applies transfer learning adapt generic linguistic knowledge specialized error correction achieves higher accuracy better recall traditional tools critically handles complex errors like subject verb disagreement tense misuse high robustness
"With the rapid development of information technology, personalized learning resource recommendation systems have shown great potential in English learning. However, current English learning resources are abundant but scattered, making it difficult for learners to obtain learning materials that efficiently meet their needs. Based on this, this study constructed a personalized recommendation system that integrates the GloVe model to capture words' distributed representation, effectively improving resource recommendations' accuracy and relevance. The recommendation system preprocesses large-scale English learning resources and generates personalized resource recommendation lists based on learners' historical learning behaviors and preferences. During the experiment, a three-month test was conducted on 100 learners with different levels of English proficiency, and corresponding usage feedback and learning effectiveness data were collected. The experimental results showed that compared to traditional recommendation methods, the recommendation system based on GloVe improved resource matching by 35% and learner satisfaction by 40%. A survey found that 80% of learners reported that recommended resources better met their learning needs, and their learning efficiency increased by an average of 25%. The system also promoted the growth of learners' vocabulary. The experimental group learners increased their vocabulary by an average of 1500 words within 3 months, significantly higher than the 800 words in the control group. The personalized recommendation system for English learning resources based on GloVe has shown excellent performance in improving the matching of learning resources, learner satisfaction, and learning outcomes, providing strong support for English learning.",rapid development information technology personalized learning resource recommendation systems shown great potential english learning however current english learning resources abundant scattered making difficult learners obtain learning materials efficiently meet needs based study constructed personalized recommendation system integrates glove model capture words distributed representation effectively improving resource recommendations accuracy relevance recommendation system preprocesses large scale english learning resources generates personalized resource recommendation lists based learners historical learning behaviors preferences experiment three month test conducted learners different levels english proficiency corresponding usage feedback learning effectiveness data collected experimental results showed compared traditional recommendation methods recommendation system based glove improved resource matching learner satisfaction survey found learners reported recommended resources better met learning needs learning efficiency increased average system also promoted growth learners vocabulary experimental group learners increased vocabulary average words within months significantly higher words control group personalized recommendation system english learning resources based glove shown excellent performance improving matching learning resources learner satisfaction learning outcomes providing strong support english learning
"As enterprise operation management faces increasingly complex multi-source heterogeneous data, dynamic resource scheduling and decision support problems, traditional optimization methods and decision-making systems can no longer meet the needs of efficient and intelligent management. In order to improve the efficiency of enterprise operation management and the performance of the decision support system, a hybrid intelligent algorithm based on FA-BP and AGA is proposed. Innovative technologies such as deep reinforcement learning, quantum evolution strategy, and microservice architecture are combined to build an enterprise management system with efficient decision support and powerful computing capabilities. The proposed system has been validated in a manufacturing enterprise, demonstrating a 25% increase in production efficiency and 18% reduction in raw material waste. Its microservice architecture enables seamless integration with existing enterprise resource planning (ERP) systems, facilitating real-time data-driven decisions. Furthermore, through multi-modal data fusion and spatiotemporal feature embedding algorithms, efficient data integration is achieved under the federated learning framework of enterprise multi-source heterogeneous data, providing comprehensive and accurate information support for enterprise decision-making. Different performances of enterprise production efficiency, raw material utilization rate, labor cost, and team task completion rate. In 87 business cycles, the highest production efficiency reached 98.76%, and the lowest was 34.56%. This shows that the enterprise has large efficiency fluctuations in some cycles, which may be affected by factors such as the external environment, production process, or equipment maintenance. Businesses need to conduct an in-depth analysis of inefficiency cycles to identify the causes and improve them. The average level of raw material utilization is 67.89%, but it can reach 83.25% at the highest.",enterprise operation management faces increasingly complex multi source heterogeneous data dynamic resource scheduling decision support problems traditional optimization methods decision making systems longer meet needs efficient intelligent management order improve efficiency enterprise operation management performance decision support system hybrid intelligent algorithm based aga proposed innovative technologies deep reinforcement learning quantum evolution strategy microservice architecture combined build enterprise management system efficient decision support powerful computing capabilities proposed system validated manufacturing enterprise demonstrating increase production efficiency reduction raw material waste microservice architecture enables seamless integration existing enterprise resource planning erp systems facilitating real time data driven decisions furthermore multi modal data fusion spatiotemporal feature embedding algorithms efficient data integration achieved federated learning framework enterprise multi source heterogeneous data providing comprehensive accurate information support enterprise decision making different performances enterprise production efficiency raw material utilization rate labor cost team task completion rate business cycles highest production efficiency reached lowest shows enterprise large efficiency fluctuations cycles may affected factors external environment production process equipment maintenance businesses need conduct depth analysis inefficiency cycles identify causes improve average level raw material utilization reach highest
"In order to improve the teaching effect of art design major in private colleges and universities, this paper combines intelligent methods to construct an intelligent network classroom teaching model for design art major in private colleges and universities. Moreover, this paper analyzes the electromagnetic field numerical calculation theory required for the simulation calculation of the design of photonic resonance coupling structure and dynamic modulation structure in combination with the actual requirements of the virtual direction of artistic design, including finite difference frequency domain method, rigorous coupled wave analysis, and time-domain coupled mode theory. In addition, this paper combines the simulation analysis to add the modulated finite difference frequency domain method and the extended model of the strict coupled wave analysis. According to the teaching experimental research, the intelligent networked classroom model for design art major in private colleges and universities proposed in this paper has good effects, and can effectively promote the teaching effect of design art in private colleges and universities.",order improve teaching effect art design major private colleges universities paper combines intelligent methods construct intelligent network classroom teaching model design art major private colleges universities moreover paper analyzes electromagnetic field numerical calculation theory required simulation calculation design photonic resonance coupling structure dynamic modulation structure combination actual requirements virtual direction artistic design including finite difference frequency domain method rigorous coupled wave analysis time domain coupled mode theory addition paper combines simulation analysis add modulated finite difference frequency domain method extended model strict coupled wave analysis according teaching experimental research intelligent networked classroom model design art major private colleges universities proposed paper good effects effectively promote teaching effect design art private colleges universities
"This study, based on the Guanzhong Road tunnel project in the core urban area of Chongqing, systematically investigates the influence of tunnel excavation on adjacent building structures by establishing a tunnel-building interaction numerical model. A two-dimensional plane strain model was developed using MIDAS/GTS software, focusing on the response mechanisms of key factors such as horizontal spacing, soil layer parameters, and building height on building deformation. The research reveals that the horizontal spacing between the tunnel and the building significantly affects the settlement distribution characteristics. When the spacing is 15 m, the settlement difference between the head and tail of the building reaches 1.94 mm, and the established displacement prediction formula effectively characterizes the nonlinear relationship between horizontal distance and displacement. The elastic modulus of the soil plays a decisive role in deformation, with the maximum settlement in silty clay formation (E is 20 MPa) increasing by 572% compared to sandy mudstone formation (E is 60 MPa), reaching 15.9 mm. Increasing building height exacerbates settlement differences, with the settlement difference of a 30 m high-rise building (2.29 mm) increasing by 487% compared to a 12 m low-rise building (0.39 mm). The study proposes 35 m as the horizontal safety spacing threshold and recommends the Cross Diaphragm (CRD) method or the large pipe roof method for highly compressible formations to control the formation loss rate (&lt;0.5%). The research findings provide a theoretical basis for risk assessment and construction method optimization in tunnel proximity construction in urban core areas.",study based guanzhong road tunnel project core urban area chongqing systematically investigates influence tunnel excavation adjacent building structures establishing tunnel building interaction numerical model two dimensional plane strain model developed using midas gts software focusing response mechanisms key factors horizontal spacing soil layer parameters building height building deformation research reveals horizontal spacing tunnel building significantly affects settlement distribution characteristics spacing settlement difference head tail building reaches established displacement prediction formula effectively characterizes nonlinear relationship horizontal distance displacement elastic modulus soil plays decisive role deformation maximum settlement silty clay formation mpa increasing compared sandy mudstone formation mpa reaching increasing building height exacerbates settlement differences settlement difference high rise building increasing compared low rise building study proposes horizontal safety spacing threshold recommends cross diaphragm crd method large pipe roof method highly compressible formations control formation loss rate research findings provide theoretical basis risk assessment construction method optimization tunnel proximity construction urban core areas
"During the operation of a photovoltaic inverter, an increase in temperature significantly affects the output power, efficiency, service life, and stability of the photovoltaic inverter. However, traditional evaluation methods based on physical experiments are time-consuming and costly, making it difficult to fully capture the dynamic performance under real conditions. This paper proposes a DT-LSTM (Digital Twin-Long Short-Term Memory) output power prediction method based on the thermoelectric coupling digital twin model of the inverter. Firstly, a digital twin model based on the equivalent circuit of the inverter is established; secondly, internal physical field data are calculated and parameter identification is performed; finally, the inverter output power is predicted and compared with traditional methods. The research results show that the prediction accuracy of the proposed method is improved, with the Mean Absolute Percentage Error (MAPE) optimized to 7.81%.",operation photovoltaic inverter increase temperature significantly affects output power efficiency service life stability photovoltaic inverter however traditional evaluation methods based physical experiments time consuming costly making difficult fully capture dynamic performance real conditions paper proposes lstm digital twin long short term memory output power prediction method based thermoelectric coupling digital twin model inverter firstly digital twin model based equivalent circuit inverter established secondly internal physical field data calculated parameter identification performed finally inverter output power predicted compared traditional methods research results show prediction accuracy proposed method improved mean absolute percentage error mape optimized
"To improve the effect of college English teaching, combines multi-source data (MSD) fusion technology to build a college English blended teaching system. Moreover, through the research on the functions of smarter classrooms and the analysis and exploration of blended teaching, this paper designs a college English blended teaching system based on MSD fusion. In addition, this paper analyzes the bandwidth-tunable optical routing module of the interference-coupled series double-ring involved in multi-data fusion. Finally, this paper studies the working principle and structural design of the module, and constructs a college English blended teaching system structure based on MSD fusion. From the evaluation results, it can be seen that the college English blended teaching system based on MSD fusion proposed in this paper can effectively improve the quality of English teaching.",improve effect college english teaching combines multi source data msd fusion technology build college english blended teaching system moreover research functions smarter classrooms analysis exploration blended teaching paper designs college english blended teaching system based msd fusion addition paper analyzes bandwidth tunable optical routing module interference coupled series double ring involved multi data fusion finally paper studies working principle structural design module constructs college english blended teaching system structure based msd fusion evaluation results seen college english blended teaching system based msd fusion proposed paper effectively improve quality english teaching
"This study aims to solve the problem of inaccurate motion recognition and modeling in the traditional human motion trajectory virtual experiment system due to the loss of motion posture during motion capture. A high-precision human motion trajectory virtual experiment system based on virtual reality technology is proposed. The system uses VR helmet, motion capture device Kinect V2 and Unity3D development platform to build a three-dimensional virtual environment, and collect and display human motion trajectory in real time. The extended Kalman filter algorithm is used to process the captured raw motion data to improve the accuracy of the motion trajectory; the genetic algorithm is used to dynamically optimize the human motion model, automatically adjust the parameters in the virtual environment, reduce system delay, and optimize the feedback time of user operations. The experimental results show that the proposed optimization scheme significantly improves the accuracy of the motion trajectory, user interaction experience and computing efficiency, and can provide a more accurate, smooth and efficient virtual experiment environment. The error does not exceed 0.3, and the input delay and feedback delay under different working conditions are less than 0.25 s. The innovation of this paper lies in the combination of Kalman filtering and genetic algorithm, which realizes the precise optimization and real-time feedback of human motion trajectory, effectively solves many limitations of traditional virtual experiment systems, and provides a new solution for the application of virtual reality technology in human motion analysis and virtual experiments.",study aims solve problem inaccurate motion recognition modeling traditional human motion trajectory virtual experiment system due loss motion posture motion capture high precision human motion trajectory virtual experiment system based virtual reality technology proposed system uses helmet motion capture device kinect unity development platform build three dimensional virtual environment collect display human motion trajectory real time extended kalman filter algorithm used process captured raw motion data improve accuracy motion trajectory genetic algorithm used dynamically optimize human motion model automatically adjust parameters virtual environment reduce system delay optimize feedback time user operations experimental results show proposed optimization scheme significantly improves accuracy motion trajectory user interaction experience computing efficiency provide accurate smooth efficient virtual experiment environment error exceed input delay feedback delay different working conditions less innovation paper lies combination kalman filtering genetic algorithm realizes precise optimization real time feedback human motion trajectory effectively solves many limitations traditional virtual experiment systems provides new solution application virtual reality technology human motion analysis virtual experiments
"This study explores the potential of developing intelligent assessment systems utilizing machine learning (ML) and natural language processing (NLP) technologies to enhance the efficiency of automated scoring and feedback for English essay writing. With the wide application of technology in the education sector, especially in language learning assessment, automated scoring systems not only optimize the allocation of teaching resources but also provide learners with immediate and objective feedback, facilitating the enhancement of their writing skills. Despite significant advancements in the application of ML and artificial intelligence (AI) in automated scoring systems, existing systems still face challenges in assessing the logic, coherence, and creativity of essay content, as well as in accurately identifying off-topic instances. This study aims to address these challenges through two main components: Firstly, an intelligent off-topic assessment mechanism for English essays is developed, utilizing text analysis techniques to accurately identify instances of deviation from the topic. Secondly, an automated scoring and feedback system for English essays based on a composite method is implemented, integrating various ML algorithms and NLP technologies to comprehensively assess the content, structure, and language usage of essays, providing practical writing improvement suggestions. Through these methods, not only is the accuracy of scoring and the effectiveness of feedback enhanced but the modernization process of English teaching and assessment is also advanced. This study aims to address the shortcomings of traditional automated scoring systems in assessing the logic and coherence of English essays, particularly in identifying off-topic responses and ensuring fairness and accuracy in scoring. By developing an intelligent assessment system that integrates multiple machine learning algorithms and natural language processing techniques, significant improvements have been achieved in the performance of automated scoring systems. These enhancements not only enhance the accuracy and consistency of scoring but also provide more specific and practical writing improvement suggestions for educational practices, thereby helping students enhance their writing skills.",study explores potential developing intelligent assessment systems utilizing machine learning natural language processing nlp technologies enhance efficiency automated scoring feedback english essay writing wide application technology education sector especially language learning assessment automated scoring systems optimize allocation teaching resources also provide learners immediate objective feedback facilitating enhancement writing skills despite significant advancements application artificial intelligence automated scoring systems existing systems still face challenges assessing logic coherence creativity essay content well accurately identifying topic instances study aims address challenges two main components firstly intelligent topic assessment mechanism english essays developed utilizing text analysis techniques accurately identify instances deviation topic secondly automated scoring feedback system english essays based composite method implemented integrating various algorithms nlp technologies comprehensively assess content structure language usage essays providing practical writing improvement suggestions methods accuracy scoring effectiveness feedback enhanced modernization process english teaching assessment also advanced study aims address shortcomings traditional automated scoring systems assessing logic coherence english essays particularly identifying topic responses ensuring fairness accuracy scoring developing intelligent assessment system integrates multiple machine learning algorithms natural language processing techniques significant improvements achieved performance automated scoring systems enhancements enhance accuracy consistency scoring also provide specific practical writing improvement suggestions educational practices thereby helping students enhance writing skills
"With the introduction of the deep learning large model TabTransformer for electric load analysis and forecasting, this study aims to compare the performance of traditional machine learning, deep learning, and Transformer-based sequential prediction algorithms on heterogeneous tabular data. We propose an interpretable deep learning large model architecture, Electric Power Load TabTransformer (EPLTT), tailored specifically to electric load forecasting with mixed heterogeneous tabular data. EPLTT leverages a Transformer architecture designed explicitly for tabular data processing, akin to GPT-series deep neural networks, efficiently handling complex structured data and providing an effective alternative to conventional models. First, we discuss the strengths and weaknesses of various algorithms addressing tabular learning problems. Subsequently, we demonstrate the configuration and training of EPLTT on real-world forecasting datasets. Finally, we achieve interpretability by applying class-weight adjustments to compensate for dataset imbalance during training. Experimental results indicate EPLTT’s superior performance over traditional models such as LightGBM regarding accuracy, precision, and specificity. Additionally, EPLTT exhibits enhanced adaptability and robustness in dataset preparation and complexity management, fulfilling practical application requirements for speed, generalization, and interpretability. The model’s vertical-domain competitiveness is validated against real-world power load forecasting scenarios, addressing class imbalance, missing values, and categorical data.",introduction deep learning large model tabtransformer electric load analysis forecasting study aims compare performance traditional machine learning deep learning transformer based sequential prediction algorithms heterogeneous tabular data propose interpretable deep learning large model architecture electric power load tabtransformer epltt tailored specifically electric load forecasting mixed heterogeneous tabular data epltt leverages transformer architecture designed explicitly tabular data processing akin gpt series deep neural networks efficiently handling complex structured data providing effective alternative conventional models first discuss strengths weaknesses various algorithms addressing tabular learning problems subsequently demonstrate configuration training epltt real world forecasting datasets finally achieve interpretability applying class weight adjustments compensate dataset imbalance training experimental results indicate epltt superior performance traditional models lightgbm regarding accuracy precision specificity additionally epltt exhibits enhanced adaptability robustness dataset preparation complexity management fulfilling practical application requirements speed generalization interpretability model vertical domain competitiveness validated real world power load forecasting scenarios addressing class imbalance missing values categorical data
"With the increasing complexity of college student work management, traditional task scheduling methods have made it difficult to meet the needs of real-time and efficiency. This study aims to explore the application of a real-time task scheduling framework based on the Hash algorithm in student work management to improve management efficiency and response speed. By analyzing the characteristics of student work management, a Hash algorithm framework suitable for real-time task scheduling is designed, which can dynamically allocate tasks and ensure the balance and efficiency of task processing. The experimental results show that after adopting this framework, the task scheduling time is shortened by 35% on average, and the task processing success rate is improved by 25%. In addition, the framework performs well in dealing with unexpected tasks, can quickly adjust resource allocation, and ensures the continuity and stability of student work management. This study not only enriches the application theory of real-time task scheduling in student work management but also provides effective technical support for practical management.",increasing complexity college student work management traditional task scheduling methods made difficult meet needs real time efficiency study aims explore application real time task scheduling framework based hash algorithm student work management improve management efficiency response speed analyzing characteristics student work management hash algorithm framework suitable real time task scheduling designed dynamically allocate tasks ensure balance efficiency task processing experimental results show adopting framework task scheduling time shortened average task processing success rate improved addition framework performs well dealing unexpected tasks quickly adjust resource allocation ensures continuity stability student work management study enriches application theory real time task scheduling student work management also provides effective technical support practical management
"The objective of this study is to discuss the use of gangue-based concrete (GBC) and the necessity of ensuring its mechanical properties meet technical and regulatory standards. This study examines GBC with varying coal gangue powder (CGP) replacement rates (0%, 10%, 20%, and 30%) and distinct water-to-cement ratios (0.35, 0.40, and 0.45). Acoustic emission tests, scanning electron microscopy tests, and axial compression tests were used in the experiments. The mechanical properties of GBC were examined by stress-strain curves, and the variation of parameters concerning content and hydrogel ratio was assessed in relation to the microstructure. Ultimately, based on the acoustic emission (AE) energy of the GBC damage process, combined with the crack compression effect, a GBC load damage model was established. The research results demonstrate that the damage evolution model developed in this study accurately represents the damage progression of GBC under varying dosages and hydrogel ratios. The structural model aligns with this paper and further experimental findings.",objective study discuss use gangue based concrete gbc necessity ensuring mechanical properties meet technical regulatory standards study examines gbc varying coal gangue powder cgp replacement rates distinct water cement ratios acoustic emission tests scanning electron microscopy tests axial compression tests used experiments mechanical properties gbc examined stress strain curves variation parameters concerning content hydrogel ratio assessed relation microstructure ultimately based acoustic emission energy gbc damage process combined crack compression effect gbc load damage model established research results demonstrate damage evolution model developed study accurately represents damage progression gbc varying dosages hydrogel ratios structural model aligns paper experimental findings
"With acceleration of globalization, English learning has become a compulsory course for many learners. However, traditional English learning methods are often inefficient and difficult to meet the needs of modern learners. Therefore, this study aims to design and apply an efficient English learning system based on the Jaro-Winkler algorithm. The Jaro-Winkler algorithm is a string similarity calculation method which can effectively measure the similarity between words so as to assist learners in memorizing and mastering English vocabulary more effectively. Firstly, this study analyzes the pain points in current English learning, such as insufficient vocabulary and poor grammar mastery, and proposes corresponding solutions. By introducing the Jaro-Winkler algorithm, the system can intelligently recommend words with high similarity for learning and improve learning efficiency. The experimental results show that learners who use this system have an average increase of 30% in vocabulary and a 20% decrease in grammatical error rate within 3 months. In addition, the system also provides a personalized learning path and real-time feedback mechanism, further enhancing learners’ learning motivation and effect. This study not only provides new ideas and methods for English learning but also provides a reference for the design of other language learning systems.",acceleration globalization english learning become compulsory course many learners however traditional english learning methods often inefficient difficult meet needs modern learners therefore study aims design apply efficient english learning system based jaro winkler algorithm jaro winkler algorithm string similarity calculation method effectively measure similarity words assist learners memorizing mastering english vocabulary effectively firstly study analyzes pain points current english learning insufficient vocabulary poor grammar mastery proposes corresponding solutions introducing jaro winkler algorithm system intelligently recommend words high similarity learning improve learning efficiency experimental results show learners use system average increase vocabulary decrease grammatical error rate within months addition system also provides personalized learning path real time feedback mechanism enhancing learners learning motivation effect study provides new ideas methods english learning also provides reference design language learning systems
"Currently, the “dual-channel” drug management system has become an important channel for hospital and external supply chain drug management. However, how to achieve more efficient drug management is still the main issue in current drug management. Therefore, in order to achieve more efficient drug management. A new “dual-channel” drug management system based on hospital information system was proposed, which uses deep bidirectional network and convolutional recurrent neural network models to identify and manage drugs. At the same time, the new system uses deconvolution and binarization methods to improve the accuracy of drug recognition. The research results show that the accuracy of drug recognition in the model can reach 95.68%, which is 13.33% higher than other models. At the same time, the recall rate was also improved by 14.89% compared to other models. After using the new system, the number of effective image recognitions increased by 10, and the hospital’s revenue growth increased by 7.43%. The new system has better drug management effects and can also improve drug sales efficiency. This has good guiding significance for hospital drug management research.",currently dual channel drug management system become important channel hospital external supply chain drug management however achieve efficient drug management still main issue current drug management therefore order achieve efficient drug management new dual channel drug management system based hospital information system proposed uses deep bidirectional network convolutional recurrent neural network models identify manage drugs time new system uses deconvolution binarization methods improve accuracy drug recognition research results show accuracy drug recognition model reach higher models time recall rate also improved compared models using new system number effective image recognitions increased hospital revenue growth increased new system better drug management effects also improve drug sales efficiency good guiding significance hospital drug management research
"Given the importance of mental health to the quality of human life and social development, as well as its central position in education in the new era, this paper focuses on the critical group of college students, aiming to optimize the teaching mode of online mental health education through technological innovation, to improve their psychological quality and coping ability. Given the significant changes in college students’ lifestyles, learning modes and communication styles in the Internet era, this paper profoundly analyses these changes’ positive and negative impacts on mental health. In particular, it is pointed out that the incidence of mental health problems among Chinese college students has risen to more than 20%, with depression, anxiety, Internet addiction, and emotional problems being particularly prominent, which seriously constrain students’ academic achievements and quality of life, and may even induce undesirable behaviors. In order to effectively deal with this situation, this paper identifies several key factors affecting the effectiveness of learning based on an in-depth analysis of students’ online psychological learning behaviors. It proposes an optimized collaborative filtering algorithm accordingly. In order to overcome the performance bottleneck and sparsity problems that traditional collaborative filtering algorithms may encounter when dealing with large-scale datasets, we propose an improved collaborative filtering algorithm based on a hybrid strategy, which achieves fine-grained modeling of students’ psychological and behavioral patterns by combining the advantages of SVD++ matrix decomposition, neural network embedding, and content filtering, and through the fusion of multi-source data and feature intersection. The algorithm significantly improves the prediction accuracy of students’ learning behaviors and achieves an increase of about 30% in the accuracy rate compared with traditional methods. This research result provides a powerful tool for accurately identifying students’ psychological needs and potential problems. It lays a solid foundation for universities to carry out personalized and high-performance online mental health education models.",given importance mental health quality human life social development well central position education new paper focuses critical group college students aiming optimize teaching mode online mental health education technological innovation improve psychological quality coping ability given significant changes college students lifestyles learning modes communication styles internet paper profoundly analyses changes positive negative impacts mental health particular pointed incidence mental health problems among chinese college students risen depression anxiety internet addiction emotional problems particularly prominent seriously constrain students academic achievements quality life may even induce undesirable behaviors order effectively deal situation paper identifies several key factors affecting effectiveness learning based depth analysis students online psychological learning behaviors proposes optimized collaborative filtering algorithm accordingly order overcome performance bottleneck sparsity problems traditional collaborative filtering algorithms may encounter dealing large scale datasets propose improved collaborative filtering algorithm based hybrid strategy achieves fine grained modeling students psychological behavioral patterns combining advantages svd matrix decomposition neural network embedding content filtering fusion multi source data feature intersection algorithm significantly improves prediction accuracy students learning behaviors achieves increase accuracy rate compared traditional methods research result provides powerful tool accurately identifying students psychological needs potential problems lays solid foundation universities carry personalized high performance online mental health education models
"This study focuses on the information processing mechanism of the new generation in the Internet and new media environment in the digital era, and reveals the evolution of their cognitive mode, value orientation and behavioral decision-making through the framework of fuzzy information analysis. Based on communication science, cognitive psychology and fuzzy logic theory, we constructed an analytical model of “information ambiguity-cognitive processing-behavioral response”, and found that the new generation has developed unique ambiguity tolerance thresholds and meaning construction strategies in the information overload environment. Using a mixed-method approach, the study combines eye-tracking experiments, semantic network analysis, and in-depth interviews to confirm the structural influence of ambiguous information on the socialization process, value identity, and risk decision-making of the new generation. The results provide theoretical support for understanding the information processing characteristics of digital natives and practical guidance for optimizing information dissemination and youth online literacy education in the new media environment.",study focuses information processing mechanism new generation internet new media environment digital reveals evolution cognitive mode value orientation behavioral decision making framework fuzzy information analysis based communication science cognitive psychology fuzzy logic theory constructed analytical model information ambiguity cognitive processing behavioral response found new generation developed unique ambiguity tolerance thresholds meaning construction strategies information overload environment using mixed method approach study combines eye tracking experiments semantic network analysis depth interviews confirm structural influence ambiguous information socialization process value identity risk decision making new generation results provide theoretical support understanding information processing characteristics digital natives practical guidance optimizing information dissemination youth online literacy education new media environment
"The conventional methods of teaching English often fall short of addressing the diverse needs of students, thereby impeding language proficiency and cross-cultural communication abilities. To enhance English instruction, artificial intelligence (AI) and natural language processing (NLP) are increasingly being employed. These technologies provide targeted learning opportunities that improve student routines and foster a deeper understanding of spoken language. Personalized education, which caters to each learner’s unique needs, forms the foundation of adaptive English teaching. This approach promotes multicultural awareness, linguistic proficiency, and confidence. This study focuses on utilizing AI and NLP to deliver personalized English instruction. It introduces an AI-driven intelligent teaching assistant—a novel framework for individualized and flexible English language education. The proposed approaches create an engaging and adaptable learning environment through advanced NLP algorithms and methodologies. By enabling quick access to information, accelerating knowledge assessment, and providing tailored training aligned with each student’s learning style, the platform reduces the cognitive load on instructors. The study proposes three strategies: designing tests and flashcards, developing innovative study plans, and monitoring and responding to student needs. The Weight-balanced AdaBoost (WB-Adaboost) algorithm was employed to develop a predictive tool that recognizes trends in student data and identifies their learning requirements. The findings have the potential to significantly advance the development of innovative instructional materials that enhance student achievement, satisfaction, and engagement. These resources can guide the development, implementation, and evaluation of AI-powered online tutoring systems in ESL programs.",conventional methods teaching english often fall short addressing diverse needs students thereby impeding language proficiency cross cultural communication abilities enhance english instruction artificial intelligence natural language processing nlp increasingly employed technologies provide targeted learning opportunities improve student routines foster deeper understanding spoken language personalized education caters learner unique needs forms foundation adaptive english teaching approach promotes multicultural awareness linguistic proficiency confidence study focuses utilizing nlp deliver personalized english instruction introduces driven intelligent teaching assistant novel framework individualized flexible english language education proposed approaches create engaging adaptable learning environment advanced nlp algorithms methodologies enabling quick access information accelerating knowledge assessment providing tailored training aligned student learning style platform reduces cognitive load instructors study proposes three strategies designing tests flashcards developing innovative study plans monitoring responding student needs weight balanced adaboost adaboost algorithm employed develop predictive tool recognizes trends student data identifies learning requirements findings potential significantly advance development innovative instructional materials enhance student achievement satisfaction engagement resources guide development implementation evaluation powered online tutoring systems esl programs
"Recycled coarse aggregate exhibits significant heterogeneity in its physical and chemical properties, along with complex interdependencies among various feature attributes. These characteristics often lead to challenges in achieving high accuracy in attribute recognition, highlighting the urgent need for a structured and intelligent analytical framework. To address this issue, this study proposes a novel hybrid approach that integrates graph convolutional networks (GCN) with attribute mathematics theory, aiming to enhance feature representation and improve recognition performance. The method begins by constructing a multi-dimensional attribute graph based on the physicochemical properties of recycled coarse aggregate, capturing the intrinsic correlations among different features. A multilayer GCN is then employed to extract deep-level, globally coupled feature representations. Subsequently, attribute mathematics theory is applied to simplify and logically abstract the output features through membership functions and covering operators, enabling effective feature selection and dimensionality reduction. The refined feature set is finally fed into a discriminant classifier to achieve accurate attribute recognition. Experimental results demonstrate the superiority of the proposed fusion model over traditional machine learning methods such as SVM, random forest, and MLP. The model achieves average recognition accuracies and recall rates of 0.89 and 0.89 across eight material categories, and 0.90 and 0.89 across seven particle size ranges, respectively. Five-fold cross-validation yields an average accuracy between 0.889 and 0.918, with a low standard deviation of 0.012, indicating strong stability and generalization performance. Moreover, the feature simplification strategy achieves an average feature reduction rate of 0.67 while retaining 0.92 of the original information. These results confirm that the proposed GCN–attribute mathematics framework significantly enhances the attribute recognition capability of recycled coarse aggregate, offering a robust and efficient solution for intelligent identification in sustainable construction materials research.",recycled coarse aggregate exhibits significant heterogeneity physical chemical properties along complex interdependencies among various feature attributes characteristics often lead challenges achieving high accuracy attribute recognition highlighting urgent need structured intelligent analytical framework address issue study proposes novel hybrid approach integrates graph convolutional networks gcn attribute mathematics theory aiming enhance feature representation improve recognition performance method begins constructing multi dimensional attribute graph based physicochemical properties recycled coarse aggregate capturing intrinsic correlations among different features multilayer gcn employed extract deep level globally coupled feature representations subsequently attribute mathematics theory applied simplify logically abstract output features membership functions covering operators enabling effective feature selection dimensionality reduction refined feature set finally fed discriminant classifier achieve accurate attribute recognition experimental results demonstrate superiority proposed fusion model traditional machine learning methods svm random forest mlp model achieves average recognition accuracies recall rates across eight material categories across seven particle size ranges respectively five fold cross validation yields average accuracy low standard deviation indicating strong stability generalization performance moreover feature simplification strategy achieves average feature reduction rate retaining original information results confirm proposed gcn attribute mathematics framework significantly enhances attribute recognition capability recycled coarse aggregate offering robust efficient solution intelligent identification sustainable construction materials research
"With the explosive growth of digital resources in libraries, the limitations of traditional recommendation systems have become increasingly prominent. The collaborative filtering algorithm is affected by the sparsity of the user-item matrix and is difficult to capture long-tail requirements. The low efficiency of association rule mining leads to serious homogenization of recommendation results. How to break through technical bottlenecks and achieve precise and diverse resource recommendations has become the key to enhancing user experience and promoting the construction of smart libraries. Therefore, a library resource recommendation model that integrates collaborative filtering and association rules is developed to optimize the accuracy and personalization of library resource recommendations. First, the research introduces user attribute features and clustering algorithms to improve collaborative filtering algorithms. Then, a library resource recommendation model is constructed by integrating association rules. Finally, the model is tested. The test results showed that in terms of recommendation diversity, when the number of neighbors was 50, the research model generated 560 unique recommendation items, which was significantly better than traditional content-based recommendation models. In terms of coverage indicators, the research model stabilized at 42%, which was 17% to 30% higher than comparison methods. In real library scenario testing, the research model could identify and recommend six randomly selected user accounts. This research provides an effective technical solution for improving the personalized service level of libraries, which has important practical value for smart libraries.",explosive growth digital resources libraries limitations traditional recommendation systems become increasingly prominent collaborative filtering algorithm affected sparsity user item matrix difficult capture long tail requirements low efficiency association rule mining leads serious homogenization recommendation results break technical bottlenecks achieve precise diverse resource recommendations become key enhancing user experience promoting construction smart libraries therefore library resource recommendation model integrates collaborative filtering association rules developed optimize accuracy personalization library resource recommendations first research introduces user attribute features clustering algorithms improve collaborative filtering algorithms library resource recommendation model constructed integrating association rules finally model tested test results showed terms recommendation diversity number neighbors research model generated unique recommendation items significantly better traditional content based recommendation models terms coverage indicators research model stabilized higher comparison methods real library scenario testing research model could identify recommend six randomly selected user accounts research provides effective technical solution improving personalized service level libraries important practical value smart libraries
"With the rapid development of computer graphics and artificial intelligence, the simulation and optimization problems in digital art and design have gradually attracted widespread attention, especially the simulation of pigment mixing effect is of great significance in digital painting and artistic creation. As a unique painting medium, the mixing effect of watercolor pigments presents complex physical and chemical characteristics, which makes the accurate simulation face great challenges. In this paper, a simulation and optimization algorithm of watercolor paint mixing effect based on graph neural network is proposed. By constructing a graph model of pigment mixing, the algorithm uses graph neural network to capture the local and global information in the process of pigment mixing, and realizes the accurate simulation of different pigment combinations. In this paper, several watercolor paint data of different brands, colors and transparencies are used for experiments. The experimental results show that compared with traditional physical simulation methods, the proposed algorithm has significant advantages in visual effect and computational efficiency. During the simulation, the color difference ΔE value of the pigment mixing was reduced by an average of 15.3%, and the calculation time was reduced by about 40%. Furthermore, a user-interactive pigment recommendation system based on reverse mapping is integrated, enabling efficient suggestions for pigment combinations that match a target color.",rapid development computer graphics artificial intelligence simulation optimization problems digital art design gradually attracted widespread attention especially simulation pigment mixing effect great significance digital painting artistic creation unique painting medium mixing effect watercolor pigments presents complex physical chemical characteristics makes accurate simulation face great challenges paper simulation optimization algorithm watercolor paint mixing effect based graph neural network proposed constructing graph model pigment mixing algorithm uses graph neural network capture local global information process pigment mixing realizes accurate simulation different pigment combinations paper several watercolor paint data different brands colors transparencies used experiments experimental results show compared traditional physical simulation methods proposed algorithm significant advantages visual effect computational efficiency simulation color difference value pigment mixing reduced average calculation time reduced furthermore user interactive pigment recommendation system based reverse mapping integrated enabling efficient suggestions pigment combinations match target color
"Faced with the rising labor costs and low construction efficiency in the construction and decoration industry, there is an urgent need to introduce automation technology to reform construction methods. Therefore, a flexible macro and micro decoration robot is designed, integrating motion redundancy optimization and active compliance control strategies. The gradient projection method is used to optimize the accuracy of the robot arm’s motion trajectory, while the active supple control is used to improve the wall surface adaptability to enhance the stability and accuracy of the finishing actuator system’s force-position tracking. The results showed that the macro/micro robotic arm was accurate in tracking the position and attitude of the trajectory, with only 0.1° in the β-Euler angle. The joint motion of the robotic arm and the driving speed of the micro robotic arm were maintained within reasonable limits. Under the premise of ensuring accurate force and position tracking as well as system stability, the stiffness coefficient should not exceed 10 to avoid frequent small oscillations during tracking and instability due to excessive stiffness. The redundant motion control algorithm and active compliance controller based on impedance control studied have improved the adaptability and accuracy of the robot arm. The designed method can achieve precise wall finishing control, which makes a significant contribution to improving construction efficiency and quality, as well as ensuring worker safety.",faced rising labor costs low construction efficiency construction decoration industry urgent need introduce automation technology reform construction methods therefore flexible macro micro decoration robot designed integrating motion redundancy optimization active compliance control strategies gradient projection method used optimize accuracy robot arm motion trajectory active supple control used improve wall surface adaptability enhance stability accuracy finishing actuator system force position tracking results showed macro micro robotic arm accurate tracking position attitude trajectory euler angle joint motion robotic arm driving speed micro robotic arm maintained within reasonable limits premise ensuring accurate force position tracking well system stability stiffness coefficient exceed avoid frequent small oscillations tracking instability due excessive stiffness redundant motion control algorithm active compliance controller based impedance control studied improved adaptability accuracy robot arm designed method achieve precise wall finishing control makes significant contribution improving construction efficiency quality well ensuring worker safety
"In this study, a safety monitoring system based on intelligent video analysis is designed and realized for the safety management needs in the high-risk operation environment of power grid construction sites. The system constructs a complete technology chain from front-end data acquisition to edge intelligent analysis to cloud decision support by integrating multimodal perception technology and deep learning algorithms. At the video monitoring level, the system adopts high-definition video acquisition and multi-source sensor data fusion program, which breaks through the limitations of traditional monitoring means in complex construction scenarios. The intelligent analysis algorithm layer integrates target detection, behavior recognition, and anomaly detection technologies and realizes real-time accurate identification of helmet/work uniform compliance, high-risk behaviors, and environmental risks through lightweight model deployment and model compression technology. The system architecture adopts a three-layer design mode, including perception layer, edge layer, and platform layer, in which the intelligent analysis gateway supports dynamic rule configuration and multi-algorithm collaborative reasoning, the data convergence platform realizes unified access and storage management of multi-protocol video streams, and the visualization interface realizes three-dimensional reconstruction of the construction scene and safety situational awareness through digital twin technology. In terms of key technology implementation, the study proposes a safety equipment detection method that integrates color features and human posture estimation, as well as an early warning mechanism for high-risk behaviors that combines geo-fencing and trajectory prediction, which significantly improves the level of intelligence and real-time response capability of power grid construction safety control. This study provides an end-to-end solution for safety monitoring in the electric power industry, which is of great theoretical value and practical significance for reducing the risk of construction safety accidents and improving the effectiveness of safety management.",study safety monitoring system based intelligent video analysis designed realized safety management needs high risk operation environment power grid construction sites system constructs complete technology chain front end data acquisition edge intelligent analysis cloud decision support integrating multimodal perception technology deep learning algorithms video monitoring level system adopts high definition video acquisition multi source sensor data fusion program breaks limitations traditional monitoring means complex construction scenarios intelligent analysis algorithm layer integrates target detection behavior recognition anomaly detection technologies realizes real time accurate identification helmet work uniform compliance high risk behaviors environmental risks lightweight model deployment model compression technology system architecture adopts three layer design mode including perception layer edge layer platform layer intelligent analysis gateway supports dynamic rule configuration multi algorithm collaborative reasoning data convergence platform realizes unified access storage management multi protocol video streams visualization interface realizes three dimensional reconstruction construction scene safety situational awareness digital twin technology terms key technology implementation study proposes safety equipment detection method integrates color features human posture estimation well early warning mechanism high risk behaviors combines geo fencing trajectory prediction significantly improves level intelligence real time response capability power grid construction safety control study provides end end solution safety monitoring electric power industry great theoretical value practical significance reducing risk construction safety accidents improving effectiveness safety management
"Fabric defect detection (DD) is critical in ensuring high-quality textile production, as defects can significantly impact aesthetic appeal and market value. Manual inspection methods are often inefficient and prone to inconsistency, motivating the development of intelligent, automated systems. Despite advancements in computer vision and deep learning (DL), many current models, such as rule-based methods involving thresholding and edge detection, still face challenges in accurately detecting subtle or complex defects under varied lighting and texture conditions. This research addresses these gaps by proposing a novel, DL-driven model named Binary Gannet Optimizer-driven Gate Adjusted Long Short-Term Memory Network (BGO-GALSTM-Net) for robust and precise defect detection and classification in textiles. A comprehensive dataset, ensuring diversity in defect types such as stains, holes, and misweaves, is used to train and evaluate the proposed model. Data preprocessing involves contrast enhancement and edge-preserving smoothing to improve defect visibility. Feature extraction uses a residual network (ResNet), allowing the model to focus on intricate defect regions. The proposed BGO-GALSTM-Net integrates temporal memory capabilities of LSTM with adaptive gating, while the Binary Gannet Optimizer fine-tunes model parameters for optimal performance. This architecture effectively processes the input image data, learns spatiotemporal patterns, and classifies defect types with high accuracy. Experimental results demonstrate superior accuracy (97.4%) along with higher F1-score, precision, and recall across various fabric types, outperforming traditional approaches. The proposed framework provides a reliable, scalable solution for real-time textile quality control, enabling manufacturers to reduce waste and maintain stringent product standards.",fabric defect detection critical ensuring high quality textile production defects significantly impact aesthetic appeal market value manual inspection methods often inefficient prone inconsistency motivating development intelligent automated systems despite advancements computer vision deep learning many current models rule based methods involving thresholding edge detection still face challenges accurately detecting subtle complex defects varied lighting texture conditions research addresses gaps proposing novel driven model named binary gannet optimizer driven gate adjusted long short term memory network bgo galstm net robust precise defect detection classification textiles comprehensive dataset ensuring diversity defect types stains holes misweaves used train evaluate proposed model data preprocessing involves contrast enhancement edge preserving smoothing improve defect visibility feature extraction uses residual network resnet allowing model focus intricate defect regions proposed bgo galstm net integrates temporal memory capabilities lstm adaptive gating binary gannet optimizer fine tunes model parameters optimal performance architecture effectively processes input image data learns spatiotemporal patterns classifies defect types high accuracy experimental results demonstrate superior accuracy along higher score precision recall across various fabric types outperforming traditional approaches proposed framework provides reliable scalable solution real time textile quality control enabling manufacturers reduce waste maintain stringent product standards
"In modern Software-Defined Networking (SDN), traffic engineering often faces critical challenges in maintaining Quality of Service (QoS) due to congestion over bottleneck links. Existing traffic engineering (TE) schemes frequently overlook the performance degradation caused by such bottlenecks, resulting in inefficient routing and poor load balancing. To address this issue, we model the traffic allocation problem as a bottleneck game with splittable flows, where users compete for shared bottleneck resources and select routes based on a link pricing strategy. We propose a distributed routing algorithm based on bottleneck game theory (DRB), which introduces a pricing application-driven network routing mechanism between users and the SDN controller. The controller periodically monitors link utilization, updates link prices accordingly, and broadcasts them to users. Each user independently selects the lowest-cost path, implicitly guided by bottleneck usage. This mechanism effectively steers traffic away from congested paths, minimizes queuing delay, and achieves global load balancing, thereby ensuring improved QoS. Extensive simulations on real-world topologies from the SNDlib dataset, including Abilene and GEANT, validate the effectiveness of DRB. Compared to state-of-the-art methods such as Hedera and ECMP, the proposed DRB algorithm consistently achieves higher throughput, lower latency, reduced jitter, and minimized packet loss under varying traffic demand levels. These results demonstrate that DRB effectively mitigates bottleneck-induced congestion and enhances QoS in SDN-based networks.",modern software defined networking sdn traffic engineering often faces critical challenges maintaining quality service qos due congestion bottleneck links existing traffic engineering schemes frequently overlook performance degradation caused bottlenecks resulting inefficient routing poor load balancing address issue model traffic allocation problem bottleneck game splittable flows users compete shared bottleneck resources select routes based link pricing strategy propose distributed routing algorithm based bottleneck game theory drb introduces pricing application driven network routing mechanism users sdn controller controller periodically monitors link utilization updates link prices accordingly broadcasts users user independently selects lowest cost path implicitly guided bottleneck usage mechanism effectively steers traffic away congested paths minimizes queuing delay achieves global load balancing thereby ensuring improved qos extensive simulations real world topologies sndlib dataset including abilene geant validate effectiveness drb compared state art methods hedera ecmp proposed drb algorithm consistently achieves higher throughput lower latency reduced jitter minimized packet loss varying traffic demand levels results demonstrate drb effectively mitigates bottleneck induced congestion enhances qos sdn based networks
"In order to additionally solve the problems of data leakage, tampering, and malicious sharing in the cloud environment, the study proposes a fog computing-based privacy data storage and query mechanism, which employs multilevel data partitioning and hybrid encrypted transmission. Meanwhile, a combined data resource sharing scheme based on virtualized shared datasets is proposed for efficient management of privacy data in enterprise sensing cloud systems. The experimental results show that the accuracy of data retrieval and recovery obtained by the research method is above 98% and are more stable in the case of data size growth. The data retrieval of the shared mechanism consumes less than 2 s, while the non-adaptive cloud storage scheme consumes up to 7 s. The experimental results show that the hybrid encryption mechanism in the scheme has a greater effect on ensuring data privacy and has a high practical value. The improved scheme proposed in the study helps to ensure data privacy, reduce data redundancy and has lower power loss under cloud data storage and query mechanism. Research not only reduces labor and time costs, but also improves the protection of private data. At the same time, it has also made practical and powerful contributions to promoting the interconnection of information systems in various industries and realizing the beautiful vision of informatization and smart life services.",order additionally solve problems data leakage tampering malicious sharing cloud environment study proposes fog computing based privacy data storage query mechanism employs multilevel data partitioning hybrid encrypted transmission meanwhile combined data resource sharing scheme based virtualized shared datasets proposed efficient management privacy data enterprise sensing cloud systems experimental results show accuracy data retrieval recovery obtained research method stable case data size growth data retrieval shared mechanism consumes less non adaptive cloud storage scheme consumes experimental results show hybrid encryption mechanism scheme greater effect ensuring data privacy high practical value improved scheme proposed study helps ensure data privacy reduce data redundancy lower power loss cloud data storage query mechanism research reduces labor time costs also improves protection private data time also made practical powerful contributions promoting interconnection information systems various industries realizing beautiful vision informatization smart life services
"The Source-Grid-Load-Storage (SGLS) system incorporates energy sources, grids, loads, and storage for effective power distribution. Optimization of addition scope and line losses is critical for refining system performance. Current approaches are inefficient, emphasizing the need for a better solution. The research addresses the optimization of SGLS systems by classifying incorporation scopes and minimizing theoretical line losses. A novel framework called, Enhanced Genetic Algorithm (EGA) is implemented for enhanced scope identification and reduced line losses, improving overall competence and sustainability. The SGLS system model includes energy sources, loads, and storage devices, interrelated in a grid. The optimization emphasizes on determining the finest configuration to integrate these components, guaranteeing minimal line losses and effectual energy flow for maintainable power distribution. EGA, includes advanced mutation, adaptive crossover, and local search methods. The proposed technique attained significant performance improvements, with reduced computation time and reduced iterations compared to standard Genetic Algorithms in optimizing SGLS configurations. Using IEEE 14-bus and custom-designed systems, the proposed method established improved optimization accuracy and reduced line losses. Results displayed 15.01 s computational time, improved competence in integrating sources, loads, and storage, outperforming traditional methods. The EGA efficiently optimizes incorporation and minimizes line losses in SGLS systems, offering significant developments over traditional methods. It offers a scalable and effective solution for modern grid systems, guaranteeing better energy management and sustainability.",source grid load storage sgls system incorporates energy sources grids loads storage effective power distribution optimization addition scope line losses critical refining system performance current approaches inefficient emphasizing need better solution research addresses optimization sgls systems classifying incorporation scopes minimizing theoretical line losses novel framework called enhanced genetic algorithm ega implemented enhanced scope identification reduced line losses improving overall competence sustainability sgls system model includes energy sources loads storage devices interrelated grid optimization emphasizes determining finest configuration integrate components guaranteeing minimal line losses effectual energy flow maintainable power distribution ega includes advanced mutation adaptive crossover local search methods proposed technique attained significant performance improvements reduced computation time reduced iterations compared standard genetic algorithms optimizing sgls configurations using ieee bus custom designed systems proposed method established improved optimization accuracy reduced line losses results displayed computational time improved competence integrating sources loads storage outperforming traditional methods ega efficiently optimizes incorporation minimizes line losses sgls systems offering significant developments traditional methods offers scalable effective solution modern grid systems guaranteeing better energy management sustainability
"Multi-attribute decision-making (MADM) relies heavily on the aggregation of fuzzy data, yet determining the most appropriate decision remains challenging, especially when information is limited or uncertain. To address this, recent research has introduced Aczel-Alsina (AA) aggregation operators (AOs) for different types of fuzzy sets. This study proposes novel Fermatean fuzzy aggregation operators by incorporating AA t-norm and AA t-conorm, along with the development of AA product and AA sum for Fermatean fuzzy sets. Additionally, we introduce two innovative operators: the Fermatean fuzzy AA weighted averaging and Fermatean fuzzy AA weighted geometric operators. The theoretical properties of these operators, including idempotency, monotonicity, and boundedness, are rigorously analyzed. To validate the effectiveness of the proposed approach, we apply it to a decision-making problem in the cross-border e-commerce environment, focusing on AI-driven consumer behavior prediction and personalized targeted advertising campaigns. A global retail firm utilizing AI algorithms to analyze large-scale consumer data is examined as a case study. Our method enhances decision-making by aligning marketing strategies with cultural and consumption patterns across different regions, ultimately improving customer engagement and increasing sales. Comparative analysis with existing methodologies demonstrates that our approach outperforms conventional techniques, offering a more accurate and adaptable decision-making framework. This research contributes to the broader understanding of AI-based personalization in business environments. It reinforces its role as a robust decision-support tool for sustainable and socially responsible global market interactions.",multi attribute decision making madm relies heavily aggregation fuzzy data yet determining appropriate decision remains challenging especially information limited uncertain address recent research introduced aczel alsina aggregation operators aos different types fuzzy sets study proposes novel fermatean fuzzy aggregation operators incorporating norm conorm along development product sum fermatean fuzzy sets additionally introduce two innovative operators fermatean fuzzy weighted averaging fermatean fuzzy weighted geometric operators theoretical properties operators including idempotency monotonicity boundedness rigorously analyzed validate effectiveness proposed approach apply decision making problem cross border commerce environment focusing driven consumer behavior prediction personalized targeted advertising campaigns global retail firm utilizing algorithms analyze large scale consumer data examined case study method enhances decision making aligning marketing strategies cultural consumption patterns across different regions ultimately improving customer engagement increasing sales comparative analysis existing methodologies demonstrates approach outperforms conventional techniques offering accurate adaptable decision making framework research contributes broader understanding based personalization business environments reinforces role robust decision support tool sustainable socially responsible global market interactions
"The convergence of 5G networks and IoT technologies is accelerating the power system into smart grid, and terminals are unable to meet service quality requirements with their own resources. Edge computing has emerged as a new solution. Considering the computing resources of idle devices, the offloading strategy and resource allocation method are jointly optimized to minimize the average delay of task procedure in a communication system with multiple terminals and single edge computing server. The problem can be divided into two subproblems, resource allocation optimization subproblem and task offloading strategy subproblem, which can be solved by Lagrange multiplier method and Nash equilibrium game algorithm, respectively. An enhanced particle swarm optimization algorithm is also introduced to generate the initial offloading strategy. Simulation results indicate that the proposed algorithm can significantly reduce the average delay of task procedure.",convergence networks iot technologies accelerating power system smart grid terminals unable meet service quality requirements resources edge computing emerged new solution considering computing resources idle devices offloading strategy resource allocation method jointly optimized minimize average delay task procedure communication system multiple terminals single edge computing server problem divided two subproblems resource allocation optimization subproblem task offloading strategy subproblem solved lagrange multiplier method nash equilibrium game algorithm respectively enhanced particle swarm optimization algorithm also introduced generate initial offloading strategy simulation results indicate proposed algorithm significantly reduce average delay task procedure
"This study presents a face recognition model for construction site safety, integrating Support Vector Machine and Gabor algorithm with incremental learning and Multi-task Cascaded Convolutional Network. The model reduces Gabor’s computational complexity while enhancing feature selection and dimensionality reduction. Tested under three lighting conditions, it achieved 99.7% and 99.1% accuracy in high- and low-light environments, respectively. At a Signal-to-Noise Ratio of 15 dB, it maintained an accuracy of 86.2%, outperforming comparison models with accuracies of 70.6%, 68.3%, and 68.1%. The proposed model ensures efficient and low-computational cost face recognition in complex environments with incomplete facial data, significant lighting variations, and limited hardware. By improving recognition accuracy and adaptability, this model enhances safety management, worker protection, and supervision of construction personnel. The study provides a practical solution for construction site safety, addressing key challenges in real-world scenarios while maintaining high efficiency and accuracy.",study presents face recognition model construction site safety integrating support vector machine gabor algorithm incremental learning multi task cascaded convolutional network model reduces gabor computational complexity enhancing feature selection dimensionality reduction tested three lighting conditions achieved accuracy high low light environments respectively signal noise ratio maintained accuracy outperforming comparison models accuracies proposed model ensures efficient low computational cost face recognition complex environments incomplete facial data significant lighting variations limited hardware improving recognition accuracy adaptability model enhances safety management worker protection supervision construction personnel study provides practical solution construction site safety addressing key challenges real world scenarios maintaining high efficiency accuracy
"With the rapid development of Internet of Things (IoT) technology, smart agriculture has become a new direction in the development of modern agriculture. This study aims to enhance the utilization of agricultural resources and strengthen the intelligent and scientific management of crop growth. Given the low efficiency, high cost, and insufficient monitoring accuracy of the current traditional agricultural monitoring methods, this study analyzes the key technologies in the domestic and international agricultural IoT field based on the current situation of China’s agriculture. A smart agricultural monitoring system is proposed that combines convolutional neural network (CNN), IoT, and blockchain technology. The system addresses issues such as low efficiency, excessive cost, and inadequate monitoring precision present in traditional agricultural information systems. Meanwhile, it adopts a modular design to enhance scalability and employs a multitasking scheduling mechanism similar to an operating system to ensure high stability. This study also proposes a blockchain-based IoT data platform and designs simulated experiments to verify the system’s performance. The dataset used in the study includes photographic materials of corn from germination to maturity across six growth stages, covering an area of approximately 15 square meters, with images captured from 9 a.m. to 6 p.m. each day. Moreover, it selects images from the corn planting period, encompassing a vast amount of image data across six growth stages, of which about 20% of data is used for the test set. The results show that the recommended model has different recognition rates for corn crops at diverse growth stages, with an overall recognition error rate, weighted averaged, ranging only from 0.18% to 3.53%. In addition, the model can predict factors affecting crop growth with high precision. The blockchain data storage model still demonstrates excellent system performance under high transaction throughput, meeting the performance requirements of IoT systems. This study fills several research gaps in intelligent agricultural management information systems and provides practical guidance and insights for the application effects of “blockchain + deep learning” technology in sustainable rural development.",rapid development internet things iot technology smart agriculture become new direction development modern agriculture study aims enhance utilization agricultural resources strengthen intelligent scientific management crop growth given low efficiency high cost insufficient monitoring accuracy current traditional agricultural monitoring methods study analyzes key technologies domestic international agricultural iot field based current situation china agriculture smart agricultural monitoring system proposed combines convolutional neural network cnn iot blockchain technology system addresses issues low efficiency excessive cost inadequate monitoring precision present traditional agricultural information systems meanwhile adopts modular design enhance scalability employs multitasking scheduling mechanism similar operating system ensure high stability study also proposes blockchain based iot data platform designs simulated experiments verify system performance dataset used study includes photographic materials corn germination maturity across six growth stages covering area approximately square meters images captured day moreover selects images corn planting period encompassing vast amount image data across six growth stages data used test set results show recommended model different recognition rates corn crops diverse growth stages overall recognition error rate weighted averaged ranging addition model predict factors affecting crop growth high precision blockchain data storage model still demonstrates excellent system performance high transaction throughput meeting performance requirements iot systems study fills several research gaps intelligent agricultural management information systems provides practical guidance insights application effects blockchain deep learning technology sustainable rural development
"In today’s competitive digital landscape, brands constantly require innovative ways to engage consumers and generate memorable experiences. Augmented reality (AR) has emerged as a powerful tool for transforming traditional brand interactions into immersive, interactive experiences. However, conventional methods often fail to attain high realism, faultless interaction, and adaptive consumer engagement. To address these challenges, this research addresses an AR-based technology that develops the creation of immersive brand experiences through interactive graphic design elements. The Dynamic Dung Beetle Optimizer-Driven Stacked Convolutional Neural Network (DDB-Stacked CNN) is employed for image recognition and AR product design optimization, enhancing user engagement in branding. To ensure high-quality AR-driven graphic design elements, image datasets, consumer interaction data, and branding elements were gathered from various sources. The collected data was preprocessed using image enhancement techniques to improve clarity, while a Kalman filter was applied to reduce noise. Dynamic Dung Beetle Optimization (DDBO) is a global search capability that enables real-time optimization of AR-driven interactive design elements, allowing brands to deliver personalized and visually compelling experiences. The framework is implemented in Python, and the findings indicate that the proposed model significantly increases user engagement and improves brand recall and accuracy compared to conventional techniques. This research highlights the transformative potential of deep learning (DL)-enhanced AR graphic design in fostering stronger brand connections, setting a new benchmark for immersive and interactive brand experiences in the digital age.",today competitive digital landscape brands constantly require innovative ways engage consumers generate memorable experiences augmented reality emerged powerful tool transforming traditional brand interactions immersive interactive experiences however conventional methods often fail attain high realism faultless interaction adaptive consumer engagement address challenges research addresses based technology develops creation immersive brand experiences interactive graphic design elements dynamic dung beetle optimizer driven stacked convolutional neural network ddb stacked cnn employed image recognition product design optimization enhancing user engagement branding ensure high quality driven graphic design elements image datasets consumer interaction data branding elements gathered various sources collected data preprocessed using image enhancement techniques improve clarity kalman filter applied reduce noise dynamic dung beetle optimization ddbo global search capability enables real time optimization driven interactive design elements allowing brands deliver personalized visually compelling experiences framework implemented python findings indicate proposed model significantly increases user engagement improves brand recall accuracy compared conventional techniques research highlights transformative potential deep learning enhanced graphic design fostering stronger brand connections setting new benchmark immersive interactive brand experiences digital age
"In the past, the evaluation of teacher’s teaching ability mainly relied on qualitative evaluation methods, often influenced by subjective factors. Nowadays, student evaluation of teaching is an essential means of monitoring teaching quality, as well as a right and obligation of students. To accurately obtain teaching evaluation results, evaluate the teaching ability, and improve teaching quality, clustering analysis of teaching evaluation data is studied. It adopts an improved K-modes clustering and a neural network prediction model based on machine learning to predict the effectiveness of online learning for students. It conducts experiments to verify it. Experimental studies have shown that the absolute error of training results is 54% in the range of 0–1, and 7% in the range of greater than 3. The similarity between the initial cluster centers selected by the random method and the cluster centers of the sample data is unstable, while the cluster centers from minimizing the sum of squared errors are closer to the sample data. The clustering effect of removing abnormal data is more concentrated. The accuracy of the clustering center distance calculation method studied is 0.9816, while other methods are 0.9321 and 0.9287. The calculated cluster center distances are stable, including values of 0.8307, 0.8547, and 0.8325. It showcases that research methods and predictive models can optimize the evaluation of higher education. Through student evaluation of teaching, it is possible to promote teaching and learning through evaluation, strengthen university teaching, and serve the national economy and society.",past evaluation teacher teaching ability mainly relied qualitative evaluation methods often influenced subjective factors nowadays student evaluation teaching essential means monitoring teaching quality well right obligation students accurately obtain teaching evaluation results evaluate teaching ability improve teaching quality clustering analysis teaching evaluation data studied adopts improved modes clustering neural network prediction model based machine learning predict effectiveness online learning students conducts experiments verify experimental studies shown absolute error training results range range greater similarity initial cluster centers selected random method cluster centers sample data unstable cluster centers minimizing sum squared errors closer sample data clustering effect removing abnormal data concentrated accuracy clustering center distance calculation method studied methods calculated cluster center distances stable including values showcases research methods predictive models optimize evaluation higher education student evaluation teaching possible promote teaching learning evaluation strengthen university teaching serve national economy society
"Cancer represents a severe hazard to human well-being, and the precise segmentation of cancer cells plays a pivotal role in cancer diagnosis and therapy. In order to enhance the accuracy and efficiency of cancer cell segmentation, this study proposes the EB0-SEGP (EfficientNet-B0 SE Block Gaussian Pyramid) model constructed based on EfficientNet-B0 and the Gaussian pyramid. The model combines the channel attention mechanism (SE Block), up-sampling layers, and NIN blocks, and is optimized on the foundation of the traditional U-Net framework. During the research process, cancer cell smear images were collected from the open-source website CSDN, and after pre-processing and data augmentation, Dataset A was constructed. Accuracy, loss value, and Dice coefficient were used as indicators to evaluate the performance of the model. The research results show that the EB0-SEGP model achieves an accuracy of 88%, a Dice coefficient of 0.88, and a loss value of 0.06 on the test set, demonstrating a notable improvement compared to U-Net and ERU-Net. Meanwhile, the standard deviation of the training curve is significantly lower than that of the comparison models, indicating a lighter degree of overfitting. This study provides a new technical means for cancer treatment and has important theoretical significance and clinical application value in the field of cancer diagnosis and treatment.",cancer represents severe hazard human well precise segmentation cancer cells plays pivotal role cancer diagnosis therapy order enhance accuracy efficiency cancer cell segmentation study proposes segp efficientnet block gaussian pyramid model constructed based efficientnet gaussian pyramid model combines channel attention mechanism block sampling layers nin blocks optimized foundation traditional net framework research process cancer cell smear images collected open source website csdn pre processing data augmentation dataset constructed accuracy loss value dice coefficient used indicators evaluate performance model research results show segp model achieves accuracy dice coefficient loss value test set demonstrating notable improvement compared net eru net meanwhile standard deviation training curve significantly lower comparison models indicating lighter degree overfitting study provides new technical means cancer treatment important theoretical significance clinical application value field cancer diagnosis treatment
"In construction engineering, material procurement costs constitute a significant portion of total project expenditures. Conventional cost optimization approaches often fail to effectively respond to abnormal events such as sudden market price fluctuations and supply chain disruptions, leading to challenges in maintaining controlled cost management. To address this issue, this study introduces a deep learning-based framework utilizing a variational autoencoder (VAE) to enhance cost prediction accuracy and improve optimization strategies under dynamic conditions. The proposed approach begins with the collection and preprocessing of construction material cost data, including the handling of missing values, outliers, and duplicate records. Principal component analysis (PCA) is then applied to extract key features and reduce data dimensionality. A VAE model is subsequently constructed, in which both encoder and decoder networks map high-dimensional data into a compact latent space. Model parameters are optimized through reparameterization techniques using the Adam optimizer to minimize reconstruction error and Kullback–Leibler (KL) divergence.",construction engineering material procurement costs constitute significant portion total project expenditures conventional cost optimization approaches often fail effectively respond abnormal events sudden market price fluctuations supply chain disruptions leading challenges maintaining controlled cost management address issue study introduces deep learning based framework utilizing variational autoencoder vae enhance cost prediction accuracy improve optimization strategies dynamic conditions proposed approach begins collection preprocessing construction material cost data including handling missing values outliers duplicate records principal component analysis pca applied extract key features reduce data dimensionality vae model subsequently constructed encoder decoder networks map high dimensional data compact latent space model parameters optimized reparameterization techniques using adam optimizer minimize reconstruction error kullback leibler divergence
"In recent years, China’s economy has undergone structural adjustments and a slowdown in growth. As a result, corporate credit risk has become increasingly important to monitor and assess accurately. Due to the low operational efficiency and high personnel costs associated with traditional credit risk assessment models, this study opted for an artificial intelligence model with support vector machines for risk assessment. To improve the accuracy and efficiency of the assessment model and make it more suitable for the financial characteristics of enterprises, a particle swarm algorithm was introduced to optimize the support vector machine model. For the choice of measurement indicators, the experiment extracted nine indicators with a cumulative contribution of 80% or more from 20 original indicators by means of principal component analysis as the experimental indicators. The model test results showed that the model had the greatest overall classification accuracy and recall rate, which were 89.44% and 58.83%, respectively. Meanwhile, the improved support vector machine model had the lowest misclassification rates for the first and second categories, at 24.45% and 16.74%, respectively. This indicates that the model is more likely to detect enterprises that can incur credit risk and can achieve the research objective with high applicability.",recent years china economy undergone structural adjustments slowdown growth result corporate credit risk become increasingly important monitor assess accurately due low operational efficiency high personnel costs associated traditional credit risk assessment models study opted artificial intelligence model support vector machines risk assessment improve accuracy efficiency assessment model make suitable financial characteristics enterprises particle swarm algorithm introduced optimize support vector machine model choice measurement indicators experiment extracted nine indicators cumulative contribution original indicators means principal component analysis experimental indicators model test results showed model greatest overall classification accuracy recall rate respectively meanwhile improved support vector machine model lowest misclassification rates first second categories respectively indicates model likely detect enterprises incur credit risk achieve research objective high applicability
"This article illustrates how design thinking can structure the development process of a practical toolkit for schools to address cyber mistreatment from pupils, parents, and other parties as a workplace issue. The process included an exploratory phase to identify gaps in schools’ understanding and handling, followed by iterative generative and evaluative phases. Based on the Psychosocial Safety Climate framework, necessary organizational capacities were defined. Prototypes of toolkit components, including vignette videos for discussions among staff, a text guide for policy and infrastructure development, and an informative video, were refined through stakeholder feedback. A summative evaluation confirmed the usefulness of these resources for engaging staff and fostering discussions on cyber mistreatment. A themed webpage was created for free access to the toolkit. This study contributes to existing debates on how academic research can impact practice by emphasizing the usefulness of design thinking in creating innovative, user-centered solutions for capacity development.",article illustrates design thinking structure development process practical toolkit schools address cyber mistreatment pupils parents parties workplace issue process included exploratory phase identify gaps schools understanding handling followed iterative generative evaluative phases based psychosocial safety climate framework necessary organizational capacities defined prototypes toolkit components including vignette videos discussions among staff text guide policy infrastructure development informative video refined stakeholder feedback summative evaluation confirmed usefulness resources engaging staff fostering discussions cyber mistreatment themed webpage created free access toolkit study contributes existing debates academic research impact practice emphasizing usefulness design thinking creating innovative user centered solutions capacity development
"This study is devoted to exploring the application effect of virtual reality (VR) and augmented reality (AR) technology in music and dance education, especially its influence on students’ learning effectiveness under the situational teaching mode. By comparing the experimental group (using VR/AR technology) and the control group (using traditional teaching methods), this study reveals that VR/AR technology has a significant positive impact on improving students’ learning motivation and satisfaction, and at the same time, it also effectively promotes students’ mastery of music and dance skills to some extent. In addition, this study also discusses the potential challenges of integrating VR/AR technology into teaching practice, including technical preparation, teaching content design and teacher training, which provides a direction for the application of educational technology in music and dance education.",study devoted exploring application effect virtual reality augmented reality technology music dance education especially influence students learning effectiveness situational teaching mode comparing experimental group using technology control group using traditional teaching methods study reveals technology significant positive impact improving students learning motivation satisfaction time also effectively promotes students mastery music dance skills extent addition study also discusses potential challenges integrating technology teaching practice including technical preparation teaching content design teacher training provides direction application educational technology music dance education
"This paper proposes a random simulation-based evaluation model for college Chinese teaching, aiming at accurately evaluating teaching effect and students’ learning progress through comprehensive and detailed evaluation index system, diversified data integration method and advanced Monte Carlo simulation technology. The evaluation system revolves around three dimensions: knowledge mastery, skill improvement, and emotional attitude to ensure the depth and breadth of evaluation. Data collection covers multiple sources of information such as academic performance, classroom interaction, and self-reflection, and undergoes rigorous preprocessing to improve the accuracy and reliability of the analysis. Based on the assumption that the teaching process is regarded as a dynamic random system, the model uses probability distribution and complex function relationship to quantify the dynamic change of students’ state, especially the improvement of key skills such as writing ability. In the experimental evaluation part, the validity of the model was verified by detailed data analysis, reliability and validity tests. Finally, the model showed high accuracy and recall in predicting students’ learning performance, proving its applicability and practicality in complex teaching environments.",paper proposes random simulation based evaluation model college chinese teaching aiming accurately evaluating teaching effect students learning progress comprehensive detailed evaluation index system diversified data integration method advanced monte carlo simulation technology evaluation system revolves around three dimensions knowledge mastery skill improvement emotional attitude ensure depth breadth evaluation data collection covers multiple sources information academic performance classroom interaction self reflection undergoes rigorous preprocessing improve accuracy reliability analysis based assumption teaching process regarded dynamic random system model uses probability distribution complex function relationship quantify dynamic change students state especially improvement key skills writing ability experimental evaluation part validity model verified detailed data analysis reliability validity tests finally model showed high accuracy recall predicting students learning performance proving applicability practicality complex teaching environments
"This paper explores the application of quantitative analysis methods to innovate cultural and creative product design, addressing the challenges designers face in understanding and translating consumer needs. The research employs an integrated framework of AHP (Analytic Hierarchy Process), QFD (Quality Function Deployment), and TRIZ (Theory of Inventive Problem Solving) as the primary tools, using the practical case of “Jiao Keshu” as a carrier to systematically illustrate the process of transforming user needs into design elements. The AHP method is utilized to prioritize and structure user needs, providing a clear design direction. QFD is then applied to quantify core needs and convert them into actionable design elements, while TRIZ resolves conflicts between these elements, optimizing product functionality and fostering innovation. This integrated approach not only enhances the scientific rigor and specificity of the design process but also validates the effectiveness of quantitative analysis tools in cultural and creative product design through practical application. By bridging cultural and creative products with consumer needs, this method offers a new perspective on achieving precise alignment and establishes a theoretical and practical foundation for innovation in cultural and creative product design.",paper explores application quantitative analysis methods innovate cultural creative product design addressing challenges designers face understanding translating consumer needs research employs integrated framework ahp analytic hierarchy process qfd quality function deployment triz theory inventive problem solving primary tools using practical case jiao keshu carrier systematically illustrate process transforming user needs design elements ahp method utilized prioritize structure user needs providing clear design direction qfd applied quantify core needs convert actionable design elements triz resolves conflicts elements optimizing product functionality fostering innovation integrated approach enhances scientific rigor specificity design process also validates effectiveness quantitative analysis tools cultural creative product design practical application bridging cultural creative products consumer needs method offers new perspective achieving precise alignment establishes theoretical practical foundation innovation cultural creative product design
"Business Environment refers to the sum of external factors and conditions related to government affairs, market, rule of law, cultural environment, etc. involved in the process of market entities’ entry, production and operation, exit, etc. The acceleration of the business environment on the economy has been verified, but the mediating role of entrepreneurial spirit is not yet clear. In order to study the relationship between entrepreneurship, business environment and regional economic development, a regression model was constructed based on the panel data of various provinces in China from 2020 to 2022, and the stepwise regression and ridge regression methods were used for analysis. Stepwise regression selects the explanatory variables that contribute the most to the model by gradually adding or removing variables. Ridge regression introduces regularization terms to solve the multicollinearity problem in the model and improve the stability of parameter estimation. The analysis results show that the coefficient of impact of the business environment on economic development is 0.33. At a confidence level of 1%, the regression coefficients between business environment and entrepreneurial innovation and entrepreneurship are 0.256 and 0.064. It is clear that business environment optimization can significantly promote regional economic development, in which entrepreneurship is the key intermediary. In addition, a quality business environment can reduce business costs, improve market vitality, encourage entrepreneurs to innovate and start businesses, and drive economic growth through efficient allocation of resources and knowledge sharing. Optimizing the business environment and fostering entrepreneurship are effective ways to promote regional economic prosperity.",business environment refers sum external factors conditions related government affairs market rule law cultural environment etc involved process market entities entry production operation exit etc acceleration business environment economy verified mediating role entrepreneurial spirit yet clear order study relationship entrepreneurship business environment regional economic development regression model constructed based panel data various provinces china stepwise regression ridge regression methods used analysis stepwise regression selects explanatory variables contribute model gradually adding removing variables ridge regression introduces regularization terms solve multicollinearity problem model improve stability parameter estimation analysis results show coefficient impact business environment economic development confidence level regression coefficients business environment entrepreneurial innovation entrepreneurship clear business environment optimization significantly promote regional economic development entrepreneurship key intermediary addition quality business environment reduce business costs improve market vitality encourage entrepreneurs innovate start businesses drive economic growth efficient allocation resources knowledge sharing optimizing business environment fostering entrepreneurship effective ways promote regional economic prosperity
"This study focuses on optimizing inference and prediction using the GLM-4 model in multi-agent systems by proposing an innovative approach that integrates generative adversarial networks (GANs) for dynamic denoising with transfer reinforcement learning (TRL). Traditional methods face challenges in noisy data environments, limited model generalization, and poor adaptability in dynamic settings. Rule-based inference methods struggle with environmental variability, and single reinforcement learning models are constrained by sample efficiency and strategy transfer limitations. This research introduces a GAN-based dynamic denoising mechanism in which the generator simulates noise distributions to create pseudo-noise samples, and the discriminator distinguishes between real and noisy data. The adversarial training iteratively enhances data quality. Experiments show that this approach improves the signal-to-noise ratio by over 20%, significantly reducing the interference of noise in GLM-4 inference. Transfer Reinforcement Learning facilitates cross-task knowledge transfer, enhancing learning efficiency in multi-agent collaborative tasks. A state-action value function-based transfer strategy is designed, enabling the migration of learned policy parameters and feature representations from source to target tasks, thereby reducing exploration costs. In multi-agent contract management experiments, this method increased convergence speed by 33% while maintaining policy stability.",study focuses optimizing inference prediction using glm model multi agent systems proposing innovative approach integrates generative adversarial networks gans dynamic denoising transfer reinforcement learning trl traditional methods face challenges noisy data environments limited model generalization poor adaptability dynamic settings rule based inference methods struggle environmental variability single reinforcement learning models constrained sample efficiency strategy transfer limitations research introduces gan based dynamic denoising mechanism generator simulates noise distributions create pseudo noise samples discriminator distinguishes real noisy data adversarial training iteratively enhances data quality experiments show approach improves signal noise ratio significantly reducing interference noise glm inference transfer reinforcement learning facilitates cross task knowledge transfer enhancing learning efficiency multi agent collaborative tasks state action value function based transfer strategy designed enabling migration learned policy parameters feature representations source target tasks thereby reducing exploration costs multi agent contract management experiments method increased convergence speed maintaining policy stability
"At present, major universities are actively carrying out innovation and entrepreneurship education courses to develop the creative potential of college students. In order to analyze the impact of participation in entrepreneurship and innovation education on college students’ creativity, the study took students from a certain province as a research sample, conducted a follow-up survey, and analyzed the current students’ creativity level and participation in entrepreneurship and innovation education. Ordinary Least Squares (OLS) regression was used to analyze the impact of entrepreneurship and innovation education on college students’ creativity. In order to enhance the credibility of the results, the propensity score matching method (PSM) was also used to verify the impact of participation in entrepreneurship and innovation education on students’ creativity. The analysis results show that the current creativity score of college students in my country is 107.6, which is at an average level; students who do not participate in entrepreneurship and innovation education have the lowest creativity score, and students who are rich in imagination and dare to take risks are more willing to actively participate in entrepreneurship and innovation education. OLS regression and PSM results show that participation in entrepreneurship and innovation education will significantly and positively affect the creativity of college students. Among them, active participation and both types of participation have significant effects on college students’ creativity, while passive participation has no significant effect. Therefore, colleges and universities need to carry out entrepreneurship and innovation education in a variety of ways, fully mobilize the enthusiasm of students to participate, and improve the level of students’ creativity. Specific recommendations include developing interactive hands-on courses, motivating students to participate in innovation competitions and entrepreneurial activities, and providing guidance and resource support to promote personalized development.",present major universities actively carrying innovation entrepreneurship education courses develop creative potential college students order analyze impact participation entrepreneurship innovation education college students creativity study took students certain province research sample conducted follow survey analyzed current students creativity level participation entrepreneurship innovation education ordinary least squares ols regression used analyze impact entrepreneurship innovation education college students creativity order enhance credibility results propensity score matching method psm also used verify impact participation entrepreneurship innovation education students creativity analysis results show current creativity score college students country average level students participate entrepreneurship innovation education lowest creativity score students rich imagination dare take risks willing actively participate entrepreneurship innovation education ols regression psm results show participation entrepreneurship innovation education significantly positively affect creativity college students among active participation types participation significant effects college students creativity passive participation significant effect therefore colleges universities need carry entrepreneurship innovation education variety ways fully mobilize enthusiasm students participate improve level students creativity specific recommendations include developing interactive hands courses motivating students participate innovation competitions entrepreneurial activities providing guidance resource support promote personalized development
"This article discusses how to use artificial intelligence technology to design a personalized sports flipped classroom to promote the effectiveness and interest of students’ sports learning. The innovation of this article has the following three points: this article uses artificial intelligence technology to design a personalized sports flipped classroom, generating personalized learning portraits, learning contents and learning paths based on multi-dimensional data such as students’ physical health, motor skills, and interests and preferences, so as to realize the high degree of personalization, intelligence, and high efficiency in sports teaching. The personalized sports flipped classroom in this paper aims to guide students to independently pre-study personalized and customized sports learning content before class, cultivate their self-study ability and interest motivation, while focusing on practical interactions in class, carrying out rich and diverse personalized sports practice activities, improving sports skills, developing sports literacy, and improving classroom efficiency and learning satisfaction. This paper verifies the effect of artificial intelligence-driven personalized sports flipped classroom design, through the two-group design of the experimental group and the control group, respectively, the two groups of students carried out a 12-weeks physical education teaching experiment, and before and after the experiment, the two groups of students conducted tests and questionnaires in the areas of physical education skills, physical fitness, learning interest, and learning satisfaction, to compare the differences in the effect of the two teaching modes, and the results show that the two teaching modes based on the results show that the personalized physical education flipped classroom model based on artificial intelligence is better than the traditional physical education teaching model in all aspects, and has significant teaching effects.",article discusses use artificial intelligence technology design personalized sports flipped classroom promote effectiveness interest students sports learning innovation article following three points article uses artificial intelligence technology design personalized sports flipped classroom generating personalized learning portraits learning contents learning paths based multi dimensional data students physical health motor skills interests preferences realize high degree personalization intelligence high efficiency sports teaching personalized sports flipped classroom paper aims guide students independently pre study personalized customized sports learning content class cultivate self study ability interest motivation focusing practical interactions class carrying rich diverse personalized sports practice activities improving sports skills developing sports literacy improving classroom efficiency learning satisfaction paper verifies effect artificial intelligence driven personalized sports flipped classroom design two group design experimental group control group respectively two groups students carried weeks physical education teaching experiment experiment two groups students conducted tests questionnaires areas physical education skills physical fitness learning interest learning satisfaction compare differences effect two teaching modes results show two teaching modes based results show personalized physical education flipped classroom model based artificial intelligence better traditional physical education teaching model aspects significant teaching effects
"This study explores the dance movement recognition technology based on deep learning, and aims to improve the recognition accuracy of dance movements by constructing and evaluating deep learning models. This paper combs the basic concepts and principles of deep learning, and analyzes the latest progress of dance action recognition, its application in different scenes and the challenges it faces. Subsequently, the process of data collection, preprocessing, labeling and feature extraction is described in detail, and the role of data preprocessing and enhancement technology in improving model performance is emphatically discussed. Based on this, this paper designs a hybrid architecture combining convolutional neural network (CNN) and recurrent neural network (RNN), and the corresponding model optimization strategy, in order to achieve higher accuracy and efficiency when dealing with complex dance sequences. The experimental design part includes the process of model training, evaluation and verification, and comprehensively tests the model performance. The results show that the proposed model performs well in many dance action recognition tasks, with high accuracy and good generalization ability. This study provides a valuable reference for the development of dance movement recognition technology, and also opens up a new way for in-depth research and application in related fields.",study explores dance movement recognition technology based deep learning aims improve recognition accuracy dance movements constructing evaluating deep learning models paper combs basic concepts principles deep learning analyzes latest progress dance action recognition application different scenes challenges faces subsequently process data collection preprocessing labeling feature extraction described detail role data preprocessing enhancement technology improving model performance emphatically discussed based paper designs hybrid architecture combining convolutional neural network cnn recurrent neural network rnn corresponding model optimization strategy order achieve higher accuracy efficiency dealing complex dance sequences experimental design part includes process model training evaluation verification comprehensively tests model performance results show proposed model performs well many dance action recognition tasks high accuracy good generalization ability study provides valuable reference development dance movement recognition technology also opens new way depth research application related fields
"Although this study systematically analyzes the development path of the red cultural creative industry based on fuzzy control and validates relevant theories and strategies using empirical data from Chongqing Shapingba District area, several limitations remain. First, the research data is primarily derived from questionnaires, which may not fully capture the developmental status and challenges of the entire red cultural and creative industry due to sample size and regional constraints. Secondly, the specific processes and practical implementation details of fuzzy control theory are not thoroughly explored, potentially limiting its applicability and effectiveness within the industry. The study’s focus is largely on identifying existing issues and proposing countermeasures, with less emphasis on forward-looking analyses of emerging challenges and problems that may arise in the future. Finally, the examination of interactions among policy, market dynamics, technology, and talent remains limited, preventing a deeper exploration of the complex relationships and comprehensive impact these factors have on industry development. Addressing these limitations in future research could enhance understanding and further support the robust development of red cultural and creative industries.",although study systematically analyzes development path red cultural creative industry based fuzzy control validates relevant theories strategies using empirical data chongqing shapingba district area several limitations remain first research data primarily derived questionnaires may fully capture developmental status challenges entire red cultural creative industry due sample size regional constraints secondly specific processes practical implementation details fuzzy control theory thoroughly explored potentially limiting applicability effectiveness within industry study focus largely identifying existing issues proposing countermeasures less emphasis forward looking analyses emerging challenges problems may arise future finally examination interactions among policy market dynamics technology talent remains limited preventing deeper exploration complex relationships comprehensive impact factors industry development addressing limitations future research could enhance understanding support robust development red cultural creative industries
"Cultural threat is one of the main explanations for opposing immigrants. However, it lacks theoretical precision and suffers from measurement issues. To remedy these problems, we conceptualize cultural threat as a zero-sum problem and specify concrete cultural rights that Muslims receive. In two online experiments in Germany, we randomly assigned people to situations where Muslim cultural rights either replace majority rights (zero-sum) or they co-exist (non-zero-sum). The willingness to accommodate Muslims’ cultural rights varies strongly by the logic of cultural threat, especially for respondents with an inclusionary mindset. They are protective of their own cultural practices, but do not perceive all gains in Muslim rights as inherently threatening. Respondents with an exclusionary mindset, however, respond less to variations in cultural threat. In contrast to people with an inclusionary mindset, they also react more negatively to Muslim than non-Muslim demands for change.",cultural threat one main explanations opposing immigrants however lacks theoretical precision suffers measurement issues remedy problems conceptualize cultural threat zero sum problem specify concrete cultural rights muslims receive two online experiments germany randomly assigned people situations muslim cultural rights either replace majority rights zero sum exist non zero sum willingness accommodate muslims cultural rights varies strongly logic cultural threat especially respondents inclusionary mindset protective cultural practices perceive gains muslim rights inherently threatening respondents exclusionary mindset however respond less variations cultural threat contrast people inclusionary mindset also react negatively muslim non muslim demands change
"With the continuous expansion of enterprise business scale, the amount of financial data is growing exponentially, and massive data has become the core resource for enterprise development. However, many enterprises have not yet established a comprehensive intelligent data analysis system, resulting in significant challenges in daily data collection and frequent loss of critical financial data, which seriously restricts the further development of enterprises. Although existing research has explored financial data processing, there are still shortcomings in the combination of algorithm efficiency and system practicality. For example, some studies focus too much on theoretical model construction, resulting in low data processing efficiency in practical applications; some systems struggle to meet the accuracy and automation requirements of enterprise analysis when faced with complex and heterogeneous financial data. In the field of intelligent computing, the chaos particle swarm optimization (PSO) algorithm provides a new approach for financial data classification and automatic analysis and processing due to its global search capability and parallel computing advantages. This article is based on the chaos particle swarm optimization algorithm, and deeply studies and designs an intelligent financial data processing system. By proposing adaptive particle swarm optimization (APSO) and simplified adaptive particle swarm optimization (RAPSO) algorithms, the data processing flow is optimized. The research results show that the system significantly improves the efficiency and automation level of financial data processing. 52% of enterprises believe that the automation level of system data analysis is high, and the RAPSO algorithm significantly shortens the data processing time compared to the APSO algorithm. However, this study only focuses on the preliminary exploration of enterprise financial data processing. In the future, algorithm application scenarios can be further expanded to optimize system functions to meet more complex business needs.",continuous expansion enterprise business scale amount financial data growing exponentially massive data become core resource enterprise development however many enterprises yet established comprehensive intelligent data analysis system resulting significant challenges daily data collection frequent loss critical financial data seriously restricts development enterprises although existing research explored financial data processing still shortcomings combination algorithm efficiency system practicality example studies focus much theoretical model construction resulting low data processing efficiency practical applications systems struggle meet accuracy automation requirements enterprise analysis faced complex heterogeneous financial data field intelligent computing chaos particle swarm optimization pso algorithm provides new approach financial data classification automatic analysis processing due global search capability parallel computing advantages article based chaos particle swarm optimization algorithm deeply studies designs intelligent financial data processing system proposing adaptive particle swarm optimization apso simplified adaptive particle swarm optimization rapso algorithms data processing flow optimized research results show system significantly improves efficiency automation level financial data processing enterprises believe automation level system data analysis high rapso algorithm significantly shortens data processing time compared apso algorithm however study focuses preliminary exploration enterprise financial data processing future algorithm application scenarios expanded optimize system functions meet complex business needs
"English is an international language and a tool for extensive communication. Learning English is conducive to the mutual communication and exchange of ideas, culture, economy, and many other aspects between different countries and ethnic groups, and is conducive to peaceful coexistence and common development among countries. Countries are increasingly cooperating and exchanging ideas in several sectors due to the broad implementation of current information technology and the rapid speed of globalization. As an international language, English is changing the way of communication. The shift in the emphasis of English language instruction from rote memorization to developing students’ listening and speaking skills will inevitably result in a shift in classroom instruction techniques. Neural networks may be used to assess collegiate English classroom teaching (ECT) techniques and practices, finally completing the following work: (1) Introduces several domestically and internationally related theories and literature on the enhancement of ECT methods and the teaching quality assessment, which will be discussed later. A neural network-based teaching assessment framework is constructed to provide a theoretical basis. (2) Big data technology was employed to obtain appropriate data samples for experiments and verification, which included an ECT technique and practice assessment system tailored to the specific needs of the school. (3) The ECT technique assessment system relies on the BPNN technology. The assessment approach based on the BPNN predicted college ECT and practice extremely well and delivered excellent outcomes, according to the findings of the study.",english international language tool extensive communication learning english conducive mutual communication exchange ideas culture economy many aspects different countries ethnic groups conducive peaceful coexistence common development among countries countries increasingly cooperating exchanging ideas several sectors due broad implementation current information technology rapid speed globalization international language english changing way communication shift emphasis english language instruction rote memorization developing students listening speaking skills inevitably result shift classroom instruction techniques neural networks may used assess collegiate english classroom teaching ect techniques practices finally completing following work introduces several domestically internationally related theories literature enhancement ect methods teaching quality assessment discussed later neural network based teaching assessment framework constructed provide theoretical basis big data technology employed obtain appropriate data samples experiments verification included ect technique practice assessment system tailored specific needs school ect technique assessment system relies bpnn technology assessment approach based bpnn predicted college ect practice extremely well delivered excellent outcomes according findings study
"Rapid and accurate assessment of college students’ mental health status is an important task in college mental health education and is also the basis for accurate intervention and personalized education services in college psychological work. The traditional evaluation methods have some problems, such as low real-time evaluation, poor evaluation effect of single-modal data, and social approval response bias. Social media big data provides a new way to predict college students’ mental health status in real time and accurately. This paper constructs a mental health prediction model based on web text data. BERT language framework is applied to psychological trait prediction, and BERT bidirectional training mode and Transformer coding module are used to mine more complete contextual semantic features and longer distance contextual dependencies, which solves the problem of enhancement vector representation of psychological trait semantic features. At the same time, considering that the difference in the internal structure of the classifier may lead to different classification effects, the fully connected layer of BERT model and the classical random forest algorithm are, respectively, used as two different classifiers in the downstream classification task to compare the model effects. The results show that BERT-B model has the greatest superiority, the prediction accuracy rate reaches 97.25%, the accuracy rate reaches 96.66%, the recall rate reaches 99.61%, the F1 value reaches 98.11%, and the AUC area reaches 99.44%, which can effectively predict the mental health status. The importance of this study is that it not only provides a fast, accurate and cost-effective mental health assessment tool for colleges and universities but also provides strong data support for personalized intervention and precise implementation of mental health services. The successful application of this innovative model marks an important step in the deep integration and value mining of social media big data in the field of mental health research and has great significance in promoting the modernization and scientific education of mental health.",rapid accurate assessment college students mental health status important task college mental health education also basis accurate intervention personalized education services college psychological work traditional evaluation methods problems low real time evaluation poor evaluation effect single modal data social approval response bias social media big data provides new way predict college students mental health status real time accurately paper constructs mental health prediction model based web text data bert language framework applied psychological trait prediction bert bidirectional training mode transformer coding module used mine complete contextual semantic features longer distance contextual dependencies solves problem enhancement vector representation psychological trait semantic features time considering difference internal structure classifier may lead different classification effects fully connected layer bert model classical random forest algorithm respectively used two different classifiers downstream classification task compare model effects results show bert model greatest superiority prediction accuracy rate reaches accuracy rate reaches recall rate reaches value reaches auc area reaches effectively predict mental health status importance study provides fast accurate cost effective mental health assessment tool colleges universities also provides strong data support personalized intervention precise implementation mental health services successful application innovative model marks important step deep integration value mining social media big data field mental health research great significance promoting modernization scientific education mental health
"With the widespread application of artificial intelligence technology in education, particularly in college English teaching, its potential for personalized instruction is gaining attention. This study aims to explore an AI-assisted personalized college English teaching model and evaluate its effectiveness in improving learning efficiency and teaching quality. Data were collected through questionnaire surveys and experimental studies on the experiences and satisfaction of students and teachers using AI teaching tools. The constructed teaching model is based on personalized learning theory and cognitive development theory, aiming to optimize the learning process and enhance learning outcomes. The experimental results showed that students using AI tools demonstrated higher improvement rates in various English skills, and both teachers and students expressed higher satisfaction with AI-assisted teaching. Despite research limitations, such as sample diversity and challenges posed by rapid technological advancements, this study provides valuable insights and recommendations for future AI applications in education.",widespread application artificial intelligence technology education particularly college english teaching potential personalized instruction gaining attention study aims explore assisted personalized college english teaching model evaluate effectiveness improving learning efficiency teaching quality data collected questionnaire surveys experimental studies experiences satisfaction students teachers using teaching tools constructed teaching model based personalized learning theory cognitive development theory aiming optimize learning process enhance learning outcomes experimental results showed students using tools demonstrated higher improvement rates various english skills teachers students expressed higher satisfaction assisted teaching despite research limitations sample diversity challenges posed rapid technological advancements study provides valuable insights recommendations future applications education
"This study, in response to the urgent need for modernization transformation of the teaching system of environmental art design in China, innovatively constructs a research framework that deeply integrates theory and practice. Based on systematic literature analysis, a teaching reform path centered on interdisciplinary integration was established. A breakthrough was made by introducing a deep belief network to construct an intelligent teaching quality assessment system, and a multi-level feature extraction architecture was adopted to achieve in-depth mining and intelligent analysis of teaching data. Experimental verification shows that this model significantly improves the evaluation efficiency and objectivity while maintaining the professionalism of expert assessment. The “foundation consolidation-ability advancement-Innovative practice” gradient curriculum system reconstruction plan proposed in the research has formed a replicable professional construction model, providing important theoretical support and practical guidance for the innovative development of design education in the new era, and effectively promoting the transformation and upgrading of traditional EAD majors towards digitalization and intelligence.",study response urgent need modernization transformation teaching system environmental art design china innovatively constructs research framework deeply integrates theory practice based systematic literature analysis teaching reform path centered interdisciplinary integration established breakthrough made introducing deep belief network construct intelligent teaching quality assessment system multi level feature extraction architecture adopted achieve depth mining intelligent analysis teaching data experimental verification shows model significantly improves evaluation efficiency objectivity maintaining professionalism expert assessment foundation consolidation ability advancement innovative practice gradient curriculum system reconstruction plan proposed research formed replicable professional construction model providing important theoretical support practical guidance innovative development design education new effectively promoting transformation upgrading traditional ead majors towards digitalization intelligence
"This article aims to explore how to use generative adversarial network (GAN) technology to simulate Chinese writing style and integrate the teaching goal of cultivating students’ “craftsman spirit” on this basis. Through this study, we hope to enhance students’ language literacy and promote their development quality. First, a generative adversarial network is used to detect and recognize text. Optimization has been carried out to address the accuracy and reliability issues of the simulation system caused by the lack of training data and the difference between real data. On this basis, we designed an automatic text modification model to improve the performance of Chinese character recognition. Finally, we integrated the teaching task of cultivating students’ craftsmanship spirit into the Chinese writing style simulation system formed by GAN, and explored effective methods for infusing craftsmanship spirit into Chinese writing teaching. The research results show that generative adversarial network algorithms have significant applicability in Chinese writing style simulation systems. Through optimization processing, we have successfully improved the effectiveness of text recognition and the accuracy of style simulation. This study indicates that a Chinese writing style simulation system based on generative adversarial networks can effectively improve students’ text recognition ability and writing style simulation accuracy. At the same time, by infusing the spirit of craftsmanship into teaching, we provide strong support for students’ language literacy and comprehensive development. This study provides new ideas and methods for future Chinese language teaching reform and innovation.",article aims explore use generative adversarial network gan technology simulate chinese writing style integrate teaching goal cultivating students craftsman spirit basis study hope enhance students language literacy promote development quality first generative adversarial network used detect recognize text optimization carried address accuracy reliability issues simulation system caused lack training data difference real data basis designed automatic text modification model improve performance chinese character recognition finally integrated teaching task cultivating students craftsmanship spirit chinese writing style simulation system formed gan explored effective methods infusing craftsmanship spirit chinese writing teaching research results show generative adversarial network algorithms significant applicability chinese writing style simulation systems optimization processing successfully improved effectiveness text recognition accuracy style simulation study indicates chinese writing style simulation system based generative adversarial networks effectively improve students text recognition ability writing style simulation accuracy time infusing spirit craftsmanship teaching provide strong support students language literacy comprehensive development study provides new ideas methods future chinese language teaching reform innovation
"Through literature review, experimental analysis, questionnaire survey, and other research methods, this study analyzes the current situation of data-driven training in police physical education. By using data-driven technology to evaluate physical training, it provides scientific theoretical guidance and technical support for improving the quality of police physical education and training. The data-driven equipment, data-driven concept, and quantitative thinking are used to analyze and practice the police physical education and training. Combined with the training results, data-driven analysis is formed to achieve data-driven training mode. This can make the teaching and training more scientific, the training organization more efficient, and the training evaluation more accurate.",literature review experimental analysis questionnaire survey research methods study analyzes current situation data driven training police physical education using data driven technology evaluate physical training provides scientific theoretical guidance technical support improving quality police physical education training data driven equipment data driven concept quantitative thinking used analyze practice police physical education training combined training results data driven analysis formed achieve data driven training mode make teaching training scientific training organization efficient training evaluation accurate
"The quick development of medical technologies has resulted in the opportunities to improve patient care. Particularly, this study focused on senior health consumers. According to that, the present study suggests a novel method for managing healthcare by combining an intelligent decision-making framework with Digital Twin (DT) technologies into a patient-centered Health Management System (HMS) especially for elderly patients. The technology uses seniors’ real-time data to build effective DT of virtual duplicates that continuously monitor and reflects each person’s health profile. With the help of these DT, real-time health management and predictive analytics are made easier, which allows the system to independently recommend individualized lifestyle adjustments and treatments based on ongoing monitoring. The present study utilizes the integrated digital twin concepts. To monitor the individual patient health data in real time, local twin is used, performed based on the condition-based monitoring (CBM) modules, whereas the global twin helps to combine the data across multiple patients and allowing broader analysis. Additionally, the decision-making process is performed with the help of fuzzy inference system (FIS) based on the system’s overall production rate. Finally, the simulation of the study is performed regarding varying health conditions of patients, which improves the healthcare outcomes through decentralized data-driven approaches. A novel prototype of “Health-Pro” was developed by the inspiration of the existing study, and tested with 43 seniors with the age between 60 and 85. 80% of participants agreed that the system supported to improved health awareness and self-management. Participants stated a promising improvement in their attitude towards controlling their health. Key prerequisites for successful implementation were also observed by the study, and these include the importance of privacy controls, adjustable interfaces and social networking factors. Overall, this innovative method of managing health opens the door for more efficient, flexible healthcare options that are sensitive to each patient’s unique demands, thereby improving senior citizens’ quality of life and health results.",quick development medical technologies resulted opportunities improve patient care particularly study focused senior health consumers according present study suggests novel method managing healthcare combining intelligent decision making framework digital twin technologies patient centered health management system hms especially elderly patients technology uses seniors real time data build effective virtual duplicates continuously monitor reflects person health profile help real time health management predictive analytics made easier allows system independently recommend individualized lifestyle adjustments treatments based ongoing monitoring present study utilizes integrated digital twin concepts monitor individual patient health data real time local twin used performed based condition based monitoring cbm modules whereas global twin helps combine data across multiple patients allowing broader analysis additionally decision making process performed help fuzzy inference system fis based system overall production rate finally simulation study performed regarding varying health conditions patients improves healthcare outcomes decentralized data driven approaches novel prototype health pro developed inspiration existing study tested seniors age participants agreed system supported improved health awareness self management participants stated promising improvement attitude towards controlling health key prerequisites successful implementation also observed study include importance privacy controls adjustable interfaces social networking factors overall innovative method managing health opens door efficient flexible healthcare options sensitive patient unique demands thereby improving senior citizens quality life health results
"Teacher-related factors significantly impact students’ higher-order thinking. However, the mechanisms of how these factors specifically affect students’ higher-order thinking in urban science classrooms are unclear, especially in the absence of empirical research based on multimodal assessment methodologies. This study explored the pathways of these factors by constructing a structural equation model and implemented a functional near-infrared spectroscopy (fNIRS) experiment in an urban science classroom to collect brain data, exploring the mechanisms of students’ higher-order thinking development. The results indicated that both teacher-student relationships and scaffolding instruction could positively influence students’ higher-order thinking, and teacher-student relationships could promote students’ higher-order thinking through the chained mediating role of teacher guidance and scaffolding instruction, while teacher guidance alone had a significant negative effect on students’ higher-order thinking. Furthermore, students’ higher-order thinking during the science class using the scaffolding strategy was related to the increased interpersonal neural synchronization (INS) in the teacher and students’ dorsolateral prefrontal cortex (DLPFC)—a neural basis associated with effective teacher-student interaction. These findings confirm the importance of the teachers’ role during students’ higher-order thinking development and provide evidence from both the structural equation modeling and brain science for the mechanisms behind higher-order thinking development.",teacher related factors significantly impact students higher order thinking however mechanisms factors specifically affect students higher order thinking urban science classrooms unclear especially absence empirical research based multimodal assessment methodologies study explored pathways factors constructing structural equation model implemented functional near infrared spectroscopy fnirs experiment urban science classroom collect brain data exploring mechanisms students higher order thinking development results indicated teacher student relationships scaffolding instruction could positively influence students higher order thinking teacher student relationships could promote students higher order thinking chained mediating role teacher guidance scaffolding instruction teacher guidance alone significant negative effect students higher order thinking furthermore students higher order thinking science class using scaffolding strategy related increased interpersonal neural synchronization ins teacher students dorsolateral prefrontal cortex dlpfc neural basis associated effective teacher student interaction findings confirm importance teachers role students higher order thinking development provide evidence structural equation modeling brain science mechanisms behind higher order thinking development
"According to the behavior of cascading dynamics of complex network, we propose an R-T real-time adjustment strategy. Different from the rigid strategies to improve the robustness of the network by changing network structure, the new proposed strategy is a timely, executable and low-cost strategy that can flexibly reduce the load when overload occurs on nodes. The mechanism of the strategies is illustrated, and two adjustment schemes, degree-based and weight-based schemes for selecting and adopting protection measures on the overload nodes are proposed. Numerical simulation is conducted in Shanghai Metro Network. The simulation results show that the proposed strategies can effectively improve the robustness of the network, and compare to the HW adjustment scheme, HD adjustment scheme have a better performance in enhancing the robustness of the network. It also reveals that taking the cost into consideration, the coverage ratio of the real-time adjustment strategy on nodes is not the greater the better, there is a relative optimal value both in the case of random attack as well as deliberate attack.",according behavior cascading dynamics complex network propose real time adjustment strategy different rigid strategies improve robustness network changing network structure new proposed strategy timely executable low cost strategy flexibly reduce load overload occurs nodes mechanism strategies illustrated two adjustment schemes degree based weight based schemes selecting adopting protection measures overload nodes proposed numerical simulation conducted shanghai metro network simulation results show proposed strategies effectively improve robustness network compare adjustment scheme adjustment scheme better performance enhancing robustness network also reveals taking cost consideration coverage ratio real time adjustment strategy nodes greater better relative optimal value case random attack well deliberate attack
"This study is an attempt to revisit the Sino-Soviet Summit of 1989 with an emphasis on relevant diplomatic activities, leadership views and intelligence assessments. More specifically, it focuses on the challenges and twists in the Sino-Soviet diplomatic course to this achievement. In doing so, the study puts the 1989 summit in the context of regional rivalries and Cold War multilateralism and discusses with examples how Mikhail Gorbachev’s foreign policy doctrine and his visit to China affected these dynamics. The study argues that this climactic event and its aftermath played a significant role in shaping Soviet–Chinese and then Russian–Chinese relations and can be considered the most significant achievement in Gorbachev’s Asian policy in the late 1980s. In this regard, the article concludes that the diplomatic course to the 1989 summit can display the ‘de-ideologisation of interstate relations’ to the benefit of more tangible bilateral issues such as peacebuilding, border security and economy. It also concludes that some aspects of this peacemaking were problematic to the USA and the Asian allies of China and the Soviet Union.",study attempt revisit sino soviet summit emphasis relevant diplomatic activities leadership views intelligence assessments specifically focuses challenges twists sino soviet diplomatic course achievement study puts summit context regional rivalries cold war multilateralism discusses examples mikhail gorbachev foreign policy doctrine visit china affected dynamics study argues climactic event aftermath played significant role shaping soviet chinese russian chinese relations considered significant achievement gorbachev asian policy late regard article concludes diplomatic course summit display ideologisation interstate relations benefit tangible bilateral issues peacebuilding border security economy also concludes aspects peacemaking problematic usa asian allies china soviet union
"This study focuses on the evaluation and promotion strategy of red cultural social influence based on big data of the Internet of Things. With the rapid development of the Internet of Things and big data technology, its application in the field of cultural communication has increasingly become the focus of research. Red culture, as a unique cultural heritage of China, its dissemination and promotion carry great significance in contemporary society. By collecting and analysing 1500 valid questionnaires, combined with descriptive statistics, reliability analysis, correlation analysis and regression analysis, this study deeply discusses how the big data of the Internet of Things can optimise the evaluation method of the social influence of red culture, and puts forward effective improvement strategies. The results show that the application of IoT big data significantly improves the public’s awareness of red culture, and its data diversity, real-time performance and depth of analysis play a key role in improving the accuracy of assessment. The public’s emotional attitude towards and participation in red culture have a significant positive impact on the improvement of awareness. Based on the above findings, this study proposes to further enhance the communication effect and social influence of red culture by enhancing the innovation and interactivity of content, strengthening the personalised recommendation system, and utilising the big data of the Internet of Things for real-time monitoring and in-depth analysis. This study provides a new perspective and empirical evidence for how to use modern information technology to promote the dissemination of traditional culture, and has important theoretical and practical significance for guiding the inheritance and development of red culture.",study focuses evaluation promotion strategy red cultural social influence based big data internet things rapid development internet things big data technology application field cultural communication increasingly become focus research red culture unique cultural heritage china dissemination promotion carry great significance contemporary society collecting analysing valid questionnaires combined descriptive statistics reliability analysis correlation analysis regression analysis study deeply discusses big data internet things optimise evaluation method social influence red culture puts forward effective improvement strategies results show application iot big data significantly improves public awareness red culture data diversity real time performance depth analysis play key role improving accuracy assessment public emotional attitude towards participation red culture significant positive impact improvement awareness based findings study proposes enhance communication effect social influence red culture enhancing innovation interactivity content strengthening personalised recommendation system utilising big data internet things real time monitoring depth analysis study provides new perspective empirical evidence use modern information technology promote dissemination traditional culture important theoretical practical significance guiding inheritance development red culture
"Currently, there is a considerable growth in the number of applications for loans of all forms. This is attributable to increased economic activity and lower interest rates on loans. However, although these loans create substantial money for financial institutions, they also pose a huge risk. The traditional approach to loan applicants includes auditors assessing them based on their prior employment experience; however, this method has the problem of being sluggish and imprecise. This article describes the creation of an intelligent loan evaluation system by inventively using oil consumption data. The system uses a decision tree model to fulfill the aim of constructing a system that can predict the rate of bad debt acquired by loan customers. As a requirement for system building, solutions to the following innovative difficulties have to be discovered. To get substantial results, a feature derivation approach and formula are used to the decision tree model. While significantly increasing review speed, the poor rate falls from 7.8% to 1.6%, while the approved rate rises from less than 17% to more than 45%. At the conclusion of this study, we attempt to enhance the method by using an ensemble approach, resulting in an overall performance improvement of roughly 5%.",currently considerable growth number applications loans forms attributable increased economic activity lower interest rates loans however although loans create substantial money financial institutions also pose huge risk traditional approach loan applicants includes auditors assessing based prior employment experience however method problem sluggish imprecise article describes creation intelligent loan evaluation system inventively using oil consumption data system uses decision tree model fulfill aim constructing system predict rate bad debt acquired loan customers requirement system building solutions following innovative difficulties discovered get substantial results feature derivation approach formula used decision tree model significantly increasing review speed poor rate falls approved rate rises less conclusion study attempt enhance method using ensemble approach resulting overall performance improvement roughly
"Deep mining at kilometer depths has become a trend in coal mining development. As a crucial method for green mining, backfill mining holds significant importance for “under-three” coal mining (mining under structures, water bodies, and railways). To investigate the surface strata movement patterns under conditions of large mining height in kilometer-deep mines, this study analyzed two working conditions—conventional mining and paste backfill mining—at a coal mine in China, based on the probability integral method and numerical simulation. The research found significant differences between the two methods calculated using the probability integral method. Under backfill mining conditions, ground deformation in the inclination and horizontal directions was only about 4.1% of that under conventional mining. Numerical simulation results indicated that during conventional mining, surface displacement showed little change when the working face advanced between 50 m and 200 m. However, a large fracture extending to the surface occurred in the roof strata at an advance distance of 200 m. When the advance reached 350 m–450 m, the rate of surface vertical displacement change slowed, with a maximum vertical subsidence of 1.26 m. In contrast, for paste backfill mining, surface horizontal displacement remained almost unchanged compared to conventional mining, but the vertical surface subsidence was reduced to 0.016 m, accounting for only 1.2% of the subsidence under conventional mining. Furthermore, backfill mining eliminated the zones and periods of intense strata subsidence. Comparing the two conditions shows that backfill mining reduces surface subsidence by dozens of times. This keeps building damage within Grade I (extremely slight damage), requiring no repair. The research results provide valuable references for understanding and controlling surface strata movement patterns in this mine and under similar mining conditions.",deep mining kilometer depths become trend coal mining development crucial method green mining backfill mining holds significant importance three coal mining mining structures water bodies railways investigate surface strata movement patterns conditions large mining height kilometer deep mines study analyzed two working conditions conventional mining paste backfill mining coal mine china based probability integral method numerical simulation research found significant differences two methods calculated using probability integral method backfill mining conditions ground deformation inclination horizontal directions conventional mining numerical simulation results indicated conventional mining surface displacement showed little change working face advanced however large fracture extending surface occurred roof strata advance distance advance reached rate surface vertical displacement change slowed maximum vertical subsidence contrast paste backfill mining surface horizontal displacement remained almost unchanged compared conventional mining vertical surface subsidence reduced accounting subsidence conventional mining furthermore backfill mining eliminated zones periods intense strata subsidence comparing two conditions shows backfill mining reduces surface subsidence dozens times keeps building damage within grade extremely slight damage requiring repair research results provide valuable references understanding controlling surface strata movement patterns mine similar mining conditions
"In order to improve the teaching effect of music appreciation course, this paper combines artificial neural network to build a model for improving music appreciation ability, and studies the two major characteristics of music wave oscillation and attenuation. Due to the existence of the frequency of the music wave, the electromagnetic wave less than its frequency cannot pass through the music wave, which makes the music wave can be used in the protection of high-power microwaves. The incident electromagnetic wave will be absorbed and reflected after entering the music wave. Therefore, a calculation model of the transmission power of the electromagnetic wave entering the music wave is proposed. From the analysis of teaching experiment data, it can be seen that the music appreciation model based on artificial neural network proposed in this paper can effectively improve students’ music appreciation ability.",order improve teaching effect music appreciation course paper combines artificial neural network build model improving music appreciation ability studies two major characteristics music wave oscillation attenuation due existence frequency music wave electromagnetic wave less frequency pass music wave makes music wave used protection high power microwaves incident electromagnetic wave absorbed reflected entering music wave therefore calculation model transmission power electromagnetic wave entering music wave proposed analysis teaching experiment data seen music appreciation model based artificial neural network proposed paper effectively improve students music appreciation ability
"With the rapid development of computer technology, the application of statistical learning and macroeconomic modeling in the study of macroeconomic uncertainty has been paid more and more attention. In this paper, the application of computer technology in macroeconomic uncertainty analysis is deeply discussed. By combining questionnaire design, data collection, model selection, implementation, and validation, it reveals how computer technology has brought unprecedented convenience and accuracy to macroeconomic research. The results show that, especially in economic strategy simulation, the application of computer parallelization technology greatly accelerates the training and verification process of the model. However, this technique also has some limitations when dealing with complex economic relationships, and researchers need to have a deep understanding of economic principles to ensure the accuracy of the model. It provides a valuable reference for economic research and practice, and lays a solid foundation for future research and practice.",rapid development computer technology application statistical learning macroeconomic modeling study macroeconomic uncertainty paid attention paper application computer technology macroeconomic uncertainty analysis deeply discussed combining questionnaire design data collection model selection implementation validation reveals computer technology brought unprecedented convenience accuracy macroeconomic research results show especially economic strategy simulation application computer parallelization technology greatly accelerates training verification process model however technique also limitations dealing complex economic relationships researchers need deep understanding economic principles ensure accuracy model provides valuable reference economic research practice lays solid foundation future research practice
"The rapid progress of Internet technology has accelerated the development of natural language processing technology. To address the current issue of poor adaptability and accuracy in cross-language text matching and translation, firstly, a multi-head attention mechanism and convolutional neural network are introduced. Moreover, a cross-language text matching model based on similarity-based attention convolutional neural network is constructed. Then, visual features are added to the Transformer model to build a real-time machine translation model based on the improved Transformer. The results showed that the accuracy of the proposed text matching model could reach 83.42% when the epoch was 4. The proposed model achieved accuracy rates of 78.96%, 77.55%, and 79.86% in the experiment of matching French, German, and Spanish with English, respectively, while the accuracy rates were 79.16%, 75.03%, and 76.54% in the experiment of matching English with three languages. In addition, as the training data size increased from 1 M to 3 M, the Bilingual Evaluation Understud score of the proposed translation model improved by 36.45%, demonstrating good scalability. In summary, the model constructed in the study not only has high accuracy and adaptability but also demonstrates significant advantages in scalability.",rapid progress internet technology accelerated development natural language processing technology address current issue poor adaptability accuracy cross language text matching translation firstly multi head attention mechanism convolutional neural network introduced moreover cross language text matching model based similarity based attention convolutional neural network constructed visual features added transformer model build real time machine translation model based improved transformer results showed accuracy proposed text matching model could reach epoch proposed model achieved accuracy rates experiment matching french german spanish english respectively accuracy rates experiment matching english three languages addition training data size increased bilingual evaluation understud score proposed translation model improved demonstrating good scalability summary model constructed study high accuracy adaptability also demonstrates significant advantages scalability
"This study is dedicated to the innovative integration of artificial intelligence technology and red cultural education resources in order to promote the modernization transformation and upgrading of red cultural education. The study first realizes the intelligent collection, arrangement, and deep value mining of red cultural education resources by using advanced technologies such as natural language processing and image recognition. On this basis, a dynamically updated and accurately matched red culture education resource base based on artificial intelligence is constructed to meet users’ personalized learning needs. The core innovation of the research lies in the design and development of a red culture education resource system integrating intelligent acquisition, intelligent analysis and intelligent pushing, which breaks through the limitations of traditional education resources in terms of form and content and creates a new mode of red culture education. In addition, the study also skillfully introduces emerging technologies such as virtual reality and augmented reality to build interactive red culture teaching scenes, which greatly enhances the experiential and participatory nature of education, and strengthens the emotional resonance and cognitive depth. In order to verify the application effect of AI-based red culture education resources in actual teaching, this study carefully designed and implemented an evaluation experiment. The experiment went through three stages, systematically collected and rigorously analyzed the data, and the results showed that the interactive red culture teaching scenario had significant teaching effects in practical application, significantly improved the attractiveness and effectiveness of red culture education, and also strongly promoted the modern dissemination and inheritance of red culture. With the advancement of technology and the continuous optimization of teaching content, this new teaching mode is expected to be popularized in a wider range, which will strongly promote the development of red culture education.",study dedicated innovative integration artificial intelligence technology red cultural education resources order promote modernization transformation upgrading red cultural education study first realizes intelligent collection arrangement deep value mining red cultural education resources using advanced technologies natural language processing image recognition basis dynamically updated accurately matched red culture education resource base based artificial intelligence constructed meet users personalized learning needs core innovation research lies design development red culture education resource system integrating intelligent acquisition intelligent analysis intelligent pushing breaks limitations traditional education resources terms form content creates new mode red culture education addition study also skillfully introduces emerging technologies virtual reality augmented reality build interactive red culture teaching scenes greatly enhances experiential participatory nature education strengthens emotional resonance cognitive depth order verify application effect based red culture education resources actual teaching study carefully designed implemented evaluation experiment experiment went three stages systematically collected rigorously analyzed data results showed interactive red culture teaching scenario significant teaching effects practical application significantly improved attractiveness effectiveness red culture education also strongly promoted modern dissemination inheritance red culture advancement technology continuous optimization teaching content new teaching mode expected popularized wider range strongly promote development red culture education
"In this paper, we take the organizational quality-specific immunity of digital technology start-ups as the starting point, and comprehensively use the martin system-TOPSIS research method to evaluate the organizational quality-specific immunity status of digital technology start-ups. The empirical results show that the martin system-TOPSIS method can effectively solve the evaluation and decision-making problem of organizational quality-specific immunity of digital technology start-ups, and the interval number multi-attribute evaluation and decision-making method based on martin system-TOPSIS is effective and feasible in solving the problem of organizational quality-specific immunity evaluation and decision-making of digital technology start-ups.",paper take organizational quality specific immunity digital technology start ups starting point comprehensively use martin system topsis research method evaluate organizational quality specific immunity status digital technology start ups empirical results show martin system topsis method effectively solve evaluation decision making problem organizational quality specific immunity digital technology start ups interval number multi attribute evaluation decision making method based martin system topsis effective feasible solving problem organizational quality specific immunity evaluation decision making digital technology start ups
"The practice and application of geographic information systems have a positive promoting effect on the development of digital cities. It can provides high-quality geographic information services, which has profound research significance. Therefore, the study improves the Mask R-CNN algorithm and combines the improved algorithm with 3D modeling to construct a new 3D modeling model for multi-source data fusion. Finally, the 3D modeling algorithm is integrated with the geographic information system, and a novel 3D modeling geographic information system model based on multi-source data fusion is proposed. In the street image segmentation task, the improved algorithm achieved an accuracy of 98.97% and a loss value of 1.08, demonstrating good stability and robustness. The proposed model can not only accurately detect and segment target images of streets, but also finely segment the boundary areas between objects. In practical applications, the research model has achieved the level I modeling accuracy standard in the 3D modeling tasks of multiple typical urban blocks, with the error controlled within a reasonable range. It has good engineering practicability and promotion value. In summary, the research proposes a new three-dimensional modeling geographic information system model based on multi-source data fusion, which has good target image recognition and segmentation capabilities, providing useful suggestions and guidance for constructing digital cities.",practice application geographic information systems positive promoting effect development digital cities provides high quality geographic information services profound research significance therefore study improves mask cnn algorithm combines improved algorithm modeling construct new modeling model multi source data fusion finally modeling algorithm integrated geographic information system novel modeling geographic information system model based multi source data fusion proposed street image segmentation task improved algorithm achieved accuracy loss value demonstrating good stability robustness proposed model accurately detect segment target images streets also finely segment boundary areas objects practical applications research model achieved level modeling accuracy standard modeling tasks multiple typical urban blocks error controlled within reasonable range good engineering practicability promotion value summary research proposes new three dimensional modeling geographic information system model based multi source data fusion good target image recognition segmentation capabilities providing useful suggestions guidance constructing digital cities
"The take-out O2O (online-to-offline) industry has seamlessly integrated into various facets of consumer life, leveraging its distinctive strengths. Within this landscape, the information disseminated through take-out O2O platform reviews has emerged as a pivotal factor influencing consumer decisions when opting for food delivery services. This study centers on scrutinizing online reviews sourced from take-out O2O platforms. It establishes a research framework wherein online reviews function as the independent variable, consumer purchase intentions as the dependent variable, perceived risk as the mediator impacting the influence of online reviews on purchase intent, and consumer trust propensity as the moderator in the relationship between online reviews and perceived risk. Hypotheses are formulated and rigorously examined through empirical methodologies to ascertain the validity of the research framework and its underpinning assumptions. The findings of this study affirm that online reviews on take-out O2O platforms, encompassing criteria such as review quantity, quality, valence, timeliness, and visual cues, wield a marked positive influence on consumer purchase intentions. Furthermore, online reviews exhibit a discernible negative impact on perceived risk. Notably, perceived risk plays a pivotal role as a partial mediator in the pathway through which online reviews influence consumer purchase intentions. Moreover, the study elucidates that consumer trust propensity operates as a vital moderator, particularly affecting how two key dimensions of online reviews—review valence and quality—influence perceived risk. This moderating effect manifests as a mechanism that introduces a mitigating influence to a certain extent.",take online offline industry seamlessly integrated various facets consumer life leveraging distinctive strengths within landscape information disseminated take platform reviews emerged pivotal factor influencing consumer decisions opting food delivery services study centers scrutinizing online reviews sourced take platforms establishes research framework wherein online reviews function independent variable consumer purchase intentions dependent variable perceived risk mediator impacting influence online reviews purchase intent consumer trust propensity moderator relationship online reviews perceived risk hypotheses formulated rigorously examined empirical methodologies ascertain validity research framework underpinning assumptions findings study affirm online reviews take platforms encompassing criteria review quantity quality valence timeliness visual cues wield marked positive influence consumer purchase intentions furthermore online reviews exhibit discernible negative impact perceived risk notably perceived risk plays pivotal role partial mediator pathway online reviews influence consumer purchase intentions moreover study elucidates consumer trust propensity operates vital moderator particularly affecting two key dimensions online reviews review valence quality influence perceived risk moderating effect manifests mechanism introduces mitigating influence certain extent
"In the era of digital transformation, libraries are faced with the challenge of finding a balance between massive resources and personalized user needs. Existing recommendation models often cannot handle complex user-item interaction networks and are difficult to fully explore the deep connection between user preferences and books. To address this problem, the study proposes a novel intelligent recommendation model that uniquely integrates the structural representation ability of Graph Neural Networks with the dynamic feature extraction ability of attention mechanisms. This innovative combination enables the simultaneous capture of complex user-item topological relationships and adaptive importance weighting of heterogeneous features, which has not been extensively explored in library recommendation systems. The model uses a multi-head attention mechanism for adaptive feature selection and combines residual connections to ensure the effective flow of information in the deep network. Experimental results show that on the Amazon book dataset, the proposed fusion model outperforms traditional deep learning methods and improves the recommendation accuracy by more than 15%. In the actual deployment of a university library, the recommendation coverage of the fusion model reached 89.2%, the user satisfaction score was 4.52 points (out of 5 points), and the recommendation accuracy in multiple subject areas remained above 85%. These results show that the fusion model effectively reveals deep relationships, solves the challenges of modeling dynamic user interests and diverse book features, and provides reliable technical support for the intelligent and personalized reading recommendation system of modern libraries.",digital transformation libraries faced challenge finding balance massive resources personalized user needs existing recommendation models often handle complex user item interaction networks difficult fully explore deep connection user preferences books address problem study proposes novel intelligent recommendation model uniquely integrates structural representation ability graph neural networks dynamic feature extraction ability attention mechanisms innovative combination enables simultaneous capture complex user item topological relationships adaptive importance weighting heterogeneous features extensively explored library recommendation systems model uses multi head attention mechanism adaptive feature selection combines residual connections ensure effective flow information deep network experimental results show amazon book dataset proposed fusion model outperforms traditional deep learning methods improves recommendation accuracy actual deployment university library recommendation coverage fusion model reached user satisfaction score points points recommendation accuracy multiple subject areas remained results show fusion model effectively reveals deep relationships solves challenges modeling dynamic user interests diverse book features provides reliable technical support intelligent personalized reading recommendation system modern libraries
"In order to explore the convergence effect of real estate consumption, this paper combines the hedonic price model and price index to analyze the convergence effect of real estate consumption and combines the regional economic structure to analyze the adjustment speed of housing prices in different regions. This paper finds that house prices tend to moderate as the distance from the inner ring area increases. Further, this paper adopts the panel unit root test, and the panel unit root test also believes that there is price convergence in the regional housing market. Moreover, this paper adopts two methods: separate regression and seemingly irrelevant regression to estimate the adjustment speed, and the results obtained by the seemingly irrelevant regression method are more effective and reliable. In addition, this paper proposes a convergence effect analysis model of real estate consumption. Through the simulation analysis, it can be seen that the regional economic structure and the convergence effect analysis model of real estate consumption proposed in this paper has a good effect.",order explore convergence effect real estate consumption paper combines hedonic price model price index analyze convergence effect real estate consumption combines regional economic structure analyze adjustment speed housing prices different regions paper finds house prices tend moderate distance inner ring area increases paper adopts panel unit root test panel unit root test also believes price convergence regional housing market moreover paper adopts two methods separate regression seemingly irrelevant regression estimate adjustment speed results obtained seemingly irrelevant regression method effective reliable addition paper proposes convergence effect analysis model real estate consumption simulation analysis seen regional economic structure convergence effect analysis model real estate consumption proposed paper good effect
"With the increasing popularity of artificial intelligence technology in the field of education, this study aims to explore the new model and new method of combining artificial intelligence and computer teaching, and evaluate its application effect in educational practice. Through questionnaires to teachers and students, the study collected information on the acceptance and feedback of AI teaching models. At the same time, through a comparative analysis of the teaching mode integrated with artificial intelligence and the traditional teaching mode, the research reveals the former’s significant advantages in improving learning effectiveness, increasing student satisfaction and promoting learning progress. In addition, this study also involves the construction, evaluation and optimization of artificial intelligence teaching models, and further discusses the effective integration of technology in the field of education. The results show that the teaching mode combined with artificial intelligence not only improves the learning efficiency but also enhances the learning experience of students. This study provides an important theoretical and practical reference for guiding the development and application of educational technology in the future.",increasing popularity artificial intelligence technology field education study aims explore new model new method combining artificial intelligence computer teaching evaluate application effect educational practice questionnaires teachers students study collected information acceptance feedback teaching models time comparative analysis teaching mode integrated artificial intelligence traditional teaching mode research reveals former significant advantages improving learning effectiveness increasing student satisfaction promoting learning progress addition study also involves construction evaluation optimization artificial intelligence teaching models discusses effective integration technology field education results show teaching mode combined artificial intelligence improves learning efficiency also enhances learning experience students study provides important theoretical practical reference guiding development application educational technology future
"The purpose of this paper is to explore the use of big data analysis methods and techniques to collect, integrate, process, analyze and visualize the training data and competition data of track and field athletes, and extract valuable information and knowledge from them, so as to provide a scientific basis and guidance for the assessment and improvement of the training effect and competitive level of track and field athletes. In this paper, 20 athletes from the Chinese national track and field team are selected as the experimental subjects, divided into two phases: baseline test and intervention test, collecting and analyzing the data of athletes’ physical fitness, technique, training and competition, respectively, and constructing prediction models using machine learning algorithms to assess and optimize the training effect and competitive level of athletes. The experimental results show that the assessment model of training effect and competitive level of track and field athletes based on big data analysis has a significant effect on the physical fitness, technique, training, and competition of the experimental subjects, and all of them have significant differences compared with the baseline test. This study confirms the validity and applicability of the training effect and competitive level assessment model of track and field athletes based on big data analysis, which provides scientific data support and guidance for the training and competition of track and field athletes, and contributes to the scientific development and level improvement of track and field sports.",purpose paper explore use big data analysis methods techniques collect integrate process analyze visualize training data competition data track field athletes extract valuable information knowledge provide scientific basis guidance assessment improvement training effect competitive level track field athletes paper athletes chinese national track field team selected experimental subjects divided two phases baseline test intervention test collecting analyzing data athletes physical fitness technique training competition respectively constructing prediction models using machine learning algorithms assess optimize training effect competitive level athletes experimental results show assessment model training effect competitive level track field athletes based big data analysis significant effect physical fitness technique training competition experimental subjects significant differences compared baseline test study confirms validity applicability training effect competitive level assessment model track field athletes based big data analysis provides scientific data support guidance training competition track field athletes contributes scientific development level improvement track field sports
"David Bissell encourages geographers to think with brain fog. In response to this, recalling work in ‘mental health geographies’ and specifically ‘geographies of delusion’, I engage with Bissell's call to consider potential empathetic solidarities with people who experience disabling neurodiversity. I suggest that geographers might listen carefully to the voices of people who dwell in fog, not to romanticise their unintentionally dissonant geographies, but instead to seek co-produced routes through which it might be possible to help moor disrupted senses of self.",david bissell encourages geographers think brain fog response recalling work mental health geographies specifically geographies delusion engage bissell call consider potential empathetic solidarities people experience disabling neurodiversity suggest geographers might listen carefully voices people dwell fog romanticise unintentionally dissonant geographies instead seek produced routes might possible help moor disrupted senses self
"With the speedy prosperity of artificial intelligence and sensor technology, the application of action recognition in football is becoming increasingly widespread. However, due to the rapid changes, complex dynamics, and diverse characteristics of football movements, traditional recognition methods face significant challenges in terms of real-time performance and accuracy. Based on this background, a football recognition model combining network wearable sensors and improved support vector machine classification algorithm is proposed. Firstly, by integrating the attitude data of accelerometers and gyroscopes, real-time dynamic features such as pitch angle, roll angle, and yaw angle are calculated, and principal component analysis is used for dimensionality reduction processing. Subsequently, the support vector machine model is optimized based on Gaussian kernel function and dynamic weighting strategy to improve classification accuracy and stability. Finally, a football action recognition model is constructed by combining wearable sensors with an improved support vector machine classification algorithm. The experiment outcomes show that the improved support vector machine algorithm achieves an action recognition accuracy of 93.8%, a recall rate of 92.5%, an F1 value of 0.92, and an inference time of 10.2 ms, which are significantly better than the comparative algorithm. In practical applications, the recognition model built has an accuracy rate of over 90% in recognizing four types of actions: standing, running, passing, and shooting, with an average recognition time as low as 9.4 ms. The research provides an efficient solution for intelligent football action recognition technology and lays the foundation for the practical application of multi-modal data fusion.",speedy prosperity artificial intelligence sensor technology application action recognition football becoming increasingly widespread however due rapid changes complex dynamics diverse characteristics football movements traditional recognition methods face significant challenges terms real time performance accuracy based background football recognition model combining network wearable sensors improved support vector machine classification algorithm proposed firstly integrating attitude data accelerometers gyroscopes real time dynamic features pitch angle roll angle yaw angle calculated principal component analysis used dimensionality reduction processing subsequently support vector machine model optimized based gaussian kernel function dynamic weighting strategy improve classification accuracy stability finally football action recognition model constructed combining wearable sensors improved support vector machine classification algorithm experiment outcomes show improved support vector machine algorithm achieves action recognition accuracy recall rate value inference time significantly better comparative algorithm practical applications recognition model built accuracy rate recognizing four types actions standing running passing shooting average recognition time low research provides efficient solution intelligent football action recognition technology lays foundation practical application multi modal data fusion
"Fashion design is a comprehensive art that combines creativity, aesthetics, technology, and other aspects. By designing and combining elements such as clothing styles, colors, and image styles, fashion works with unique styles and practical functions are created. However, the existing clothing design methods all have defects such as poor image accuracy and long generation cycle. Therefore, this study optimizes the Cycle Consistency-Generative Adversarial Network by introducing Least Squares and Efficient Channel Attention Network, and constructs a brand-new intelligent fashion design image generation model based on this. The results show that this model achieves an accuracy of 97.1%, an average success rate of 96.8%, and a 37 dB image signal-to-noise ratio, significantly outperforming the comparison models. Meanwhile, when generating a substantial volume of clothing designs, the model maintained a peak response time of merely 229 ms and a minimal loss rate of 0.17%. And it achieved a score of 97.3 in a questionnaire based on the esthetic ratings of 200 users. In conclusion, the proposed model not only generates higher-quality, more detailed images but also ensures high precision and success rates. The clothing designs are widely accepted by the public, meet modern esthetic standards, and effectively address the challenges of existing fashion design methods, and is conducive to designing higher-quality fashion garments.",fashion design comprehensive art combines creativity aesthetics technology aspects designing combining elements clothing styles colors image styles fashion works unique styles practical functions created however existing clothing design methods defects poor image accuracy long generation cycle therefore study optimizes cycle consistency generative adversarial network introducing least squares efficient channel attention network constructs brand new intelligent fashion design image generation model based results show model achieves accuracy average success rate image signal noise ratio significantly outperforming comparison models meanwhile generating substantial volume clothing designs model maintained peak response time merely minimal loss rate achieved score questionnaire based esthetic ratings users conclusion proposed model generates higher quality detailed images also ensures high precision success rates clothing designs widely accepted public meet modern esthetic standards effectively address challenges existing fashion design methods conducive designing higher quality fashion garments
"As the earliest form of writing and historical cultural heritage in China, oracle bone inscriptions carry immeasurable density of historical information and potential for academic research. However, due to the age and preservation environment, the quality of many oracle bone images is severely impaired, resulting in problems such as text edge wear, loss of details, and blurred handwriting, which greatly limits the ability of scholars to accurately interpret the contents of the oracle bones and explore them in depth. In this paper, we propose a generative adversarial network (GAN)-based image enhancement and calibration system, which realizes multi-level information recovery and geometric correction from micro to macro by applying an advanced deep learning model to capture the minute features, delicate textures and original layouts of oracle bone images. The model in this paper consists of two main parts, that is, generator network G and discriminator network D. The task of generator G is to generate images consistent with the real data distribution based on random noise variables and possibly additional condition information (e.g., category labels, attributes, etc.), which should mimic or satisfy as much as possible the features of the real image under the given conditions. The discriminator D, on the other hand, is responsible for receiving the input real image or the fake image generated by the generator and outputs a probability value that reflects the probability that it considers the image to be a real sample, that is, the truthfulness or trustworthiness of the image. The model in this paper achieves the functions of image quality assessment and authenticity judgment, image recovery and optimization, image enhancement and proofreading by means of adversarial training. Experimental evaluations are carried out on two representative oracle image datasets, OBI-100 and OBI-300, and the effectiveness and superiority of this paper’s method in improving the clarity and readability of oracle images, as well as accurately recognizing oracle characters and extracting oracle information, are verified by comparing it with other image enhancement and reweighting methods. The effectiveness and superiority of the method is verified. The method of this paper provides a new technical means for the research and inheritance of oracle bone inscriptions.",earliest form writing historical cultural heritage china oracle bone inscriptions carry immeasurable density historical information potential academic research however due age preservation environment quality many oracle bone images severely impaired resulting problems text edge wear loss details blurred handwriting greatly limits ability scholars accurately interpret contents oracle bones explore depth paper propose generative adversarial network gan based image enhancement calibration system realizes multi level information recovery geometric correction micro macro applying advanced deep learning model capture minute features delicate textures original layouts oracle bone images model paper consists two main parts generator network discriminator network task generator generate images consistent real data distribution based random noise variables possibly additional condition information category labels attributes etc mimic satisfy much possible features real image given conditions discriminator hand responsible receiving input real image fake image generated generator outputs probability value reflects probability considers image real sample truthfulness trustworthiness image model paper achieves functions image quality assessment authenticity judgment image recovery optimization image enhancement proofreading means adversarial training experimental evaluations carried two representative oracle image datasets obi obi effectiveness superiority paper method improving clarity readability oracle images well accurately recognizing oracle characters extracting oracle information verified comparing image enhancement reweighting methods effectiveness superiority method verified method paper provides new technical means research inheritance oracle bone inscriptions
"The MSE surveillance system provides the ability to detect changes in the efficiency of the drilling system. MSE can be expressed mathematically in terms of controllable parameters Weight on Bit (WOB), Rate of Penetration (ROP), Revolutions per Minute (RPM) etc. Once the optimum WOB was determined, the correlation with Torque and Penetration per Revolution was used to determine optimum values for those parameters for a given drilling situation. The MSE surveillance systerm provides the ability to detect changes in the efficiency of the drilling system, more or less continuously. The use of MSE surveillance is a key feature of well planning and operational practice, MSE analysis has resulted in redesigned in areas as diverse as well control practices, bit selection, BHA and ECD design, makeup torque, directional target sizing, and motor differential ratings. Through lots investigation and research, my research group has redesigned the MSE calculation modes and developed a MSE software system and applied in some wells and developed a software, whose ROP was increased by an average of 40% on several pilot wells in Changning area.",mse surveillance system provides ability detect changes efficiency drilling system mse expressed mathematically terms controllable parameters weight bit wob rate penetration rop revolutions per minute rpm etc optimum wob determined correlation torque penetration per revolution used determine optimum values parameters given drilling situation mse surveillance systerm provides ability detect changes efficiency drilling system less continuously use mse surveillance key feature well planning operational practice mse analysis resulted redesigned areas diverse well control practices bit selection bha ecd design makeup torque directional target sizing motor differential ratings lots investigation research research group redesigned mse calculation modes developed mse software system applied wells developed software whose rop increased average several pilot wells changning area
"In recent years, the task of generating captions for videos has become a prominent research focus, with its main challenge being how to effectively capture essential semantic elements – such as objects, actions, and their spatial-temporal relationships – from abundant and redundant visual content. To address this challenge, earlier methods generally concentrate on either extracting representative clips across multiple frames (global level) or locating salient areas within single frames (local level). Many existing methods tend to ignore the fundamental hierarchical organization of videos, where identifying representative frames should come before locating informative regions. To tackle this limitation, we propose G2L, a hierarchical attention framework that (1) selects salient clips &amp; frames via differentiable Gumbel Top-K sampling and (2) refines region-level context for caption generation. Extensive experiments conducted on the widely adopted benchmarks MSVD and MSR-VTT confirm that our method achieves notable improvements over existing state-of-the-art approaches. Ablations confirm that the global-to-local cascade and dual-branch optimization jointly account for the gain.",recent years task generating captions videos become prominent research focus main challenge effectively capture essential semantic elements objects actions spatial temporal relationships abundant redundant visual content address challenge earlier methods generally concentrate either extracting representative clips across multiple frames global level locating salient areas within single frames local level many existing methods tend ignore fundamental hierarchical organization videos identifying representative frames come locating informative regions tackle limitation propose hierarchical attention framework selects salient clips amp frames via differentiable gumbel top sampling refines region level context caption generation extensive experiments conducted widely adopted benchmarks msvd msr vtt confirm method achieves notable improvements existing state art approaches ablations confirm global local cascade dual branch optimization jointly account gain
"The integration of biometric data and sensory stimuli is reshaping architectural computing, expanding design possibilities beyond traditional visual and geometric parameters. Through ‘Hydnum’, named after a genus of fungi known for its distinctive geometric mycelial networks, we demonstrate the integration of real-time biodata with virtual and physical elements to create dynamic spatial experiences. Our computational framework synthesizes olfactory, auditory, and visual elements through a sophisticated scent delivery mechanism, bio-inspired geometric structures, and real-time audio synthesis, all modulated by users’ physiological responses mapped to Russell’s Circumplex Model of Affect. The system leverages biosignal processing and transformations to engage underutilized senses in architectural design, while the project’s modular scent system and cyber-physical artifacts create responsive environments. Through empirical testing and systematic integration of multiple sensory channels, this research advances biodigital architecture by providing a comprehensive framework for spaces that actively engage with users’ emotional and physiological states.",integration biometric data sensory stimuli reshaping architectural computing expanding design possibilities beyond traditional visual geometric parameters hydnum named genus fungi known distinctive geometric mycelial networks demonstrate integration real time biodata virtual physical elements create dynamic spatial experiences computational framework synthesizes olfactory auditory visual elements sophisticated scent delivery mechanism bio inspired geometric structures real time audio synthesis modulated users physiological responses mapped russell circumplex model affect system leverages biosignal processing transformations engage underutilized senses architectural design project modular scent system cyber physical artifacts create responsive environments empirical testing systematic integration multiple sensory channels research advances biodigital architecture providing comprehensive framework spaces actively engage users emotional physiological states
"In power distribution networks, the correct identification of network topology and parameters is critical in maintaining efficient operating conditions. Operative efficiency and reliability are major goals in such networks. Satisfying these conditions necessitates a clear understanding of network parameters and topology, particularly in low-voltage power distribution networks, which are typically characterized by complicated patterns and fluctuating load demands. This research presents a novel approach utilizing the Voltage Regulator Weighted Topology Recurrent Neural Network (VR-WT-RNN) for joint identification of parameters and topology in low-voltage distribution networks. The Low-Voltage Network Dataset is used to capture time-series data from five nodes in a low-voltage electrical distribution network, and Z-score normalization was used for data preprocessing. The VR-WT-RNN approach combines weighted topology considerations within a recurrent neural network structure, enhancing its ability to accurately model and predict sophisticated network behaviors. Using records of voltage regulator settings, load profiles, and network configurations, the model can predict both network parameters and topology. The performance of the VR-WT-RNN model shows an effectiveness of up to 5% accuracy improvement for Transformer 2, accompanied by significant gains in NMI and ARI over traditional methods. Further, the responsiveness of the model towards dynamic variations of network conditions marks its usability within changing operating conditions.",power distribution networks correct identification network topology parameters critical maintaining efficient operating conditions operative efficiency reliability major goals networks satisfying conditions necessitates clear understanding network parameters topology particularly low voltage power distribution networks typically characterized complicated patterns fluctuating load demands research presents novel approach utilizing voltage regulator weighted topology recurrent neural network rnn joint identification parameters topology low voltage distribution networks low voltage network dataset used capture time series data five nodes low voltage electrical distribution network score normalization used data preprocessing rnn approach combines weighted topology considerations within recurrent neural network structure enhancing ability accurately model predict sophisticated network behaviors using records voltage regulator settings load profiles network configurations model predict network parameters topology performance rnn model shows effectiveness accuracy improvement transformer accompanied significant gains nmi ari traditional methods responsiveness model towards dynamic variations network conditions marks usability within changing operating conditions
"Multimodal sentiment analysis is a well-known field in AI that integrates text, video, and audio to gain a better understanding of people’s emotions. However, this field still faces two major challenges: first, it is difficult to effectively eliminate noise interference in multimodal data; Second, the existing research focuses on the fusion mechanism between modes, ignoring the similarity and heterogeneity between modes, which leads to the deviation of emotional analysis. Therefore, this paper proposes a deep fusion model based on Transformer. We define the polarity vector (PV) and the intensity vector (SV) based on the emotion analysis strategy of PS-Mixer to judge the polarity (positive, negative, or neutral) and intensity (0–3 range) of emotions, respectively. PV fuses text and video features, and SV fuses text and audio features and improves the ability of feature expression by introducing a cross-fusion strategy. Transformer architecture enhances the generalization performance and large-scale data processing ability of the model. The experimental results show that the performance of this model on MOSI and MOSEI datasets is better than the existing multi-modal sentiment analysis methods, which provides a new idea for research in this field.",multimodal sentiment analysis well known field integrates text video audio gain better understanding people emotions however field still faces two major challenges first difficult effectively eliminate noise interference multimodal data second existing research focuses fusion mechanism modes ignoring similarity heterogeneity modes leads deviation emotional analysis therefore paper proposes deep fusion model based transformer define polarity vector intensity vector based emotion analysis strategy mixer judge polarity positive negative neutral intensity range emotions respectively fuses text video features fuses text audio features improves ability feature expression introducing cross fusion strategy transformer architecture enhances generalization performance large scale data processing ability model experimental results show performance model mosi mosei datasets better existing multi modal sentiment analysis methods provides new idea research field
"Accurate medical image segmentation models can efficiently assist healthcare professionals in diagnosis. Segmentation methods based on Convolutional Neural Networks (CNNs) are effective in extracting local features. However, due to their inherently limited receptive fields, they exhibit shortcomings in integrating global dependencies and extracting multi-scale features. This limitation of CNNs has prompted researchers to explore Transformer-based approaches. This shift is driven by the unique self-attention mechanism of Transformers, which effectively models global dependencies and facilitates multi-scale feature extraction. In this study, we designed a medical image segmentation model called FRISFormer, based on a U-shaped architecture. FRISFormer is entirely built on the Transformer architecture and possesses the characteristic of being effectively trainable without the need for pre-trained models. Specifically, the innovation of FRISFormer is primarily reflected in two key aspects: (1) FRISFormer refines the features extracted by the Efficient Self-Attention (ESA) module through a Feature Refinement Feed-forward Network (FRFN), further achieving deep deconstruction and enhancement of features. (2) FRISFormer replaces the classic skip connections with a ReMixed Transformer Context Bridge, effectively promoting the correlation between global dependencies and local context. This study tested FRISFormer on the multi-organ segmentation dataset (Synapse) and the skin lesion segmentation dataset (ISIC 2018). On the Synapse dataset, FRISFormer improved the test metric by 0.50, while on the ISIC dataset, the test metric improved by 0.23. The experimental results fully demonstrate the effectiveness and superiority of FRISFormer in feature representation and segmentation accuracy.",accurate medical image segmentation models efficiently assist healthcare professionals diagnosis segmentation methods based convolutional neural networks cnns effective extracting local features however due inherently limited receptive fields exhibit shortcomings integrating global dependencies extracting multi scale features limitation cnns prompted researchers explore transformer based approaches shift driven unique self attention mechanism transformers effectively models global dependencies facilitates multi scale feature extraction study designed medical image segmentation model called frisformer based shaped architecture frisformer entirely built transformer architecture possesses characteristic effectively trainable without need pre trained models specifically innovation frisformer primarily reflected two key aspects frisformer refines features extracted efficient self attention module feature refinement feed forward network frfn achieving deep deconstruction enhancement features frisformer replaces classic skip connections remixed transformer context bridge effectively promoting correlation global dependencies local context study tested frisformer multi organ segmentation dataset synapse skin lesion segmentation dataset isic synapse dataset frisformer improved test metric isic dataset test metric improved experimental results fully demonstrate effectiveness superiority frisformer feature representation segmentation accuracy
"In order to improve the privacy protection effect, this paper combines the intelligent data recognition algorithm to research the privacy protection algorithm, constructs an intelligent privacy protection model, and uses the iterative dynamic programming algorithm to optimally group the original histogram structure. Moreover, this paper uses an optimized exponential mechanism to randomly adjust the optimal grouping structure to protect group privacy. In addition, this paper finds the mean of each group, and uses the Laplacian mechanism to inject random noise into each group, and analyzes the differential privacy histogram publishing algorithm under the optimized structure. Finally, this paper obtains a privacy protection algorithm model suitable for big data systems on this basis. The simulation study shows that the privacy protection algorithm model based on intelligent data recognition constructed in this paper can play an important role in privacy protection in the context of big data.",order improve privacy protection effect paper combines intelligent data recognition algorithm research privacy protection algorithm constructs intelligent privacy protection model uses iterative dynamic programming algorithm optimally group original histogram structure moreover paper uses optimized exponential mechanism randomly adjust optimal grouping structure protect group privacy addition paper finds mean group uses laplacian mechanism inject random noise group analyzes differential privacy histogram publishing algorithm optimized structure finally paper obtains privacy protection algorithm model suitable big data systems basis simulation study shows privacy protection algorithm model based intelligent data recognition constructed paper play important role privacy protection context big data
"With the rapid development of big data and smart education, the application of diagnostic learning analysis technology has become an important trend in the field of education. The purpose of this study is to explore the application effect of this technology in education, especially in the aspects of personalized learning and teaching efficiency improvement. The attitudes of educators and learners were collected through questionnaires, the practical application effect of the technology was verified by experimental design, and the effectiveness of the technology was analyzed deeply through model construction and evaluation. The results show that diagnostic learning analytics can significantly improve teaching quality and learning outcomes, despite data privacy and technology acceptance challenges. This study reveals the application potential of this technology in the field of education, and provides a valuable reference for future research in related fields. Despite the limitations of sample representation and long-term effect analysis, this study still has important guiding significance for the future development of educational technology.",rapid development big data smart education application diagnostic learning analysis technology become important trend field education purpose study explore application effect technology education especially aspects personalized learning teaching efficiency improvement attitudes educators learners collected questionnaires practical application effect technology verified experimental design effectiveness technology analyzed deeply model construction evaluation results show diagnostic learning analytics significantly improve teaching quality learning outcomes despite data privacy technology acceptance challenges study reveals application potential technology field education provides valuable reference future research related fields despite limitations sample representation long term effect analysis study still important guiding significance future development educational technology
"The fractional order accumulation calculation process in traditional grey system theory is too complex and has poor effectiveness in predicting carbon dioxide emissions in practical applications. A new non-equidistant grey model is proposed to address the above issues, which introduces a C-type fractional order cumulative derivative to simplify the calculation of fractional order accumulation. At the same time, introducing a dynamic fractional order adjustment strategy can adaptively optimize the fractional order value based on data characteristics, enhancing the flexibility and accuracy of the model in predicting carbon dioxide emissions. Thus, the C-type fractional order cumulative non-uniform grey model (1,1) is obtained. The research results show that compared with existing advanced non-equidistant GM (1,1) models based on integration, non-equidistant grey prediction models based on residual correction, and hybrid multi-scale machine learning models, the research method achieves higher accuracy while maintaining high computational efficiency. Its average absolute percentage error in performance testing is the lowest at only 1.987%, significantly better than other models. In practical applications, the research method has an average absolute percentage error of less than 10% in both foreign and domestic data prediction. Overall, this model can be applied in practical prediction of carbon dioxide emissions and provide data reference for achieving the goal of “carbon neutrality.”",fractional order accumulation calculation process traditional grey system theory complex poor effectiveness predicting carbon dioxide emissions practical applications new non equidistant grey model proposed address issues introduces type fractional order cumulative derivative simplify calculation fractional order accumulation time introducing dynamic fractional order adjustment strategy adaptively optimize fractional order value based data characteristics enhancing flexibility accuracy model predicting carbon dioxide emissions thus type fractional order cumulative non uniform grey model obtained research results show compared existing advanced non equidistant models based integration non equidistant grey prediction models based residual correction hybrid multi scale machine learning models research method achieves higher accuracy maintaining high computational efficiency average absolute percentage error performance testing lowest significantly better models practical applications research method average absolute percentage error less foreign domestic data prediction overall model applied practical prediction carbon dioxide emissions provide data reference achieving goal carbon neutrality
"Based on Mie scattering theory, an electromagnetic (EM) wave loss model of spherical wave-absorbing aggregates with ferrites uniformly distributed in ceramsite was established. In addition, the Mie series solution for calculating the EM scattering of homogeneous dielectric spheres was derived. Through MATLAB programming, the attenuation coefficient, scattering coefficient, and absorption coefficient of functional aggregates were calculated, and the effects of wave absorber content, particle size, and frequency of the incident wave on the aggregate EM loss performance were studied. These results showed improved EM wave loss capacity of ferrite-containing ceramsite aggregates when the amount of the wave absorber was 20%, the particle radius was between 5 and 10 mm, and the frequency of the incident wave was between 8 and 20 GHz.",based mie scattering theory electromagnetic wave loss model spherical wave absorbing aggregates ferrites uniformly distributed ceramsite established addition mie series solution calculating scattering homogeneous dielectric spheres derived matlab programming attenuation coefficient scattering coefficient absorption coefficient functional aggregates calculated effects wave absorber content particle size frequency incident wave aggregate loss performance studied results showed improved wave loss capacity ferrite containing ceramsite aggregates amount wave absorber particle radius frequency incident wave ghz
"To explore the effective training strategies of college students’ basketball special ability, this paper conducts a comparative study of functional training and core strength training on college students’ basketball special ability training through intelligent methods. Moreover, this study integrates monocular visual SLAM and LSD-SLAM algorithms into the training method through intelligent methods. Monocular visual SLAM is used to achieve 3D point cloud reconstruction based on LSD-SLAM algorithm, combined with basketball training features to construct an intelligent recognition system. The LSD-SLAM algorithm is used in training to accurately track athlete movements, provide depth information, optimize posture estimation, and assist in functional training and core strength training. The two work together to enhance college basketball specific abilities. The results show that both functional training and core strength training have certain effects on the cultivation of the college students’ basketball special ability, so it is necessary to combine the two to improve the college students’ basketball special ability.",explore effective training strategies college students basketball special ability paper conducts comparative study functional training core strength training college students basketball special ability training intelligent methods moreover study integrates monocular visual slam lsd slam algorithms training method intelligent methods monocular visual slam used achieve point cloud reconstruction based lsd slam algorithm combined basketball training features construct intelligent recognition system lsd slam algorithm used training accurately track athlete movements provide depth information optimize posture estimation assist functional training core strength training two work together enhance college basketball specific abilities results show functional training core strength training certain effects cultivation college students basketball special ability necessary combine two improve college students basketball special ability
"In recent years, the network freight industry in China has been developing rapidly, and has become an important force in increasing efficiency in logistics, but there is still a lack of effective portrait methods for network freight enterprises in the society. This paper combines the database of network freight transport regulatory system of the Chinese Ministry of Transport, proposes the XGBoost model portrait label classification algorithm based on ant colony optimisation, and researches the research on the portrait of network freight enterprises by taking a certain province in the system as an example. By comparing with the ranking of network freight transport enterprises released by China Federation of Logistics &amp; Purchasing (CFLP), the reasonableness of network freight transport enterprise portrait is verified, and comparing with logistic regression, random forest, KNN and other methods, the label classification accuracy of enterprise portrait method based on ACO-XGBoost reaches 95.71%, and the overall performance of the model is optimal. And the sensitivity analysis of the influence of business reliability characteristics is conducted, and it is determined that the normal rate of overload regulation, the normal rate of vehicle track and the driver qualification compliance rate are important factors affecting the enterprise portrait of network freight transport. The result proves the new algorithm can quickly and accurately find the optimal solution for profiling network freight enterprises.",recent years network freight industry china developing rapidly become important force increasing efficiency logistics still lack effective portrait methods network freight enterprises society paper combines database network freight transport regulatory system chinese ministry transport proposes xgboost model portrait label classification algorithm based ant colony optimisation researches research portrait network freight enterprises taking certain province system example comparing ranking network freight transport enterprises released china federation logistics amp purchasing cflp reasonableness network freight transport enterprise portrait verified comparing logistic regression random forest knn methods label classification accuracy enterprise portrait method based aco xgboost reaches overall performance model optimal sensitivity analysis influence business reliability characteristics conducted determined normal rate overload regulation normal rate vehicle track driver qualification compliance rate important factors affecting enterprise portrait network freight transport result proves new algorithm quickly accurately find optimal solution profiling network freight enterprises
"This paper selects sample data from state-owned A-share listed companies from 2007 to 2018. Based on the calculation and determination of excessive on-the-job consumption, three indicators are defined to measure the depth of enterprise reform: diversity of mixed subjects, deepness of mixed subjects, and check and balance degree of mixed main body. Models are established and empirical tests are conducted through group analysis. The results show that all three indicators—diversity of mixed subjects, deepness of mixed subjects, and check and balance degree of mixed main body—significantly inhibit the level of excessive on-the-job consumption by executives, and this inhibitory effect is significant at the level of 1%. This paper finds that the higher the degree of enterprise reform, the lower the level of excess consumption; By the types of industries, compared with monopoly industries, the inhibitory effect of enterprise reform on excess consumption is more obvious in non-monopoly industries. Further research shows that in areas with a high degree of marketization, mixed ownership has a stronger inhibitory effect on excess consumption.",paper selects sample data state owned share listed companies based calculation determination excessive job consumption three indicators defined measure depth enterprise reform diversity mixed subjects deepness mixed subjects check balance degree mixed main body models established empirical tests conducted group analysis results show three indicators diversity mixed subjects deepness mixed subjects check balance degree mixed main body significantly inhibit level excessive job consumption executives inhibitory effect significant level paper finds higher degree enterprise reform lower level excess consumption types industries compared monopoly industries inhibitory effect enterprise reform excess consumption obvious non monopoly industries research shows areas high degree marketization mixed ownership stronger inhibitory effect excess consumption
"At present, when planning the allocation of recreational green space in urban and rural areas, there is often a cognitive bias against rural recreational green space, and planners, designers, and government officials often think that the countryside is a productive green space, so they do not set up a special leisure green space for rural residents to use. Therefore, this study first analyzed the relevant data of the National Leisure Life Survey and the Guizhou Provincial Statistical Survey Corps Family Housing Survey, and compared and discussed the degree of urbanization, the distance to leisure places, the demand for community parks, and the demand for activity centers. Secondly, 112 residents and 236 residents sampled by random telephone were selected from the household income and expenditure survey of the Guizhou Provincial Statistical Survey Corps, and the questionnaire survey and empirical research were carried out to further evaluate the differences in residents’ demand for green space according to the different attributes of urban and rural residences (urban, intermediary, and rural). The results of this study show that both the data analysis and the empirical research obtained by the Guizhou Provincial Statistical Survey Corps confirm that urban and rural residents have a demand for recreational green space, indicating that no significant difference was found in the demand for recreational green space between urban and rural residents. Green space is important for both urban and rural residents, and the results of this study hope to reverse the bias of policymakers on the planning strategy of urban and rural recreational green space.",present planning allocation recreational green space urban rural areas often cognitive bias rural recreational green space planners designers government officials often think countryside productive green space set special leisure green space rural residents use therefore study first analyzed relevant data national leisure life survey guizhou provincial statistical survey corps family housing survey compared discussed degree urbanization distance leisure places demand community parks demand activity centers secondly residents residents sampled random telephone selected household income expenditure survey guizhou provincial statistical survey corps questionnaire survey empirical research carried evaluate differences residents demand green space according different attributes urban rural residences urban intermediary rural results study show data analysis empirical research obtained guizhou provincial statistical survey corps confirm urban rural residents demand recreational green space indicating significant difference found demand recreational green space urban rural residents green space important urban rural residents results study hope reverse bias policymakers planning strategy urban rural recreational green space
"In response to the severe outbreak of COVID-19, the world is making every effort to promote vaccination. The Chinese government reports on the progress of the epidemic and related information about vaccines through the media, especially the official media, which has some influence on residents’ attitude and vaccination behavior. But few studies have discussed the potential public health impact of vaccines on social media and analyzed them alongside the impact of official media. In addition, from the perspective of SEIR models and the diffusion of knowledge, it is proposed that information about the epidemic situation and knowledge of vaccines before infection will prompt rural residents in central China to correctly understand vaccines and take the initiative to vaccinate. A survey of 1453 rural residents in Hubei, Henan, and Anhui provinces in central China showed that residents’ belief in official media coverage affecting vaccination behavior was established, while entertainment network platforms hindered the dissemination of health knowledge due to excessive entertainment information. In addition, perceived effectiveness, perceived security, perceived hazard, and knowledge related to vaccines can affect vaccination behavior. In this study, the influence factors of media information on vaccination behavior were discussed in theory and practice.",response severe outbreak covid world making every effort promote vaccination chinese government reports progress epidemic related information vaccines media especially official media influence residents attitude vaccination behavior studies discussed potential public health impact vaccines social media analyzed alongside impact official media addition perspective seir models diffusion knowledge proposed information epidemic situation knowledge vaccines infection prompt rural residents central china correctly understand vaccines take initiative vaccinate survey rural residents hubei henan anhui provinces central china showed residents belief official media coverage affecting vaccination behavior established entertainment network platforms hindered dissemination health knowledge due excessive entertainment information addition perceived effectiveness perceived security perceived hazard knowledge related vaccines affect vaccination behavior study influence factors media information vaccination behavior discussed theory practice
"This study aims to explore an artificial intelligence (AI) based red culture content production and recommendation system, and analyze how the system can effectively improve users’ contact frequency, cognitive depth, satisfaction and participation in red culture content, as well as promote users’ positive emotional responses. Through regression analysis and correlation analysis of 759 valid questionnaires, this study verified four core research hypotheses: AI recommendation system can significantly increase users’ exposure to red cultural content; enhance the depth of users’ cognition of red culture; increase user satisfaction and engagement; and eliciting positive emotional responses from users. The results show that personalized recommendation mechanisms play a key role in improving user experience, while strengthening the emotional resonance of content can significantly improve user engagement and communication willingness. In addition, this study also puts forward suggestions such as optimizing the algorithm of the recommendation system, multi-channel operation, and establishing a continuous monitoring and evaluation mechanism, so as to further improve the intelligent communication effect of red cultural content. This study provides theoretical and empirical support for the intelligent dissemination of red culture, and has important practical significance and academic value for the use of AI technology to promote red culture.",study aims explore artificial intelligence based red culture content production recommendation system analyze system effectively improve users contact frequency cognitive depth satisfaction participation red culture content well promote users positive emotional responses regression analysis correlation analysis valid questionnaires study verified four core research hypotheses recommendation system significantly increase users exposure red cultural content enhance depth users cognition red culture increase user satisfaction engagement eliciting positive emotional responses users results show personalized recommendation mechanisms play key role improving user experience strengthening emotional resonance content significantly improve user engagement communication willingness addition study also puts forward suggestions optimizing algorithm recommendation system multi channel operation establishing continuous monitoring evaluation mechanism improve intelligent communication effect red cultural content study provides theoretical empirical support intelligent dissemination red culture important practical significance academic value use technology promote red culture
"China’s tobacco industry ranks first in the world in terms of output. It can be said that tobacco is one of the pillar industries in China. Cigarette boxes are the main product produced by tobacco companies. In the production process, manually screening defective cigarette boxes is time-consuming and labor-intensive. This system uses YOLOv5 to automatically detect cigarette box defect images and provide defect category warnings. The system mainly includes four parts: the image acquisition unit, image processing unit, processing execution unit, and data acquisition and analysis unit. According to the defect type, the workshop will produce defective cigarette packs of brands such as Fengheshan and Red Diamond, and the defects of each brand will be identified and classified. Recognition accuracy is up to 99.9%. This system achieves the detection of cigarette box surface defects, effectively improving the detection capability of cigarette box defects in the automated production line.",china tobacco industry ranks first world terms output said tobacco one pillar industries china cigarette boxes main product produced tobacco companies production process manually screening defective cigarette boxes time consuming labor intensive system uses yolov automatically detect cigarette box defect images provide defect category warnings system mainly includes four parts image acquisition unit image processing unit processing execution unit data acquisition analysis unit according defect type workshop produce defective cigarette packs brands fengheshan red diamond defects brand identified classified recognition accuracy system achieves detection cigarette box surface defects effectively improving detection capability cigarette box defects automated production line
"With the advancement of technology, the inclusion problem of piezoelectric materials has gradually become a key research object. The aim of this study is to solve the relative rigid body displacement of Eshelby inclusion in piezoelectric materials. An improved Eshelby boundary element method is proposed, which combines the displacement extrapolation method to accurately calculate the mechanical properties of piezoelectric materials by introducing a strength factor. The improved method not only improves the calculation accuracy, but also effectively deals with the relative displacement of inclusions in piezoelectric materials. In the experiment, gallium arsenide is taken as an object, and the effectiveness of the improved method is verified by comparing J-integral method with stress method. The experimental results show that when the intensity factor is 1, 2, and 3, the error rates of generalized displacement calculation are 0.67%, 0.46%, and 0.39%, respectively. The errors of this method are 0.15%, 0.12%, 0.16%, 0.14%, and 0.16%, respectively, when measuring the relative displacement of inclusions and rigid bodies. These results show that the Eshelby boundary element method improved by displacement extrapolation has significant advantages in the treatment of boundary conditions and displacement analysis of piezoelectric materials. The improved method can effectively solve the problem of relative displacement of inclusions in piezoelectric materials, and has wide application prospect and high practical value.",advancement technology inclusion problem piezoelectric materials gradually become key research object aim study solve relative rigid body displacement eshelby inclusion piezoelectric materials improved eshelby boundary element method proposed combines displacement extrapolation method accurately calculate mechanical properties piezoelectric materials introducing strength factor improved method improves calculation accuracy also effectively deals relative displacement inclusions piezoelectric materials experiment gallium arsenide taken object effectiveness improved method verified comparing integral method stress method experimental results show intensity factor error rates generalized displacement calculation respectively errors method respectively measuring relative displacement inclusions rigid bodies results show eshelby boundary element method improved displacement extrapolation significant advantages treatment boundary conditions displacement analysis piezoelectric materials improved method effectively solve problem relative displacement inclusions piezoelectric materials wide application prospect high practical value
"Aesthetic education for aviation majors is an educational model that takes aviation majors as the main body, takes aesthetic education as the means, and aims to cultivate innovative talents in aviation majors. This paper elaborates the application of big data and artificial intelligence technology in the promotion of aesthetic education in aviation majors, aiming to improve the intelligence, personalization and adaptability of aesthetic education in aviation majors, and enhance the quality and level of education in aviation majors. The investigators designs a service system for aviation professional aesthetic education, that realizes the functions of personalized diagnosis, intelligent recommendation, dynamic adjustment and evaluation feedback for students based on big data and artificial intelligence technology, providing technical support for the promotion and innovation of aviation professional aesthetic education. In this paper, a semester-long teaching practice was carried out to teach aviation professional aesthetic education to students in the experimental group and the control group, and the experimental effect was assessed and analyzed through questionnaire survey, achievement analysis, teaching evaluation and other methods. The authors put forward the concept of aviation professional aesthetic education, clarify its cultivation goal, teaching content and evaluation standard, and provide reference for the theory and practice of aviation professional aesthetic education. The authors construct the theoretical framework and service system of aviation professional aesthetic education and realize the intelligence, personalization, and adaptability of aviation professional aesthetic education by using big data and artificial intelligence technology, which provides technical support for the promotion and innovation of aviation professional aesthetic education. The authors conducted an empirical study to verify the effectiveness and advantages of the service system of aviation professional aesthetic education, and found that the students in the experimental group made significant progress in the knowledge, ability, attitude and interest of aviation professional aesthetic education, which provided a data basis for the implementation and improvement of aviation professional aesthetic education.",aesthetic education aviation majors educational model takes aviation majors main body takes aesthetic education means aims cultivate innovative talents aviation majors paper elaborates application big data artificial intelligence technology promotion aesthetic education aviation majors aiming improve intelligence personalization adaptability aesthetic education aviation majors enhance quality level education aviation majors investigators designs service system aviation professional aesthetic education realizes functions personalized diagnosis intelligent recommendation dynamic adjustment evaluation feedback students based big data artificial intelligence technology providing technical support promotion innovation aviation professional aesthetic education paper semester long teaching practice carried teach aviation professional aesthetic education students experimental group control group experimental effect assessed analyzed questionnaire survey achievement analysis teaching evaluation methods authors put forward concept aviation professional aesthetic education clarify cultivation goal teaching content evaluation standard provide reference theory practice aviation professional aesthetic education authors construct theoretical framework service system aviation professional aesthetic education realize intelligence personalization adaptability aviation professional aesthetic education using big data artificial intelligence technology provides technical support promotion innovation aviation professional aesthetic education authors conducted empirical study verify effectiveness advantages service system aviation professional aesthetic education found students experimental group made significant progress knowledge ability attitude interest aviation professional aesthetic education provided data basis implementation improvement aviation professional aesthetic education
"This paper focuses on the innovative application of big data technology in the field of digitization of ancient literary texts, and puts forward a new mode and method of using big data technology to realize the whole-process design and implementation, in-depth analysis and excavation of ancient literary texts, in view of the inadequacy of the current digitization of ancient literary texts in terms of comprehensiveness and depth. The research practice shows that with the help of big data technology, not only has remarkable results been achieved in data collection and integration, text processing standardization, and efficient data management, but also in the application of specific technological tools such as Bayesian Intelligent Error Correction Algorithm, bilstm-CRF Ancient Literature Text Structuring and Semantic Annotation, and BERT Sentiment Connotation Analysis Model, which demonstrates excellent performance. The results show that by adopting advanced digitization research methods, the recognition accuracy of ancient texts, literary references, author information and publication year reaches 98%, 97%, 97%, and 98.5%, respectively, and after standardization and structuring, the heterogeneity of each data item is greatly reduced, the degree of structuring is significantly improved, and the data usability and accessibility are also enhanced. Meanwhile, the evaluation data of error correction algorithm efficacy, structuring function, and sentiment analysis function show that the model is outstanding in reducing the error rate, improving the degree of structuring, and performing sentiment analysis, especially for the processing of ancient texts and literary references, the error correction effect and the accuracy rate of sentiment analysis are especially obvious. In summary, this study has successfully constructed a comprehensive and in-depth digitization system of ancient literary texts through big data technology, which not only greatly improves the quality and usability of the data, but also broadens the perspectives and dimensions of ancient literature research.",paper focuses innovative application big data technology field digitization ancient literary texts puts forward new mode method using big data technology realize whole process design implementation depth analysis excavation ancient literary texts view inadequacy current digitization ancient literary texts terms comprehensiveness depth research practice shows help big data technology remarkable results achieved data collection integration text processing standardization efficient data management also application specific technological tools bayesian intelligent error correction algorithm bilstm crf ancient literature text structuring semantic annotation bert sentiment connotation analysis model demonstrates excellent performance results show adopting advanced digitization research methods recognition accuracy ancient texts literary references author information publication year reaches respectively standardization structuring heterogeneity data item greatly reduced degree structuring significantly improved data usability accessibility also enhanced meanwhile evaluation data error correction algorithm efficacy structuring function sentiment analysis function show model outstanding reducing error rate improving degree structuring performing sentiment analysis especially processing ancient texts literary references error correction effect accuracy rate sentiment analysis especially obvious summary study successfully constructed comprehensive depth digitization system ancient literary texts big data technology greatly improves quality usability data also broadens perspectives dimensions ancient literature research
"With the continuous development of big data and artificial intelligence technology, the accounting and auditing industry is also experiencing unprecedented changes. The purpose of this study is to explore the application of big data in accounting audit and its impact on audit quality and efficiency. Through in-depth analysis of existing literature and empirical research, the study found that big data can not only improve the accuracy and timeliness of audits but also promote the professional development of auditors and the sustainable development of the industry. At the same time, this study also focuses on the specific application of emerging technologies such as artificial intelligence and machine learning in auditing and analyzes the challenges and ethical issues that these technologies may bring. Finally, the study proposes a series of implementation strategies and best practices to guide audit practices, education and training, and policy development. Overall, this study provides a strong theoretical support and practical guidance for the innovation and development of the audit industry, and helps to promote the accounting audit into a smarter and more efficient new era.",continuous development big data artificial intelligence technology accounting auditing industry also experiencing unprecedented changes purpose study explore application big data accounting audit impact audit quality efficiency depth analysis existing literature empirical research study found big data improve accuracy timeliness audits also promote professional development auditors sustainable development industry time study also focuses specific application emerging technologies artificial intelligence machine learning auditing analyzes challenges ethical issues technologies may bring finally study proposes series implementation strategies best practices guide audit practices education training policy development overall study provides strong theoretical support practical guidance innovation development audit industry helps promote accounting audit smarter efficient new
"Forest ecological protection faces unprecedented challenges with global climate change and the intensification of human activities. Traditional protection methods are difficult to adapt to the complex and changeable ecological environment, and innovative technical means are urgently needed. This study aims to integrate the Internet of Things and deep reinforcement learning technology to build a forest ecological protection strategy optimization and decision support system to improve protection efficiency and scientificity. A comprehensive perception of forest ecology is achieved by deploying sensoreal times to monitor forest environmental parameters in real life, such as temperature, humidity, soil nutrients, etc. Based on the deep reinforcement learning algorithm, a protection strategy optimization model is established, and the optimal strategy is learned by simulating the protection measures in different situations. Experimental results show that compared with traditional methods, this system can improve the effectiveness of the protection strategy by 30% and significantly reduce the waste of resources. In the simulated 1000-hectare forest area, after adopting this system, the vegetation coverage rate increased by 20%, and the species diversity index increased by 15%. In addition, the system also has a real-time decision support function, which can dynamically adjust protection strategies according to real-time data to respond to sudden environmental changes. This study provides a new technical path for forest ecological protection and a useful reference for research in related fields.",forest ecological protection faces unprecedented challenges global climate change intensification human activities traditional protection methods difficult adapt complex changeable ecological environment innovative technical means urgently needed study aims integrate internet things deep reinforcement learning technology build forest ecological protection strategy optimization decision support system improve protection efficiency scientificity comprehensive perception forest ecology achieved deploying sensoreal times monitor forest environmental parameters real life temperature humidity soil nutrients etc based deep reinforcement learning algorithm protection strategy optimization model established optimal strategy learned simulating protection measures different situations experimental results show compared traditional methods system improve effectiveness protection strategy significantly reduce waste resources simulated hectare forest area adopting system vegetation coverage rate increased species diversity index increased addition system also real time decision support function dynamically adjust protection strategies according real time data respond sudden environmental changes study provides new technical path forest ecological protection useful reference research related fields
"With the emergence of new network services, they have put forward extremely high quality of service requirements for transmission links. A high-performance priority queue scheduling mechanism driven by artificial intelligence is proposed to address the low efficiency and easy blocking of traditional priority queue scheduling mechanisms in software defined network data planes. This mechanism avoids queue head blocking by improving the deficit round robin algorithm, ensures fair allocation of output bandwidth for differentiated fine-grained priority queues, and guarantees maximum forwarding delay. The results show that this method significantly improves performance compared to traditional algorithms, as it can process and forward data packets faster, thereby reducing latency and increasing throughput. Specifically, the average round-trip time of the deficit round robin algorithm is 8.21 ms, compared to 3.89 ms for this method, which reduces 4.72 ms. The highest average throughput of the deficit round robin algorithm is 60.01 Mbps, compared to 105.99 Mbps for this method, which improves by 76.62%. After improving the algorithm, the software defined a network data plane system with an average frame delay of 2.81 ms and a packet loss rate of 4.50%. The amount of data transmitted by the improved system was basically the same as when it was not attacked, with an availability rate of 98.6%. The improved algorithm can better ensure the smoothness of transmission services. This provides a new direction for future research on new network architecture technologies and has certain economic value.",emergence new network services put forward extremely high quality service requirements transmission links high performance priority queue scheduling mechanism driven artificial intelligence proposed address low efficiency easy blocking traditional priority queue scheduling mechanisms software defined network data planes mechanism avoids queue head blocking improving deficit round robin algorithm ensures fair allocation output bandwidth differentiated fine grained priority queues guarantees maximum forwarding delay results show method significantly improves performance compared traditional algorithms process forward data packets faster thereby reducing latency increasing throughput specifically average round trip time deficit round robin algorithm compared method reduces highest average throughput deficit round robin algorithm mbps compared mbps method improves improving algorithm software defined network data plane system average frame delay packet loss rate amount data transmitted improved system basically attacked availability rate improved algorithm better ensure smoothness transmission services provides new direction future research new network architecture technologies certain economic value
"In order to extract the semantic information from a large number of student comments on the online education platform, this paper investigates and develops a Chinese comment classification model using the Voting and BiLSTM algorithm. The model classified the course reviews from two aspects of sentiment and content. The sentiment aspect was divided into three categories of “Positive,” “Negative,” and “Neutral,” and the content aspect was divided into three categories of “course,” “platform environment,” and “other.” In the collection and processing of data sets, an effective data set that accurately represents the characteristics of the education field is constructed by utilizing comments obtained from the NetEase Cloud course platform. The HuggingFace open source Bert pre-training model is then employed for word vector training. In the model construction, based on the Voting and BiLSTM model classification algorithm, a weighted fusion Bi_Voting strategy is proposed, and the classification principle based on SVM and the resampling enhance module are introduced. The experimental results show that the classification model has significant advantages in terms of accuracy, recall rate, and F1 value. In addition, to obtain more comprehensive information, we also conducted an in-depth analysis of different categories of reviews using hierarchical clustering and TextRank keyword extraction.",order extract semantic information large number student comments online education platform paper investigates develops chinese comment classification model using voting bilstm algorithm model classified course reviews two aspects sentiment content sentiment aspect divided three categories positive negative neutral content aspect divided three categories course platform environment collection processing data sets effective data set accurately represents characteristics education field constructed utilizing comments obtained netease cloud course platform huggingface open source bert pre training model employed word vector training model construction based voting bilstm model classification algorithm weighted fusion voting strategy proposed classification principle based svm resampling enhance module introduced experimental results show classification model significant advantages terms accuracy recall rate value addition obtain comprehensive information also conducted depth analysis different categories reviews using hierarchical clustering textrank keyword extraction
"How to effectively combine power Internet of Things and blockchain technology in the integrated energy service scenario remains to be further explored. This paper aims to improve the energy collection and monitoring effect of integrated energy system through intelligent Internet of Things technology, and enhance the stability and reliability of integrated energy system operation. In addition, this paper proposes multi-source clock and multi-timing interface, studies and designs clock synchronization and timing mechanism covering the whole data collection network segment, designs multi-source weighted timing mechanism, designs timing mechanism among key network segments, and improves the vertical and horizontal time management and time synchronization capabilities of the system, thus improving the consistency of integrated energy information collection and monitoring data collection. It can be seen from the research that the energy collection and monitoring system of integrated energy system combined with intelligent Internet of Things technology proposed in this paper has good results, and the energy data collected by this system is more accurate, and some calculation formulas such as network energy consumption are introduced, which lays a solid foundation for the monitoring and analysis of the platform. This study not only promotes the operation stability of integrated energy system but also promotes the application development of Internet of Things technology in energy system. Therefore, in the follow-up study, we can further study the application of intelligent Internet of Things technology in integrated energy system decision-making.",effectively combine power internet things blockchain technology integrated energy service scenario remains explored paper aims improve energy collection monitoring effect integrated energy system intelligent internet things technology enhance stability reliability integrated energy system operation addition paper proposes multi source clock multi timing interface studies designs clock synchronization timing mechanism covering whole data collection network segment designs multi source weighted timing mechanism designs timing mechanism among key network segments improves vertical horizontal time management time synchronization capabilities system thus improving consistency integrated energy information collection monitoring data collection seen research energy collection monitoring system integrated energy system combined intelligent internet things technology proposed paper good results energy data collected system accurate calculation formulas network energy consumption introduced lays solid foundation monitoring analysis platform study promotes operation stability integrated energy system also promotes application development internet things technology energy system therefore follow study study application intelligent internet things technology integrated energy system decision making
"Understanding the factors that drive dual-task interference—the performance decline when two tasks are performed concurrently—is critical for designing effective interfaces in multitasking environments such as health care and aviation. This study examines the influence of resource conflict, visual angle separation, and task priority on dual-task interference. Data was collected from 39 participants performing a visual tracking task and a digit task in virtual reality (VR). Using Multiple Resource Theory’s (MRT) computational model, an iterative approach was employed to evaluate each factor’s contribution. Incorporating resource conflict improved model fit over a demand-only baseline, and adding a visual penalty to account for angle separation further enhanced prediction accuracy. Task-specific results showed greater interference in the digit task, suggesting that task priority influences susceptibility to interference. Overall, the results highlight the importance of incorporating resource conflict and perceptual factors, such as visual angle separation, into computational models of dual-task interference.",understanding factors drive dual task interference performance decline two tasks performed concurrently critical designing effective interfaces multitasking environments health care aviation study examines influence resource conflict visual angle separation task priority dual task interference data collected participants performing visual tracking task digit task virtual reality using multiple resource theory mrt computational model iterative approach employed evaluate factor contribution incorporating resource conflict improved model fit demand baseline adding visual penalty account angle separation enhanced prediction accuracy task specific results showed greater interference digit task suggesting task priority influences susceptibility interference overall results highlight importance incorporating resource conflict perceptual factors visual angle separation computational models dual task interference
"To accurately extract the topological features of brain functional networks from electroencephalogram signals of patients with schizophrenia, this study proposes a functional brain network model that uses a graph convolutional neural network to integrate frequency bands, segment lengths, and functional connectivity indexes. It combines time-frequency domain features and brain network topological features to divide electroencephalogram signals into different frequency bands and length epochs. This allows for the analysis of characteristic values of electroencephalogram signals of schizophrenic patients. Through comparative trials, the study evaluates the suggested model’s performance. The results revealed that the proposed model obtained an average accuracy of 91.12% using the phase-locked value functional connectivity metric at Theta frequency band and 6-s segment length, which was 2.85%, 8.74%, and 3.95% higher compared to the support vector machine, convolutional neural network, and temporal convolutional network models, respectively, and demonstrated a superior recognition performance. In addition, network topology analysis revealed that the parietal region was an important region in the brain network of schizophrenic patients. The proposed model is reliable and feasible for detecting schizophrenic patients. It can extract topological features from electroencephalogram signals to characterize the functional brain networks of patients with schizophrenia. These features can then be correlated with clinical symptom salience, which is expected to aid in diagnosis.",accurately extract topological features brain functional networks electroencephalogram signals patients schizophrenia study proposes functional brain network model uses graph convolutional neural network integrate frequency bands segment lengths functional connectivity indexes combines time frequency domain features brain network topological features divide electroencephalogram signals different frequency bands length epochs allows analysis characteristic values electroencephalogram signals schizophrenic patients comparative trials study evaluates suggested model performance results revealed proposed model obtained average accuracy using phase locked value functional connectivity metric theta frequency band segment length higher compared support vector machine convolutional neural network temporal convolutional network models respectively demonstrated superior recognition performance addition network topology analysis revealed parietal region important region brain network schizophrenic patients proposed model reliable feasible detecting schizophrenic patients extract topological features electroencephalogram signals characterize functional brain networks patients schizophrenia features correlated clinical symptom salience expected aid diagnosis
"With the growing maturity of the “technology” + “teaching” integration model, the ethical and legal issues related to AI in English language teaching have become more and more prominent. However, the previous literature has not systematically explored the factors that cause AI ethics and law in English language teaching, nor has it made suggestions to address these factors. Therefore, this paper explores in depth the origins, manifestations, and solutions of these ethical and legal issues. Firstly, through the literature review method, we explore the ethical and legal issues surrounding AI in English language teaching and its impact on teachers and students, and find that these ethical and legal issues have a significant effect on enhancing the legal awareness of students and teachers. Moreover, based on the literature review method, this paper identifies eight influencing factors and determines the weights of each factor through the method of multiple linear regression, and it can be seen that automated decision-making and accountability (X3) and teacher roles and competence (X4) have the greatest weights and are the key factors leading to ethical and legal issues. Finally, this paper also formulates targeted recommendations in response to the experimental results, such as respecting students’ and teachers’ privacy and autonomy, maintaining teachers’ professionalism and accountability, and complying with laws, regulations, and ethical guidelines.",growing maturity technology teaching integration model ethical legal issues related english language teaching become prominent however previous literature systematically explored factors cause ethics law english language teaching made suggestions address factors therefore paper explores depth origins manifestations solutions ethical legal issues firstly literature review method explore ethical legal issues surrounding english language teaching impact teachers students find ethical legal issues significant effect enhancing legal awareness students teachers moreover based literature review method paper identifies eight influencing factors determines weights factor method multiple linear regression seen automated decision making accountability teacher roles competence greatest weights key factors leading ethical legal issues finally paper also formulates targeted recommendations response experimental results respecting students teachers privacy autonomy maintaining teachers professionalism accountability complying laws regulations ethical guidelines
"Interpreting teaching and research need a large number of real, high-quality interpreting corpus, but the existing interpreting corpus has many shortcomings, such as small scale, single type, and uneven quality. In this paper, we utilize big data technology to build a powerful, easy-to-use and open-sharing English-Chinese interpreting corpus database to provide rich and diverse high-quality interpreting examples for the teaching and research of interpreting. We collect English-Chinese interpreting data of various types, scenarios, topics, and levels from the Internet, TV broadcasts, and other channels, clean, standardize, slice, align, and annotate the data, store the metadata information in XML format, and design and implement the structure, functions, and interfaces of the corpus. This paper mainly introduces the data method, model construction, and application effect of the corpus, including the collection, organization, annotation, storage, management, retrieval, analysis, display, and application of the corpus.",interpreting teaching research need large number real high quality interpreting corpus existing interpreting corpus many shortcomings small scale single type uneven quality paper utilize big data technology build powerful easy use open sharing english chinese interpreting corpus database provide rich diverse high quality interpreting examples teaching research interpreting collect english chinese interpreting data various types scenarios topics levels internet broadcasts channels clean standardize slice align annotate data store metadata information xml format design implement structure functions interfaces corpus paper mainly introduces data method model construction application effect corpus including collection organization annotation storage management retrieval analysis display application corpus
"This study explores how to use big data analysis technology to conduct a comprehensive, objective and in-depth assessment of the communication effect of China’s foreign discourse culture, with a view to optimizing the communication strategy and enhancing the effectiveness of international communication. This study systematically comprehends the theory of foreign discourse culture and soft power, the application of big data technology in the field of cultural communication, the current status of domestic and international research on the assessment of the communication effect of foreign discourse culture, as well as the construction of the theoretical framework for the assessment of the communication effect of foreign discourse culture based on big data and the setting of the research hypotheses. This study collects big data resources from various Internet media platforms such as social media, news media reports, international public opinion databases, online forums and blogs, and carries out data cleansing and standardization to lay a data foundation for the subsequent analysis. This study adopts two major data mining techniques, text mining and social network analysis, as well as a variety of data mining algorithms, such as topic modeling, sentiment analysis, and influence calculation, to conduct in-depth mining and analysis of the data on the cultural communication of foreign discourses, and develops a communication path recognition system for tracking and recording the trajectory of the dissemination of foreign discourses and constructing a communication network mapping. In addition, this paper demonstrates the effect of the model through specific cases. It is found that through the deep mining of big data, we can intuitively see the degree of influence of different discourse contents, communication channels, time nodes, and other factors on the communication effect, thus providing a scientific basis and decision-making support for policy makers and communication practitioners.",study explores use big data analysis technology conduct comprehensive objective depth assessment communication effect china foreign discourse culture view optimizing communication strategy enhancing effectiveness international communication study systematically comprehends theory foreign discourse culture soft power application big data technology field cultural communication current status domestic international research assessment communication effect foreign discourse culture well construction theoretical framework assessment communication effect foreign discourse culture based big data setting research hypotheses study collects big data resources various internet media platforms social media news media reports international public opinion databases online forums blogs carries data cleansing standardization lay data foundation subsequent analysis study adopts two major data mining techniques text mining social network analysis well variety data mining algorithms topic modeling sentiment analysis influence calculation conduct depth mining analysis data cultural communication foreign discourses develops communication path recognition system tracking recording trajectory dissemination foreign discourses constructing communication network mapping addition paper demonstrates effect model specific cases found deep mining big data intuitively see degree influence different discourse contents communication channels time nodes factors communication effect thus providing scientific basis decision making support policy makers communication practitioners
"This study focuses on designing an online learning system using an enhanced differential evolution K-Means clustering algorithm. With the growing popularity of online learning, the efficient distribution of personalized learning resources has become a key research area. By addressing the limitations of traditional methods and integrating advanced optimization techniques, the proposed system offers superior performance in terms of clustering accuracy, convergence speed, and adaptability. The detailed analysis of optimization algorithms’ categories and recent developments further positions this research within the broader context of educational technology advancements. The clustering accuracy of the improved algorithm reaches 37.13% when processing the Iris dataset. This relatively low accuracy is partly attributed to the Iris dataset’s inherent complexity, including class overlap and subtle feature differences between clusters. It also indicates potential for further optimization, such as refining the distance measurement method for overlapping classes. The experimental evaluation shows that on the Iris and Wine datasets from the UCI Machine Learning Repository, the clustering accuracy of the improved algorithm reaches 37.13% and 41.22%, respectively. Compared with the traditional K-Means algorithm, its convergence speed is increased by 4.5 times, where convergence speed is measured by both the number of iterations required to reach stable clustering results and the computational time.",study focuses designing online learning system using enhanced differential evolution means clustering algorithm growing popularity online learning efficient distribution personalized learning resources become key research area addressing limitations traditional methods integrating advanced optimization techniques proposed system offers superior performance terms clustering accuracy convergence speed adaptability detailed analysis optimization algorithms categories recent developments positions research within broader context educational technology advancements clustering accuracy improved algorithm reaches processing iris dataset relatively low accuracy partly attributed iris dataset inherent complexity including class overlap subtle feature differences clusters also indicates potential optimization refining distance measurement method overlapping classes experimental evaluation shows iris wine datasets uci machine learning repository clustering accuracy improved algorithm reaches respectively compared traditional means algorithm convergence speed increased times convergence speed measured number iterations required reach stable clustering results computational time
"Virtual simulation of surveying instruments is a specific aspect of virtual measurement that combines three technologies: three-dimensional (3D) modeling, virtual simulation, and computer technology. In order to solve problems such as instrument damage, shortage of practical instruments for students, and difficulties in practical training, this paper takes the level instrument in surveying instruments as an example, and uses virtual reality simulation technology, Unity3D technology, 3ds Max modeling technology, and related computer technologies to build a virtual simulation of the level instrument. The research mainly focuses on the following aspects: (1) Using 3ds Max software, the tripod model was established, and the telescope model, objective lens model, spiral model, and base model of the level instrument were also established, and then combined into a complete level instrument. Finally, the tripod model and the level instrument model were imported into Unity3D for assembly and use. (2) Using C# programming language, SQL Server database, and Unity3D, a virtual terrain was built to simulate the cognitive simulation of the level instrument, the principle of centering and leveling simulation, and the principle of height measurement simulation. (3) The cognitive simulation of the level instrument was carried out in the Unity3D-based surveying instrument cognitive simulation system; the simulation of the principle of level instrument height measurement was also performed. Through the reading of the level staff, the elevation was calculated. This virtual simulation system can be used for classroom teaching demonstrations and student experiments, effectively reducing the damage of expensive surveying instruments, easing equipment shortages, and improving teaching effectiveness.",virtual simulation surveying instruments specific aspect virtual measurement combines three technologies three dimensional modeling virtual simulation computer technology order solve problems instrument damage shortage practical instruments students difficulties practical training paper takes level instrument surveying instruments example uses virtual reality simulation technology unity technology max modeling technology related computer technologies build virtual simulation level instrument research mainly focuses following aspects using max software tripod model established telescope model objective lens model spiral model base model level instrument also established combined complete level instrument finally tripod model level instrument model imported unity assembly use using programming language sql server database unity virtual terrain built simulate cognitive simulation level instrument principle centering leveling simulation principle height measurement simulation cognitive simulation level instrument carried unity based surveying instrument cognitive simulation system simulation principle level instrument height measurement also performed reading level staff elevation calculated virtual simulation system used classroom teaching demonstrations student experiments effectively reducing damage expensive surveying instruments easing equipment shortages improving teaching effectiveness
"With the acceleration of urbanization and the vigorous rise of tourism, smart tourism, as an emerging business format, is playing an increasingly important role in the sustainable development of cities. This study aims to explore how smart tourism can promote urban sustainable development through spatial optimization and verify its application effect through empirical analysis. The research background covers the current challenges cities face, such as tight space resources, increasing environmental pressure and diversified tourism demand. The urban tourism space is refined and dynamically managed by constructing a smart tourism space optimization model combined with GIS technology, big data analysis, and artificial intelligence algorithm. The experimental results show that after implementing the smart tourism strategy, the utilization efficiency of urban tourism space has increased by 25%, tourist satisfaction has increased by 30%, and carbon emissions have decreased by 15%. In addition, smart tourism has also promoted the coordinated development of the urban economy, society and environment and promoted the transformation and upgrading of the tourism industry. This study not only provides new ideas and methods for urban sustainable development but also provides scientific basis and reference cases for the practical application of smart tourism.",acceleration urbanization vigorous rise tourism smart tourism emerging business format playing increasingly important role sustainable development cities study aims explore smart tourism promote urban sustainable development spatial optimization verify application effect empirical analysis research background covers current challenges cities face tight space resources increasing environmental pressure diversified tourism demand urban tourism space refined dynamically managed constructing smart tourism space optimization model combined gis technology big data analysis artificial intelligence algorithm experimental results show implementing smart tourism strategy utilization efficiency urban tourism space increased tourist satisfaction increased carbon emissions decreased addition smart tourism also promoted coordinated development urban economy society environment promoted transformation upgrading tourism industry study provides new ideas methods urban sustainable development also provides scientific basis reference cases practical application smart tourism
"This paper examines the construction and application of a digital protection and dissemination system for red cultural resources based on the Internet of Things (IoT) and big data technologies. By utilizing high-precision sensors and big data tools, the system enables real-time monitoring and data collection of red cultural resources. Additionally, the random forest model is applied for feature extraction and classification prediction. The research encompasses data acquisition and remote processing, model construction and optimization, IoT architecture design, and application system development. Through system evaluation and user feedback analysis, the effectiveness of the digital protection measures and the communication impact are assessed, demonstrating improvements in both the protection of red cultural resources and user engagement. The optimized IoT architecture and model significantly enhance data accuracy and system stability, offering robust support for the long-term protection and widespread dissemination of red cultural resources. This study offers a scientific approach and practical pathway for the digital transformation of red cultural heritage, serving as a valuable reference in the field of cultural heritage protection.",paper examines construction application digital protection dissemination system red cultural resources based internet things iot big data technologies utilizing high precision sensors big data tools system enables real time monitoring data collection red cultural resources additionally random forest model applied feature extraction classification prediction research encompasses data acquisition remote processing model construction optimization iot architecture design application system development system evaluation user feedback analysis effectiveness digital protection measures communication impact assessed demonstrating improvements protection red cultural resources user engagement optimized iot architecture model significantly enhance data accuracy system stability offering robust support long term protection widespread dissemination red cultural resources study offers scientific approach practical pathway digital transformation red cultural heritage serving valuable reference field cultural heritage protection
"Scientific higher-order thinking (S-HOT) is a key competency for cultivating innovation in science and technology. However, its influencing pathways remain unclear. This study integrates structural equation modeling and fuzzy set qualitative comparative analysis to explore how individual and environmental factors such as parental engagement, family atmosphere, scientific critical and judgmental ability, learning engagement, extracurricular reading, and teaching methods shape junior high school students’ S-HOT. Based on data from 124 students in Beijing, the findings show that scientific critical and judgmental ability positively affects S-HOT. Extracurricular reading enhances S-HOT indirectly through learning engagement or in combination with scientific critical and judgmental ability. However, excessive learning engagement may have a negative impact. fsQCA reveals six distinct factor combinations that contribute to S-HOT, highlighting the presence of multiple effective pathways. These findings offer both theoretical insight and practical guidance for the targeted development of higher-order thinking in science education.",scientific higher order thinking hot key competency cultivating innovation science technology however influencing pathways remain unclear study integrates structural equation modeling fuzzy set qualitative comparative analysis explore individual environmental factors parental engagement family atmosphere scientific critical judgmental ability learning engagement extracurricular reading teaching methods shape junior high school students hot based data students beijing findings show scientific critical judgmental ability positively affects hot extracurricular reading enhances hot indirectly learning engagement combination scientific critical judgmental ability however excessive learning engagement may negative impact fsqca reveals six distinct factor combinations contribute hot highlighting presence multiple effective pathways findings offer theoretical insight practical guidance targeted development higher order thinking science education
"Was the COVID-19 pandemic orchestrated by a secret elite, or does the government simply filter certain truths? While both statements reflect conspiracy thinking, they refer to different rationalities. This study contributes to the debate on the definitions of conspiracy mentality, suspicion and theories by examining their relationships with trust. Analysing a nationally representative survey data from Spain, the study shows that lower levels of trust predict greater levels of belief in conspiracy theories and suspicion, while conspiracy mentality remains stable regardless of trust levels. The research concludes that different forms of conspiracy thinking have distinct social significance and do not relate to trust in the same way.",covid pandemic orchestrated secret elite government simply filter certain truths statements reflect conspiracy thinking refer different rationalities study contributes debate definitions conspiracy mentality suspicion theories examining relationships trust analysing nationally representative survey data spain study shows lower levels trust predict greater levels belief conspiracy theories suspicion conspiracy mentality remains stable regardless trust levels research concludes different forms conspiracy thinking distinct social significance relate trust way
"To address the challenge of interpreting complex spatiotemporal patterns in multimodal English learning behaviors, this study proposes a novel computational framework integrating dynamic graph construction with adaptive interpretable machine learning. First, multi-source data fusion technology is used to integrate logs, data, and evaluation scores to construct a time-series learning behavior graph. Second, the XGBoost LSTM (long short-term memory) hybrid model is used to predict learning effectiveness, and the hierarchical sampling SHAP interpretation algorithm is proposed simultaneously. Third, a three-level warning mechanism is constructed using dynamic threshold sliding windows. Key behavioral characteristics are identified through SHAP values and a risk probability model is created using logistic regression. Validated across educational and industrial datasets, the framework enhances interpretability in predictive maintenance scenario, demonstrating cross-domain applicability in IoT sensor networks and healthcare monitoring systems.",address challenge interpreting complex spatiotemporal patterns multimodal english learning behaviors study proposes novel computational framework integrating dynamic graph construction adaptive interpretable machine learning first multi source data fusion technology used integrate logs data evaluation scores construct time series learning behavior graph second xgboost lstm long short term memory hybrid model used predict learning effectiveness hierarchical sampling shap interpretation algorithm proposed simultaneously third three level warning mechanism constructed using dynamic threshold sliding windows key behavioral characteristics identified shap values risk probability model created using logistic regression validated across educational industrial datasets framework enhances interpretability predictive maintenance scenario demonstrating cross domain applicability iot sensor networks healthcare monitoring systems
"This paper discusses efficiency, spatial pattern characteristics and impact mechanism of green development in Chengdu-Chongqing City Group from 2005 to 2018 by using various methods such as the SBM model, spatial autocorrelation analysis, and the spatial panel metrology model. It is found that the green development efficiency of Chengdu-Chongqing City Group has been gradually improved as a whole with decreasing regional differences year by year; the green development efficiency of the city group has a significant positive spatial autocorrelation and spatial combined effect. There are other factors with obvious indirect effects, including economic growth, urbanization ratio, industrial structure, pollution index, population scale, foreign direct investment and marketization level, and so on. However, the direct and indirect effects of government intervention and human capital offset each other in opposite directions; thus, the total effect is no longer of significance. In addition, some factors are of insignificant indirect effects, such as government intervention, patent quantity, and population, indicating that these factors have no obvious influence on the green development efficiency of neighboring areas.",paper discusses efficiency spatial pattern characteristics impact mechanism green development chengdu chongqing city group using various methods sbm model spatial autocorrelation analysis spatial panel metrology model found green development efficiency chengdu chongqing city group gradually improved whole decreasing regional differences year year green development efficiency city group significant positive spatial autocorrelation spatial combined effect factors obvious indirect effects including economic growth urbanization ratio industrial structure pollution index population scale foreign direct investment marketization level however direct indirect effects government intervention human capital offset opposite directions thus total effect longer significance addition factors insignificant indirect effects government intervention patent quantity population indicating factors obvious influence green development efficiency neighboring areas
"As society evolves, people have become increasingly reliant on the internet, and network access control technologies ensure the security of network environments. However, traditional network access control technologies generally suffer from limitations such as static verification cycles, single authorization methods, and a lack of continuous behavior tracking, making it difficult to address dynamic and complex network threats. To address this, the study proposes a network access control model based on the zero-trust security concept, integrating dynamic authorization verification methods to keep user verification in a continuous and dynamic state, thereby enhancing the system’s overall protective capabilities. Through simulation experiments, the results show that compared to KNN, SVM, and MLP models, the proposed ZT-DA model achieves an accuracy rate of 98.4% and a false positive rate of 3.8% when accessing 400 times, with clearer classification boundaries, enabling more accurate distinction between normal and abnormal behavior. When the sample size is 200, the trust calculation times for different stages are 78 ms, 65 ms, and 70 ms, respectively. In practical applications, the model achieved an interception rate of 100%, a false interception rate of 0%, and a control range of 100%. The research results indicate that the model can monitor threat behaviors in real time, effectively intercept malicious attacks, and enhance information security protection capabilities with a lower false positive rate while ensuring a smooth user network experience.",society evolves people become increasingly reliant internet network access control technologies ensure security network environments however traditional network access control technologies generally suffer limitations static verification cycles single authorization methods lack continuous behavior tracking making difficult address dynamic complex network threats address study proposes network access control model based zero trust security concept integrating dynamic authorization verification methods keep user verification continuous dynamic state thereby enhancing system overall protective capabilities simulation experiments results show compared knn svm mlp models proposed model achieves accuracy rate false positive rate accessing times clearer classification boundaries enabling accurate distinction normal abnormal behavior sample size trust calculation times different stages respectively practical applications model achieved interception rate false interception rate control range research results indicate model monitor threat behaviors real time effectively intercept malicious attacks enhance information security protection capabilities lower false positive rate ensuring smooth user network experience
"Focusing on the issue of loss of image spatial structure information in the current oil painting style recognition research, this paper first preprocesses the oil painting image, then combines the spatial information and grayscale information to improve the fuzzy clustering algorithm (FCM) (EFCM), and utilizes the EFCM to segment the oil painting image. On this basis, the fourth-order tensor training samples are constructed for the segmented oil painting images, and the oil painting image features are extracted using multilinear principal component analysis (MPCA). Finally, the support vector machine (SVM) optimized by the improved particle swarm algorithm (EAPSO) is used for oil painting style recognition. The simulation outcome demonstrates that the offered model has an average recognition accuracy (mAP) of 96.2%, which is better than the comparison model, and provides a new technical path for the accurate recognition of oil painting style.",focusing issue loss image spatial structure information current oil painting style recognition research paper first preprocesses oil painting image combines spatial information grayscale information improve fuzzy clustering algorithm fcm efcm utilizes efcm segment oil painting image basis fourth order tensor training samples constructed segmented oil painting images oil painting image features extracted using multilinear principal component analysis mpca finally support vector machine svm optimized improved particle swarm algorithm eapso used oil painting style recognition simulation outcome demonstrates offered model average recognition accuracy map better comparison model provides new technical path accurate recognition oil painting style
"Children’s storied play, or in other words, their fictive world-building, has much to teach us with regard to understanding story and how story pedagogy might be otherwise shaped in curricular spaces. In this article, we explore this proposition by drawing on interconnected and entangled moments of children’s outdoor storied play in a Canadian first-grade class. By mapping the children’s movements within three storied-play encounters, we animate how what could appear as the most ordinary and mundane ways of composing story in outdoor, open-ended and make-believe play, offers a lively form of protestation to the predominant Western narrative paradigm of beginning-middle-end story structure. Bringing the three moments into conversation with literary theorist Marielle Macé’s concept of narrative stylization, we attend to ways story lives and grows within children’s storied and playful worldbuilding. Ultimately, we argue that limiting instructional methods to that which is centred around the objectification and rationalization of storying (i.e., typical beginning-middle-end story structure) can displace other pedagogical directions that recognize and extend the myriad ways in which stories are made, learned, and understood. We conclude the article with an exploration of how educators might promote and build upon the narrative stylizations – fiction-in-the-doing – that children are always already engaging in their storied play.",children storied play words fictive world building much teach regard understanding story story pedagogy might otherwise shaped curricular spaces article explore proposition drawing interconnected entangled moments children outdoor storied play canadian first grade class mapping children movements within three storied play encounters animate could appear ordinary mundane ways composing story outdoor open ended make believe play offers lively form protestation predominant western narrative paradigm beginning middle end story structure bringing three moments conversation literary theorist marielle macé concept narrative stylization attend ways story lives grows within children storied playful worldbuilding ultimately argue limiting instructional methods centred around objectification rationalization storying typical beginning middle end story structure displace pedagogical directions recognize extend myriad ways stories made learned understood conclude article exploration educators might promote build upon narrative stylizations fiction children always already engaging storied play
"With the rapid development of the information age, traditional resume screening methods have been difficult to meet the urgent needs of enterprises for efficient and accurate talent selection when dealing with massive recruitment data. Based on this, this study proposes a resume screening and career matching model based on Fuzzy Natural Language Processing (Fuzzy NLP), which aims to optimize the intelligent matching process between talents and positions by using cutting-edge technology. The model deeply integrates fuzzy logic theory and semantic analysis technology, and constructs a multi-dimensional semantic understanding framework to refine the analysis of resumes and job descriptions, effectively eliminate the semantic ambiguity and uncertainty in natural language, and break through the bottleneck of traditional methods at the level of semantic understanding. In the empirical research phase, the research team systematically tested a real-world dataset of 5000 resumes and 1000 job information. Experimental results show that compared with the traditional keyword matching method, the proposed model achieves a significant improvement of 25% to 85% in the matching accuracy, and accurately captures the potential relationship between job requirements and candidates’ abilities through dynamic weight adjustment and semantic similarity measurement mechanism. In terms of screening efficiency, the model helps optimize the recruitment process, shortens the manual review time by 30%, and significantly reduces labor and time costs. Especially when dealing with fuzzy semantic descriptions and polysemy word scenarios, the model reduces the false positive rate by 20% through context perception and fuzzy rule reasoning, showing excellent semantic understanding ability and robustness. This study not only verifies the practical value and technical advantages of fuzzy natural language processing technology in the field of human resource management but also provides theoretical support and methodological reference for promoting the intelligent transformation of the recruitment process, which is expected to build a more efficient and accurate talent screening system for enterprises and help human resource management move towards a new stage of intelligent and scientific development.",rapid development information age traditional resume screening methods difficult meet urgent needs enterprises efficient accurate talent selection dealing massive recruitment data based study proposes resume screening career matching model based fuzzy natural language processing fuzzy nlp aims optimize intelligent matching process talents positions using cutting edge technology model deeply integrates fuzzy logic theory semantic analysis technology constructs multi dimensional semantic understanding framework refine analysis resumes job descriptions effectively eliminate semantic ambiguity uncertainty natural language break bottleneck traditional methods level semantic understanding empirical research phase research team systematically tested real world dataset resumes job information experimental results show compared traditional keyword matching method proposed model achieves significant improvement matching accuracy accurately captures potential relationship job requirements candidates abilities dynamic weight adjustment semantic similarity measurement mechanism terms screening efficiency model helps optimize recruitment process shortens manual review time significantly reduces labor time costs especially dealing fuzzy semantic descriptions polysemy word scenarios model reduces false positive rate context perception fuzzy rule reasoning showing excellent semantic understanding ability robustness study verifies practical value technical advantages fuzzy natural language processing technology field human resource management also provides theoretical support methodological reference promoting intelligent transformation recruitment process expected build efficient accurate talent screening system enterprises help human resource management move towards new stage intelligent scientific development
"Current e-commerce transaction encryption and secure logistics sharing methods, despite using advanced data encryption, still face risks of data leaks and payment security vulnerabilities, sometimes due to weaknesses in network protocols. To address these challenges, this study introduces a blockchain-based approach combined with the RSA asymmetric encryption algorithm, which enables secure decryption without direct key transmission. Experimental results demonstrate that the proposed solution significantly reduces the communication overhead compared to that of existing dual-layer consensus and IoT-based architectures. With 40 nodes, it achieves just 302 transactions of overhead at 50 TPS and a minimal system memory usage of 0.502% at 40 TPS. Furthermore, at a high throughput of 120 TPS, the algorithm completes transactions in only 53.47 ms—far faster than competing methods. These improvements enhance both security and efficiency, offering a more reliable framework for e-commerce transactions and logistics data sharing while strengthening trust in digital trade.",current commerce transaction encryption secure logistics sharing methods despite using advanced data encryption still face risks data leaks payment security vulnerabilities sometimes due weaknesses network protocols address challenges study introduces blockchain based approach combined rsa asymmetric encryption algorithm enables secure decryption without direct key transmission experimental results demonstrate proposed solution significantly reduces communication overhead compared existing dual layer consensus iot based architectures nodes achieves transactions overhead tps minimal system memory usage tps furthermore high throughput tps algorithm completes transactions far faster competing methods improvements enhance security efficiency offering reliable framework commerce transactions logistics data sharing strengthening trust digital trade
"Infectious diseases have always been a focus of global public health attention. However, the current methods for analyzing and predicting the trend of infectious disease transmission are not comprehensive, resulting in significant discrepancies in the analysis results. To address this issue, a differential equation model of infectious diseases with latent periods is proposed for Susceptible-Exposed-Infectious-Recovered-Susceptible. On this basis, a staged model combined with logistic regression is established. Finally, in response to the problem of missing data related to infectious diseases in certain regions, LSTM networks and autoregressive integrated moving average models are used for prediction to assist differential equation models in disease transmission analysis. Through empirical analysis, it is known that the fitting degree between the results obtained from the infectious disease differential equation model proposed in the study and the actual results reaches 0.95, with a root mean square error of 0.03 and an average relative error of 3.5%. The prediction model proposed in the study achieves an accuracy of 92.72%, reaching convergence accuracy and convergence loss values after only 23 and 22 iterations. The differential equation model for infectious diseases proposed in the study can more accurately predict the spread trend of infectious diseases and provide scientific basis for public health decision-making.",infectious diseases always focus global public health attention however current methods analyzing predicting trend infectious disease transmission comprehensive resulting significant discrepancies analysis results address issue differential equation model infectious diseases latent periods proposed susceptible exposed infectious recovered susceptible basis staged model combined logistic regression established finally response problem missing data related infectious diseases certain regions lstm networks autoregressive integrated moving average models used prediction assist differential equation models disease transmission analysis empirical analysis known fitting degree results obtained infectious disease differential equation model proposed study actual results reaches root mean square error average relative error prediction model proposed study achieves accuracy reaching convergence accuracy convergence loss values iterations differential equation model infectious diseases proposed study accurately predict spread trend infectious diseases provide scientific basis public health decision making
"To improve the recognition accuracy of pavement crack target detection robots based on image processing, this research proposes a road crack recognition model built on the improved YOLOv4 algorithm. The enhanced YOLOv4 model, which utilizes optimized K-means and attention mechanisms for improved detection, is combined with a binocular vision calibration method for accurate camera setup. Feature points are extracted and matched using an accelerated robust feature algorithm to measure crack widths, resulting in a novel binocular vision-based road crack detection system. The experiment demonstrated that the detection accuracy of the proposed enhanced YOLOv4 exceeded 97% on datasets, with recall, F1, AUC, and precision values of 0.973, 0.986, 0.985, and 0.989, respectively. The accuracy of the research model exceeded 98%, and the fit with the sample data exceeded 96%. The research methods can effectively achieve intelligent and accurate detection of road cracks, which has positive significance for road maintenance work.",improve recognition accuracy pavement crack target detection robots based image processing research proposes road crack recognition model built improved yolov algorithm enhanced yolov model utilizes optimized means attention mechanisms improved detection combined binocular vision calibration method accurate camera setup feature points extracted matched using accelerated robust feature algorithm measure crack widths resulting novel binocular vision based road crack detection system experiment demonstrated detection accuracy proposed enhanced yolov exceeded datasets recall auc precision values respectively accuracy research model exceeded fit sample data exceeded research methods effectively achieve intelligent accurate detection road cracks positive significance road maintenance work
"While natural language processing (NLP) has made significant strides in high-resource languages (HRLs), many languages still lack sufficient representation in training data. Many more such languages remain underrepresented in training data. Cross-lingual transfer learning (CTLT) has been proposed as an exciting use case where an application of these text classifiers trained on HRLs will be extended to low-resource languages (LRLs), mostly using zero and few-shot techniques. This notwithstanding, conventional multilingual language models usually underperform in some languages. Using classification along with language modeling does not work for all language combinations for optimal results. The approach that we propose is MT-TC: a new hybrid framework upon which the traditional translate-test approach can be revisited and refined. That is, MT-TC combines a text classifier trained in the high-resource language with a neural machine translation (NMT) model. Because it enjoys “soft” differentiable translations, backpropagation can be performed end-to-end on an architecture to jointly fine-tune both components. We evaluate MT-TC on Taxi1500 dataset with 20 typologically diverse languages. The macro F1-score reaches 0.88 for the proposed method, outperforming baseline multilingual models by as much as 12.5% in low-resource settings. These results show that under the aspect of minimum-annotated data availability, MT-TC performs significantly well and qualifies itself as a very within-real-world-scalable solution for a lot of multilingual NLP applications.",natural language processing nlp made significant strides high resource languages hrls many languages still lack sufficient representation training data many languages remain underrepresented training data cross lingual transfer learning ctlt proposed exciting use case application text classifiers trained hrls extended low resource languages lrls mostly using zero shot techniques notwithstanding conventional multilingual language models usually underperform languages using classification along language modeling work language combinations optimal results approach propose new hybrid framework upon traditional translate test approach revisited refined combines text classifier trained high resource language neural machine translation nmt model enjoys soft differentiable translations backpropagation performed end end architecture jointly fine tune components evaluate taxi dataset typologically diverse languages macro score reaches proposed method outperforming baseline multilingual models much low resource settings results show aspect minimum annotated data availability performs significantly well qualifies within real world scalable solution lot multilingual nlp applications
"The development of classroom teaching evaluation in China is still in its infancy, despite the country’s rapid growth in music education. The quality of music instruction is not scientifically and reasonably evaluated based on actual teaching quality. When it comes to teaching and evaluating music, music teachers simply follow the classroom teaching evaluation indicators and models of any other topic in the school. An evaluation system that is merely a form, and does not have any meaningful impact on evaluation, feedback, or promotion is inevitable as a result of this. That’s why a deep-level, in-depth study of music classroom teaching’s quality evaluation system is still needed, along with a theoretical-and-practical combination. This work takes the music teaching as the starting point to carry out the corresponding research. First, this work builds a music teaching quality evaluation system via B/S architecture. It includes three parts: client, application unit, and database. The application unit includes user management, online evaluation, data management, evaluation result query and analysis. After users enter the system, they will score the teaching quality. Second, this work proposes the use of multi-scale convolution to extract data features, and proposes an improved attention mechanism to construct MS-IATT-CNN for evaluating music teaching quality. Third, this work has carried out a comprehensive and systematic experiment, and the experimental data has verified the feasibility of this work.",development classroom teaching evaluation china still infancy despite country rapid growth music education quality music instruction scientifically reasonably evaluated based actual teaching quality comes teaching evaluating music music teachers simply follow classroom teaching evaluation indicators models topic school evaluation system merely form meaningful impact evaluation feedback promotion inevitable result deep level depth study music classroom teaching quality evaluation system still needed along theoretical practical combination work takes music teaching starting point carry corresponding research first work builds music teaching quality evaluation system via architecture includes three parts client application unit database application unit includes user management online evaluation data management evaluation result query analysis users enter system score teaching quality second work proposes use multi scale convolution extract data features proposes improved attention mechanism construct iatt cnn evaluating music teaching quality third work carried comprehensive systematic experiment experimental data verified feasibility work
"Intelligent manufacturing is the direction of transformation and upgrading in the manufacturing industry, reflecting the level of manufacturing. This article uses three third-party recruitment platforms, Zhaopin, 51Job, and Boss Zhipin as information sources, and searches for keywords such as “intelligent manufacturing” and “intelligent production.” The region is limited to Fujian Province, and recruitment data is collected using a Houyi data collector to crawl 363 valid recruitment information from September 2022 to September 2023. Based on the data collected above, analyze the demand and influencing factors for intelligent manufacturing talents in enterprises in Fujian Province. How to match the talents needed for the continuous development and growth of the intelligent manufacturing industry in Fujian Province is an urgent problem that needs to be solved. Based on the above analysis, countermeasures and suggestions are proposed from four levels: individuals, universities, enterprises, and society: Students continuously enrich their theoretical knowledge system, actively form teams with classmates, and actively participate in enterprise internships. Employed employees in intelligent manufacturing positions in enterprises actively participate in skill training organized by the company and apply cutting-edge technology to their work. Universities should fully communicate with industry enterprises, optimize the setting of professional disciplines and training programs, increase opportunities for students to practice in enterprises, guide students to obtain professional qualifications in intelligent manufacturing, participate in intelligent manufacturing competitions, and strengthen the construction of a dual teacher team. Enterprises should fully communicate with universities and establish a training mechanism that integrates industry and education. Rely on industry associations, organize employees to participate in intelligent manufacturing skills training, set up an intelligent manufacturing Internet sharing platform, and do a good job in technical personnel service. Society should vigorously promote the development prospects and potential of intelligent manufacturing, and guide job seekers to choose the field of intelligent manufacturing for development.",intelligent manufacturing direction transformation upgrading manufacturing industry reflecting level manufacturing article uses three third party recruitment platforms zhaopin job boss zhipin information sources searches keywords intelligent manufacturing intelligent production region limited fujian province recruitment data collected using houyi data collector crawl valid recruitment information september september based data collected analyze demand influencing factors intelligent manufacturing talents enterprises fujian province match talents needed continuous development growth intelligent manufacturing industry fujian province urgent problem needs solved based analysis countermeasures suggestions proposed four levels individuals universities enterprises society students continuously enrich theoretical knowledge system actively form teams classmates actively participate enterprise internships employed employees intelligent manufacturing positions enterprises actively participate skill training organized company apply cutting edge technology work universities fully communicate industry enterprises optimize setting professional disciplines training programs increase opportunities students practice enterprises guide students obtain professional qualifications intelligent manufacturing participate intelligent manufacturing competitions strengthen construction dual teacher team enterprises fully communicate universities establish training mechanism integrates industry education rely industry associations organize employees participate intelligent manufacturing skills training set intelligent manufacturing internet sharing platform good job technical personnel service society vigorously promote development prospects potential intelligent manufacturing guide job seekers choose field intelligent manufacturing development
"Along with the continuous promotion of curriculum reform, English has become a very important course content in the stage of higher vocational education, which is an important path to cultivate English-speaking professional talents and integrate them into social development. At the same time, the continuous promotion of educational reform, but also the diversified teaching methods into the English curriculum, the application of multimodal teaching mode in higher vocational English teaching, is bound to enhance the quality of English teaching to a greater extent. Based on this, this paper uses the hierarchical analysis method to construct a multimodal intelligent classroom teaching evaluation index system for higher vocational English, and teachers, media, and students are assigned three indicators, of which teachers occupy 20%, teaching media 60%, and students 20%. And puts forward the corresponding teaching optimization strategy, from adopting multimodal English teaching resources, creating a multimodal English teaching atmosphere, developing multimodal English teaching activities, and implementing multimodal English teaching evaluation, to put forward corresponding teaching suggestions. And puts forward the corresponding teaching optimization strategy, from adopting multimodal English teaching resources, creating a multimodal English teaching atmosphere, formulating multimodal English teaching activities, and implementing multimodal English teaching evaluation, so as to provide certain reference for the improvement of higher vocational English teaching level and the application of multimodal intelligent classroom teaching mode.",along continuous promotion curriculum reform english become important course content stage higher vocational education important path cultivate english speaking professional talents integrate social development time continuous promotion educational reform also diversified teaching methods english curriculum application multimodal teaching mode higher vocational english teaching bound enhance quality english teaching greater extent based paper uses hierarchical analysis method construct multimodal intelligent classroom teaching evaluation index system higher vocational english teachers media students assigned three indicators teachers occupy teaching media students puts forward corresponding teaching optimization strategy adopting multimodal english teaching resources creating multimodal english teaching atmosphere developing multimodal english teaching activities implementing multimodal english teaching evaluation put forward corresponding teaching suggestions puts forward corresponding teaching optimization strategy adopting multimodal english teaching resources creating multimodal english teaching atmosphere formulating multimodal english teaching activities implementing multimodal english teaching evaluation provide certain reference improvement higher vocational english teaching level application multimodal intelligent classroom teaching mode
"As aerobics becomes an increasingly popular form of exercise, the demand for precise movement standards has grown. Traditional human pose recognition models no longer meet the practical requirements of aerobics scenarios. To address this, the study is based on open pose estimation technology, optimized with attention mechanisms and graph neural networks, and proposes a hybrid human pose recognition model. Performance validation and ablation experiments show that the model have an accuracy of 95% and a loss value as low as 0.0089. The highest score for human key points is 7.5, with angle error and position error reduced to 4.9% and 5.4%, respectively, outperforming the base algorithm. This highlights the success of the proposed optimization and enhancement techniques. In practical application comparison experiments, the recognition model achieves a running time of 8 ms when recognizing 150 images, significantly outperforming the comparison models. In multi-person recognition experiments, the proposed model reaches an accuracy of 93%. Additionally, the model shows superior performance in visualizing human pose recognition in practical scenarios. These results indicate that the model has high recognition accuracy and robustness, and can adapt to various real-world applications, meeting the high demands for human pose recognition in aerobics.",aerobics becomes increasingly popular form exercise demand precise movement standards grown traditional human pose recognition models longer meet practical requirements aerobics scenarios address study based open pose estimation technology optimized attention mechanisms graph neural networks proposes hybrid human pose recognition model performance validation ablation experiments show model accuracy loss value low highest score human key points angle error position error reduced respectively outperforming base algorithm highlights success proposed optimization enhancement techniques practical application comparison experiments recognition model achieves running time recognizing images significantly outperforming comparison models multi person recognition experiments proposed model reaches accuracy additionally model shows superior performance visualizing human pose recognition practical scenarios results indicate model high recognition accuracy robustness adapt various real world applications meeting high demands human pose recognition aerobics
"To improve the intelligent management of e-commerce customer data, this study proposes an intelligent classification method for e-commerce customers based on an improved extreme gradient boosting tree model. The method uses an improved particle swarm optimization algorithm to optimize the hyperparameters of the extreme gradient boosting tree, so as to improve the model’s ability to adapt to complex data. The outcomes indicated that the proposed model achieved 91.6% in terms of accuracy, which was a 5.0% improvement over the baseline model. The recall rate was 88.9% and the F1 value was 90.2%. In addition, the proposed method was applied to an e-commerce platform. The obtained customer categorization accuracy was 92.3%, the recall rate was 92.1%, the processing cost was 0.8 yuan/thousand times, and the processing efficiency was 480,000 items/day. In terms of robustness, the accuracy using the proposed method was stable at around 90%, with AUC fluctuations ranging from 0.91 to 0.95, and the maximum memory fluctuation of 15.6%. The CPU utilization rate always remained within the safety threshold of 92%. In terms of real-time, the average response time was 28 ms for a single request, 210 ms for batch processing, and 41 ms under peak traffic. The CPU utilization in the three cases was 22%, 48%, and 61%, respectively. The study’s findings demonstrate that the suggested approach can offer e-commerce clients an accurate and effective intelligent categorization solution.",improve intelligent management commerce customer data study proposes intelligent classification method commerce customers based improved extreme gradient boosting tree model method uses improved particle swarm optimization algorithm optimize hyperparameters extreme gradient boosting tree improve model ability adapt complex data outcomes indicated proposed model achieved terms accuracy improvement baseline model recall rate value addition proposed method applied commerce platform obtained customer categorization accuracy recall rate processing cost yuan thousand times processing efficiency items day terms robustness accuracy using proposed method stable around auc fluctuations ranging maximum memory fluctuation cpu utilization rate always remained within safety threshold terms real time average response time single request batch processing peak traffic cpu utilization three cases respectively study findings demonstrate suggested approach offer commerce clients accurate effective intelligent categorization solution
"The period of college students is an important life stage in the transition from teenagers to youth, and it is the period with the most violent physiological and psychological changes in the growth of whole life. WI-FI has brought many positive changes to the lifestyle of college students, and has also had many negative effects on college students. Under the WI-FI environment, college students’ health is worrying. The number of “head-downers” on campus has increased sharply, and the phenomenon of dependence on the WI-FI has become more and more serious, which seriously endangers the health of students. In view of this, this paper conducts an in-depth analysis in the WI-FI environment, hoping to understand its specific impact and propose effective solutions based on its existing problems. By analyzing the impact of WI-FI environment on college students’ physical and mental health, find out the specific types of morbidity caused by WI-FI environment on college students’ physical and mental health. The data of a college student group in a university are collected to establish a judgment model and recognition model based on deep learning. The model integrates convolutional neural networks (CNN) and machine learning algorithms to comprehensively assess the health status of college students. This initiative helps to better implement the guiding principle of “health first” in school education and promotes campus culture construction.",period college students important life stage transition teenagers youth period violent physiological psychological changes growth whole life brought many positive changes lifestyle college students also many negative effects college students environment college students health worrying number head downers campus increased sharply phenomenon dependence become serious seriously endangers health students view paper conducts depth analysis environment hoping understand specific impact propose effective solutions based existing problems analyzing impact environment college students physical mental health find specific types morbidity caused environment college students physical mental health data college student group university collected establish judgment model recognition model based deep learning model integrates convolutional neural networks cnn machine learning algorithms comprehensively assess health status college students initiative helps better implement guiding principle health first school education promotes campus culture construction
"We present UPE-YOLOv7, a novel UAV-based framework for real-time tailings pollution monitoring, combining an enhanced YOLOv7 detector with multi-modal image enhancement. Targeting challenges like dust, low-light, and small low-saliency targets in mining areas, our method introduces: (1) a multi-scale pyramid network for noise-resistant feature extraction, (2) a hybrid edge-texture enhancer to sharpen contamination boundaries, (3) a context-aware semantic module for background suppression, and (4) a self-adaptive denoiser for degraded images. Experiments on a custom UAV dataset show UPE-YOLOv7 achieves 82.3% mAP (vs YOLOv7’s 75.6%), with stable performance across target sizes and noise levels. This demonstrates its potential as a scalable solution for automated environmental monitoring in challenging mining scenarios.",present upe yolov novel uav based framework real time tailings pollution monitoring combining enhanced yolov detector multi modal image enhancement targeting challenges like dust low light small low saliency targets mining areas method introduces multi scale pyramid network noise resistant feature extraction hybrid edge texture enhancer sharpen contamination boundaries context aware semantic module background suppression self adaptive denoiser degraded images experiments custom uav dataset show upe yolov achieves map yolov stable performance across target sizes noise levels demonstrates potential scalable solution automated environmental monitoring challenging mining scenarios
"With the development of Internet of Things technology, blockchain consensus technology has been extensively applied in various fields. To address the low efficiency in blockchain consensus, the research proposes a clustering blockchain consensus algorithm. The new algorithm reduces the communication complexity through the hierarchical network architecture and dynamic clustering mechanism, and optimizes the consensus process by combining the node reputation evaluation model. Moreover, the algorithm innovatively introduces cyclic redundancy check technology to enhance data validation. The research results showed that the new algorithm significantly improved the consensus efficiency of blockchain in wireless sensor networks. The block generation delay was only 338 ms, and the average throughput reached 793. This is better than that of the “Practical Byzantine Fault Tolerance based on Evaluation and Voting Mechanism” (0.45%, 398.5 ms, and 738). Under high load conditions, the probability of transaction authentication failure was 0.38%. The new algorithm can significantly improve the efficiency of blockchain consensus through clustering and network layering, which is significant for the development of blockchain in wireless sensor network areas.",development internet things technology blockchain consensus technology extensively applied various fields address low efficiency blockchain consensus research proposes clustering blockchain consensus algorithm new algorithm reduces communication complexity hierarchical network architecture dynamic clustering mechanism optimizes consensus process combining node reputation evaluation model moreover algorithm innovatively introduces cyclic redundancy check technology enhance data validation research results showed new algorithm significantly improved consensus efficiency blockchain wireless sensor networks block generation delay average throughput reached better practical byzantine fault tolerance based evaluation voting mechanism high load conditions probability transaction authentication failure new algorithm significantly improve efficiency blockchain consensus clustering network layering significant development blockchain wireless sensor network areas
"In order to further promote the innovative design of College English teaching and improve the level of English teaching, this paper proposes a college English teaching evaluation system based on multimodal discourse analysis. Firstly, a new teaching evaluation model was constructed to deeply explore the problems of multimodal discourse in English textbooks from the perspective of multimodal discourse analysis. Secondly, applying multimodal discourse analysis to English teaching evaluation provides new perspectives and methods for evaluation. Finally, an English teaching evaluation algorithm and scheme were designed based on the MATLAB 7.0 environment, and the feasibility of the algorithm was tested through alpha coefficient. The consistency of teachers’ evaluation of students’ learning status was determined, and the English teaching evaluation mode was innovated. The results show that the alpha coefficient is between 0.00 and 1.00, which also shows that there is a strong consistency between the two projects. At the same time, it also shows the feasibility of the system and the effectiveness of the algorithm.",order promote innovative design college english teaching improve level english teaching paper proposes college english teaching evaluation system based multimodal discourse analysis firstly new teaching evaluation model constructed deeply explore problems multimodal discourse english textbooks perspective multimodal discourse analysis secondly applying multimodal discourse analysis english teaching evaluation provides new perspectives methods evaluation finally english teaching evaluation algorithm scheme designed based matlab environment feasibility algorithm tested alpha coefficient consistency teachers evaluation students learning status determined english teaching evaluation mode innovated results show alpha coefficient also shows strong consistency two projects time also shows feasibility system effectiveness algorithm
"Generative artificial intelligence has swept into design research and practice, with popular image generators and language model interfaces now being used across disciplines and design stages. The multimodal language models exhibit fundamentally different characteristics from earlier machine learning approaches, and present unique challenges and opportunities for designers. A key question for design research arises: How can the capabilities of these large multimodal models become part of our design tools? This work investigates how the recent development of reasoning language models in particular can be utilized for early stage exploratory design. We outline a set of principles to build effective tools with these models by drawing from research in artificial intelligence. Specifically, we explore the application of the new reasoning model paradigm in computational design scripting. We present a system which successfully generates computer-aided design (CAD) scripts in Grasshopper. The system is compared to existing CAD AI approaches for its capabilities and characteristics. High latency and the absence of a user-model feedback loop still restrict real-time exploration in our system. Our developed principles can further inform broad exploratory tools and motivate inquiries into multimodal reasoning for design. By demonstrating a new CAD approach, we show large models’ utility as creative tools. Future research should address latency, enhance interactivity, and address other design tasks beyond CAD.",generative artificial intelligence swept design research practice popular image generators language model interfaces used across disciplines design stages multimodal language models exhibit fundamentally different characteristics earlier machine learning approaches present unique challenges opportunities designers key question design research arises capabilities large multimodal models become part design tools work investigates recent development reasoning language models particular utilized early stage exploratory design outline set principles build effective tools models drawing research artificial intelligence specifically explore application new reasoning model paradigm computational design scripting present system successfully generates computer aided design cad scripts grasshopper system compared existing cad approaches capabilities characteristics high latency absence user model feedback loop still restrict real time exploration system developed principles inform broad exploratory tools motivate inquiries multimodal reasoning design demonstrating new cad approach show large models utility creative tools future research address latency enhance interactivity address design tasks beyond cad
"Traditional campus services are often difficult to meet the different needs of students, faculty, and school administrators. It is necessary to design a future campus intelligent service platform, which uses micro-service component architecture to achieve modularity and scalability. Combined with intelligent computer technology, this paper combines intelligent technology to build a smart campus (SC) system under the concept of digital intelligence empowerment, designs a fit calculation formula based on user portraits, and organically integrates user portraits with learning resource recommendations. This paper chooses collaborative filtering algorithm and association rule algorithm and makes use of users’ professional characteristics to supplement them to generate recommended candidate sets. Finally, through the experimental study, we can see that the user portrait recommendation method proposed in this paper is suitable for teaching resources recommendation in SC, and through systematic evaluation, we can see that the SC system proposed in this paper is theoretically reliable.",traditional campus services often difficult meet different needs students faculty school administrators necessary design future campus intelligent service platform uses micro service component architecture achieve modularity scalability combined intelligent computer technology paper combines intelligent technology build smart campus system concept digital intelligence empowerment designs fit calculation formula based user portraits organically integrates user portraits learning resource recommendations paper chooses collaborative filtering algorithm association rule algorithm makes use users professional characteristics supplement generate recommended candidate sets finally experimental study see user portrait recommendation method proposed paper suitable teaching resources recommendation systematic evaluation see system proposed paper theoretically reliable
"In order to improve the effect of collection information management, this paper uses intelligent computer technology to build a cultural and creative element resource library based on the collection information management system. This paper optimizes the design of TPBF by analyzing the false positive rate of the two-stage Bloomfilter query algorithm, determines the use conditions of the new algorithm, and obtains the optimal parameters of the two-stage Bloomfilter query algorithm according to the model. Aiming at the limited improvement of the standard Bloomfilter algorithm and its extended algorithm to reduce the false positive rate, this paper proposes a two-stage Bloomfilter algorithm. Moreover, this paper analyzes the parameters when the algorithm achieves the optimal result, and then proves that the result achieved by the algorithm is consistent with the theoretical expectation through simulation experiments. The experimental research shows that the cultural and creative element resource library based on the collection information management system proposed in this paper can effectively improve the information processing effect of museum collections.",order improve effect collection information management paper uses intelligent computer technology build cultural creative element resource library based collection information management system paper optimizes design tpbf analyzing false positive rate two stage bloomfilter query algorithm determines use conditions new algorithm obtains optimal parameters two stage bloomfilter query algorithm according model aiming limited improvement standard bloomfilter algorithm extended algorithm reduce false positive rate paper proposes two stage bloomfilter algorithm moreover paper analyzes parameters algorithm achieves optimal result proves result achieved algorithm consistent theoretical expectation simulation experiments experimental research shows cultural creative element resource library based collection information management system proposed paper effectively improve information processing effect museum collections
"As the world transitions toward a more sustainable and digitalized energy landscape, the management of grid assets has become increasingly complex and critical to ensuring the efficient delivery of electricity. The integration of renewable energy systems (RES) with grid-connected load infrastructures has proven to be an effective solution for enhancing system reliability and reducing transmission losses. However, this integration often introduces power quality (PQ) issues, such as voltage sags, swells, and fluctuations, which can compromise grid stability. To address these challenges, this research proposes a novel solution for sustainable digital grid asset management using the Unified Power Quality Conditioner (UPQC) and Weighted Salp Swarm Driven One-Class Support Vector Machine (WSS-OCSVM), which integrates the Weighted Salp Swarm Algorithm (WSSA) with Weighted One-Class Support Vector Machine (WOCSVM). The approach aims to mitigate PQ disturbances, optimize asset disposal strategies, and evaluate the ecological impact of grid components. The methodology is implemented in several stages: first, the UPQC is used to regulate power quality, followed by the integration of WSS-OCSVM to optimize decision-making processes for asset management. This system is further enhanced by the SEASWARM platform, which enables intelligent adaptation and decision-making for asset disposal, considering environmental factors like carbon emissions and recyclability. The efficacy of the suggested method is confirmed by MATLAB simulation studies, which show better performance than existing techniques. Although evaluated on the 39-bus system, the suggested UPQC + WSS-OCSVM approach produced results of 99.94% accuracy. The results indicate that the UPQC + WSS-OCSVM model moderates PQ disturbances and also optimizes asset lifecycle management and promotes ecological sustainability. This comprehensive solution offers a significant advancement in the development of intelligent, resilient, and environmentally conscious modern power grids.",world transitions toward sustainable digitalized energy landscape management grid assets become increasingly complex critical ensuring efficient delivery electricity integration renewable energy systems res grid connected load infrastructures proven effective solution enhancing system reliability reducing transmission losses however integration often introduces power quality issues voltage sags swells fluctuations compromise grid stability address challenges research proposes novel solution sustainable digital grid asset management using unified power quality conditioner upqc weighted salp swarm driven one class support vector machine wss ocsvm integrates weighted salp swarm algorithm wssa weighted one class support vector machine wocsvm approach aims mitigate disturbances optimize asset disposal strategies evaluate ecological impact grid components methodology implemented several stages first upqc used regulate power quality followed integration wss ocsvm optimize decision making processes asset management system enhanced seaswarm platform enables intelligent adaptation decision making asset disposal considering environmental factors like carbon emissions recyclability efficacy suggested method confirmed matlab simulation studies show better performance existing techniques although evaluated bus system suggested upqc wss ocsvm approach produced results accuracy results indicate upqc wss ocsvm model moderates disturbances also optimizes asset lifecycle management promotes ecological sustainability comprehensive solution offers significant advancement development intelligent resilient environmentally conscious modern power grids
"The development of big data technology has brought more developable opportunities to the field of English teaching. This article discusses the role and strategies of big data in English teaching from the perspective of learning situation analysis, and proposes a learning situation analysis model based on big data, which utilizes data science and artificial intelligence technology to achieve personalized and intelligent teaching. The article introduces the background and goals of teaching change, describes the theoretical framework and technical components of the learning situation analysis model, and demonstrates the methods and effects of personalized teaching, and the process and results of experimental evaluation. The article also gives suggestions for promoting big data-accurate teaching and emphasizes the potential and prospects of big data in English teaching.",development big data technology brought developable opportunities field english teaching article discusses role strategies big data english teaching perspective learning situation analysis proposes learning situation analysis model based big data utilizes data science artificial intelligence technology achieve personalized intelligent teaching article introduces background goals teaching change describes theoretical framework technical components learning situation analysis model demonstrates methods effects personalized teaching process results experimental evaluation article also gives suggestions promoting big data accurate teaching emphasizes potential prospects big data english teaching
"With rapid development of information technology, virtual simulation technology has gradually integrated into the teaching system of vocational colleges and has become an important means to improve teaching quality and effect. However, the existing teaching feedback mechanism is often single and lagging, and it is not easy to comprehensively and in real time reflect the teaching effect and students’ learning status. Therefore, this study aims to explore how virtual simulation technology can optimize the multi-level feedback mechanism in the information teaching of vocational colleges to realize the precise regulation and personalized guidance of the teaching process. Vocational colleges pay attention to cultivating practical skills, and virtual simulation technology can provide a working environment close to accurate, effectively making up for the shortcomings of traditional teaching. However, the lack of adequate feedback mechanism makes it difficult to maximize the teaching effect. This study constructed a multi-level feedback system based on virtual simulation technology, including three levels: real-time interactive feedback, phased evaluation feedback and comprehensive analysis feedback. In the experiment, we selected 1000 students and 50 teachers from three vocational colleges as the research objects and carried out a one-semester teaching practice. The experimental results show that after the introduction of a multi-level feedback mechanism, students’ academic performance has improved by 20% on average, their practical skills have improved by 25%, and their learning satisfaction has reached 90%. At the same time, teachers’ teaching efficiency has increased by 15%, and their teaching satisfaction has reached 95%. In addition, the accuracy rate of real-time feedback of the system reaches 88%, the accuracy rate of phased evaluation reaches 92%, and the accuracy rate of comprehensive analysis reaches 95%. Virtual simulation technology optimizes the multi-level feedback mechanism in information teaching of vocational colleges, which can significantly improve teaching and students’ learning effects, and provide strong support for innovation and development of vocational education.",rapid development information technology virtual simulation technology gradually integrated teaching system vocational colleges become important means improve teaching quality effect however existing teaching feedback mechanism often single lagging easy comprehensively real time reflect teaching effect students learning status therefore study aims explore virtual simulation technology optimize multi level feedback mechanism information teaching vocational colleges realize precise regulation personalized guidance teaching process vocational colleges pay attention cultivating practical skills virtual simulation technology provide working environment close accurate effectively making shortcomings traditional teaching however lack adequate feedback mechanism makes difficult maximize teaching effect study constructed multi level feedback system based virtual simulation technology including three levels real time interactive feedback phased evaluation feedback comprehensive analysis feedback experiment selected students teachers three vocational colleges research objects carried one semester teaching practice experimental results show introduction multi level feedback mechanism students academic performance improved average practical skills improved learning satisfaction reached time teachers teaching efficiency increased teaching satisfaction reached addition accuracy rate real time feedback system reaches accuracy rate phased evaluation reaches accuracy rate comprehensive analysis reaches virtual simulation technology optimizes multi level feedback mechanism information teaching vocational colleges significantly improve teaching students learning effects provide strong support innovation development vocational education
"In the evolving landscape of digital transformation and intelligent systems, automated asset management is crucial for enhancing operational efficiency and strategic resource planning. This research introduces a novel hybrid model, Logistic Regression-Weighted Random Forest (LR-WRF), combining the benefits of Logistic Regression (LR) and Weighted Random Forest (WRF) to automate asset classification and disposal predictions. The system collects historical data, including asset ID, location, usage history, maintenance logs, and physical condition, which are pre-processed to remove inconsistencies and normalise formats. Principal Component Analysis (PCA) is applied for feature extraction, reducing data dimensionality while retaining essential information. A Weighted Random Forest (WRF) classifier is then used to categorise assets into operational states such as Active, Degraded, or Obsolete. To predict disposal eligibility, a Logistic Regression model assesses whether an asset should be retained, sold, recycled, or discarded. The experimental results show that the LR-WRF model provides high classification accuracy and offers effective disposal recommendations, significantly reducing manual intervention and ensuring compliance with asset lifecycle policies. The proposed model was evaluated across a variety of metrics, including accuracy (98.82%), precision, recall, and F1-score. This research advances the field of intelligent asset management by integrating machine learning techniques into automated decision-making processes, thereby enhancing the efficiency, accuracy, and timeliness of asset management practices in organisations.",evolving landscape digital transformation intelligent systems automated asset management crucial enhancing operational efficiency strategic resource planning research introduces novel hybrid model logistic regression weighted random forest wrf combining benefits logistic regression weighted random forest wrf automate asset classification disposal predictions system collects historical data including asset location usage history maintenance logs physical condition pre processed remove inconsistencies normalise formats principal component analysis pca applied feature extraction reducing data dimensionality retaining essential information weighted random forest wrf classifier used categorise assets operational states active degraded obsolete predict disposal eligibility logistic regression model assesses whether asset retained sold recycled discarded experimental results show wrf model provides high classification accuracy offers effective disposal recommendations significantly reducing manual intervention ensuring compliance asset lifecycle policies proposed model evaluated across variety metrics including accuracy precision recall score research advances field intelligent asset management integrating machine learning techniques automated decision making processes thereby enhancing efficiency accuracy timeliness asset management practices organisations
"With rapid development of information technology, intelligent education has become an important research direction. English learning is an integral part of education, and its learning behavior analysis and teaching optimization have attracted much attention. Based on the SSA-GRU (Singular Spectrum Analysis-Gated Recurrent Unit) model, this study aims to deeply explore characteristics of English learning behavior to provide a scientific basis for teaching optimization. Firstly, a comprehensive learning behavior database is constructed by collecting online learning data of English learners, including learning time, interaction frequency, and homework completion. Subsequently, the SSA-GRU model is used to mine the data deeply, and the potential laws and trends in learning behavior are effectively extracted. The experimental results show that compared with traditional analysis methods, the SSA-GRU model improves the accuracy of behavioral pattern recognition by 15.3% and trend prediction by 12.7%. Further analysis shows that learners’ learning time is positively correlated with their achievements (the correlation coefficient is 0.78), and the influence of interaction frequency on the learning effect is particularly significant (the improvement effect is 18.4%). This study proposes targeted teaching optimization strategies based on these analysis results, including personalized learning path recommendations and interactive teaching activity design. Practical application shows that these strategies have effectively improved the teaching effect, with students’ achievement increasing by 10.2% on average and learning satisfaction rising by 22.5%. This study not only enriches the theory of intelligent education but also provides strong support for English teaching practice, showing the broad application prospect of the SSA-GRU model in English learning behavior analysis and teaching optimization.",rapid development information technology intelligent education become important research direction english learning integral part education learning behavior analysis teaching optimization attracted much attention based ssa gru singular spectrum analysis gated recurrent unit model study aims deeply explore characteristics english learning behavior provide scientific basis teaching optimization firstly comprehensive learning behavior database constructed collecting online learning data english learners including learning time interaction frequency homework completion subsequently ssa gru model used mine data deeply potential laws trends learning behavior effectively extracted experimental results show compared traditional analysis methods ssa gru model improves accuracy behavioral pattern recognition trend prediction analysis shows learners learning time positively correlated achievements correlation coefficient influence interaction frequency learning effect particularly significant improvement effect study proposes targeted teaching optimization strategies based analysis results including personalized learning path recommendations interactive teaching activity design practical application shows strategies effectively improved teaching effect students achievement increasing average learning satisfaction rising study enriches theory intelligent education also provides strong support english teaching practice showing broad application prospect ssa gru model english learning behavior analysis teaching optimization
"With the acceleration of urbanization, the imbalance between urban and rural spatial distribution and resource allocation has become increasingly prominent. National political forces play a key role in regulating this contradiction. This study aims to explore how national political forces affect the spatial distribution and resource allocation of urban and rural areas through policy intervention. The research background covers the complexity of urban and rural development under globalization and the guiding role of national policies in regional development. By constructing a comprehensive analysis model, it is found that the adjustment coefficient of national policies on urban and rural population distribution reaches 0.75, which significantly promotes the urbanization process. Regarding resource allocation, policy intervention has narrowed the difference in resource allocation between urban and rural areas by 20%, effectively alleviating the problem of resource inequality. In addition, the regional development pattern optimization index has increased by 30% under the guidance of policies, indicating the important role of national political forces in optimizing spatial structure and promoting the integrated development of urban and rural areas. The results of this study not only reveal the regulation mechanism of national political forces in urban and rural development and provide a scientific basis for future policy formulation, emphasizing the core position of the state in promoting the coordinated development of urban and rural areas.",acceleration urbanization imbalance urban rural spatial distribution resource allocation become increasingly prominent national political forces play key role regulating contradiction study aims explore national political forces affect spatial distribution resource allocation urban rural areas policy intervention research background covers complexity urban rural development globalization guiding role national policies regional development constructing comprehensive analysis model found adjustment coefficient national policies urban rural population distribution reaches significantly promotes urbanization process regarding resource allocation policy intervention narrowed difference resource allocation urban rural areas effectively alleviating problem resource inequality addition regional development pattern optimization index increased guidance policies indicating important role national political forces optimizing spatial structure promoting integrated development urban rural areas results study reveal regulation mechanism national political forces urban rural development provide scientific basis future policy formulation emphasizing core position state promoting coordinated development urban rural areas
"In today’s rapidly evolving digital landscape, Power Marketing Management Platforms (PMMPs) have emerged as essential tools for businesses seeking to optimize their marketing strategies. These platforms integrate advanced technologies and data analytics to deliver targeted campaigns, automate processes, and generate valuable insights into customer behavior. However, traditional marketing approaches often face challenges such as data silos, poor cross-channel integration, and a limited ability to adapt to shifting consumer behaviors. This research investigates the application of machine learning (ML) algorithms to enhance electricity consumption analysis and user engagement within PMMPs. A comprehensive framework is proposed, incorporating advanced analytical models for user segmentation, behavioral analysis, and consumption forecasting. The K-Means clustering algorithm segments users based on their electricity consumption patterns, enabling a more tailored service approach. To analyze temporal shifts in user behavior and forecast consumption trends, this research introduces the Dynamic Random Synthesized Light Gradient Boosting Machine (DR-LightGBM). This model delivers actionable insights that support grid scheduling and resource optimization. The system develops personalized marketing strategies, such as dynamic pricing models and energy-saving incentives, to enhance user participation and promote energy conservation. The effectiveness of the proposed system is evaluated using several performance metrics. RMSE (38.11 kW), along with MAPE, MAE, and training time, assess the accuracy of consumption forecasting, while engagement rate and reduction in energy usage serve as business-relevant indicators of marketing impact. The experimental results highlight the significant potential of machine learning-driven approaches to improve user experience and operational efficiency in modern PMMPs.",today rapidly evolving digital landscape power marketing management platforms pmmps emerged essential tools businesses seeking optimize marketing strategies platforms integrate advanced technologies data analytics deliver targeted campaigns automate processes generate valuable insights customer behavior however traditional marketing approaches often face challenges data silos poor cross channel integration limited ability adapt shifting consumer behaviors research investigates application machine learning algorithms enhance electricity consumption analysis user engagement within pmmps comprehensive framework proposed incorporating advanced analytical models user segmentation behavioral analysis consumption forecasting means clustering algorithm segments users based electricity consumption patterns enabling tailored service approach analyze temporal shifts user behavior forecast consumption trends research introduces dynamic random synthesized light gradient boosting machine lightgbm model delivers actionable insights support grid scheduling resource optimization system develops personalized marketing strategies dynamic pricing models energy saving incentives enhance user participation promote energy conservation effectiveness proposed system evaluated using several performance metrics rmse along mape mae training time assess accuracy consumption forecasting engagement rate reduction energy usage serve business relevant indicators marketing impact experimental results highlight significant potential machine learning driven approaches improve user experience operational efficiency modern pmmps
"With the rapid development of IoT technology, the nonlinear IoT environment has increasing requirements for the intelligence and resource efficiency of embedded systems. Based on the current background of Internet of Things technology, this study focuses on the design and optimization strategy of intelligent prediction and control algorithms for embedded systems under resource constraints. The proposed algorithm innovatively integrates the long short-term memory network (LSTM) of deep learning and the proximal policy optimization algorithm (PPO) of reinforcement learning, LSTM processes long-term dependencies in IoT time series data, accurately captures the dynamic changes of the system, and the PPO helps the agent to optimize the control strategy in real time according to the environmental feedback. Through theoretical analysis and experimental verification, a comprehensive optimization framework combining computing efficiency improvement, storage optimization and energy management is proposed to achieve the best matching of algorithm performance and hardware resources. Experimental data shows that the optimized algorithm performs well in handling nonlinear data prediction tasks, reducing the average computation time by 37%, reducing storage requirements by more than 60%, and reducing energy consumption by 45%. Especially in the nonlinear temperature prediction experiment, the optimization algorithm maintains the same prediction accuracy as the original version, and the response speed is faster and the anti-interference ability is stronger in the face of data peaks, which fully demonstrates the advantages of the algorithm optimization. Compared with the existing methods, the new algorithm overcomes the difficulty of traditional algorithms in dealing with the nonlinear and high-dimensional problems of IoT data in terms of prediction accuracy, and greatly reduces the error. In terms of control performance, it breaks through the limitations of fixed strategies and maintains the efficient and stable operation of the system. It is also highly adaptable and robust. This research fills the gap in the design of intelligent algorithms for embedded systems in the nonlinear Internet of Things environment, and provides strong technical support for enterprises and research institutions to develop the next generation of Internet of Things applications.",rapid development iot technology nonlinear iot environment increasing requirements intelligence resource efficiency embedded systems based current background internet things technology study focuses design optimization strategy intelligent prediction control algorithms embedded systems resource constraints proposed algorithm innovatively integrates long short term memory network lstm deep learning proximal policy optimization algorithm ppo reinforcement learning lstm processes long term dependencies iot time series data accurately captures dynamic changes system ppo helps agent optimize control strategy real time according environmental feedback theoretical analysis experimental verification comprehensive optimization framework combining computing efficiency improvement storage optimization energy management proposed achieve best matching algorithm performance hardware resources experimental data shows optimized algorithm performs well handling nonlinear data prediction tasks reducing average computation time reducing storage requirements reducing energy consumption especially nonlinear temperature prediction experiment optimization algorithm maintains prediction accuracy original version response speed faster anti interference ability stronger face data peaks fully demonstrates advantages algorithm optimization compared existing methods new algorithm overcomes difficulty traditional algorithms dealing nonlinear high dimensional problems iot data terms prediction accuracy greatly reduces error terms control performance breaks limitations fixed strategies maintains efficient stable operation system also highly adaptable robust research fills gap design intelligent algorithms embedded systems nonlinear internet things environment provides strong technical support enterprises research institutions develop next generation internet things applications
"The cross-border e-commerce industry is experiencing unprecedented growth, and big data technology has great potential as a key tool to support its operations and management. This paper mainly studies the application and effect of big data in cross-border e-commerce logistics demand forecasting and resource allocation. First, it discusses how big data can improve the forecasting accuracy of cross-border e-commerce logistics demand and conducts empirical research by constructing different forecasting models, including time series analysis and machine learning algorithms. Secondly, it explores the application of big data in logistics resource allocation, including warehouse management, transportation route optimization and supply chain collaboration, and evaluates the effectiveness of resource allocation strategies through mathematical modeling. In addition, the methods of this study are compared with existing research, and the advantages and limitations of big data in this field are discussed, and its implications and significance for practical business are analyzed. The research results show that through big data technology, cross-border e-commerce logistics demand prediction is more accurate and resource allocation is more efficient, but at the same time, it is necessary to pay attention to the challenges in data quality and privacy protection.",cross border commerce industry experiencing unprecedented growth big data technology great potential key tool support operations management paper mainly studies application effect big data cross border commerce logistics demand forecasting resource allocation first discusses big data improve forecasting accuracy cross border commerce logistics demand conducts empirical research constructing different forecasting models including time series analysis machine learning algorithms secondly explores application big data logistics resource allocation including warehouse management transportation route optimization supply chain collaboration evaluates effectiveness resource allocation strategies mathematical modeling addition methods study compared existing research advantages limitations big data field discussed implications significance practical business analyzed research results show big data technology cross border commerce logistics demand prediction accurate resource allocation efficient time necessary pay attention challenges data quality privacy protection
"With the widespread application of 3D human animation, traditional skeleton reconstruction and motion data prediction methods still have limitations in terms of modeling capabilities, trajectory discontinuity, and response delays when dealing with point cloud density, multi-view posture changes, and missing key points. To address this, this study proposes a 3D human animation and motion data generation model that integrates mean shift clustering, graph structure neural network modeling, and Kalman filter prediction. This method achieves adaptive extraction of key bone points through dual-ring neighborhood density clustering, combines local geometry and poses semantics to construct a complete skeleton topology, and introduces a state recursive mechanism for missing point repair and temporal smoothing in the output stage. The experiment showed that the model achieved the highest F1 values of 94.83% and 93.94% on the Human 3.6 M dataset and the Carnegie Mellon University motion capture dataset, with a skeleton point missing rate as low as 6.21%. It maintained an average opinion score of up to 4.78 under multi-view and complex lighting conditions, with an average processing delay of no more than 34 ms. In terms of reconstruction accuracy, acceleration smoothness, and output integrity under extreme poses, this model outperformed other comparison models. This model has significant advantages in accuracy, stability, and real-time performance, providing reliable technical support for high-quality animation generation, motion capture, and intelligent virtual human systems.",widespread application human animation traditional skeleton reconstruction motion data prediction methods still limitations terms modeling capabilities trajectory discontinuity response delays dealing point cloud density multi view posture changes missing key points address study proposes human animation motion data generation model integrates mean shift clustering graph structure neural network modeling kalman filter prediction method achieves adaptive extraction key bone points dual ring neighborhood density clustering combines local geometry poses semantics construct complete skeleton topology introduces state recursive mechanism missing point repair temporal smoothing output stage experiment showed model achieved highest values human dataset carnegie mellon university motion capture dataset skeleton point missing rate low maintained average opinion score multi view complex lighting conditions average processing delay terms reconstruction accuracy acceleration smoothness output integrity extreme poses model outperformed comparison models model significant advantages accuracy stability real time performance providing reliable technical support high quality animation generation motion capture intelligent virtual human systems
"With rapid development of digital age, the brand LOGO is an essential visual identity for enterprises, and its design, identification, and analysis have become a research hotspot in computer vision and image processing. Traditional manual recognition methods are inefficient, and it isn’t easy to meet needs of large-scale, rapid, and accurate. Therefore, automatic recognition and analysis technology based on computer vision and image processing emerged. This study aims to explore an efficient and accurate automatic identification and analysis method of brand LOGO design to improve the intelligent level of brand management. Firstly, the research constructs a large-scale brand LOGO database, covering LOGO images of various styles, industries, and complex backgrounds. A complete LOGO recognition and analysis system is designed to use convolutional neural networks (CNN) as the primary model in deep learning, combined with key technologies such as image preprocessing, feature extraction, and classification recognition. In the experimental stage, through training and testing of 10,000 LOGO images, the results show that recognition accuracy rate of the system reaches 95.8%, which is nearly 15 percentage points higher than that of traditional methods. At same time, in analyzing LOGO design elements, such as color, shape, and texture, the system’s accuracy rate reached 92.3%, which provides strong data support for brand design. The influence of different network structures and parameter adjustments on the recognition effect is also discussed, and the model’s performance is optimized by cross-validation. This study not only realizes the automatic identification and analysis of brand LOGO but also provides new ideas and solutions for intelligent applications in related fields.",rapid development digital age brand logo essential visual identity enterprises design identification analysis become research hotspot computer vision image processing traditional manual recognition methods inefficient easy meet needs large scale rapid accurate therefore automatic recognition analysis technology based computer vision image processing emerged study aims explore efficient accurate automatic identification analysis method brand logo design improve intelligent level brand management firstly research constructs large scale brand logo database covering logo images various styles industries complex backgrounds complete logo recognition analysis system designed use convolutional neural networks cnn primary model deep learning combined key technologies image preprocessing feature extraction classification recognition experimental stage training testing logo images results show recognition accuracy rate system reaches nearly percentage points higher traditional methods time analyzing logo design elements color shape texture system accuracy rate reached provides strong data support brand design influence different network structures parameter adjustments recognition effect also discussed model performance optimized cross validation study realizes automatic identification analysis brand logo also provides new ideas solutions intelligent applications related fields
"Logistics delivery efficiency and environmental sustainability have become important research areas in the construction of smart cities. In order to further enhance the effectiveness of sustainable logistics and distribution operations in existing smart cities and reduce energy consumption, a multi-objective problem function is constructed for electric vehicle logistics and distribution. Based on the cuckoo algorithm, this study introduces adaptive step size adjustment, bird egg dynamic discovery strategy, and flight reconstruction strategy to optimize the search accuracy and global convergence ability of distribution paths. Finally, a new logistics distribution operation path mechanism is proposed. The new algorithm had an average delivery energy consumption of 149.67 J–160.72 J and a task processing time of 6.32s–8.42s. Compared with the other three advanced planning algorithms, it significantly reduced delivery costs and resource occupation, and improved delivery efficiency by more than 25%. The lowest delivery cost per kilometer was only 1.1 yuan, and the highest delivery efficiency was 40 kilometers per hour. From this, the method can effectively improve operational efficiency and achieve sustainable development in smart city logistics distribution, providing certain reference value for the subsequent development of this field.",logistics delivery efficiency environmental sustainability become important research areas construction smart cities order enhance effectiveness sustainable logistics distribution operations existing smart cities reduce energy consumption multi objective problem function constructed electric vehicle logistics distribution based cuckoo algorithm study introduces adaptive step size adjustment bird egg dynamic discovery strategy flight reconstruction strategy optimize search accuracy global convergence ability distribution paths finally new logistics distribution operation path mechanism proposed new algorithm average delivery energy consumption task processing time compared three advanced planning algorithms significantly reduced delivery costs resource occupation improved delivery efficiency lowest delivery cost per kilometer yuan highest delivery efficiency kilometers per hour method effectively improve operational efficiency achieve sustainable development smart city logistics distribution providing certain reference value subsequent development field
"In complex urban environments, traditional path planning methods have significant shortcomings in terms of safety assurance, multi-objective path optimization, and personalized travel recommendations. To address these issues, this paper proposes a reinforcement learning-based path planning algorithm with a strategy-guided mechanism and further constructs a path optimization model suitable for multi-destination travel scenarios. This method introduces a safety-aware potential field and a composite reward mechanism to guide the agent to achieve a dynamic balance between path length and safety. In the experimental section, a dataset incorporating map and urban public safety information was constructed, and 800 rounds of path learning simulation experiments were conducted. The results show that the convergence time of the proposed algorithm is 32% shorter than that of the greedy strategy-based method and 27% shorter than that of the policy enhancement method. Additionally, the average path length is reduced by over 100 m, and the safety score improves by over 14%. In multi-destination travel tests, when the number of target points was 20, the total path length was reduced by 3.00% compared to the distance matrix method and by 0.15% compared to the genetic algorithm, verifying its scalability and stability in complex scenarios. The research results indicate that this method can provide efficient, safe, and personalized path planning solutions for urban travelers, offering broad application prospects.",complex urban environments traditional path planning methods significant shortcomings terms safety assurance multi objective path optimization personalized travel recommendations address issues paper proposes reinforcement learning based path planning algorithm strategy guided mechanism constructs path optimization model suitable multi destination travel scenarios method introduces safety aware potential field composite reward mechanism guide agent achieve dynamic balance path length safety experimental section dataset incorporating map urban public safety information constructed rounds path learning simulation experiments conducted results show convergence time proposed algorithm shorter greedy strategy based method shorter policy enhancement method additionally average path length reduced safety score improves multi destination travel tests number target points total path length reduced compared distance matrix method compared genetic algorithm verifying scalability stability complex scenarios research results indicate method provide efficient safe personalized path planning solutions urban travelers offering broad application prospects
"To improve the accuracy of intelligent detection of business logic vulnerabilities, code auditing techniques were used to construct a code attribute graph. Subsequently, breadth-first search algorithm in graph algorithm was used and bidirectional search algorithm was introduced to improve it. Finally, the study introduced large language model for assisted verification to form the final detection model. The outcomes revealed that the highest values of accuracy and recall of the proposed model were 98.31% and 97.54%, respectively, and the mean values of the leakage rate and false alarm rate of the model were 8.06% and 9.19%, respectively. In addition, in practical applications, the maximum values of the accuracy rate of this model on structured query language injection, command injection, buffer overflow, and cross-site scripting attack vulnerabilities were 97.06%, 98.53%, 98.33%, and 98.46%, respectively. The novelty of the research lies in the combination of large language models and improved graph algorithms to construct code attribute graphs and optimize search algorithms. This approach achieves efficient and accurate detection of business logic vulnerabilities while significantly reducing resource consumption and providing new ideas for network security protection.",improve accuracy intelligent detection business logic vulnerabilities code auditing techniques used construct code attribute graph subsequently breadth first search algorithm graph algorithm used bidirectional search algorithm introduced improve finally study introduced large language model assisted verification form final detection model outcomes revealed highest values accuracy recall proposed model respectively mean values leakage rate false alarm rate model respectively addition practical applications maximum values accuracy rate model structured query language injection command injection buffer overflow cross site scripting attack vulnerabilities respectively novelty research lies combination large language models improved graph algorithms construct code attribute graphs optimize search algorithms approach achieves efficient accurate detection business logic vulnerabilities significantly reducing resource consumption providing new ideas network security protection
"With the acceleration of urbanization, the design of urban landscape sculpture, as an important part of urban culture, has attracted increasing attention. This study aims to explore geospatial analysis and sustainable aesthetic, innovative design in urban landscape sculpture design. Through field investigation and GIS technology analysis of 30 urban landscape sculptures at home and abroad, it is found that geospatial factors such as topography, landform and climate significantly influence the design and layout of sculptures. The experimental results show that rational use of geospatial information can improve the harmony between sculpture and the environment by 45%. In terms of sustainable aesthetic innovation design, this study puts forward the concept of “ecological sculpture,” which combines local materials and green construction technology to achieve low-carbon and environmental protection of sculpture design. Case studies show that sculpture projects with sustainable design concepts have reduced maintenance costs by 30% while increasing public satisfaction to 85%. This study also discusses the application of digital technology in sculpture design. Through 3D modelling and virtual reality technology, the visualization and optimization of the design process are realized, and the design efficiency is improved by 20%. Geospatial analysis and sustainable aesthetic innovative design play an important role in urban landscape sculpture design, which not only enhances the artistic value of sculpture but also promotes the sustainable development of cities.",acceleration urbanization design urban landscape sculpture important part urban culture attracted increasing attention study aims explore geospatial analysis sustainable aesthetic innovative design urban landscape sculpture design field investigation gis technology analysis urban landscape sculptures home abroad found geospatial factors topography landform climate significantly influence design layout sculptures experimental results show rational use geospatial information improve harmony sculpture environment terms sustainable aesthetic innovation design study puts forward concept ecological sculpture combines local materials green construction technology achieve low carbon environmental protection sculpture design case studies show sculpture projects sustainable design concepts reduced maintenance costs increasing public satisfaction study also discusses application digital technology sculpture design modelling virtual reality technology visualization optimization design process realized design efficiency improved geospatial analysis sustainable aesthetic innovative design play important role urban landscape sculpture design enhances artistic value sculpture also promotes sustainable development cities
"The use of information technology (IT) in today’s classrooms is crucial due to the very rapid pace at which technological developments are created. Information technology provides a unique set of possibilities in the field of political and ideological education for grabbing students’ interest, encouraging critical thinking, and generating knowledgeable citizens. However, choosing the best information technology-enabled solution is a challenging challenge, especially for companies like Central Universities. To properly address this challenge, it is essential to establish a rigorous assessment system that considers a multitude of state characteristics, such as authenticity, resource efficiency, inclusiveness, and ethical values. Using the q-ROFS environment and the ENTROPY and WASPAS techniques, our study demonstrates a contemporary approach to tackling this important problem. Therefore, Central University and other similar institutions will benefit from this integration by receiving accurate data and practical solutions to the challenges of technology integration, which they can then handle in a precise and firm way. In this opening section, we lay the framework for a substantial investigation into the ways in which our proposed paradigm has the potential to revolutionize political and ideological education in the contemporary digital era.",use information technology today classrooms crucial due rapid pace technological developments created information technology provides unique set possibilities field political ideological education grabbing students interest encouraging critical thinking generating knowledgeable citizens however choosing best information technology enabled solution challenging challenge especially companies like central universities properly address challenge essential establish rigorous assessment system considers multitude state characteristics authenticity resource efficiency inclusiveness ethical values using rofs environment entropy waspas techniques study demonstrates contemporary approach tackling important problem therefore central university similar institutions benefit integration receiving accurate data practical solutions challenges technology integration handle precise firm way opening section lay framework substantial investigation ways proposed paradigm potential revolutionize political ideological education contemporary digital
"The Ordos Basin is characterized by low-permeability oil and gas reservoirs. In the past, the recovery factors for such reservoirs were often predicted through laboratory experiments or numerical simulations, and methods for improving recovery were proposed based on reservoir characteristics. However, due to the differences among blocks, neither laboratory experiments nor numerical simulations have yet established a unified prediction standard for effect of oil flooding, and there is a lack of comparability between the production capacities of different blocks. In order to investigate the impact of different injection schemes on the oil flooding effect in the same type of reservoirs, this paper takes the Ordos Basin oil and gas reservoirs as an example and, considering the main factors controlling enhanced oil recovery, conducts calculations and analyses. A neural network-based method for predicting the recovery factors of different blocks is proposed, and the effects of injection volume, timing, concentration, and rate on the recovery factors are analyzed. The new method proposed in this study has important academic value for the subsequent optimization of enhanced oil recovery schemes in the same type of blocks.",ordos basin characterized low permeability oil gas reservoirs past recovery factors reservoirs often predicted laboratory experiments numerical simulations methods improving recovery proposed based reservoir characteristics however due differences among blocks neither laboratory experiments numerical simulations yet established unified prediction standard effect oil flooding lack comparability production capacities different blocks order investigate impact different injection schemes oil flooding effect type reservoirs paper takes ordos basin oil gas reservoirs example considering main factors controlling enhanced oil recovery conducts calculations analyses neural network based method predicting recovery factors different blocks proposed effects injection volume timing concentration rate recovery factors analyzed new method proposed study important academic value subsequent optimization enhanced oil recovery schemes type blocks
"To improve the accuracy of analyzing the development of real estate finance, this article has made specific improvements to traditional machine learning algorithms. The improvement method mainly applies the generalized linear model to conduct in-depth analysis and exploration of the development effect of real estate finance, and comprehensively uses the liquid type logarithmic linear model and the overall logarithmic linear model. These models are used to evaluate the impact of various combinations of real estate finance development methods on the effectiveness of incoming calls and visits, in order to recommend relatively better types of real estate finance development. By analyzing the model, this article can preliminarily simulate the investment standards for future real estate finance development and evaluate their development effects in advance. Compared with traditional real estate financial analysis methods, the method proposed in this paper demonstrates significant advantages in prediction accuracy and application scope. Based on experimental simulation research, it is known that the simulation method for the development of real estate finance based on improved machine learning algorithms has good results and provides strong support for decision-making in the field of real estate finance.",improve accuracy analyzing development real estate finance article made specific improvements traditional machine learning algorithms improvement method mainly applies generalized linear model conduct depth analysis exploration development effect real estate finance comprehensively uses liquid type logarithmic linear model overall logarithmic linear model models used evaluate impact various combinations real estate finance development methods effectiveness incoming calls visits order recommend relatively better types real estate finance development analyzing model article preliminarily simulate investment standards future real estate finance development evaluate development effects advance compared traditional real estate financial analysis methods method proposed paper demonstrates significant advantages prediction accuracy application scope based experimental simulation research known simulation method development real estate finance based improved machine learning algorithms good results provides strong support decision making field real estate finance
"Geographical scholarship on the agency and practices of peoples of African descent now flourishes across a diverse range of frameworks and approaches. This article highlights three salient trajectories: Black geographies, geographies of the Black Atlantic and Black ecologies – subfields that, despite considerable overlaps, have developed with little direct dialogue or collaboration. We begin by summarizing their respective methodological and theoretical contributions, then explore points of convergence that hold promise for future work and finally culminate by thinking with political–ecological struggles for land and life across the Black diaspora. Placing these fields in conversation, we reflect on the analytical avenues that open when studies of the Black knowledge and agency embedded in Atlantic landscapes and livelihoods meet the commitments to Black livingness and insurgent ecological politics made visible by Black geographies and Black ecologies.",geographical scholarship agency practices peoples african descent flourishes across diverse range frameworks approaches article highlights three salient trajectories black geographies geographies black atlantic black ecologies subfields despite considerable overlaps developed little direct dialogue collaboration begin summarizing respective methodological theoretical contributions explore points convergence hold promise future work finally culminate thinking political ecological struggles land life across black diaspora placing fields conversation reflect analytical avenues open studies black knowledge agency embedded atlantic landscapes livelihoods meet commitments black livingness insurgent ecological politics made visible black geographies black ecologies
"With the rapid development and wide application of artificial intelligence (AI) technology in the field of education, this study aims to explore and evaluate the application strategies of AI in the optimization of teaching resources. Using a combination of questionnaires, data analysis, and machine learning models, the study provides an in-depth analysis of the possibilities and practical effects of AI in optimizing teaching resources. The questionnaire collected the attitudes and use of AI teaching resources by educators and students, and the random forest model was used to evaluate the effectiveness of AI in optimizing teaching resources. The results show that educators and students generally agree that AI can significantly improve teaching quality and meet personalized learning needs. However, the study also reveals data sample bias and limitations in the model’s ability to generalize. Overall, this study provides an empirical basis for understanding the potential of AI in the optimization of educational resources, while pointing to the direction of future research.",rapid development wide application artificial intelligence technology field education study aims explore evaluate application strategies optimization teaching resources using combination questionnaires data analysis machine learning models study provides depth analysis possibilities practical effects optimizing teaching resources questionnaire collected attitudes use teaching resources educators students random forest model used evaluate effectiveness optimizing teaching resources results show educators students generally agree significantly improve teaching quality meet personalized learning needs however study also reveals data sample bias limitations model ability generalize overall study provides empirical basis understanding potential optimization educational resources pointing direction future research
"Tourism e-commerce is a mode of combining e-commerce and tourism industry to realize online transactions in tourism industry. This paper aims to explore the construction and application of tourism e-commerce platform based on cloud computing, firstly analyzing the characteristics and advantages of cloud computing technology, as well as the current situation of the development of tourism e-commerce and the existing problems, and then proposing the architecture and functional modules of the tourism e-commerce platform based on cloud computing, followed by the introduction of the platform’s design methodology and implementation technology, and finally verifying, through examples, the platform’s feasibility and effectiveness, providing a new solution for the digital transformation of the tourism industry.",tourism commerce mode combining commerce tourism industry realize online transactions tourism industry paper aims explore construction application tourism commerce platform based cloud computing firstly analyzing characteristics advantages cloud computing technology well current situation development tourism commerce existing problems proposing architecture functional modules tourism commerce platform based cloud computing followed introduction platform design methodology implementation technology finally verifying examples platform feasibility effectiveness providing new solution digital transformation tourism industry
"In order to find the relationship between enterprise value and cash flow, so that enterprises can create enterprise value quickly and efficiently. Based on the theory of nonlinear structural equation model, with the help of SPSS26.0 and AMOS21.0 software, this paper makes an empirical analysis on the financial data of China’s large-scale mechanical equipment and instrumentation manufacturing industry in 2023. Firstly, the structural equation model is constructed by drawing the path diagram of nonlinear structural equation. Secondly, the model is tested, and finally relevant conclusions are drawn. The empirical results show that the company’s cash payment ability, cash acquisition ability and cash turnover ability are positively correlated with the company’s value. Therefore, enterprises should closely link cash flow management with enterprise value creation, and enhance enterprise value through scientific and effective cash flow management, so as to realize the financial management goal of maximizing enterprise value.",order find relationship enterprise value cash flow enterprises create enterprise value quickly efficiently based theory nonlinear structural equation model help spss amos software paper makes empirical analysis financial data china large scale mechanical equipment instrumentation manufacturing industry firstly structural equation model constructed drawing path diagram nonlinear structural equation secondly model tested finally relevant conclusions drawn empirical results show company cash payment ability cash acquisition ability cash turnover ability positively correlated company value therefore enterprises closely link cash flow management enterprise value creation enhance enterprise value scientific effective cash flow management realize financial management goal maximizing enterprise value
"The banking system is composed of a large number of financial institutions closely connected through various transactions and relationships, forming a complex network that is prone to risk contagion. At the same time, financial markets are influenced by factors such as market volatility, economic cycles, and unpredictable external shocks, further increasing the complexity of systematic risk modeling and prediction. Therefore, to improve the accuracy and reduce the complexity of systemic risk in banks, this study proposes an assessment method for systemic risk in banks based on stochastic differential equations. The results showed that compared with other methods, the model fitting and prediction accuracy of assessing systemic risk in banks based on stochastic differential equations were the highest. When the number of experiments was 1,000, the model fitting degree and prediction accuracy were 0.991 and 96.3%, respectively. The extreme event capture rate and risk premium correlation of evaluating experimental data samples from 30 listed banks using a stochastic differential equation model have been improved. When the number of experiments was 1,000, the extreme event capture rate and risk premium correlation using the SDE model were 93.3% and 0.85. The proposed bank systematic risk assessment method based on stochastic differential equations has superior performance. In real-world banking scenarios, this method captures the dynamic behavior and randomness of the banking system, enabling real-time monitoring and early warning of systemic risks, improving risk management efficiency, and ensuring financial market stability.",banking system composed large number financial institutions closely connected various transactions relationships forming complex network prone risk contagion time financial markets influenced factors market volatility economic cycles unpredictable external shocks increasing complexity systematic risk modeling prediction therefore improve accuracy reduce complexity systemic risk banks study proposes assessment method systemic risk banks based stochastic differential equations results showed compared methods model fitting prediction accuracy assessing systemic risk banks based stochastic differential equations highest number experiments model fitting degree prediction accuracy respectively extreme event capture rate risk premium correlation evaluating experimental data samples listed banks using stochastic differential equation model improved number experiments extreme event capture rate risk premium correlation using sde model proposed bank systematic risk assessment method based stochastic differential equations superior performance real world banking scenarios method captures dynamic behavior randomness banking system enabling real time monitoring early warning systemic risks improving risk management efficiency ensuring financial market stability
"In recent years, the scale of Internet information infrastructure has expanded rapidly, and its threat assessment is becoming more and more important. Assessing the threat of Internet information infrastructure can find out the most vulnerable link in the network node and provide a reference for taking correct network security defense measures. In view of the low accuracy of current Internet information infrastructure threat assessment methods, the study combined relevant data on Internet information infrastructure and constructed a knowledge graph of basic Internet information. This study proposes an Internet infrastructure threat assessment model based on knowledge atlas. In the effectiveness analysis of the model, the most vulnerable network node was Dom5, and it was found that the vulnerability of this node was affected by Dom1, Dom2, Dom3, Dom4, Dom7, and Dom8. Subsequently, in the analysis of the application effect of the model, the average F1 value and average recall rate of the model were 96.13% and 97.26%, respectively, which were better than the comparative model. The results show that the model is effective and has practical application value. This model not only has a high accuracy rate and recall rate in identifying vulnerable nodes but also provides valuable insights for network security defense strategies. This model accurately locates vulnerable nodes, analyzes their vulnerability impacts, and takes proactive measures to enhance network security.",recent years scale internet information infrastructure expanded rapidly threat assessment becoming important assessing threat internet information infrastructure find vulnerable link network node provide reference taking correct network security defense measures view low accuracy current internet information infrastructure threat assessment methods study combined relevant data internet information infrastructure constructed knowledge graph basic internet information study proposes internet infrastructure threat assessment model based knowledge atlas effectiveness analysis model vulnerable network node dom found vulnerability node affected dom dom dom dom dom dom subsequently analysis application effect model average value average recall rate model respectively better comparative model results show model effective practical application value model high accuracy rate recall rate identifying vulnerable nodes also provides valuable insights network security defense strategies model accurately locates vulnerable nodes analyzes vulnerability impacts takes proactive measures enhance network security
"The real economy has been seriously hit in recent years, and the government has introduced relevant economic policies to stabilize and promote the development of the real economy. However, the policy uncertainty negatively affects the development of manufacturing enterprises, and the relevant influencing factors need to be analyzed for manufacturing enterprises to achieve long-term stable development. In this study, regression analysis is used to investigate manufacturing enterprises’ technological innovation’ and economic policy uncertainty (EPU)’s connection. Manufacturing enterprises’ innovation activities are negative correlated with EPU and leads to a sustainable lack of technological innovation in enterprises in the results. Meanwhile, in terms of R&amp;D investment and invention patents, EPU has a greater degree of influence on technological innovation of non-state manufacturing enterprises. The proposed methods and viewpoints can provide a scientific research method for measuring the quality of enterprise innovation, that is, using the knowledge breadth method to test the quality of enterprise patents, effectively avoiding the drawbacks of using the number of patents to measure the quality of enterprise innovation.",real economy seriously hit recent years government introduced relevant economic policies stabilize promote development real economy however policy uncertainty negatively affects development manufacturing enterprises relevant influencing factors need analyzed manufacturing enterprises achieve long term stable development study regression analysis used investigate manufacturing enterprises technological innovation economic policy uncertainty epu connection manufacturing enterprises innovation activities negative correlated epu leads sustainable lack technological innovation enterprises results meanwhile terms amp investment invention patents epu greater degree influence technological innovation non state manufacturing enterprises proposed methods viewpoints provide scientific research method measuring quality enterprise innovation using knowledge breadth method test quality enterprise patents effectively avoiding drawbacks using number patents measure quality enterprise innovation
"This study explores the landscape design of elevated roadside slopes in China’s mountainous cities, aiming to enhance urban environmental aesthetics through an integrated framework of aesthetic and ecological theories. By establishing an aesthetic evaluation system for main road slopes, the research analyzes factors influencing visual landscape quality—including plant configuration, geological conditions, and cultural context—and investigates their correlation with landscape art assessment. Treating mountain slopes as windows for urban civilization, cultural carriers, and ecological nodes, the study addresses the challenge of transforming municipal engineering slopes into symbols of urban humanity. The research integrates 200+ domestic and international literatures, field surveys of 15 main road slopes (over 2 km) in Chongqing, Guiyang, and Kunming, questionnaires from 300 road users and 50 designers, and 10+ local policy documents. Slopes were selected for geological representativeness, visibility (≤50 m from roads), safety (slope &lt;60°), and diversity (soil/rock types). Empirical analysis includes: (1) status assessment (recording 35% slope data, diagnosing 40% soil erosion and 30% vegetation degradation); (2) evaluation system construction (12 indicators via AHP, with 25% weight for color richness and 30% for morphological diversity); (3) design optimization (30+ plant species for color-seasonal coordination, boosting vegetation coverage to &gt;80% through engineering-biological measures); and (4) effect monitoring (40% coverage increase and 30% erosion reduction within 6 months, with feedback-driven refinements). Findings show that the aesthetic-ecological design model balances visual appeal (e.g., seasonal color gradients) with ecological resilience (e.g., native vine soil fixation), validated by AHP-weighted morphological diversity as the primary aesthetic driver. The study provides a scientific framework for mountainous city slope landscaping, demonstrating its potential to reconcile urban development with environmental sustainability and offering a replicable model for similar regions.",study explores landscape design elevated roadside slopes china mountainous cities aiming enhance urban environmental aesthetics integrated framework aesthetic ecological theories establishing aesthetic evaluation system main road slopes research analyzes factors influencing visual landscape quality including plant configuration geological conditions cultural context investigates correlation landscape art assessment treating mountain slopes windows urban civilization cultural carriers ecological nodes study addresses challenge transforming municipal engineering slopes symbols urban humanity research integrates domestic international literatures field surveys main road slopes chongqing guiyang kunming questionnaires road users designers local policy documents slopes selected geological representativeness visibility roads safety slope diversity soil rock types empirical analysis includes status assessment recording slope data diagnosing soil erosion vegetation degradation evaluation system construction indicators via ahp weight color richness morphological diversity design optimization plant species color seasonal coordination boosting vegetation coverage engineering biological measures effect monitoring coverage increase erosion reduction within months feedback driven refinements findings show aesthetic ecological design model balances visual appeal seasonal color gradients ecological resilience native vine soil fixation validated ahp weighted morphological diversity primary aesthetic driver study provides scientific framework mountainous city slope landscaping demonstrating potential reconcile urban development environmental sustainability offering replicable model similar regions
"With globalization accelerating, the international dissemination and integration of Chinese traditional culture in education have become crucial. This study evaluates the influence of Chinese traditional culture on college students’ ideals and beliefs across China, the United States, Germany, Japan, and India by analyzing data collected through online questionnaires, social media, learning management systems, and big data analysis. Findings reveal that while Chinese culture is widely recognized globally, its educational impact varies significantly among international students. These differences stem from cultural communication strategies, as well as the cultural backgrounds, education systems, and social values of the receiving countries. The study highlights the need to respect and adapt to cultural differences and adopt innovative communication strategies to enhance international students’ awareness and participation in Chinese culture. By applying big data analysis, this research provides quantitative evaluations and deep insights into the international communication and educational integration of Chinese culture, guiding future cross-cultural education research and promoting global cultural exchange and understanding.",globalization accelerating international dissemination integration chinese traditional culture education become crucial study evaluates influence chinese traditional culture college students ideals beliefs across china united states germany japan india analyzing data collected online questionnaires social media learning management systems big data analysis findings reveal chinese culture widely recognized globally educational impact varies significantly among international students differences stem cultural communication strategies well cultural backgrounds education systems social values receiving countries study highlights need respect adapt cultural differences adopt innovative communication strategies enhance international students awareness participation chinese culture applying big data analysis research provides quantitative evaluations deep insights international communication educational integration chinese culture guiding future cross cultural education research promoting global cultural exchange understanding
"With the rapid development of technology, the massive amount of data accumulated in the field of economic management contains rich information, but traditional analysis methods are difficult to effectively explore its complex features and patterns. This study focuses on using CNN models to solve the problem of feature extraction and modeling in economic management big data, and explores its application value and contribution in this field. The CNN model, with its powerful ability to automatically extract features and capture local correlations in data, can break through the bottleneck of traditional methods in processing high-dimensional and nonlinear economic data. By preprocessing economic big data and applying CNN models, key features from multi-source heterogeneous data such as tax data and market transaction data can be efficiently extracted, avoiding the subjectivity and limitations of manual feature extraction. Through statistical analysis, these characteristics not only reveal the significant differences between China’s population and social economy in the eastern and western regions, but also show the overall trend of the economic development of cities and regions through time and space analysis. The application of CNN models provides a new technological path for economic data analysis, which helps to improve the accuracy of economic forecasting, optimize economic management decisions, and is of great significance for promoting the development of research and practice in the economic field.",rapid development technology massive amount data accumulated field economic management contains rich information traditional analysis methods difficult effectively explore complex features patterns study focuses using cnn models solve problem feature extraction modeling economic management big data explores application value contribution field cnn model powerful ability automatically extract features capture local correlations data break bottleneck traditional methods processing high dimensional nonlinear economic data preprocessing economic big data applying cnn models key features multi source heterogeneous data tax data market transaction data efficiently extracted avoiding subjectivity limitations manual feature extraction statistical analysis characteristics reveal significant differences china population social economy eastern western regions also show overall trend economic development cities regions time space analysis application cnn models provides new technological path economic data analysis helps improve accuracy economic forecasting optimize economic management decisions great significance promoting development research practice economic field
"Traditional object repair methods usually rely on complex algorithms and a lot of manual labor, making it difficult to repair large and complex objects. In order to improve the efficiency of repairing large and complex targets and reduce the workload of object repair, an object repair model based on digital twin technology is developed. A charge-coupled device in the mechanical arm collects image data from the target object and transfers it to the real-virtual coordinate matching module. The attention mechanism is introduced to construct a multiscale residual UNet object image segmentation model for real-virtual coordinate matching to deal with the object repair trajectory planning problem. When verifying the function of the model, the loss degree, repair model parameters, and object repair error are used as evaluation indicators. The results of the model performance test revealed that the proposed multiscale residual UNet model for object image segmentation exhibited an accuracy of 0.942 while maintaining a loss value of 0.099 at steady state. Compared to the traditional UNet model, the study’s model had fewer parameters by 20.02 million and a slightly improved prediction accuracy of 0.01 on the self-built dataset. Additionally, the inclusion of the attention module enhanced the prediction accuracy by 0.02 M without adding too many parameters. The experiment demonstrated that the deviation between the predicted and actual object center coordinates was less than 1 mm, both horizontally and vertically. This sub-millimeter accuracy allowed for precise virtual-to-real alignment, which was essential for the operation of high-fidelity digital twins. It also ensured reliable robotic manipulation in demanding applications, such as precision component repair and cultural heritage restoration. Furthermore, the use of a dual-robot cooperative approach proved more effective. It was more effective in completing complex repair tasks. It increased system repair capability. It enabled omni-directional repairs previously unattainable with a single robot arm. Crucially, this dual-robot strategy reduced the time required for complex object repair to under 15 s. This represented a 40% improvement in speed over single-robot operation, while maintaining a repair pass rate of 99.2%.",traditional object repair methods usually rely complex algorithms lot manual labor making difficult repair large complex objects order improve efficiency repairing large complex targets reduce workload object repair object repair model based digital twin technology developed charge coupled device mechanical arm collects image data target object transfers real virtual coordinate matching module attention mechanism introduced construct multiscale residual unet object image segmentation model real virtual coordinate matching deal object repair trajectory planning problem verifying function model loss degree repair model parameters object repair error used evaluation indicators results model performance test revealed proposed multiscale residual unet model object image segmentation exhibited accuracy maintaining loss value steady state compared traditional unet model study model fewer parameters million slightly improved prediction accuracy self built dataset additionally inclusion attention module enhanced prediction accuracy without adding many parameters experiment demonstrated deviation predicted actual object center coordinates less horizontally vertically sub millimeter accuracy allowed precise virtual real alignment essential operation high fidelity digital twins also ensured reliable robotic manipulation demanding applications precision component repair cultural heritage restoration furthermore use dual robot cooperative approach proved effective effective completing complex repair tasks increased system repair capability enabled omni directional repairs previously unattainable single robot arm crucially dual robot strategy reduced time required complex object repair represented improvement speed single robot operation maintaining repair pass rate
"Accurately locating product target information is crucial for improving competitiveness and brand image. However, traditional methods are often inefficient and lack robustness in complex visual environments. This study proposes an improved product target information localization model, which takes a dense connection module as the backbone to extract multi-scale feature information. Then, a dynamic convolution module is employed to adaptively fuse responses from different convolution kernels, while an attention mechanism is introduced to enhance key regions. Finally, a multi-stage feature refinement module is applied to progressively optimize edge and structural information, thereby generating high-quality saliency maps and improving localization accuracy and model robustness. Compared with the baseline model without refinement, introducing three feature refinement modules increases the F-measure by 0.026, while dynamic convolution achieves an optimal F-measure of 0.951. Moreover, the combination of two feature refinement modules and dynamic convolution reduces the MAE by 0.018. Compared with four state-of-the-art models (Capsal, PiCANet, PoolNet, and DGRL), I-PFPN consistently outperforms them in F-measure and PR curve evaluations. In practice, the model completes product target localization within approximately 0.5 s per image, making it a fast and effective tool for enterprise-level applications in dynamic market environments.",accurately locating product target information crucial improving competitiveness brand image however traditional methods often inefficient lack robustness complex visual environments study proposes improved product target information localization model takes dense connection module backbone extract multi scale feature information dynamic convolution module employed adaptively fuse responses different convolution kernels attention mechanism introduced enhance key regions finally multi stage feature refinement module applied progressively optimize edge structural information thereby generating high quality saliency maps improving localization accuracy model robustness compared baseline model without refinement introducing three feature refinement modules increases measure dynamic convolution achieves optimal measure moreover combination two feature refinement modules dynamic convolution reduces mae compared four state art models capsal picanet poolnet dgrl pfpn consistently outperforms measure curve evaluations practice model completes product target localization within approximately per image making fast effective tool enterprise level applications dynamic market environments
"With the continuous development of construction industrialization, prefabricated buildings have been widely used due to their advantages of resource conservation, shortened construction period, environmental protection, and low pollution. However, the structural optimization design and construction scheduling optimization of prefabricated building concrete components urgently need to be solved to meet the sustainable development. Therefore, the research adopts the dragonfly algorithm optimized by multiple strategies to optimize the overall structure of prefabricated concrete components in buildings. BIM technology is applied to build a construction scheduling optimization model that satisfies resource constraints and structural constraints to optimize the scheduling plan of prefabricated components during assembly construction and find the shortest construction period and optimal scheduling plan. The experimental results show that in the comparison of intelligent algorithm iteration curves, the improved algorithm was superior to the standard DA, Nelder-Mead, NS-FPA, and CPSO algorithms. In the multi-modal function test, the optimal result of the research model was 17.54, while the standard DA, Nelder-Mead, NS-FPA, and LMA algorithms were 72.5, 52.5, and 20.3, respectively. The convergence and optimization performance of the research model were both the best. In the specific construction optimization, the improved algorithm effectively reduced the cost of concrete components and increased the ultimate bearing capacity of welded joints by 6.24%. The construction scheduling optimization model based on BIM technology saved 0.2631 working days, or 6.31 h, improving the work efficiency and reducing cost. This indicates that the research results have application value in the field of prefabricated building optimization.",continuous development construction industrialization prefabricated buildings widely used due advantages resource conservation shortened construction period environmental protection low pollution however structural optimization design construction scheduling optimization prefabricated building concrete components urgently need solved meet sustainable development therefore research adopts dragonfly algorithm optimized multiple strategies optimize overall structure prefabricated concrete components buildings bim technology applied build construction scheduling optimization model satisfies resource constraints structural constraints optimize scheduling plan prefabricated components assembly construction find shortest construction period optimal scheduling plan experimental results show comparison intelligent algorithm iteration curves improved algorithm superior standard nelder mead fpa cpso algorithms multi modal function test optimal result research model standard nelder mead fpa lma algorithms respectively convergence optimization performance research model best specific construction optimization improved algorithm effectively reduced cost concrete components increased ultimate bearing capacity welded joints construction scheduling optimization model based bim technology saved working days improving work efficiency reducing cost indicates research results application value field prefabricated building optimization
"The rapid growth of social media platforms has radically changed the dynamics by which artistic content is disseminated, enabling the development of new paradigms for artistic production and audience engagement. This study undertakes an in-depth examination and visualization framework aimed at explicating the spread of artistic content on major social media platforms. By consolidating and analyzing data from Instagram, Twitter, TikTok, Pinterest, and Behance, this study examines 500,000 art-related posts over a 12-month period to identify key dissemination patterns and viral dynamics. The methodological framework utilizes state-of-the-art machine learning algorithms, including deep neural networks for extracting visual features and graph-based approaches for modeling diffusion dynamics, supplemented by advanced visualization techniques to explain complex dynamics of dissemination. Findings reveal that the spread of artistic content follows certain temporal and spatial dynamics, with the visual appeal of an artwork, posting times, and network effects constituting key drivers of virality. The visualization framework utilized integrates interactive network graphs, temporal heat maps, and multi-dimensional scaling to represent dissemination pathways, thus enabling real-time tracking and pattern detection. The predictive models achieve an accuracy level of 87.3% in predicting the viral potential, reflecting a significant performance boost compared to conventional baseline techniques. This study offers novel insights into digital art consumption, provides actionable suggestions for artists and cultural institutions, and establishes a theoretical foundation for understanding the diffusion of creative content in interconnected systems. The proposed framework has practical implications in terms of how content can be optimized, audience engagement enhanced, and platform design improved, effectively bridging the gap between computational social science and problems relevant to digital humanities.",rapid growth social media platforms radically changed dynamics artistic content disseminated enabling development new paradigms artistic production audience engagement study undertakes depth examination visualization framework aimed explicating spread artistic content major social media platforms consolidating analyzing data instagram twitter tiktok pinterest behance study examines art related posts month period identify key dissemination patterns viral dynamics methodological framework utilizes state art machine learning algorithms including deep neural networks extracting visual features graph based approaches modeling diffusion dynamics supplemented advanced visualization techniques explain complex dynamics dissemination findings reveal spread artistic content follows certain temporal spatial dynamics visual appeal artwork posting times network effects constituting key drivers virality visualization framework utilized integrates interactive network graphs temporal heat maps multi dimensional scaling represent dissemination pathways thus enabling real time tracking pattern detection predictive models achieve accuracy level predicting viral potential reflecting significant performance boost compared conventional baseline techniques study offers novel insights digital art consumption provides actionable suggestions artists cultural institutions establishes theoretical foundation understanding diffusion creative content interconnected systems proposed framework practical implications terms content optimized audience engagement enhanced platform design improved effectively bridging gap computational social science problems relevant digital humanities
"With the rapid development of artificial intelligence technology, its application in the field of college sports health management has become a new focus of research. The purpose of this study is to explore and evaluate the application effect of artificial intelligence technology in college sports health management. The study first collected and processed relevant data, including health examination data, physical activity data, and dietary habits information. Random forest and neural network models were then used for data analysis, with a focus on predicting and improving student health. Through the training, validation, and testing of the model, this study not only demonstrates the potential of artificial intelligence in health data analysis but also puts forward targeted health management recommendations. Although studies have shown that the model performs well in data analysis and prediction, there are still limitations in data representation, model generalization ability, and practical application feasibility. Future research can focus on expanding the scope of data collection, enhancing model generalization, and exploring more practical technical solutions.",rapid development artificial intelligence technology application field college sports health management become new focus research purpose study explore evaluate application effect artificial intelligence technology college sports health management study first collected processed relevant data including health examination data physical activity data dietary habits information random forest neural network models used data analysis focus predicting improving student health training validation testing model study demonstrates potential artificial intelligence health data analysis also puts forward targeted health management recommendations although studies shown model performs well data analysis prediction still limitations data representation model generalization ability practical application feasibility future research focus expanding scope data collection enhancing model generalization exploring practical technical solutions
"With the increasing demand for garbage classification, images and videos are required to complement each other in real-time scenarios. Therefore, the research of garbage detection and classification is still of long-term value. This paper introduces a novel garbage detection and sorting framework called MCMM that incorporates multi-column convolution and matrix-multiplication (MatMul)-free based Transformer. Specially, the multi-column convolution is designed to enhance the performance of image processing tasks through multi-scale feature extraction and adaptation to objects of varying sizes. MatMul-free Transformer significantly reduces computational complexity and hardware overhead by eliminating matrix multiplication, while maintaining high model performance. The experimental evaluation shows that the MCMM network achieves better results in both qualitative and quantitative methods compared to existing competing methods.",increasing demand garbage classification images videos required complement real time scenarios therefore research garbage detection classification still long term value paper introduces novel garbage detection sorting framework called mcmm incorporates multi column convolution matrix multiplication matmul free based transformer specially multi column convolution designed enhance performance image processing tasks multi scale feature extraction adaptation objects varying sizes matmul free transformer significantly reduces computational complexity hardware overhead eliminating matrix multiplication maintaining high model performance experimental evaluation shows mcmm network achieves better results qualitative quantitative methods compared existing competing methods
"The English teaching network course mechanism based on the ecological environment of the Internet of Things based on edge computing is a very key existence in the development process of modern society. Its strong existence significance and purpose are mainly to further improve the level of science and technology in China. Combined with the development process of the Internet of Things in the field of ecological environment, the standard progress of the Internet of Things in the ecological environment is sorted out. The main purpose of this paper is to introduce this technology into the public perspective from various advanced physical calculation formulas. In this paper, the main method to verify the online course mechanism of English teaching in the ecological environment of the Internet of Things is to carry out more brand-new technical research based on the traditional research background. Different calculation methods are also the most important online course mechanism of English teaching. Finally, the conclusion and result we get is that the English teaching mechanism of the ecological environment of the Edge computing Internet of Things has a strong role and value.",english teaching network course mechanism based ecological environment internet things based edge computing key existence development process modern society strong existence significance purpose mainly improve level science technology china combined development process internet things field ecological environment standard progress internet things ecological environment sorted main purpose paper introduce technology public perspective various advanced physical calculation formulas paper main method verify online course mechanism english teaching ecological environment internet things carry brand new technical research based traditional research background different calculation methods also important online course mechanism english teaching finally conclusion result get english teaching mechanism ecological environment edge computing internet things strong role value
"Under the dual backdrop of the innovation-driven development strategy and the coordinated development of the Beijing–Tianjin–Hebei region, the growth of county-level technology-based small and medium-sized enterprises (SMEs) in Hebei Province has been constrained by regional heterogeneity in resource endowments and industrial foundations, which conflicts with the uniform application of support policies. Accurately identifying spatial differentiation patterns in policy effectiveness has become essential for optimizing policy implementation. Existing studies have primarily employed traditional models such as ordinary least squares (OLS), overlooking the spatial correlation of county-level economic systems. Moreover, limited attention has been paid to the local spatial heterogeneity of policy variables and enterprise development indicators, the nonlinear characteristics of policy effects at the county scale, and the quantification of spatial spillover effects of support policies. To address these limitations, moderation models, threshold regression models, and the Spatial Durbin Model (SDM) were integrated in this study to construct a moderating effect measurement model of support policies. Through this approach, the spatially differentiated effects of support policies for technology-based SMEs across counties in Hebei Province were quantified. The nonlinear moderating mechanisms of policy instruments and their spatial transmission patterns were systematically examined. By integrating spatial econometric techniques with policy effect evaluation frameworks, this study proposes a novel paradigm for regional policy research and offers evidence-based guidance for designing differentiated policy measures aimed at enhancing the effectiveness of support initiatives in Hebei Province.",dual backdrop innovation driven development strategy coordinated development beijing tianjin hebei region growth county level technology based small medium sized enterprises smes hebei province constrained regional heterogeneity resource endowments industrial foundations conflicts uniform application support policies accurately identifying spatial differentiation patterns policy effectiveness become essential optimizing policy implementation existing studies primarily employed traditional models ordinary least squares ols overlooking spatial correlation county level economic systems moreover limited attention paid local spatial heterogeneity policy variables enterprise development indicators nonlinear characteristics policy effects county scale quantification spatial spillover effects support policies address limitations moderation models threshold regression models spatial durbin model sdm integrated study construct moderating effect measurement model support policies approach spatially differentiated effects support policies technology based smes across counties hebei province quantified nonlinear moderating mechanisms policy instruments spatial transmission patterns systematically examined integrating spatial econometric techniques policy effect evaluation frameworks study proposes novel paradigm regional policy research offers evidence based guidance designing differentiated policy measures aimed enhancing effectiveness support initiatives hebei province
"With the increasing demand for analyzing accuracy and real-time performance in soccer games, traditional athlete tracking algorithms are challenged to perform in complex scenes. The main objective of the research is to cope with the challenges such as occlusion and lighting changes in complex scenes by improving the detection speed and accuracy. To this end, a novel soccer player tracking detection model is investigated. First, channel pruning and layer pruning techniques are used to optimize YOLOv3, which reduces the computational complexity and the number of parameters, while ensuring high detection accuracy. Second, the feature enhancement module and offset sampling mechanism are designed in combination with the improved DeepSORT multi-target tracking algorithm. By combining the Kalman filter and the Hungarian algorithm, the accuracy of target prediction and trajectory association is further improved. The experiment outcomes show that on the publicly available football network version 2 dataset, the F1 value of the model reaches 94.56%, and the average processing time is only 0.74 s. On the football player tracking and recognition dataset, the F1 value of the model is 95.27% and the processing time is 0.77 s. In addition, the loss rate is the lowest in occluded scenes, at 8.34%, and the highest trajectory consistency reaches 92.59% in lighting changing environments. From this, the model proposed by the research is significantly superior to existing methods in multiple key performance indicators, and has excellent detection capabilities and practical application potential. It can provide certain technical support for the analysis of penalty decisions and technical guidance in future football matches.",increasing demand analyzing accuracy real time performance soccer games traditional athlete tracking algorithms challenged perform complex scenes main objective research cope challenges occlusion lighting changes complex scenes improving detection speed accuracy end novel soccer player tracking detection model investigated first channel pruning layer pruning techniques used optimize yolov reduces computational complexity number parameters ensuring high detection accuracy second feature enhancement module offset sampling mechanism designed combination improved deepsort multi target tracking algorithm combining kalman filter hungarian algorithm accuracy target prediction trajectory association improved experiment outcomes show publicly available football network version dataset value model reaches average processing time football player tracking recognition dataset value model processing time addition loss rate lowest occluded scenes highest trajectory consistency reaches lighting changing environments model proposed research significantly superior existing methods multiple key performance indicators excellent detection capabilities practical application potential provide certain technical support analysis penalty decisions technical guidance future football matches
"In order to improve the medium and long-term forecast effect of regional economic development level, this paper combines the echo state network (TESN) to construct a medium and long-term forecast model for regional economic development level. Through the structural characteristics of the deep echo state network and the update criterion of the reserve pool (TRP), this paper improves the deep echo state network, and proposes the deep echo state network on the basis of TESN, and improves the single hidden layer structure of TESN. The improved deep echo state network only collects the state of TRP connected to the output layer, thereby reducing the size of the state matrix of TRP. The experimental study shows that the medium and long-term forecast effect of the regional economic level proposed in this paper is good.",order improve medium long term forecast effect regional economic development level paper combines echo state network tesn construct medium long term forecast model regional economic development level structural characteristics deep echo state network update criterion reserve pool trp paper improves deep echo state network proposes deep echo state network basis tesn improves single hidden layer structure tesn improved deep echo state network collects state trp connected output layer thereby reducing size state matrix trp experimental study shows medium long term forecast effect regional economic level proposed paper good
"This study is motivated by the need to improve the efficiency of iterative methods for solving nonlinear systems, overcoming some limitations of Newton’s method and other classical schemes. Building on the recent work of Singh, Sharma, and Kumar—which introduced a self-accelerating parameter dependent on the last iterate—an in-depth stability analysis of these without memory methods in a vectorial framework is proposed. The main objective is to evaluate how parameters’ selection and variation affect the process’s convergence and stability using tools from discrete dynamical systems, bifurcation diagrams, and dynamical planes. The results demonstrate that certain members of the family, particularly those with fourth-order convergence, exhibit superior stability properties compared to the initially proposed fifth-order scheme. Additionally, numerical tests on various nonlinear systems confirm the theoretical stability results. The stable methods of these families can be successfully applied on realistic problems such as preliminary orbit determination and on the nonlinear systems resulting from the discretization of Fisher’s equation used in population genetics and ecology, convection-diffusion equations appearing in engineering problems, etc.",study motivated need improve efficiency iterative methods solving nonlinear systems overcoming limitations newton method classical schemes building recent work singh sharma kumar introduced self accelerating parameter dependent last iterate depth stability analysis without memory methods vectorial framework proposed main objective evaluate parameters selection variation affect process convergence stability using tools discrete dynamical systems bifurcation diagrams dynamical planes results demonstrate certain members family particularly fourth order convergence exhibit superior stability properties compared initially proposed fifth order scheme additionally numerical tests various nonlinear systems confirm theoretical stability results stable methods families successfully applied realistic problems preliminary orbit determination nonlinear systems resulting discretization fisher equation used population genetics ecology convection diffusion equations appearing engineering problems etc
"Deep analysis of ideological and political education (IPE) data is vital for enhancing pedagogical precision. Addressing limitations of current methods—particularly in comprehending complex semantics, integrating domain knowledge, and evaluating civic-political (Si-Pol) characteristics—this study proposes an improved deep learning algorithm. Our framework augments semantic understanding by structuring external knowledge, employs a hierarchical attention mechanism to capture key textual information, and incorporates Si-Pol features into the loss function design. Evaluated on the COAE dataset, the model achieved macro-averaged F1 scores of 87.3% and 83.6% on the critical tasks of sentiment orientation recognition and values consistency assessment, respectively. This represents a significant improvement of 3.7% to 8.2% over baseline models (TextCNN, BiLSTM-Attention, base Transformer). The research provides a novel pathway for more accurate and interpretable analysis of IPE effectiveness.",deep analysis ideological political education ipe data vital enhancing pedagogical precision addressing limitations current methods particularly comprehending complex semantics integrating domain knowledge evaluating civic political pol characteristics study proposes improved deep learning algorithm framework augments semantic understanding structuring external knowledge employs hierarchical attention mechanism capture key textual information incorporates pol features loss function design evaluated coae dataset model achieved macro averaged scores critical tasks sentiment orientation recognition values consistency assessment respectively represents significant improvement baseline models textcnn bilstm attention base transformer research provides novel pathway accurate interpretable analysis ipe effectiveness
"This paper proposes a capacity configuration method for a microgrid composed of a photovoltaic (PV) power generation system and a hybrid energy storage system (battery storage + supercapacitors). The core of this method involves constructing a mixed-integer linear programming (MILP) model and incorporating a battery aging model to determine the retirement time of the energy storage system, thereby optimizing the microgrid capacity configuration. Additionally, this paper explores the integration of supercapacitors (SCs) into the microgrid capacity configuration to effectively mitigate battery aging, enhancing the economic performance and operational efficiency of the microgrid. The proposed method is validated through real-world case studies, demonstrating its practical applicability in microgrid development. Moreover, the case study analysis compares the proposed method with traditional methods that do not consider battery aging factors and conducts a sensitivity analysis regarding changes in energy storage procurement cost parameters. The results demonstrate that the proposed method not only effectively addresses the challenges posed by battery aging but also offers superior economic and technical performance in microgrid optimization planning and capacity configuration.",paper proposes capacity configuration method microgrid composed photovoltaic power generation system hybrid energy storage system battery storage supercapacitors core method involves constructing mixed integer linear programming milp model incorporating battery aging model determine retirement time energy storage system thereby optimizing microgrid capacity configuration additionally paper explores integration supercapacitors scs microgrid capacity configuration effectively mitigate battery aging enhancing economic performance operational efficiency microgrid proposed method validated real world case studies demonstrating practical applicability microgrid development moreover case study analysis compares proposed method traditional methods consider battery aging factors conducts sensitivity analysis regarding changes energy storage procurement cost parameters results demonstrate proposed method effectively addresses challenges posed battery aging also offers superior economic technical performance microgrid optimization planning capacity configuration
"Polyethylene is often used as the outer protective layer of optical cables due to its excellent dielectric, corrosion resistance, and mechanical properties. However, when facing high-voltage optical cable lines, its insulation ability is slightly insufficient. Therefore, a new type of optical cable sheath has been studied and prepared. This sheath is suitable for optical cable systems with voltage levels of 10 kV and below, and can meet the insulation and protection requirements in power communication scenarios. Firstly, different forms of magnesium oxide were prepared by hydrothermal and redox methods, and added to a polyethylene solution. The magnesium oxide/polyethylene composite film was prepared by a solution casting process with thermal crosslinking at 180°C for 15 min. To further improve the performance of composite films, a new type of optical cable sheath material, magnesium oxide/polyethylene composite film, was prepared by combining high thermal conductivity alumina with fibrous magnesium oxide/polyethylene. The results showed that when the amount of fibrous magnesium oxide in different forms of magnesium oxide/polyethylene composite films was 1.5 phr, the breakdown strength could reach 468 kV/mm, which was 26.49% higher than that of pure polyethylene, and the dielectric capacity increased by 5%. The direct current breakdown strength of the alumina/magnesium oxide/polyethylene composite film was 518 kV/mm, which was 14.11% higher than that of the magnesium oxide/polyethylene composite film. The above data indicated that the insulation performance of the modified optical cable sheath was greatly improved, which can adapt to higher voltage optical cable protection. The research provides important references for the design and optimization of new optical cable sheath materials, as well as new ideas and methods for material research in related application fields.",polyethylene often used outer protective layer optical cables due excellent dielectric corrosion resistance mechanical properties however facing high voltage optical cable lines insulation ability slightly insufficient therefore new type optical cable sheath studied prepared sheath suitable optical cable systems voltage levels meet insulation protection requirements power communication scenarios firstly different forms magnesium oxide prepared hydrothermal redox methods added polyethylene solution magnesium oxide polyethylene composite film prepared solution casting process thermal crosslinking min improve performance composite films new type optical cable sheath material magnesium oxide polyethylene composite film prepared combining high thermal conductivity alumina fibrous magnesium oxide polyethylene results showed amount fibrous magnesium oxide different forms magnesium oxide polyethylene composite films phr breakdown strength could reach higher pure polyethylene dielectric capacity increased direct current breakdown strength alumina magnesium oxide polyethylene composite film higher magnesium oxide polyethylene composite film data indicated insulation performance modified optical cable sheath greatly improved adapt higher voltage optical cable protection research provides important references design optimization new optical cable sheath materials well new ideas methods material research related application fields
"Scientific physical training management is vital to improve the police’s practical literacy and law enforcement capabilities, and it is also crucial in making the police force move towards formalization and professionalism. Good physical fitness is an essential foundation for the police to perform tasks and an important way to improve and apply police techniques and tactics. As the management of police physical training is a complex process, it is a qualitative concept with “fuzziness,” and it is impossible to accurately and ideally describe its content quantitatively only by classical and random mathematics. To carry out scientific quantitative analysis and objective evaluation of its advantages and disadvantages and to promote the development of police physical training and comprehensively improve the level of police physical fitness. This paper studies the application effect of four kinds of Fuzzy (Fuzzy mathematics) calculation methods in the management evaluation of police physical training by combining the fuzzy formula method with a questionnaire survey. The study results show that using four Fuzzy (fuzzy mathematics) scoring methods can determine the quality level of any police physical training management and rank the comprehensive scores from high too low to distinguish the ranking order clearly. The four Fuzzy (fuzzy mathematics) scoring methods are both scientific and reasonable: (reliability), and is simple and easy to implement (validity). Especially in sports research, quantitative evaluation of people or things in a “fuzzy state” is worthy of praise for development and application. Finally, we concluded that the four Fuzzy (fuzzy mathematics) calculation methods have good feasibility and operability.",scientific physical training management vital improve police practical literacy law enforcement capabilities also crucial making police force move towards formalization professionalism good physical fitness essential foundation police perform tasks important way improve apply police techniques tactics management police physical training complex process qualitative concept fuzziness impossible accurately ideally describe content quantitatively classical random mathematics carry scientific quantitative analysis objective evaluation advantages disadvantages promote development police physical training comprehensively improve level police physical fitness paper studies application effect four kinds fuzzy fuzzy mathematics calculation methods management evaluation police physical training combining fuzzy formula method questionnaire survey study results show using four fuzzy fuzzy mathematics scoring methods determine quality level police physical training management rank comprehensive scores high low distinguish ranking order clearly four fuzzy fuzzy mathematics scoring methods scientific reasonable reliability simple easy implement validity especially sports research quantitative evaluation people things fuzzy state worthy praise development application finally concluded four fuzzy fuzzy mathematics calculation methods good feasibility operability
"To determine the optimal trajectory of mobile robots in a complex environment and to enhance the effectiveness of search efficiency and path optimization, an improved ant colony (ACO) algorithm is proposed. A fusion algorithm of improved particle swarm optimization (PSO) and ACO algorithm is proposed to solve for optimal paths. Firstly, we improve the PSO and adaptively adjust the inertia weight to facilitate the particle search. Besides, we disturb the particles with the chaotic variables to increase the convergence speed. Secondly, the pheromone distribution on the path is adjusted based on the optimal solution of PSO to solve the initial pheromone insufficiency issue in the ACO algorithm. Then, to optimize the update strategy, we adaptively adjust the pheromone intensity values. We also provide a wide range of control schemes for updating the parameters for balancing the search capability and convergence. The results in the simulation environment show that our algorithm is more effective than other improved algorithms. The improved fusion algorithm converges faster and finds shorter optimal path. The method can improve the comprehensive performance and achieve the rapid path planning for mobile robots.",determine optimal trajectory mobile robots complex environment enhance effectiveness search efficiency path optimization improved ant colony aco algorithm proposed fusion algorithm improved particle swarm optimization pso aco algorithm proposed solve optimal paths firstly improve pso adaptively adjust inertia weight facilitate particle search besides disturb particles chaotic variables increase convergence speed secondly pheromone distribution path adjusted based optimal solution pso solve initial pheromone insufficiency issue aco algorithm optimize update strategy adaptively adjust pheromone intensity values also provide wide range control schemes updating parameters balancing search capability convergence results simulation environment show algorithm effective improved algorithms improved fusion algorithm converges faster finds shorter optimal path method improve comprehensive performance achieve rapid path planning mobile robots
"Dance, as a visual art, carries the different emotions of dancers through its movements. In response to the problem of low accuracy in traditional emotion recognition methods for dance action emotion recognition, this study proposes a dance action emotion recognition method that integrates convolutional neural networks and long short-term memory networks. It extracts features of dance action images through convolutional neural networks, recognizes actions utilizing long short-term memory networks, and then constructs a dance action emotion recognition model. Finally, by incorporating attention mechanisms into the recognition model, the key information of dance movements is focused. The experiment showed that the recognition accuracy of the research model reached 0.9472, with a root mean square error of 0.5124, significantly better than other models. In the practical application analysis of the proposed method, the recognition accuracy of the four different emotions of happiness, fear, relaxation, and sadness in dance movements reached 98.21%, 99.24%, 97.32%, and 98.49%, and was more practical than the comparative models. The above outcomes indicate that the research method can enhance the efficiency and accuracy of dance action emotion recognition, and provide a reference for subsequent scholars to conduct research in emotion recognition.",dance visual art carries different emotions dancers movements response problem low accuracy traditional emotion recognition methods dance action emotion recognition study proposes dance action emotion recognition method integrates convolutional neural networks long short term memory networks extracts features dance action images convolutional neural networks recognizes actions utilizing long short term memory networks constructs dance action emotion recognition model finally incorporating attention mechanisms recognition model key information dance movements focused experiment showed recognition accuracy research model reached root mean square error significantly better models practical application analysis proposed method recognition accuracy four different emotions happiness fear relaxation sadness dance movements reached practical comparative models outcomes indicate research method enhance efficiency accuracy dance action emotion recognition provide reference subsequent scholars conduct research emotion recognition
"This study aims to develop an efficient slope monitoring and deformation prediction method to address the geological disaster problems caused by slope deformation in infrastructure construction. The slope monitoring system and deformation prediction model were studied and constructed based on the building information model, support vector machine, and cuckoo search algorithm. Experiments showed that the improved cuckoo search algorithm had the best convergence and the optimal solution quality. The hypervolume reached 0.977, the inverse substitution distance reached 0.108. Moreover, this method had the fastest solution time, and the highest coverage of inflection points in the solution set. The optimized deformation prediction model had a strong generalization ability and achieved the best prediction effect in practical applications. The deviation between the predicted value and the true value was the smallest. The prediction accuracy of the optimized model was the highest, and the sum of squared errors reached 0.121. The prediction effect and efficiency of this method were the best, and the prediction time reached 6.46 seconds. This research can promptly identify potential safety hazards, provide theoretical support for formulating scientific slope treatment plans, and reduce casualties and property losses.",study aims develop efficient slope monitoring deformation prediction method address geological disaster problems caused slope deformation infrastructure construction slope monitoring system deformation prediction model studied constructed based building information model support vector machine cuckoo search algorithm experiments showed improved cuckoo search algorithm best convergence optimal solution quality hypervolume reached inverse substitution distance reached moreover method fastest solution time highest coverage inflection points solution set optimized deformation prediction model strong generalization ability achieved best prediction effect practical applications deviation predicted value true value smallest prediction accuracy optimized model highest sum squared errors reached prediction effect efficiency method best prediction time reached seconds research promptly identify potential safety hazards provide theoretical support formulating scientific slope treatment plans reduce casualties property losses
"The urban resilient distribution network is a crucial component of a resilient power network, facing various risks such as operational failures, climate impacts, and network attacks. These risks pose significant threats to the security of the power networks. However, the existing methods still have some problems, such as incomplete risk awareness indicators and inaccurate identification of power grid operational state. To ensure the secure and stable operation of an urban power grid, a risk situational awareness model for the resilient distribution network cyber-physical system (CPS) based on data density is developed, along with an enhanced Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS). Firstly, a more comprehensive risk situation index system is proposed to reflect the characteristics of power grid security situation and provide assessment basis for situation understanding and prediction. Second, the data density is used to determine the typical state of each node in order to distinguish between risk and normal states. Thirdly, a neural hierarchical interpolation algorithm with enhanced predictive capabilities is presented for forecasting future operational characteristics. Finally, a neural random network is used to classify the future states of nodes, and the part that does not belong to the typical state is identified as the risk state, so as to realize the risk situational awareness. The model is validated using actual data from a regional distribution network in East China. The results indicate that the proposed method achieves an identification rate exceeding 95% for all types of risks, and both the index system and the model can improve the situational awareness accuracy of each node by about 2%. Therefore, the proposed method can perceive the future risk posture of CPS nodes in the distribution network and have higher accuracy compared to traditional methods.",urban resilient distribution network crucial component resilient power network facing various risks operational failures climate impacts network attacks risks pose significant threats security power networks however existing methods still problems incomplete risk awareness indicators inaccurate identification power grid operational state ensure secure stable operation urban power grid risk situational awareness model resilient distribution network cyber physical system cps based data density developed along enhanced neural hierarchical interpolation time series forecasting hits firstly comprehensive risk situation index system proposed reflect characteristics power grid security situation provide assessment basis situation understanding prediction second data density used determine typical state node order distinguish risk normal states thirdly neural hierarchical interpolation algorithm enhanced predictive capabilities presented forecasting future operational characteristics finally neural random network used classify future states nodes part belong typical state identified risk state realize risk situational awareness model validated using actual data regional distribution network east china results indicate proposed method achieves identification rate exceeding types risks index system model improve situational awareness accuracy node therefore proposed method perceive future risk posture cps nodes distribution network higher accuracy compared traditional methods
"In recent years, wall art has become an important part of urban culture. However, image generation and restoration technologies still have limitations, especially in terms of image structure coherence and the consistency of artistic style. Issues such as abrupt textures and discontinuous edges at image stitching points are prominent. To address these problems, this study proposes a wall art image stitching and generation method that integrates Mask Generative Adversarial Network and Fine Generative Adversarial Network. The final model design performs excellently on public image datasets. In terms of generated image quality, the proposed model achieves Frechet Inception Distance and initial scores of 8.2 and 8.6, respectively, outperforming comparative models. Moreover, the model’s no-reference image quality evaluation, measured using the BRISQUE index, reaches 11.8, indicating that the generated images have a certain degree of realism. The proposed improved model demonstrates stronger applicability and reliability in the task of wall art image stitching and generation. The findings of this study provide technical support for the digital restoration, artistic creation, and urban visual design of wall art. It can improve the automatic generation and restoration quality of wall art images, promote the development of wall art creation technology, and support its practical application in digital cultural industries and urban esthetic improvement.",recent years wall art become important part urban culture however image generation restoration technologies still limitations especially terms image structure coherence consistency artistic style issues abrupt textures discontinuous edges image stitching points prominent address problems study proposes wall art image stitching generation method integrates mask generative adversarial network fine generative adversarial network final model design performs excellently public image datasets terms generated image quality proposed model achieves frechet inception distance initial scores respectively outperforming comparative models moreover model reference image quality evaluation measured using brisque index reaches indicating generated images certain degree realism proposed improved model demonstrates stronger applicability reliability task wall art image stitching generation findings study provide technical support digital restoration artistic creation urban visual design wall art improve automatic generation restoration quality wall art images promote development wall art creation technology support practical application digital cultural industries urban esthetic improvement
"The experiment proposes an English learning video recommendation method based on user preference differences with sequential recommendation. The experiment first introduces a collaborative filtering algorithm to preprocess user behavior and filter abnormalities and mutations in user behavior to further improve the accuracy. Then a deep learning algorithm for sequence recommendation based on user behavior is proposed, which mainly uses short-term user behavior for learning and representation. In addition, a model based on the attention mechanism is introduced to represent the long- and short-term user behaviors. At the same time, the difference between the long- and short-term behaviors is utilized for selective learning, which solves the need of identifying the change of user interests in educational scenarios. The results indicated that when the number of iterations reaches 250, the research method has a minimum loss value of 0.658. In the comparison of model accuracy, at the 50th iteration, the accuracy of the constructed method is as high as 94.89%. In the comparison of recommendation time, when the data volume is 4 MB, the recommended time of the research method is 0.072 s. When the data volume is 20 MB, the recommended time of the research method is always less than 0.100 s. The results indicate that the research method is the most effective approach for creating English learning videos. Furthermore, the reliability of data transmission is consistently high, ensuring that students’ online learning needs are met with accuracy.",experiment proposes english learning video recommendation method based user preference differences sequential recommendation experiment first introduces collaborative filtering algorithm preprocess user behavior filter abnormalities mutations user behavior improve accuracy deep learning algorithm sequence recommendation based user behavior proposed mainly uses short term user behavior learning representation addition model based attention mechanism introduced represent long short term user behaviors time difference long short term behaviors utilized selective learning solves need identifying change user interests educational scenarios results indicated number iterations reaches research method minimum loss value comparison model accuracy iteration accuracy constructed method high comparison recommendation time data volume recommended time research method data volume recommended time research method always less results indicate research method effective approach creating english learning videos furthermore reliability data transmission consistently high ensuring students online learning needs met accuracy
"At present, for ecological efficiency evaluation, the traditional data envelopment analysis has the problem that the same model is difficult to take into account different proportions of economic and environmental efficiency analysis. This study innovatively proposes a dual-objective data envelopment analysis (DEA) ecological efficiency model that combines environmental efficiency and economic efficiency. This method is based on data inclusion analysis and addresses the limitations of traditional data envelopment analysis (DEA) in balancing economic and environmental efficiency ratios. The results showed that, compared to traditional methods, the new method considered cities with lower ecological efficiency more comprehensively. This was because it incorporated unexpected outputs, such as carbon emissions, and dynamic weight allocation. The provincial ENE was about 0.55 (new method), lower than the traditional data envelopment analysis’s 0.6, reflecting a more rigorous assessment of the model’s environmental impact. The environmental efficiency of a province was about 0.55, while the environmental efficiency of cities in the eastern, central, and western regions was about 0.75, 0.42, and 0.42, respectively. The environmental efficiency of cities in the north and south regions was around 0.35 and 0.73, respectively. The economic efficiency of the whole province was around 0.97, and the economic efficiency of the cities in the east, center, and west regions was around 0.99, 0.98, and 0.95, respectively, which were all higher than 0.95 and above. It demonstrated that the economic efficiency of the cities in the east, center and west regions was higher. The economic efficiency of cities in the north and south regions was around 0.965 and 0.995, respectively, and the economic efficiency of the province was higher at around 0.977. The ecological efficiency of the whole province was around 0.52. The ecological efficiency of cities in the east, center, and west regions was around 0.72, 0.41, and 0.42, respectively. The ecological efficiency of cities in the north and south regions was around 0.38 and 0.73, respectively. From 2020 to 2024, the changes in ecological efficiency in the province were more stable. The economic efficiency of all cities in the period from 2018 to 2020 was generally high, and some cities even reached a perfect score. It shows that the study proposes that the method can provide a new research idea for cracking the contradiction between resource environment and regional development.",present ecological efficiency evaluation traditional data envelopment analysis problem model difficult take account different proportions economic environmental efficiency analysis study innovatively proposes dual objective data envelopment analysis dea ecological efficiency model combines environmental efficiency economic efficiency method based data inclusion analysis addresses limitations traditional data envelopment analysis dea balancing economic environmental efficiency ratios results showed compared traditional methods new method considered cities lower ecological efficiency comprehensively incorporated unexpected outputs carbon emissions dynamic weight allocation provincial ene new method lower traditional data envelopment analysis reflecting rigorous assessment model environmental impact environmental efficiency province environmental efficiency cities eastern central western regions respectively environmental efficiency cities north south regions around respectively economic efficiency whole province around economic efficiency cities east center west regions around respectively higher demonstrated economic efficiency cities east center west regions higher economic efficiency cities north south regions around respectively economic efficiency province higher around ecological efficiency whole province around ecological efficiency cities east center west regions around respectively ecological efficiency cities north south regions around respectively changes ecological efficiency province stable economic efficiency cities period generally high cities even reached perfect score shows study proposes method provide new research idea cracking contradiction resource environment regional development
"To address data lag, subjectivity, and challenges in tracking emotional-spatiotemporal dynamics in traditional smart volunteer service evaluation, this study proposes a quality assessment system integrating spatiotemporal sentiment analysis. The system employs a self-attention mechanism for deep fusion of multimodal data (text, images, and speech) and develops a cross-modal spatiotemporal analysis method using graph convolutional networks. Implemented on a cloud platform, it enables real-time service monitoring and dynamic evaluation. Testing achieved an 89.0 F1-score and 48.0% accuracy on CMU-MOSI dataset. Ablation experiments on Yelp datasets showed Hit Rates of 44%/23% (k = 10) and 74%/48% (k = 50), with NDCG values reaching 28%/13% and 38%/18%, respectively. The system demonstrated superior sentiment analysis precision and assessment reliability, offering an intelligent solution for optimizing volunteer service management and decision-making. This advancement promotes the evolution of intelligent, precise evaluation frameworks in volunteer services.",address data lag subjectivity challenges tracking emotional spatiotemporal dynamics traditional smart volunteer service evaluation study proposes quality assessment system integrating spatiotemporal sentiment analysis system employs self attention mechanism deep fusion multimodal data text images speech develops cross modal spatiotemporal analysis method using graph convolutional networks implemented cloud platform enables real time service monitoring dynamic evaluation testing achieved score accuracy cmu mosi dataset ablation experiments yelp datasets showed hit rates ndcg values reaching respectively system demonstrated superior sentiment analysis precision assessment reliability offering intelligent solution optimizing volunteer service management decision making advancement promotes evolution intelligent precise evaluation frameworks volunteer services
"The need for player emotion in game product design and operation is growing in relevance given the explosive growth of the game sector. As a mainstream competitive genre, MOBA games involve regular player interactions and strong emotional fluctuations that quickly call for effective emotion tendency research techniques to improve user experience and product tuning impacts. In this study, we build an emotional model framework based on enhanced deep reinforcement learning (DRL) and propose a model entitled MOBA-SentDRL for MOBA environment. The results reveal that MOBA-SentDRL beats the conventional model in several criteria by building an actual MOBA game user sentiment dataset and assessing it in a range of comparison studies. The work motivates the future direction of multimodal and multi-strategy fusion for emotional computing and offers a practical method for the emotion interpretation and interaction of intelligent gaming goods.",need player emotion game product design operation growing relevance given explosive growth game sector mainstream competitive genre moba games involve regular player interactions strong emotional fluctuations quickly call effective emotion tendency research techniques improve user experience product tuning impacts study build emotional model framework based enhanced deep reinforcement learning drl propose model entitled moba sentdrl moba environment results reveal moba sentdrl beats conventional model several criteria building actual moba game user sentiment dataset assessing range comparison studies work motivates future direction multimodal multi strategy fusion emotional computing offers practical method emotion interpretation interaction intelligent gaming goods
"Aiming at the problem of insufficient real-time dynamic risk modeling in Human Factors Engineering (HFE), this study proposes the Parameterized Bayesian Network (PBN-KL) optimization method. The method fuses multi-source physiological, cognitive, and environmental parameters and significantly improves risk prevention and control effectiveness through KL scatter-driven adaptive structure learning and real-time decision engine. In the validation of NASA-TLX and UCI-HAR datasets, the prevention and control accuracy is improved by 15.2%, the response delay is reduced by 40.7%, and the prevention and control success rate reaches 92.3% compared with the traditional method. The framework provides an interpretable and real-time active security solution for high interaction scenarios under the national strategic needs of intelligent manufacturing “digital twin” and precision medicine, bridging the gap between theoretical modeling and practical application needs.",aiming problem insufficient real time dynamic risk modeling human factors engineering hfe study proposes parameterized bayesian network pbn optimization method method fuses multi source physiological cognitive environmental parameters significantly improves risk prevention control effectiveness scatter driven adaptive structure learning real time decision engine validation nasa tlx uci har datasets prevention control accuracy improved response delay reduced prevention control success rate reaches compared traditional method framework provides interpretable real time active security solution high interaction scenarios national strategic needs intelligent manufacturing digital twin precision medicine bridging gap theoretical modeling practical application needs
"A robust video processing technology that integrates adaptive Kalman filtering and optimized ORB algorithm is proposed to address the problems of image blur and feature matching failure caused by mechanical vibration and extreme lighting changes in industrial video surveillance. By dynamically adjusting the covariance matrix of KF process noise, the synergistic suppression of high-frequency vibration and pulse type illumination noise can be achieved. The experimental results show that optimizing the ORB algorithm enhances the invariance of feature point direction and scale through the Gaussian difference pyramid and grayscale centroid method, and the success rate of feature point matching is improved by 63.1% compared to the traditional method. In extreme lighting changes and dynamic scenes, the algorithm reduces image pixel offset by 56%–67% and increases peak signal-to-noise ratio by an average of 32%–34%, significantly better than mainstream methods such as histogram equalization and contrast-limited adaptive histogram equalization. By simplifying the calculation process of ORB feature descriptors, the algorithm’s processing speed increases by 57% compared to the traditional SIFT method, meeting the timeliness requirements of industrial monitoring. In four typical industrial scenarios, the pixel matching accuracy reaches 85.23%, 78.34%, and 83.56% in rotation, scale, and lighting variation scenarios. Compared with gyroscope technology and traditional ORB algorithm, the peak signal-to-noise ratio of the proposed method is improved by an average of 2.56 dB and 1.67 dB, respectively. The above results indicate that the proposed method has better image denoising and robustness effects in real industrial environments than traditional video image processing techniques and is of great significance for maintaining the clarity of video surveillance images in harsh industrial environments.",robust video processing technology integrates adaptive kalman filtering optimized orb algorithm proposed address problems image blur feature matching failure caused mechanical vibration extreme lighting changes industrial video surveillance dynamically adjusting covariance matrix process noise synergistic suppression high frequency vibration pulse type illumination noise achieved experimental results show optimizing orb algorithm enhances invariance feature point direction scale gaussian difference pyramid grayscale centroid method success rate feature point matching improved compared traditional method extreme lighting changes dynamic scenes algorithm reduces image pixel offset increases peak signal noise ratio average significantly better mainstream methods histogram equalization contrast limited adaptive histogram equalization simplifying calculation process orb feature descriptors algorithm processing speed increases compared traditional sift method meeting timeliness requirements industrial monitoring four typical industrial scenarios pixel matching accuracy reaches rotation scale lighting variation scenarios compared gyroscope technology traditional orb algorithm peak signal noise ratio proposed method improved average respectively results indicate proposed method better image denoising robustness effects real industrial environments traditional video image processing techniques great significance maintaining clarity video surveillance images harsh industrial environments
"To enhance the performance and technological sophistication of ceramic products, this study integrates artificial neural network techniques to develop a system for identifying the technological grade of ceramics, employing advanced ceramic image recognition methods to assess and validate the quality level of the products. Moreover, the numerical calculation program of the physical quantity of Mie scattering theory is written on the MATLAB software to analyze the numerical value of scattering series of ceramic craft products, the relationship between scattering coefficient and particle size, and the relationship between scattered light flux and particle size. In addition, this paper conducts graph analysis to study and understand the properties of Mie scattering theory more deeply. Finally, the image algorithm is applied to construct the system. Simulation results indicate that the ceramic technology level identification and processing system, developed using artificial neural network techniques, effectively supports the manufacturing and classification of contemporary ceramic products.",enhance performance technological sophistication ceramic products study integrates artificial neural network techniques develop system identifying technological grade ceramics employing advanced ceramic image recognition methods assess validate quality level products moreover numerical calculation program physical quantity mie scattering theory written matlab software analyze numerical value scattering series ceramic craft products relationship scattering coefficient particle size relationship scattered light flux particle size addition paper conducts graph analysis study understand properties mie scattering theory deeply finally image algorithm applied construct system simulation results indicate ceramic technology level identification processing system developed using artificial neural network techniques effectively supports manufacturing classification contemporary ceramic products
"As music gradually becomes an indispensable part of people’s lives, the optimization of music recommendation systems has become a research hotspot. In response to the problems of poor recommendation accuracy and low degree of personalization in music recommendation systems, this article combined the optimized LGCN-A (Light Graph Convolution Network-Attitude) algorithm to conduct personalized modeling and recommendation research in music recommendation systems. Firstly, the collected data is cleaned and normalized, and feature data of users and music are extracted. Then, a graph structure between users and music is constructed, and each user and music node is initialized with a feature vector operation. The original feature vectors of users and music can be mapped to the embedding space through linear transformation, and the embedding vector can be initialized using Gaussian distribution and regularized. Finally, based on the traditional NGCF (Neural Graph Collaborative Filtering) algorithm, lightweight processing can be achieved by reducing nonlinear activation functions and feature transformation steps, and self-attention mechanism can be introduced to assign different weights to different users and music. The experiment is based on the public dataset Million Song Dataset to predict the interaction between users and music, and generate a recommendation list. The results show that at k of 20, the recommendation hit rate of the LGCN-A algorithm reached 0.95, which is 0.19 higher than the traditional NGCF algorithm, and the overlap is only 21.43%. The LGCN-A algorithm has improved the accuracy of recommendations in music recommendation systems, ensuring different levels of user personalization and providing strong support for the further development of recommendation systems.",music gradually becomes indispensable part people lives optimization music recommendation systems become research hotspot response problems poor recommendation accuracy low degree personalization music recommendation systems article combined optimized lgcn light graph convolution network attitude algorithm conduct personalized modeling recommendation research music recommendation systems firstly collected data cleaned normalized feature data users music extracted graph structure users music constructed user music node initialized feature vector operation original feature vectors users music mapped embedding space linear transformation embedding vector initialized using gaussian distribution regularized finally based traditional ngcf neural graph collaborative filtering algorithm lightweight processing achieved reducing nonlinear activation functions feature transformation steps self attention mechanism introduced assign different weights different users music experiment based public dataset million song dataset predict interaction users music generate recommendation list results show recommendation hit rate lgcn algorithm reached higher traditional ngcf algorithm overlap lgcn algorithm improved accuracy recommendations music recommendation systems ensuring different levels user personalization providing strong support development recommendation systems
"Employee satisfaction, retention, and productivity are all directly impacted by employee remuneration, which is a key factor in determining organizational effectiveness. Despite the significance of remuneration, many firms find it difficult to put in place structures that are fair and accommodating of the diverse workforce. This study suggests a thorough, data-driven approach for segmenting data using adaptive clustering in order to maximize employee remuneration. The paradigm allows for accurate and comprehensible employee stratification by including new indicators like the wage-to-Performance Ratio (SPR), wage growth rate, and satisfaction elasticity. A real-world corporate dataset was used, and to guarantee data quality, extensive preparation was done, including feature engineering, normalization, and outlier elimination. Different employee segments were created using adaptive clustering, which was assessed against conventional K-Means and directed by internal validation indices (such as the elbow method and silhouette score). Strategic pay changes are informed by empirical evaluation’s actionable findings, which include identifying strong achievers who are underpaid and long-tenured employees who are stagnant. HR professionals may identify high-potential employee cohorts and adopt data-driven, performance-aligned remuneration plans with the help of the suggested strategy. Additionally, the approach supports evidence-based human capital management in a variety of operational situations by exhibiting scalability and flexibility across different organizational structures.",employee satisfaction retention productivity directly impacted employee remuneration key factor determining organizational effectiveness despite significance remuneration many firms find difficult put place structures fair accommodating diverse workforce study suggests thorough data driven approach segmenting data using adaptive clustering order maximize employee remuneration paradigm allows accurate comprehensible employee stratification including new indicators like wage performance ratio spr wage growth rate satisfaction elasticity real world corporate dataset used guarantee data quality extensive preparation done including feature engineering normalization outlier elimination different employee segments created using adaptive clustering assessed conventional means directed internal validation indices elbow method silhouette score strategic pay changes informed empirical evaluation actionable findings include identifying strong achievers underpaid long tenured employees stagnant professionals may identify high potential employee cohorts adopt data driven performance aligned remuneration plans help suggested strategy additionally approach supports evidence based human capital management variety operational situations exhibiting scalability flexibility across different organizational structures
"Education is vital for talent development, and assessing English teaching quality helps teachers improve their methods. However, traditional assessment models are inefficient and time-consuming. To address this, this study proposes a new teaching quality assessment model using a forward rule fast-matching algorithm. This approach improves data processing efficiency by removing irrelevant data through a rule-driven data cleaning. The model integrates a deep belief network with the fast-matching algorithm to create an effective evaluation tool, along with a student performance prediction model. Finally, for model validation, the study used a self-collected dataset that included English performance data from 286 students, which was later expanded to include English performance data from 3500 students from 10 universities of different sizes. Experimental results show that the model performs well, achieving a PR curve area of 0.87 and an F1 value of 0.93. After regularization, the model reduced time consumption by 6.18% and space usage by 3.23%. The average matching time was 319 ms, with 927 matches on average—both better than existing methods. Moreover, the difference between actual and predicted English scores was small, with detection accuracy above 93%. In conclusion, the proposed model offers higher efficiency, accuracy, and reliability in evaluating English teaching quality. The research contributes by reducing time costs and improving evaluation effectiveness, offering a practical solution for real-world teaching assessments. It also advances deep learning by combining deep belief networks with forward rule fast-matching techniques.",education vital talent development assessing english teaching quality helps teachers improve methods however traditional assessment models inefficient time consuming address study proposes new teaching quality assessment model using forward rule fast matching algorithm approach improves data processing efficiency removing irrelevant data rule driven data cleaning model integrates deep belief network fast matching algorithm create effective evaluation tool along student performance prediction model finally model validation study used self collected dataset included english performance data students later expanded include english performance data students universities different sizes experimental results show model performs well achieving curve area value regularization model reduced time consumption space usage average matching time matches average better existing methods moreover difference actual predicted english scores small detection accuracy conclusion proposed model offers higher efficiency accuracy reliability evaluating english teaching quality research contributes reducing time costs improving evaluation effectiveness offering practical solution real world teaching assessments also advances deep learning combining deep belief networks forward rule fast matching techniques
"Effective agricultural production supply chain management is the key to optimizing the intelligent transformation strategy of ecological agriculture. Traditional research faces huge challenges in resource optimization and dynamic decision-making. Therefore, this paper first abstracts the intelligent management problem of ecological agriculture as a multi-agent sequential decision-making problem. Then, the cosine function is adopted to improve the Transformer model (EOTransformer). For the entire agricultural production supply chain involved in the cooperation, maintain the encoder and decoder structure of EOTransformer. At each time step, agent observations are encoded by passing them through the encoder to obtain their latent representations. The decoder generates optimal actions of each agent through sequential autoregression to obtain the best strategy. Experimental outcome demonstrates that the suggested approach achieves an approximate 90% reduction in parameter scale over standard approach, greatly reducing the complexity of the algorithm.",effective agricultural production supply chain management key optimizing intelligent transformation strategy ecological agriculture traditional research faces huge challenges resource optimization dynamic decision making therefore paper first abstracts intelligent management problem ecological agriculture multi agent sequential decision making problem cosine function adopted improve transformer model eotransformer entire agricultural production supply chain involved cooperation maintain encoder decoder structure eotransformer time step agent observations encoded passing encoder obtain latent representations decoder generates optimal actions agent sequential autoregression obtain best strategy experimental outcome demonstrates suggested approach achieves approximate reduction parameter scale standard approach greatly reducing complexity algorithm
"Accurate and intelligent management of laboratory equipment is essential in modern university environments, yet traditional manual monitoring methods suffer from inefficiency, lack of timeliness, and weak data integration. To address these challenges, this study proposes a complete detection and management framework based on an improved YOLOv4 model integrated with Internet of Things (IoT) technologies. First, a TV-L1 optical flow-based preprocessing method is designed to extract high-information keyframes from video streams. Then, K-means++ clustering and an Efficient Channel Attention (ECA) module are incorporated into the YOLOv4 architecture to optimize anchor box allocation and channel-wise feature emphasis. The improved model demonstrates superior detection accuracy and robustness under varying lighting and occlusion conditions. Furthermore, a multidimensional user evaluation and a temporal consistency test validate its performance in real-world laboratory monitoring, achieving an accuracy of 0.98, IoU of 0.95, and a low ID switch rate of 1.3 per 100 frames. This research provides an integrated and scalable technical approach to smart laboratory management, supporting real-time monitoring, device accountability, and IoT-based deployment scenarios.",accurate intelligent management laboratory equipment essential modern university environments yet traditional manual monitoring methods suffer inefficiency lack timeliness weak data integration address challenges study proposes complete detection management framework based improved yolov model integrated internet things iot technologies first optical flow based preprocessing method designed extract high information keyframes video streams means clustering efficient channel attention eca module incorporated yolov architecture optimize anchor box allocation channel wise feature emphasis improved model demonstrates superior detection accuracy robustness varying lighting occlusion conditions furthermore multidimensional user evaluation temporal consistency test validate performance real world laboratory monitoring achieving accuracy iou low switch rate per frames research provides integrated scalable technical approach smart laboratory management supporting real time monitoring device accountability iot based deployment scenarios
"To address the limitations of inaccuracy and data fragmentation in educational assessment systems, this study proposes a novel computational framework integrating federated learning (FL) with knowledge distillation. By modelling students as edge clients and institutional servers as central aggregators, the framework enables distributed collaborative training while preserving data privacy. Crucially, we optimize the FL aggregation strategy through a hierarchical knowledge distillation mechanism, where local models are guided by distilled global knowledge to enhance parameter efficiency. Experimental validation demonstrates a 10.3% improvement in assessment accuracy compared to baseline methods, alongside a 17% reduction in communication overhead. The proposed model showcases scalable computational efficiency for large-scale educational data analysis, with potential applications extending to scientific data fusion scenarios in IoT and edge computing environments.",address limitations inaccuracy data fragmentation educational assessment systems study proposes novel computational framework integrating federated learning knowledge distillation modelling students edge clients institutional servers central aggregators framework enables distributed collaborative training preserving data privacy crucially optimize aggregation strategy hierarchical knowledge distillation mechanism local models guided distilled global knowledge enhance parameter efficiency experimental validation demonstrates improvement assessment accuracy compared baseline methods alongside reduction communication overhead proposed model showcases scalable computational efficiency large scale educational data analysis potential applications extending scientific data fusion scenarios iot edge computing environments
"To optimize resource allocation in free trade zones (FTZs), this study proposes a computational synergy mechanism leveraging multimodal network modeling. We integrate five-dimensional data streams—capital flows, logistics networks, human mobility, institutional policies, and digital information—through multimodal feature fusion. Key innovations include: (1) A dynamic graph embedding framework using graph attention networks (GATs) to model cross-factor dependencies; (2) A multi-agent coordination algorithm combining Nash bargaining with proximal policy optimization (PPO) for resource conflict resolution; (3) Real-time resource scheduling via heuristic search and stream processing. Validated with China’s pilot FTZ operational data, cross-border clearance workflows accelerated by 32.4% through dynamic scheduling.",optimize resource allocation free trade zones ftzs study proposes computational synergy mechanism leveraging multimodal network modeling integrate five dimensional data streams capital flows logistics networks human mobility institutional policies digital information multimodal feature fusion key innovations include dynamic graph embedding framework using graph attention networks gats model cross factor dependencies multi agent coordination algorithm combining nash bargaining proximal policy optimization ppo resource conflict resolution real time resource scheduling via heuristic search stream processing validated china pilot ftz operational data cross border clearance workflows accelerated dynamic scheduling
"To deeply explore the potential correlation between in-class grades and extracurricular training plans, a big data mining and analysis model for colleges and universities based on the improved FP-growth algorithm is studied and proposed. Based on the traditional FP-growth algorithm, this model integrates the C4.5 partitioning strategy and optimizes the FP-growth tree structure, significantly improving the mining efficiency and accuracy of big data in colleges and universities. Compared with the traditional FP-growth association rule algorithm, the running time of the research model is only 9 minutes, which is 12 minutes shorter. The simulation results show a strong correlation between the in-class grades and the grades of social practice and campus cultural activities. The confidence levels exceed 80%, the accuracy rate reaches 92.32%, and the loss value is only 0.18. The accuracy rate of the research model increases by 17.97% compared with the traditional model. From this, the model proposed by the research has excellent data mining and data analysis capabilities, and can provide a new suggestion and direction for student management in the field of education.",deeply explore potential correlation class grades extracurricular training plans big data mining analysis model colleges universities based improved growth algorithm studied proposed based traditional growth algorithm model integrates partitioning strategy optimizes growth tree structure significantly improving mining efficiency accuracy big data colleges universities compared traditional growth association rule algorithm running time research model minutes minutes shorter simulation results show strong correlation class grades grades social practice campus cultural activities confidence levels exceed accuracy rate reaches loss value accuracy rate research model increases compared traditional model model proposed research excellent data mining data analysis capabilities provide new suggestion direction student management field education
"The proliferation of online network services has led to a steady rise in gambling-related activities, posing serious challenges to cyber governance and public security. Despite significant efforts by law enforcement agencies to curb online gambling, its dynamic and covert nature continues to hinder effective regulation. In this study, we propose a novel identification framework that leverages multimodal signals—including webpage text, visual content, and embedded image text—to detect gambling websites with high precision. Our approach integrates these heterogeneous data sources into a unified model, achieving robust representation and classification across diverse website structures. Extensive experiments on a domain-specific dataset demonstrate that our method significantly outperforms traditional baselines, reaching an accuracy of 99.3%. This work contributes an effective and scalable technical solution to assist real-world gambling crime detection and opens new directions for multimodal modeling in security applications.",proliferation online network services led steady rise gambling related activities posing serious challenges cyber governance public security despite significant efforts law enforcement agencies curb online gambling dynamic covert nature continues hinder effective regulation study propose novel identification framework leverages multimodal signals including webpage text visual content embedded image text detect gambling websites high precision approach integrates heterogeneous data sources unified model achieving robust representation classification across diverse website structures extensive experiments domain specific dataset demonstrate method significantly outperforms traditional baselines reaching accuracy work contributes effective scalable technical solution assist real world gambling crime detection opens new directions multimodal modeling security applications
"A new urbanization evaluation index system is constructed based on the “Five-in-One” general layout. The entropy weight TOPSIS method calculates the new urbanization index of 16 prefecture-level cities in Anhui Province from 2006 to 2021. The differences and evolution patterns of the new urbanization level in different regions are analyzed using kernel density estimation and the Dagum Gini coefficient. The main driving forces for improving the new urbanization level are analyzed using principal component analysis. The results show that: (1) The new urbanization level of cities in Anhui Province generally exhibits an uneven distribution of “high in the southeast and low in the northwest,” with Hefei city consistently leading the province in the new urbanization level. (2) The main driving forces of new urbanization in Anhui Province vary at different times, with economic development and ecological civilization becoming key driving forces for new urbanization in Anhui Province from 2014 to 2021.",new urbanization evaluation index system constructed based five one general layout entropy weight topsis method calculates new urbanization index prefecture level cities anhui province differences evolution patterns new urbanization level different regions analyzed using kernel density estimation dagum gini coefficient main driving forces improving new urbanization level analyzed using principal component analysis results show new urbanization level cities anhui province generally exhibits uneven distribution high southeast low northwest hefei city consistently leading province new urbanization level main driving forces new urbanization anhui province vary different times economic development ecological civilization becoming key driving forces new urbanization anhui province
"Traditional Chinese medicine has played a good part in the prevention and treatment of many diseases due to its low toxicity, effectiveness and ability to regulate the functions of the whole body from multiple targets. Traditional Chinese medicine compound can also play a synergistic effect between a variety of single-flavor Chinese medicines to act on multiple targets of the disease, and then achieve therapeutic effects. During recent years, many researchers have begun to concentrate on the pharmacological activity of traditional Chinese medicine, and clinical studies have also shown that the traditional Chinese medicine in the adjuvant treatment of AD is remarkable, and the development of effective drugs to improve the course of AD from natural herbs has become a hot spot in today’s research. Therefore, this article reviews the current status of traditional Chinese medicine in the treatment of AD, in order to provide new ideas for related research.",traditional chinese medicine played good part prevention treatment many diseases due low toxicity effectiveness ability regulate functions whole body multiple targets traditional chinese medicine compound also play synergistic effect variety single flavor chinese medicines act multiple targets disease achieve therapeutic effects recent years many researchers begun concentrate pharmacological activity traditional chinese medicine clinical studies also shown traditional chinese medicine adjuvant treatment remarkable development effective drugs improve course natural herbs become hot spot today research therefore article reviews current status traditional chinese medicine treatment order provide new ideas related research
"With the rapid development of e-commerce and the increasing diversity of user demands, personalized recommendations on online shopping platforms have become an important direction of research in the field of e-commerce, which is extremely important for promoting the intelligent transformation of the e-commerce industry. Personalized recommendations on online shopping platforms directly promote the growth of e-commerce sales by improving user experience and satisfaction; at the same time, through the application of intelligent recommendation technology, it accelerates the technological innovation of the e-commerce industry and the changes in the competitive landscape. However, current personalized recommendation algorithms still have problems such as poor algorithm performance, long iteration time, and low recommendation precision. To better achieve personalized recommendations for online shopping, this article applies deep Q-network (DQN) to explore personalized recommendation algorithms in depth. In the article, data is first collected and preprocessed through data augmentation, and based on the deep Q-network, a user–product interaction matrix is constructed. Afterward, based on user behavior characteristic data, user behavior modeling is implemented. Then a DQN-based learning architecture is constructed by combining long-term interests, short-term interests, and prediction modules, and a small batch gradient descent method is used to train the function. Finally, this article evaluates the performance of the algorithm. The research results show that when the length of the recommendation list is 1, the accuracy, recall, and mean average precision of the DQN algorithm are 0.924, 0.875, and 0.901, respectively. The recommendation algorithm based on DQN achieves good results in indicators of recommendation accuracy, recall, and mean average precision. In this article, DQN is applied, and the online shopping recommendation system is optimized by combining deep learning and reinforcement learning, significantly improving the quality and efficiency of recommendations and promoting the intelligent development of the e-commerce industry.",rapid development commerce increasing diversity user demands personalized recommendations online shopping platforms become important direction research field commerce extremely important promoting intelligent transformation commerce industry personalized recommendations online shopping platforms directly promote growth commerce sales improving user experience satisfaction time application intelligent recommendation technology accelerates technological innovation commerce industry changes competitive landscape however current personalized recommendation algorithms still problems poor algorithm performance long iteration time low recommendation precision better achieve personalized recommendations online shopping article applies deep network dqn explore personalized recommendation algorithms depth article data first collected preprocessed data augmentation based deep network user product interaction matrix constructed afterward based user behavior characteristic data user behavior modeling implemented dqn based learning architecture constructed combining long term interests short term interests prediction modules small batch gradient descent method used train function finally article evaluates performance algorithm research results show length recommendation list accuracy recall mean average precision dqn algorithm respectively recommendation algorithm based dqn achieves good results indicators recommendation accuracy recall mean average precision article dqn applied online shopping recommendation system optimized combining deep learning reinforcement learning significantly improving quality efficiency recommendations promoting intelligent development commerce industry
"In news text clustering optimization, the current text clustering optimization methods have the drawback of poor clustering optimization effect. In response to this issue, this study introduces an improved density peak clustering algorithm to cluster news text data, and uses the k-means clustering algorithm to optimize mixed text. On the basis of improving the density peak clustering algorithm, a new clustering optimization method is proposed by combining the feature word pairing method that can extract features from text information. The experiment showed that the research algorithm had an average absolute error and root mean square error of 1.035 and 0.963 when optimizing the clustering of news texts, and the clustering accuracy of this algorithm reached 94.36%, significantly higher than other algorithms. The optimization method achieved an accuracy of 89.67% in extracting text feature words for clustering optimization of different types of news texts, which was significantly higher than the comparison method. The above results indicate that the proposed method has a good clustering optimization effect on news texts, providing a technical support for the field of text clustering research.",news text clustering optimization current text clustering optimization methods drawback poor clustering optimization effect response issue study introduces improved density peak clustering algorithm cluster news text data uses means clustering algorithm optimize mixed text basis improving density peak clustering algorithm new clustering optimization method proposed combining feature word pairing method extract features text information experiment showed research algorithm average absolute error root mean square error optimizing clustering news texts clustering accuracy algorithm reached significantly higher algorithms optimization method achieved accuracy extracting text feature words clustering optimization different types news texts significantly higher comparison method results indicate proposed method good clustering optimization effect news texts providing technical support field text clustering research
"With the opening of the big data era, the exponential growth of IoT medical data provides powerful data support for the medical and health care fields. To effectively utilize these data, the study innovatively utilizes direct method for learning a linear non-Gaussian acyclic model with graph capsule neural network and proposes a causal inference technique based on IoT medical data disease prediction and risk assessment model. The experimental results indicated that the proposed method of the study could effectively clarify the causal relationship between disease features and thus enhance the stability of the model compared with the popular disease prediction and risk assessment methods of the same type. The research model achieved 86.01%, 94.26%, 95.03%, 95.12%, and 94.21% classification accuracy for the causality maps of diabetes dataset, stroke dataset, heart failure dataset, heart disease dataset, and cardiovascular disease dataset, respectively. In addition, the research model also achieved 97.13%, 97.26%, and 77.89% for precision, recall, and F1 value on the test set, respectively. It had good disease prediction and risk assessment ability. The research results are of great significance in improving the accuracy and efficiency of disease prediction and risk assessment. It can not only provide strong technical support for the application in related fields, but also is expected to promote the further development and application of causal inference technique in disease prediction and risk assessment.",opening big data exponential growth iot medical data provides powerful data support medical health care fields effectively utilize data study innovatively utilizes direct method learning linear non gaussian acyclic model graph capsule neural network proposes causal inference technique based iot medical data disease prediction risk assessment model experimental results indicated proposed method study could effectively clarify causal relationship disease features thus enhance stability model compared popular disease prediction risk assessment methods type research model achieved classification accuracy causality maps diabetes dataset stroke dataset heart failure dataset heart disease dataset cardiovascular disease dataset respectively addition research model also achieved precision recall value test set respectively good disease prediction risk assessment ability research results great significance improving accuracy efficiency disease prediction risk assessment provide strong technical support application related fields also expected promote development application causal inference technique disease prediction risk assessment
"The individualized demand for cigarettes from customers is growing quickly with the rapid expansion of e-commerce, posing a significant challenge to the finished cigarette warehouse system. This study proposes an automated warehouse picking optimization method based on the FP-growth algorithm and an improved genetic algorithm. This method significantly improves warehouse operational efficiency by optimizing product combination packing, storage location of goods, and order batching. Specifically, the study introduced two mechanisms to improve the genetic algorithm and optimize the storage location of goods: a dynamic parameter adaptive mechanism and an evolutionary reversal operator. It also combined the improved K-means algorithm (IKMA) for order batching optimization. Compared with existing methods, this study achieved rapid convergence to an approximate optimal solution and significant reductions in warehouse inbound and outbound operations and picking time. These reductions were 16.8–32.0% and 33.2%, respectively. Compared to the other two methods, the enhanced genetic algorithm stabilized the total number of bin entries and exited after approximately 420 iterations of product location optimization. These results indicate that the method proposed in this study has significant advantages in responding to the rapid changes in order demand in the e-commerce environment, providing a new direction for warehouse management.",individualized demand cigarettes customers growing quickly rapid expansion commerce posing significant challenge finished cigarette warehouse system study proposes automated warehouse picking optimization method based growth algorithm improved genetic algorithm method significantly improves warehouse operational efficiency optimizing product combination packing storage location goods order batching specifically study introduced two mechanisms improve genetic algorithm optimize storage location goods dynamic parameter adaptive mechanism evolutionary reversal operator also combined improved means algorithm ikma order batching optimization compared existing methods study achieved rapid convergence approximate optimal solution significant reductions warehouse inbound outbound operations picking time reductions respectively compared two methods enhanced genetic algorithm stabilized total number bin entries exited approximately iterations product location optimization results indicate method proposed study significant advantages responding rapid changes order demand commerce environment providing new direction warehouse management
"The popularity of volleyball is increasing. Athletes must follow correct techniques for practical training and fair competition. However, errors frequently occur during training or matches, and traditional methods struggle to identify them accurately. This adversely affects athletes’ training effectiveness and the fairness of volleyball matches. This article presented a volleyball error technique action recognition method based on visual image technology. The camera device was used to obtain visual images of volleyball actions, and support vector machine technology was used to classify the skeletal features of athletes in volleyball action images to identify volleyball error technique actions. This article conducted a 10-day analysis identifying incorrect technical movements among 50 volleyball players. The experimental results showed that the average recognition accuracy of the traditional human recognition judgment method and the error passing technique in this article was about 72.09% and 95.59%, respectively. The average recognition accuracy of this article’s traditional human identification judgment method and the error pad technique was about 68.11% and 94.33%, respectively. Therefore, sports volleyball error technology action recognition based on visual image technology can improve the accuracy of error technology action recognition.",popularity volleyball increasing athletes must follow correct techniques practical training fair competition however errors frequently occur training matches traditional methods struggle identify accurately adversely affects athletes training effectiveness fairness volleyball matches article presented volleyball error technique action recognition method based visual image technology camera device used obtain visual images volleyball actions support vector machine technology used classify skeletal features athletes volleyball action images identify volleyball error technique actions article conducted day analysis identifying incorrect technical movements among volleyball players experimental results showed average recognition accuracy traditional human recognition judgment method error passing technique article respectively average recognition accuracy article traditional human identification judgment method error pad technique respectively therefore sports volleyball error technology action recognition based visual image technology improve accuracy error technology action recognition
"Throughout the existing literature, the research on the legislative view of artificial intelligence is mainly divided into two modes: policy-driven and market economy-driven; and this paper is based on the legal interpretation theory and analysis method. The above two models of artificial intelligence legislation are analyzed. The policy-driven view of AI legislation is based on the power-based approach, focusing on the national security issues involved in the development of the AI industry, as well as ex post-facto remedies for criminal offenses. The market economy-driven AI legislation model is based on the rights-based approach, focusing on the ex-ante prevention of civil infringement and moral and ethical issues involved in the development of the AI industry. This paper argues that AI legislation should integrate the “hard law path” and the “soft law path,” and comprehensively consider the factors of “legality” and “legitimacy.”",throughout existing literature research legislative view artificial intelligence mainly divided two modes policy driven market economy driven paper based legal interpretation theory analysis method two models artificial intelligence legislation analyzed policy driven view legislation based power based approach focusing national security issues involved development industry well post facto remedies criminal offenses market economy driven legislation model based rights based approach focusing prevention civil infringement moral ethical issues involved development industry paper argues legislation integrate hard law path soft law path comprehensively consider factors legality legitimacy
"In response to teachers’ insufficient knowledge management ability, this study intends to optimize the important parameters of support vector machines using lightning search algorithms to obtain improved algorithms. Research constructs a new evaluation model for teacher knowledge management ability based on improved algorithms. The performance comparison analysis of the improved algorithm proposed in the study showed that the accuracy and area under the PR curve of the algorithm were 93.47% and 0.8, superior to the comparison algorithm. In empirical analysis, using this evaluation model to improve teachers’ knowledge management ability resulted in teacher satisfaction scores of 94 points and student satisfaction scores of 96 points, both better than before the improvement. The above results indicated that the improved algorithm and teacher knowledge management ability evaluation model had good performance. Therefore, this model can be used to improve teachers’ knowledge management ability, thereby promoting the overall quality development of school teachers.",response teachers insufficient knowledge management ability study intends optimize important parameters support vector machines using lightning search algorithms obtain improved algorithms research constructs new evaluation model teacher knowledge management ability based improved algorithms performance comparison analysis improved algorithm proposed study showed accuracy area curve algorithm superior comparison algorithm empirical analysis using evaluation model improve teachers knowledge management ability resulted teacher satisfaction scores points student satisfaction scores points better improvement results indicated improved algorithm teacher knowledge management ability evaluation model good performance therefore model used improve teachers knowledge management ability thereby promoting overall quality development school teachers
"In graphic design, designers often need to consider multiple goals, such as the esthetics of the design, the accuracy of conveying information, and the visual experience of the user. Multi-objective collaborative optimization can help designers find the best balance point between multiple objectives and improve the quality and effectiveness of their designs. Therefore, research proposed a graphic design method that combines user label preference information with multi-objective optimization, which combines user preference information with PSO (particle swarm optimization) algorithm for multi-objective optimization. The findings showed that when the number of training samples was small, the contour coefficients of each model changed significantly, and the clustering effect of the models was unstable. As the amount of training samples increased, the clustering effect of each model tended to stabilize. The contour coefficients based on improved K-means, K-means++, and classical K-means algorithms were 0.743, 0.707, and 0.546, respectively, indicating that the improved K-means algorithm had the best clustering effect in this study. As the number of users increases, the user accuracy of the research method improves to 0.88. The recall rate of the research method is consistently higher than other comparative methods, at approximately 0.93. The proposed methods in graphic design can effectively integrate user preference information, meet the needs of graphic design, and provide reference for designers in graphic design.",graphic design designers often need consider multiple goals esthetics design accuracy conveying information visual experience user multi objective collaborative optimization help designers find best balance point multiple objectives improve quality effectiveness designs therefore research proposed graphic design method combines user label preference information multi objective optimization combines user preference information pso particle swarm optimization algorithm multi objective optimization findings showed number training samples small contour coefficients model changed significantly clustering effect models unstable amount training samples increased clustering effect model tended stabilize contour coefficients based improved means means classical means algorithms respectively indicating improved means algorithm best clustering effect study number users increases user accuracy research method improves recall rate research method consistently higher comparative methods approximately proposed methods graphic design effectively integrate user preference information meet needs graphic design provide reference designers graphic design
"With the development of smart grids, short-term net load prediction for distributed energy is increasingly valued, net load prediction can fully explore the difference between electricity load and renewable energy output, and is the basis for energy management and optimal scheduling. Considering that the correlation between source and charge is not considered in the traditional statistical model, and the prediction accuracy is poor, this paper proposes a short-term net load prediction method for distributed energy based on the Spatio-Temporal Graph Convolution Networks (STGCNs) and Attention mechanism. First, the time series of factors such as wind power, solar power, historical characteristics of actual load, and weather environment in the integrated energy system are mapped into the data form of graph structure, and the correlation between variables is calculated using the Maximum Information Coefficient (MIC). It is used as the weighted value of the connected edges of nodes to construct the adjacency matrix and initialize the graph network. Then, STGCN is used to autonomous feature extraction of the time series. STGCN can consider the uncertainties at both ends of the source and load at the same time, and fully extract the internal relationship between the source and load variables in the distributed energy graph network. Finally, the Attention mechanism is used to strengthen the feature extraction ability of STGCN model for time series data, improving the short-term net load prediction accuracy. The experiment uses the real power data set of an integrated energy system in a region for verification. Compared with other data-driven methods, the fusion model trained and generated has higher prediction accuracy. At the same time, the trained generated model can adapt to different scale data and different prediction lengths in practice, and has strong generalization ability.",development smart grids short term net load prediction distributed energy increasingly valued net load prediction fully explore difference electricity load renewable energy output basis energy management optimal scheduling considering correlation source charge considered traditional statistical model prediction accuracy poor paper proposes short term net load prediction method distributed energy based spatio temporal graph convolution networks stgcns attention mechanism first time series factors wind power solar power historical characteristics actual load weather environment integrated energy system mapped data form graph structure correlation variables calculated using maximum information coefficient mic used weighted value connected edges nodes construct adjacency matrix initialize graph network stgcn used autonomous feature extraction time series stgcn consider uncertainties ends source load time fully extract internal relationship source load variables distributed energy graph network finally attention mechanism used strengthen feature extraction ability stgcn model time series data improving short term net load prediction accuracy experiment uses real power data set integrated energy system region verification compared data driven methods fusion model trained generated higher prediction accuracy time trained generated model adapt different scale data different prediction lengths practice strong generalization ability
"With the development of the economy and society and the deepening of economic globalization, traditional cross-border e-commerce data management methods are no longer able to meet practical needs. In response to this, research is conducted on the title and attribute data of popular cross-border e-commerce products, and natural language processing models (BERT) and deep autoencoder models are improved to construct a multi-dimensional cross-border e-commerce data visualization platform based on deep learning. The specific innovation includes three aspects. In terms of model construction, research is conducted on optimizing the vocabulary extension layer and multi head attention mechanism of BERT, and combining it with a deep autoencoder with classification constraints to construct a multi-dimensional cross-border e-commerce data classification model (MDCCBDL) based on deep neural networks, achieving deep fusion of title and attribute features. In terms of performance validation, the model achieved an average accuracy of 86.3% and an F1 score of 83.7% on six Amazon datasets, which were 6.2% and 4.7% higher than the baseline model with a single data, respectively; On 9 cross-platform datasets including eBay and Wish, the average accuracy was 87.6%, verifying the generalization ability. The recall rate and average accuracy are 84% and 97%, respectively, which are 7.7% and 3.2% higher than those of deep interest networks. In terms of platform applications, the visualization platform achieved an average response time of 0.79 s and an error rate of 0.35% in 1000 concurrent multi-environment tests. The product selection time was shortened by 43.9% compared to traditional methods, and the success rate increased to over 80%. The research results indicate that the model improves classification accuracy and representation ability through multi-dimensional feature fusion, and the platform has time and efficiency advantages in cross-border e-commerce product selection management, providing a practical solution for intelligent management of industry data.",development economy society deepening economic globalization traditional cross border commerce data management methods longer able meet practical needs response research conducted title attribute data popular cross border commerce products natural language processing models bert deep autoencoder models improved construct multi dimensional cross border commerce data visualization platform based deep learning specific innovation includes three aspects terms model construction research conducted optimizing vocabulary extension layer multi head attention mechanism bert combining deep autoencoder classification constraints construct multi dimensional cross border commerce data classification model mdccbdl based deep neural networks achieving deep fusion title attribute features terms performance validation model achieved average accuracy score six amazon datasets higher baseline model single data respectively cross platform datasets including ebay wish average accuracy verifying generalization ability recall rate average accuracy respectively higher deep interest networks terms platform applications visualization platform achieved average response time error rate concurrent multi environment tests product selection time shortened compared traditional methods success rate increased research results indicate model improves classification accuracy representation ability multi dimensional feature fusion platform time efficiency advantages cross border commerce product selection management providing practical solution intelligent management industry data
"Alzheimer’s disease is a degenerative disorder of the central nervous system. In the early stage, its symptoms can be easily confused with those of aging, causing patients to miss the optimal treatment period and exacerbating the condition. This study aims to assist doctors in diagnosing Alzheimer’s disease and predicting the progression of mild cognitive impairment by using graph convolutional neural networks. The CA-GCN model was constructed by building an adjacency matrix based on sample similarity, introducing Chebyshev convolution for hierarchical analysis, and leveraging an adaptive mechanism based on clinical information to flexibly adjust the relationships among samples. The experimental results demonstrate that the CA-GCN model has a stable accuracy in diagnosis (AUC, 0.97) and prediction experiments (AUC, 0.88), outperforming common machine learning algorithms. This model improves diagnostic accuracy and assists clinical decision-making, predict disease progression, and thus treat patients in a timely manner, reducing the burden on families.",alzheimer disease degenerative disorder central nervous system early stage symptoms easily confused aging causing patients miss optimal treatment period exacerbating condition study aims assist doctors diagnosing alzheimer disease predicting progression mild cognitive impairment using graph convolutional neural networks gcn model constructed building adjacency matrix based sample similarity introducing chebyshev convolution hierarchical analysis leveraging adaptive mechanism based clinical information flexibly adjust relationships among samples experimental results demonstrate gcn model stable accuracy diagnosis auc prediction experiments auc outperforming common machine learning algorithms model improves diagnostic accuracy assists clinical decision making predict disease progression thus treat patients timely manner reducing burden families
"Customer churn prediction and analysis on e-commerce platforms are key to e-commerce systems, providing strong support for e-commerce and traffic planning. As the advancement and application of deep learning, e-commerce platform customer churn prediction models with traditional XGBoost algorithms have emerged one after another. However, traditional XGBoost algorithms have many drawbacks and limitations. These problems with slow training speed, accuracy loss, overfitting, and inability to handle large-scale data. Therefore, an improved XGBoost algorithm is introduced to improve accuracy under the influence of data mining and minimal training resources. Experimental results demonstrate that the proposed model achieves an accuracy of 78.7%, a prediction precision of 73.5%, and a recall rate of 58.7%. For core and important-development customer categories, the AUC values reached 86.7% and 86.5%, respectively. Compared to baseline models such as SVM, BP neural networks, and C4.5 decision trees, the improved XGBoost demonstrates superior accuracy and stability. These findings confirm that the predicted values are closer to the true labels and that the method provides more reliable and accurate support for customer segmentation and retention strategy development on e-commerce platforms.",customer churn prediction analysis commerce platforms key commerce systems providing strong support commerce traffic planning advancement application deep learning commerce platform customer churn prediction models traditional xgboost algorithms emerged one another however traditional xgboost algorithms many drawbacks limitations problems slow training speed accuracy loss overfitting inability handle large scale data therefore improved xgboost algorithm introduced improve accuracy influence data mining minimal training resources experimental results demonstrate proposed model achieves accuracy prediction precision recall rate core important development customer categories auc values reached respectively compared baseline models svm neural networks decision trees improved xgboost demonstrates superior accuracy stability findings confirm predicted values closer true labels method provides reliable accurate support customer segmentation retention strategy development commerce platforms
"With the development and maturation of artificial intelligence (AI) technology, AI service contexts have become a new scenario in the service industry. This study explores the impact of three types of customer behaviors on front-line employee service silence in the context of AI service contact compatibility in China’s service industry. It also examines the mediating role of employee emotional reactions and the moderating effect of employee AI perception. The study empirically tests these relationships using data from 657 samples in China’s hotel service industry. The results show that customer participation and positive feedback behaviors have a significant negative impact on employee service silence, with employee emotional experiences mediating the relationship between customer behaviors and service silence.",development maturation artificial intelligence technology service contexts become new scenario service industry study explores impact three types customer behaviors front line employee service silence context service contact compatibility china service industry also examines mediating role employee emotional reactions moderating effect employee perception study empirically tests relationships using data samples china hotel service industry results show customer participation positive feedback behaviors significant negative impact employee service silence employee emotional experiences mediating relationship customer behaviors service silence
"Riga plates serve as electromagnetic actuators originally designed to minimize hydrodynamic drag and pressure. Their utility has since broadened to drag reduction in submarines, micro-coolers, biomedical flow control, and thermal reactors. This study computationally simulates and statistically analyzes the energy transfer efficiency of an electro-magnetohydrodynamic (EMHD) Casson trihybrid nanofluid flowing past a vertical radiant Riga plate in scenarios involving natural and forced convection, considering the effects of viscous dissipation, suction, and the nanomaterials’ shape factor. The governing model is solved using a hybrid spectral technique, with numerical results validated against benchmark data. According to the results of the current work, suction augmentation diminishes the tri-hybrid Casson nanofluid’s velocity and drag forces. Elevating the values of mixed convection or Casson factors reduces the Casson tri-hybrid nanofluid’s temperature and enhances its energy transfer. The incorporation of non-spherical nanoparticles reduces the Nusselt number by 0.23%–5.3% and increases skin friction by 1.6%–14% in comparison to using spherical nanoparticles. Multiple regression indicates that thermal radiation is the most positive contributor in boosting energy transfer rates. These insights may advance predictive capabilities for thermal management in energy systems and guide next-generation EMHD reactor design.",riga plates serve electromagnetic actuators originally designed minimize hydrodynamic drag pressure utility since broadened drag reduction submarines micro coolers biomedical flow control thermal reactors study computationally simulates statistically analyzes energy transfer efficiency electro magnetohydrodynamic emhd casson trihybrid nanofluid flowing past vertical radiant riga plate scenarios involving natural forced convection considering effects viscous dissipation suction nanomaterials shape factor governing model solved using hybrid spectral technique numerical results validated benchmark data according results current work suction augmentation diminishes tri hybrid casson nanofluid velocity drag forces elevating values mixed convection casson factors reduces casson tri hybrid nanofluid temperature enhances energy transfer incorporation non spherical nanoparticles reduces nusselt number increases skin friction comparison using spherical nanoparticles multiple regression indicates thermal radiation positive contributor boosting energy transfer rates insights may advance predictive capabilities thermal management energy systems guide next generation emhd reactor design
"With the impact of the “Internet+” wave, the “fourth Industrial revolution” and the “epidemic era,” the traditional service model has been radically changed. Green sustainable development has been extended to the service field and new technologies, new models, and new business forms of green service have been brought into being. Based on the original data of 213 service-oriented corporates, this paper adopts the questionnaire survey method to explore the mechanism between green service innovation carried out by service-oriented enterprises and consumer happiness based on the perspectives of consumers’ “value identity” and “extra-role altruistic behavior.” This study aims to solve the problem of how green service innovation provided by corporates translates into consumer happiness. The findings are as follows: Value identity has a mediating effect between corporate green service innovation and consumer extra-role altruistic behavior (beneficial to corporates and environment), and consumer extra-role altruistic behavior has a serial mediating effect between corporate green service innovation and consumer happiness. Resource heterogeneity positively moderates the relationship between corporate green service innovation and consumer value identity. Resource coupling also positively moderates the relationship between consumers’ value identity and their extra-role altruistic behavior. The conclusion extends the research boundary of enterprise green service innovation and adds a new perspective and context. Moreover, the research results also provide reference for the service innovation transformation of related corporates.",impact internet wave fourth industrial revolution epidemic traditional service model radically changed green sustainable development extended service field new technologies new models new business forms green service brought based original data service oriented corporates paper adopts questionnaire survey method explore mechanism green service innovation carried service oriented enterprises consumer happiness based perspectives consumers value identity extra role altruistic behavior study aims solve problem green service innovation provided corporates translates consumer happiness findings follows value identity mediating effect corporate green service innovation consumer extra role altruistic behavior beneficial corporates environment consumer extra role altruistic behavior serial mediating effect corporate green service innovation consumer happiness resource heterogeneity positively moderates relationship corporate green service innovation consumer value identity resource coupling also positively moderates relationship consumers value identity extra role altruistic behavior conclusion extends research boundary enterprise green service innovation adds new perspective context moreover research results also provide reference service innovation transformation related corporates
"The uncertainty of power load is one of the important research directions in demand response uncertainty. Accurate and effective power system load forecasting is an important prerequisite for ensuring the safety, stable operation, and normal production of the power grid. To improve the accuracy of short-term load forecasting in power systems under demand response scenarios, this paper proposes a Transformer load forecasting method that considers demand response potential. Firstly, the change law of response uncertainty with electricity price difference and consumer psychology principles are used to quantify the power demand response results under different probability conditions. Then, Transformer neural networks are used to extract features from user historical load, temperature, electricity price, and other time series data. Finally, a multi-head self-attention mechanism is used to pay attention to the structural relationship between time series data, analyze the importance of input variables at each historical moment on the current load, and achieve high-precision prediction of user load and demand response potential. This article takes industrial users as an example to predict the power load and demand response regulation power of the general component manufacturing industry. Through comparative analysis with actual data, the effectiveness of the proposed method is verified. Compared with other existing methods, the Transformer model that considers demand response performs well in power load forecasting, providing a certain theoretical basis for evaluating the potential of demand response. The subsequent work will study the characteristics of electric, hot, and cold loads and their coupling relationships under the difference of electricity prices, and improve the forecasting performance of user loads and Demand Response Regulation power, so as to reduce the power generation and operation costs of the grid.",uncertainty power load one important research directions demand response uncertainty accurate effective power system load forecasting important prerequisite ensuring safety stable operation normal production power grid improve accuracy short term load forecasting power systems demand response scenarios paper proposes transformer load forecasting method considers demand response potential firstly change law response uncertainty electricity price difference consumer psychology principles used quantify power demand response results different probability conditions transformer neural networks used extract features user historical load temperature electricity price time series data finally multi head self attention mechanism used pay attention structural relationship time series data analyze importance input variables historical moment current load achieve high precision prediction user load demand response potential article takes industrial users example predict power load demand response regulation power general component manufacturing industry comparative analysis actual data effectiveness proposed method verified compared existing methods transformer model considers demand response performs well power load forecasting providing certain theoretical basis evaluating potential demand response subsequent work study characteristics electric hot cold loads coupling relationships difference electricity prices improve forecasting performance user loads demand response regulation power reduce power generation operation costs grid
"Volunteering is one of the quality education contents for college students. Studying the varied motivations of college students’ volunteering promotes the implementation of motivation education of college students’ volunteering, and exploring the influencing factors and relationships of college students’ volunteering behavior helps formulate appropriate systems and policies to promote college students’ volunteering activities. The motivations of volunteering behavior were analyzed using Exploratory Factor Analysis (EFA) to understand their classification and role, and the influencing factors were analyzed using Structural Equation Modeling (SEM) to study the occurrence path of volunteering behavior. The results of the study showed that utilitarian motivation has little influence on volunteering behavior, and volunteering ability and environment have a mediation effect on volunteering behavior, and volunteering environment has a significant moderation effect on volunteering ability and motivation.",volunteering one quality education contents college students studying varied motivations college students volunteering promotes implementation motivation education college students volunteering exploring influencing factors relationships college students volunteering behavior helps formulate appropriate systems policies promote college students volunteering activities motivations volunteering behavior analyzed using exploratory factor analysis efa understand classification role influencing factors analyzed using structural equation modeling sem study occurrence path volunteering behavior results study showed utilitarian motivation little influence volunteering behavior volunteering ability environment mediation effect volunteering behavior volunteering environment significant moderation effect volunteering ability motivation
"Taking data from eight sub-industries of equipment manufacturing industry in Shaanxi province from 2012 to 2020 as samples, an evaluation index system of science and technology innovation capability in Shaanxi equipment manufacturing industry is constructed, to evaluate the innovation capability of each sub-industry by principal component analysis method and entropy method, and analyze their dynamic evolution trends combined with kernel density. The results show that the innovation capability of railway, shipbuilding, aerospace, and other transportation equipment manufacturing industries is far ahead of other seven sub-industries in Shaanxi. The contribution of the two indexes, including R&amp;D expenditure and Proportion of sales revenue from new products, is higher than other indexes, and the gap in the capability is expanding over time. Therefore, intelligent transformation in Shaanxi equipment manufacturing industry is an inevitable choice. It is necessary to strengthen investment in new equipment and technologies, focus on research and development of new products, effectively expand channels for obtaining funds, and give full of the driving effect of dominant industries to enhance the innovation capability in Shaanxi equipment manufacturing industry.",taking data eight sub industries equipment manufacturing industry shaanxi province samples evaluation index system science technology innovation capability shaanxi equipment manufacturing industry constructed evaluate innovation capability sub industry principal component analysis method entropy method analyze dynamic evolution trends combined kernel density results show innovation capability railway shipbuilding aerospace transportation equipment manufacturing industries far ahead seven sub industries shaanxi contribution two indexes including amp expenditure proportion sales revenue new products higher indexes gap capability expanding time therefore intelligent transformation shaanxi equipment manufacturing industry inevitable choice necessary strengthen investment new equipment technologies focus research development new products effectively expand channels obtaining funds give full driving effect dominant industries enhance innovation capability shaanxi equipment manufacturing industry
"Chronic kidney disease (CKD)-associated anemia, driven by erythropoietin (EPO) deficiency and iron dysregulation, significantly impacts patient survival and quality of life. Conventional therapies like erythropoiesis-stimulating agents (ESAs) face limitations including injection dependency, cardiovascular risks, and iron intolerance. Vadadustat, an oral hypoxia-inducible factor prolyl hydroxylase inhibitor (HIF-PHI), offers a novel mechanism by stabilizing HIF-α subunits to enhance endogenous EPO production, improve iron utilization, and promote erythropoiesis. Pharmacokinetically, Vadadustat exhibits rapid absorption (peak: 2–3 hours), dose-proportional exposure, minimal accumulation, and 99.5% plasma protein binding, with adjustments recommended in dialysis-dependent CKD due to reduced clearance. Phase III clinical trials demonstrate non-inferiority of Vadadustat versus Darbepoetin Alfa in maintaining target hemoglobin levels across dialysis-dependent (DD-CKD) and non-dialysis-dependent (NDD-CKD) populations. However, while Vadadustat met cardiovascular safety endpoints in DD-CKD (INNO2VATE trials), it showed a higher major adverse cardiovascular event (MACE) risk in NDD-CKD (HR: 1.17; 95% CI: 1.01–1.36). Common adverse events include hypertension (14%), gastrointestinal disturbances (13%), and thrombosis. Drug interactions with oral iron, OAT3 inhibitors (e.g., allopurinol), and BCRP substrates require monitoring. Approved for adults with dialysis-dependent CKD (≥3 months duration), Vadadustat provides an effective oral alternative to ESAs, with potential advantages in iron mobilization and flexible thrice-weekly dosing. Future studies should address long-term cardiovascular safety in non-dialysis populations.",chronic kidney disease ckd associated anemia driven erythropoietin epo deficiency iron dysregulation significantly impacts patient survival quality life conventional therapies like erythropoiesis stimulating agents face limitations including injection dependency cardiovascular risks iron intolerance vadadustat oral hypoxia inducible factor prolyl hydroxylase inhibitor hif phi offers novel mechanism stabilizing hif subunits enhance endogenous epo production improve iron utilization promote erythropoiesis pharmacokinetically vadadustat exhibits rapid absorption peak hours dose proportional exposure minimal accumulation plasma protein binding adjustments recommended dialysis dependent ckd due reduced clearance phase iii clinical trials demonstrate non inferiority vadadustat versus darbepoetin alfa maintaining target hemoglobin levels across dialysis dependent ckd non dialysis dependent ndd ckd populations however vadadustat met cardiovascular safety endpoints ckd inno vate trials showed higher major adverse cardiovascular event mace risk ndd ckd common adverse events include hypertension gastrointestinal disturbances thrombosis drug interactions oral iron oat inhibitors allopurinol bcrp substrates require monitoring approved adults dialysis dependent ckd months duration vadadustat provides effective oral alternative potential advantages iron mobilization flexible thrice weekly dosing future studies address long term cardiovascular safety non dialysis populations
"The coupled lateral-torsional vibration of thin-walled beams under moving harmonic loads holds critical engineering relevance. For example, vehicles traversing uneven roads induce vertical oscillations that translate into harmonic excitations on thin-walled beams. Based on Vlasov’s thin-walled beam theory, this study establishes closed-form time-domain solutions for the lateral-torsional displacements of mono-symmetric thin-walled beams by transforming the coupled partial differential equations into ordinary differential equations via modal expansion, and solving them through Laplace transform and its inverse. A finite element solution program based on the Newmark-β algorithm is created for validation purposes, with verification results demonstrating close alignment between analytical and numerical solutions. A key innovation involves representing a single vehicle load as two eccentric lateral forces in parameter analysis, enhancing the accuracy of simulating vehicle-induced dynamics. Furthermore, parametric studies quantitatively characterize the effects of critical control parameters (excitation frequency, moving speed, span length, load eccentricity, and warping stiffness) on lateral-torsional coupling displacements. Key conclusions are that torsional displacement is more velocity-sensitive; warping suppresses peak amplitudes. These findings provide a profound foundation for designing thin-walled bridges to avoid resonance.",coupled lateral torsional vibration thin walled beams moving harmonic loads holds critical engineering relevance example vehicles traversing uneven roads induce vertical oscillations translate harmonic excitations thin walled beams based vlasov thin walled beam theory study establishes closed form time domain solutions lateral torsional displacements mono symmetric thin walled beams transforming coupled partial differential equations ordinary differential equations via modal expansion solving laplace transform inverse finite element solution program based newmark algorithm created validation purposes verification results demonstrating close alignment analytical numerical solutions key innovation involves representing single vehicle load two eccentric lateral forces parameter analysis enhancing accuracy simulating vehicle induced dynamics furthermore parametric studies quantitatively characterize effects critical control parameters excitation frequency moving speed span length load eccentricity warping stiffness lateral torsional coupling displacements key conclusions torsional displacement velocity sensitive warping suppresses peak amplitudes findings provide profound foundation designing thin walled bridges avoid resonance
"With the acceleration of urbanization and the rise of cultural industries, the planning and advancement of urban cultural industry has become an important research object. To study the advancement of urban cultural industry planning, this study proposes a comprehensive fuzzy algorithm research system. The methodology dynamically optimizes indicator weights through a fusion of static weights (determined via Analytic Hierarchy Process) and dynamic weights (derived from Euclidean distance-based similarity normalization). A multi-dimensional digital indicator system spanning digital factor investment, innovation environment, and industry competition is constructed, with competitiveness quantitatively assessed using entropy weighting. FP-Growth further mines implicit association rules among indicators to enhance correlation analysis. Experiments show that when the number of people involved is 400 thousand, the calculation time of the research method is 330 s, which is significantly improved compared with the performance of the comparison method which exceeds 900 s. In the runtime data volume test, the retained data volume generated by the research method in 500 s was only approximately 130M, which was much less than 310M and 220M of the comparison methods. During the data processing, the data error rate in the retained data tends to be stable and fluctuates between 2% and 3%. The research methods have obtained accurate and effective analysis results on the promotion of urban cultural industry planning in both types of countries. This indicates that the research method has good performance and practical application effects and can provide effective references for urban cultural industry planning.",acceleration urbanization rise cultural industries planning advancement urban cultural industry become important research object study advancement urban cultural industry planning study proposes comprehensive fuzzy algorithm research system methodology dynamically optimizes indicator weights fusion static weights determined via analytic hierarchy process dynamic weights derived euclidean distance based similarity normalization multi dimensional digital indicator system spanning digital factor investment innovation environment industry competition constructed competitiveness quantitatively assessed using entropy weighting growth mines implicit association rules among indicators enhance correlation analysis experiments show number people involved thousand calculation time research method significantly improved compared performance comparison method exceeds runtime data volume test retained data volume generated research method approximately much less comparison methods data processing data error rate retained data tends stable fluctuates research methods obtained accurate effective analysis results promotion urban cultural industry planning types countries indicates research method good performance practical application effects provide effective references urban cultural industry planning
"With the progress of technology and the development of the times, the competition among enterprises has become more and more intense, and various risks affect the development of enterprises. To improve enterprise risk assessment accuracy, the research proposes a model based on improved neural network, using the improved Latent Dirichlet Allocation (Teo-LDA) model to classify and analyze customer reviews, and combining the improved fuzzy neural network to fuzzify and analyze customer attributes and features. The experimental data showed that the accuracy of the improved Teo-LDA-based customer review analysis method was up to 0.85 and the accuracy was up to 0.82. The average accuracy of the improved fuzzy neural network model for risk assessment was 94.4%, which was 15.8% more accurate than the Back Propagation (BP) neural network model, and 5.1% more accurate than the Genetic Algorithm (GA) neural network. The results indicate that trained by adaptive hybrid method, the improved fuzzy neural network model is the most effective in achieving accurate assessment of customer churn risk in enterprises.",progress technology development times competition among enterprises become intense various risks affect development enterprises improve enterprise risk assessment accuracy research proposes model based improved neural network using improved latent dirichlet allocation teo lda model classify analyze customer reviews combining improved fuzzy neural network fuzzify analyze customer attributes features experimental data showed accuracy improved teo lda based customer review analysis method accuracy average accuracy improved fuzzy neural network model risk assessment accurate back propagation neural network model accurate genetic algorithm neural network results indicate trained adaptive hybrid method improved fuzzy neural network model effective achieving accurate assessment customer churn risk enterprises
"China’s economy has developed rapidly, but its ecological civilization has been seriously affected. Traditional theoretical e-commerce construction cannot improve the efficiency of e-commerce construction. Therefore, it is of great practical significance to conduct in-depth research on countermeasures for e-commerce construction from the perspective of artificial intelligence algorithms. This article analyzes the problems in the construction of EC (which may refer to related systems such as e-commerce), and uses BP neural network to train the input and output layers, which can quickly identify environmental hazards, detect ecological hazards, and improve the quality of EC; Also utilizing it to identify and classify garbage, incorporating the network into input data learning and outputting categories; Implement non-linear mapping of input and output through BP neural network, transmit signals and predict future natural disasters. In addition, in addition to technical analysis, other countermeasures are proposed for EC construction issues in order to provide inspiration for e-commerce construction. After analyzing the BP neural network, this article conducts experimental comparisons on the recognition and classification accuracy of different types of garbage in e-commerce. The results showed that the BP neural network had recognition accuracy and classification accuracy of over 80% for recyclable waste, kitchen waste, and hazardous waste, with specific rates of 81.21%, 85.00%, 83.70%, and 81.51%, 83.29%, and 87.34%, respectively. The data proves that BP neural network is powerful in garbage classification and recognition, contributing to the construction of EC.",china economy developed rapidly ecological civilization seriously affected traditional theoretical commerce construction improve efficiency commerce construction therefore great practical significance conduct depth research countermeasures commerce construction perspective artificial intelligence algorithms article analyzes problems construction may refer related systems commerce uses neural network train input output layers quickly identify environmental hazards detect ecological hazards improve quality also utilizing identify classify garbage incorporating network input data learning outputting categories implement non linear mapping input output neural network transmit signals predict future natural disasters addition addition technical analysis countermeasures proposed construction issues order provide inspiration commerce construction analyzing neural network article conducts experimental comparisons recognition classification accuracy different types garbage commerce results showed neural network recognition accuracy classification accuracy recyclable waste kitchen waste hazardous waste specific rates respectively data proves neural network powerful garbage classification recognition contributing construction
"The purpose of this study is to provide an overview of new ways to decision-making in English language schools that make use of information technology (IT) solutions. Learning management systems (LMSs) that are housed on the cloud, mobile applications for tailored language learning, chatbots driven by artificial intelligence for language practice, virtual reality technologies for language learning, and data analytics tools for tracking student progress are the five distinct possibilities that are presented at this time. On the basis of eight criteria, each alternative is given careful examination. These criteria are as follows: cost-effectiveness, flexibility, customization, security, integration, and scaling. A crucial component of the decision-making process is the CRITIC-AROMAN method, which functions inside a Fermatean fuzzy (FF) environment. Through an analysis of the relationships between the various criteria, CRITIC guarantees that a comprehensive assessment of the relevance of each criterion is carried out by accurately balancing them. Then, in order to measure the effectiveness of the alternatives, AROMAN employs ratio evaluations and ranks them according to how well they performed in comparison to these criteria. However, the FF environment offers a flexible framework for dealing with imprecise data, despite the fact that the decision-making process is riddled with ambiguity. In the end, these techniques make it possible to conduct an organized and comprehensive evaluation of the many IT-driven options that may be implemented in English as Second Language (ESL) classrooms. This, in turn, makes it possible to make informed decisions that are in accordance with the objectives and priorities of the institution.",purpose study provide overview new ways decision making english language schools make use information technology solutions learning management systems lmss housed cloud mobile applications tailored language learning chatbots driven artificial intelligence language practice virtual reality technologies language learning data analytics tools tracking student progress five distinct possibilities presented time basis eight criteria alternative given careful examination criteria follows cost effectiveness flexibility customization security integration scaling crucial component decision making process critic aroman method functions inside fermatean fuzzy environment analysis relationships various criteria critic guarantees comprehensive assessment relevance criterion carried accurately balancing order measure effectiveness alternatives aroman employs ratio evaluations ranks according well performed comparison criteria however environment offers flexible framework dealing imprecise data despite fact decision making process riddled ambiguity end techniques make possible conduct organized comprehensive evaluation many driven options may implemented english second language esl classrooms turn makes possible make informed decisions accordance objectives priorities institution
"To address challenges such as insufficient real-time performance, balancing detection accuracy with model size, and interference from complex lighting and occlusion in orchard environments, this study proposes a lightweight and efficient apple detection model, YOLO-EAD (efficient apple detection). The model enhances YOLOv8 by replacing its backbone with the EfficientViT-T network to reduce computational complexity, introducing a self-attention-based detection head (SA-detect) to streamline detection branches, and integrating a coordinate attention (CA) mechanism in the Neck layer to improve feature focus. Additionally, the SIoU loss function is adopted for more precise bounding box regression. These enhancements collectively reduce model size to 4.1 MB and computational complexity to 9.3 GFlops while achieving a high mAP@0.5 of 96.7%. Compared to the original YOLOv8s, this model reduces complexity by 67.5% with improved detection accuracy and robustness under varied lighting and occlusion conditions, making it suitable for real-time applications on edge devices in agricultural robotics.",address challenges insufficient real time performance balancing detection accuracy model size interference complex lighting occlusion orchard environments study proposes lightweight efficient apple detection model yolo ead efficient apple detection model enhances yolov replacing backbone efficientvit network reduce computational complexity introducing self attention based detection head detect streamline detection branches integrating coordinate attention mechanism neck layer improve feature focus additionally siou loss function adopted precise bounding box regression enhancements collectively reduce model size computational complexity gflops achieving high map compared original yolov model reduces complexity improved detection accuracy robustness varied lighting occlusion conditions making suitable real time applications edge devices agricultural robotics
"The purpose of this study is to explore the impact of classroom interactive Chinese teaching mode based on IoT technology on university students’ language learning. The study adopted the methods of questionnaire survey and experimental research, and selected two grades of Chinese teaching classes of a university in a city as the research object, totaling 120 college students, divided into experimental group and control group, and conducted a 6-month experimental teaching. The experimental group adopted the classroom interactive Chinese teaching mode based on IoT technology, and the control group adopted the traditional Chinese teaching mode. Before and after the experiment, the college students in the two groups were tested on their comprehensive language ability and questionnaire survey on their satisfaction with language learning, and at the same time, the college students in the experimental group were interviewed to collect their evaluations and feedbacks on the teaching mode. The experimental results show that the comprehensive language ability and language learning satisfaction of the college students in the experimental group are significantly higher than those in the control group, and both of them are significantly improved during the experimental period, which indicates that the teaching mode has a positive impact on college students' language learning. The results of the questionnaires and interviews also showed that the college students had a positive attitude toward the teaching mode, believing that it could make the language classroom more vivid and interesting, increase the interaction between teachers and students as well as among students, and improve the efficiency and quality of language learning. This study provides theoretical guidance and practical reference for the reform and innovation of interactive Chinese teaching, as well as an outlook for further expanding the scope and extending the duration of the study.",purpose study explore impact classroom interactive chinese teaching mode based iot technology university students language learning study adopted methods questionnaire survey experimental research selected two grades chinese teaching classes university city research object totaling college students divided experimental group control group conducted month experimental teaching experimental group adopted classroom interactive chinese teaching mode based iot technology control group adopted traditional chinese teaching mode experiment college students two groups tested comprehensive language ability questionnaire survey satisfaction language learning time college students experimental group interviewed collect evaluations feedbacks teaching mode experimental results show comprehensive language ability language learning satisfaction college students experimental group significantly higher control group significantly improved experimental period indicates teaching mode positive impact college students language learning results questionnaires interviews also showed college students positive attitude toward teaching mode believing could make language classroom vivid interesting increase interaction teachers students well among students improve efficiency quality language learning study provides theoretical guidance practical reference reform innovation interactive chinese teaching well outlook expanding scope extending duration study
"In the Internet of Things environment, the energy consumption of trust transmission has a significant impact on ensuring the stable operation of the network. To effectively manage and control energy consumption, this study proposes a study on intelligent perception of energy consumption through trust transmission among Internet of Things nodes. This study intends to effectively control the energy consumption of Internet of Things nodes and provide new solutions for energy management in the Internet of Things. Firstly, it conducts research on the intelligent perception trust model for Internet of Things nodes to ensure accurate calculation and evaluation of node trust, and to adapt to changes in the Internet of Things environment. Secondly, a trust transmission energy consumption intelligent perception model based on multi-agent nodes in the Internet of Things is constructed. To improve the efficiency and accuracy of trust transmission and control energy consumption, the concept of multi-agent is introduced and combined with energy consumption intelligent perception technology. The experimental results show that the energy consumption of the model decreases from the initial 0.56 to 0.24 with the change in the number of recombinations, a decrease of approximately 57%. As the number of cycles changes, it decreases from the initial 0.51 to 0.22, a decrease of approximately 57%. It is evident that it has significant advantages in energy consumption control. It can be seen that this study explores the differences in energy consumption control among different trust models, providing an important reference for subsequent model selection and optimization.",internet things environment energy consumption trust transmission significant impact ensuring stable operation network effectively manage control energy consumption study proposes study intelligent perception energy consumption trust transmission among internet things nodes study intends effectively control energy consumption internet things nodes provide new solutions energy management internet things firstly conducts research intelligent perception trust model internet things nodes ensure accurate calculation evaluation node trust adapt changes internet things environment secondly trust transmission energy consumption intelligent perception model based multi agent nodes internet things constructed improve efficiency accuracy trust transmission control energy consumption concept multi agent introduced combined energy consumption intelligent perception technology experimental results show energy consumption model decreases initial change number recombinations decrease approximately number cycles changes decreases initial decrease approximately evident significant advantages energy consumption control seen study explores differences energy consumption control among different trust models providing important reference subsequent model selection optimization
"The significance of this research lies in the application of data mining techniques to analyze badminton competitions, allowing for the identification of players’ tactics, strategies, and weaknesses. By uncovering these insights, targeted training and improvement plans can be developed, enhancing player performance and optimizing competitive outcomes. Association rules in data mining techniques are used, and the Apriori algorithm in association rules is analyzed and improved. The ACARMI algorithm (an improved algorithm of constrained association rule mining based on item) is then applied. Then, data from six top badminton competitions is collected and converted into Boolean data suitable for this algorithm. A data mining module is designed to mine the tactical association rules of all players, and a specific analysis is conducted on one of the players, successfully analyzing their tactical strategies. Finally, to verify the effectiveness of the algorithm proposed in this article, FP-Growth algorithm and DHP algorithm are applied to comparative experiments. The experiment shows that compared with the FP-Growth algorithm, the initial and final running times of the algorithm in this article with minimum support and different constraint numbers are around “355 ms and 75 ms,” “150 ms and 45 ms,” respectively; similarly, compared with the DHP algorithm, they are around “165 ms and 35 ms,” “175 ms and 55 ms,” respectively. Under two sets of experiments, the algorithm proposed in this article presents better operational efficiency. This article demonstrates that the use of data mining techniques can provide effective tactical optimization, opponent research, real-time decision support, and long-term development planning assistance for athletes and coaches.",significance research lies application data mining techniques analyze badminton competitions allowing identification players tactics strategies weaknesses uncovering insights targeted training improvement plans developed enhancing player performance optimizing competitive outcomes association rules data mining techniques used apriori algorithm association rules analyzed improved acarmi algorithm improved algorithm constrained association rule mining based item applied data six top badminton competitions collected converted boolean data suitable algorithm data mining module designed mine tactical association rules players specific analysis conducted one players successfully analyzing tactical strategies finally verify effectiveness algorithm proposed article growth algorithm dhp algorithm applied comparative experiments experiment shows compared growth algorithm initial final running times algorithm article minimum support different constraint numbers around respectively similarly compared dhp algorithm around respectively two sets experiments algorithm proposed article presents better operational efficiency article demonstrates use data mining techniques provide effective tactical optimization opponent research real time decision support long term development planning assistance athletes coaches
"Pulse repetition interval (PRI) selection significantly impacts the performance of modern radar systems. Although fixed PRI radar systems have made significant advancements, they face inherent challenges such as vulnerability to interference, deception, and high sidelobe levels. To mitigate these issues, random PRI radar systems have been developed, offering improved multitarget detection and enhanced anti-jamming capabilities. However, random PRI introduces complexity in sidelobe suppression, which is critical for maintaining radar system performance. This paper reviews sidelobe suppression techniques in random PRI radar, focusing on the Cyclic Algorithm-New (CAN) and various window functions, including Hanning, Hamming, Kaiser, and Chebyshev windows. By leveraging the CAN algorithm’s unique capability to optimize phase in the frequency domain, this study transfers the traditional trade-off between main lobe width and sidelobe suppression to edge effects. The results demonstrate that applying time-domain windowing to the time-domain sequence (which was optimized in the frequency domain via the CAN algorithm) significantly improves sidelobe suppression, with the Chebyshev window showing superior performance in high-precision scenarios. This work provides a solid theoretical and practical foundation for enhancing radar system performance in complex electromagnetic environments.",pulse repetition interval pri selection significantly impacts performance modern radar systems although fixed pri radar systems made significant advancements face inherent challenges vulnerability interference deception high sidelobe levels mitigate issues random pri radar systems developed offering improved multitarget detection enhanced anti jamming capabilities however random pri introduces complexity sidelobe suppression critical maintaining radar system performance paper reviews sidelobe suppression techniques random pri radar focusing cyclic algorithm new various window functions including hanning hamming kaiser chebyshev windows leveraging algorithm unique capability optimize phase frequency domain study transfers traditional trade main lobe width sidelobe suppression edge effects results demonstrate applying time domain windowing time domain sequence optimized frequency domain via algorithm significantly improves sidelobe suppression chebyshev window showing superior performance high precision scenarios work provides solid theoretical practical foundation enhancing radar system performance complex electromagnetic environments
"In the context of Open innovation, it has become an important way for manufacturing enterprises to use knowledge network embedding (KNE) to achieve intelligent transformation (IT). This study combines the theory of absorptive capacity (AC) and constructs a chain mediation model of “KNE-PAC (potential absorptive capacity)-AAC (actual absorptive capacity)-IT” based on the logical chain of “resources-capabilities-results” to explore the internal mechanism of KNE. 208 manufacturing enterprises are used as research samples. The empirical results based on hierarchical regression analysis and Bootstrap method indicate that: (1) KNE positively affects IT; (2) PAC plays a complete mediating role between KNE and IT; (3) AAC does not have a mediating effect between KNE and IT; (4) PAC and AAC play a chain mediating role of KNE on IT. This study expands the research on the impact of KNE on IT, promotes the theoretical exploration of AC on IT, and provides practical reference for manufacturing enterprises to achieve IT in the era of open innovation.",context open innovation become important way manufacturing enterprises use knowledge network embedding kne achieve intelligent transformation study combines theory absorptive capacity constructs chain mediation model kne pac potential absorptive capacity aac actual absorptive capacity based logical chain resources capabilities results explore internal mechanism kne manufacturing enterprises used research samples empirical results based hierarchical regression analysis bootstrap method indicate kne positively affects pac plays complete mediating role kne aac mediating effect kne pac aac play chain mediating role kne study expands research impact kne promotes theoretical exploration provides practical reference manufacturing enterprises achieve open innovation
"While it has become somewhat commonplace to lament the lack of attention eschatology has received in the modern era, the interest this area of Christian doctrine does receive often seems to be in service of present existential and ethical concerns. This article lays the foundation for a theology of the resurrection of the body. In it, I argue that theological reflection on the resurrection of the body must be grounded in distinctively theological premises that are integrated within the overarching fabric of Christian doctrine. Building on the work of the late John Webster, this author argues that dogmatic reflection on the resurrection of the body must focus on it as a species of the self-communicative action of the Triune God ad extra and the effects of that action wherein God works to secure a people of his own possession who might love and worship him.",become somewhat commonplace lament lack attention eschatology received modern interest area christian doctrine receive often seems service present existential ethical concerns article lays foundation theology resurrection body argue theological reflection resurrection body must grounded distinctively theological premises integrated within overarching fabric christian doctrine building work late john webster author argues dogmatic reflection resurrection body must focus species self communicative action triune god extra effects action wherein god works secure people possession might love worship
"With the national protection of intangible cultural heritage increasing, the inheritance and innovation of music culture has become a hot topic. Music education in colleges and universities must ensure the inheritance and innovation of music. However, there is a lack of music culture inheritance and innovation model based on the Internet of Things. Therefore, based on the IoT metal system, this paper describes the theoretical framework and specific realization steps, etc. of the model. The construction of the model requires the deployment of infrastructure, the construction of a cultural library of music resources and the development of a basic application model. This paper cooperated with a university to conduct a semester-long experimental test work. The results show that the model constructed in this paper can improve students’ music learning motivation and music classroom participation, as well as music innovation consciousness and music professionalism compared with the traditional model.",national protection intangible cultural heritage increasing inheritance innovation music culture become hot topic music education colleges universities must ensure inheritance innovation music however lack music culture inheritance innovation model based internet things therefore based iot metal system paper describes theoretical framework specific realization steps etc model construction model requires deployment infrastructure construction cultural library music resources development basic application model paper cooperated university conduct semester long experimental test work results show model constructed paper improve students music learning motivation music classroom participation well music innovation consciousness music professionalism compared traditional model
"This study explores the intelligent design method of customized furniture based on modular theory and artificial intelligence technology, aiming to improve design efficiency and customer satisfaction. The research focuses on how to optimize the customized furniture design process through modular design and artificial intelligence technology, achieving a balance between personalization and cost-effectiveness. The research methods involve building modular design processes, utilizing artificial intelligence to assist in module selection and combination optimization, and applying augmented reality technology in design. The results showed that the proposed model performed the best on the training set, with an accuracy of 92%, a precision of 88%, a recall of 90%, an F1 score of 89%, an area under the curve ratio of 95%, and a logarithmic loss of 0.21. The corresponding values on the validation set decreased to 89%, 85%, 86%, 85%, and 93%, while the logarithmic loss increased to 0.28. In addition, in the performance comparison, the structural similarity index value of the model increased from 0.350 to 0.912, and the peak signal-to-noise ratio increased from 13.542 decibels to 22.759 decibels, demonstrating good performance. These evaluation metrics (accuracy, precision, recall, and F1 score) were used to assess the classification performance of the AI model in selecting optimal module combinations and generating rendered design schemes that best match user preferences. The proposed approach, named Modular Customized Furniture Intelligent Design System (MoCFIDS), integrates modular theory with AI-assisted module optimization and augmented reality for design validation. Comparative experiments with state-of-the-art models such as CycleGAN and MUNIT showed that MoCFIDS significantly outperformed them in terms of SSIM, PSNR, and computational efficiency, highlighting its superior capability in producing high-quality, user-preferred furniture designs. The research significance lies in the fact that the modular intelligent design method for customized furniture has improved the efficiency and quality of customized furniture design, delivering more personalized and cost-effective design outcomes.",study explores intelligent design method customized furniture based modular theory artificial intelligence technology aiming improve design efficiency customer satisfaction research focuses optimize customized furniture design process modular design artificial intelligence technology achieving balance personalization cost effectiveness research methods involve building modular design processes utilizing artificial intelligence assist module selection combination optimization applying augmented reality technology design results showed proposed model performed best training set accuracy precision recall score area curve ratio logarithmic loss corresponding values validation set decreased logarithmic loss increased addition performance comparison structural similarity index value model increased peak signal noise ratio increased decibels decibels demonstrating good performance evaluation metrics accuracy precision recall score used assess classification performance model selecting optimal module combinations generating rendered design schemes best match user preferences proposed approach named modular customized furniture intelligent design system mocfids integrates modular theory assisted module optimization augmented reality design validation comparative experiments state art models cyclegan munit showed mocfids significantly outperformed terms ssim psnr computational efficiency highlighting superior capability producing high quality user preferred furniture designs research significance lies fact modular intelligent design method customized furniture improved efficiency quality customized furniture design delivering personalized cost effective design outcomes
"With the advent of the digital era, the application of big data in the field of cross-border e-commerce is increasing. Cross-border e-commerce has become a new model of “Internet + trade,” bringing new opportunities to traditional foreign trade industries and manufacturing. This paper aims to explore how consumer behavior influences the development and implementation of cross-border e-commerce marketing strategies in the context of big data. First of all, this study reviews the relevant theories of big data and e-commerce marketing, and analyzes the current development status and challenges faced by cross-border e-commerce. Then, the quantitative research method is used to collect and preprocess the selected sample data. By constructing a prediction model, this study identifies the key variables that affect consumers’ purchasing decisions, and empirically validates the model. The results show that big data analytics can effectively predict consumer behavior and provide support for cross-border e-commerce marketing strategies. Finally, this paper analyzes the practical application and enlightenment of the research results on cross-border e-commerce marketing, trying to provide theoretical guidance and practical reference for cross-border e-commerce enterprises to develop more accurate and effective marketing strategies under the big data environment.",advent digital application big data field cross border commerce increasing cross border commerce become new model internet trade bringing new opportunities traditional foreign trade industries manufacturing paper aims explore consumer behavior influences development implementation cross border commerce marketing strategies context big data first study reviews relevant theories big data commerce marketing analyzes current development status challenges faced cross border commerce quantitative research method used collect preprocess selected sample data constructing prediction model study identifies key variables affect consumers purchasing decisions empirically validates model results show big data analytics effectively predict consumer behavior provide support cross border commerce marketing strategies finally paper analyzes practical application enlightenment research results cross border commerce marketing trying provide theoretical guidance practical reference cross border commerce enterprises develop accurate effective marketing strategies big data environment
"The trend of network violence and false information dissemination is getting faster and faster. This paper uses private key signature and public key verification to ensure the authenticity and integrity of digital contracts. The decentralization and invariance of blockchain technology can strengthen data protection and privacy protection, and it can solve the problem that traditional manual methods cannot effectively reduce network violence and false information dissemination. In terms of cyber violence, 128 netizens in City A experienced cyber bullying in 2021, and 52 netizens experienced cyber bullying in 2022. Compared with the number of people who experienced cyberbullying in 2021, the number of people who experienced cyberbullying in 2022 decreased by 59.38%. In terms of false information, the number of false advertising events in City A in 2022 decreased by 60% compared with 2021. This study highlights the positive role of digital contracts in mitigating internet violence and curbing the spread of false information, contributing to the creation of a safer, more trustworthy, and healthier online environment.",trend network violence false information dissemination getting faster faster paper uses private key signature public key verification ensure authenticity integrity digital contracts decentralization invariance blockchain technology strengthen data protection privacy protection solve problem traditional manual methods effectively reduce network violence false information dissemination terms cyber violence netizens city experienced cyber bullying netizens experienced cyber bullying compared number people experienced cyberbullying number people experienced cyberbullying decreased terms false information number false advertising events city decreased compared study highlights positive role digital contracts mitigating internet violence curbing spread false information contributing creation safer trustworthy healthier online environment
"In the current context of information data sharing, with the rapid development of communication technology and intelligence, malicious program patch control technology has been widely applied. To address the issues of malicious program propagation control, this study uses the deep deterministic gradient algorithm to design a malicious program patch control method. On the basis of analyzing the propagation mechanism of malicious programs, intrusion detection systems are used to model the malicious programs. The rehabilitation model for susceptible infected individuals is applied to describe the process of malware transmission and construct a composite malware patch propagation model. A malicious program patch control method based on the dual deep Q-network algorithm is designed by introducing a composite malicious program patch propagation model. The dual deep Q-network algorithm could achieve network equilibrium in 45 time steps. Under the attack of malicious programs with different hit rates, the peak proportion of susceptible devices reached 0.07, 0.02, and 0.286, respectively. The number of devices infected by high hit rate malicious programs was 2.81 times that of devices infected by low hit rate malicious programs. In dynamic network environments, the DDPV method showed good adaptability, could effectively control the propagation of malicious programs under different dynamic conditions, and maintained high network gains and patch success rates. Therefore, adopting the designed malicious program patch control method can effectively suppress the spread of malicious programs by quickly identifying and sending patches, providing strong support for building a secure network environment.",current context information data sharing rapid development communication technology intelligence malicious program patch control technology widely applied address issues malicious program propagation control study uses deep deterministic gradient algorithm design malicious program patch control method basis analyzing propagation mechanism malicious programs intrusion detection systems used model malicious programs rehabilitation model susceptible infected individuals applied describe process malware transmission construct composite malware patch propagation model malicious program patch control method based dual deep network algorithm designed introducing composite malicious program patch propagation model dual deep network algorithm could achieve network equilibrium time steps attack malicious programs different hit rates peak proportion susceptible devices reached respectively number devices infected high hit rate malicious programs times devices infected low hit rate malicious programs dynamic network environments ddpv method showed good adaptability could effectively control propagation malicious programs different dynamic conditions maintained high network gains patch success rates therefore adopting designed malicious program patch control method effectively suppress spread malicious programs quickly identifying sending patches providing strong support building secure network environment
"In the context of educational digital transformation and the deep integration of intelligent technologies, Mixed Reality (MR) technology has enabled the creation of immersive collaborative environments, offering a novel paradigm for student team-based innovation and design practice. However, the exponential growth of interaction data and the dynamic complexity of collaborative scenarios in MR environments have presented a critical challenge: accurately capturing behavioral features within both human-computer interaction (HCI) and social collaborative interaction (SCI) to improve collaborative efficiency. Existing research demonstrates three major limitations: (i) single-modality data are insufficient to reflect the social context of team collaboration; (ii) static feature extraction methods fail to accommodate the evolving demands of design tasks; and (iii) inadequate modeling of team role differentiation has resulted in suboptimal interdisciplinary collaboration recommendations. To address these issues, a dynamic interaction recommendation method based on dual interaction relationship inference was proposed. A five-layered processing model—comprising an input layer, embedding layer, student preference extraction layer, team preference extraction layer, and prediction layer—was constructed. Dynamic student operation trajectories were captured using Gated Recurrent Unit (GRU) networks, while global and local attention mechanisms were employed to integrate team interaction data, enabling multi-level modeling of both personalized preferences and collaborative contexts. This study represents the first integration of dual interaction relationships into a recommendation framework, overcoming the limitations of conventional single-modality approaches. A dynamic, role-sensitive solution is thereby provided for intelligent collaboration systems in MR-based educational environments, offering significant theoretical and practical implications for the digital transformation of education.",context educational digital transformation deep integration intelligent technologies mixed reality technology enabled creation immersive collaborative environments offering novel paradigm student team based innovation design practice however exponential growth interaction data dynamic complexity collaborative scenarios environments presented critical challenge accurately capturing behavioral features within human computer interaction hci social collaborative interaction sci improve collaborative efficiency existing research demonstrates three major limitations single modality data insufficient reflect social context team collaboration static feature extraction methods fail accommodate evolving demands design tasks iii inadequate modeling team role differentiation resulted suboptimal interdisciplinary collaboration recommendations address issues dynamic interaction recommendation method based dual interaction relationship inference proposed five layered processing model comprising input layer embedding layer student preference extraction layer team preference extraction layer prediction layer constructed dynamic student operation trajectories captured using gated recurrent unit gru networks global local attention mechanisms employed integrate team interaction data enabling multi level modeling personalized preferences collaborative contexts study represents first integration dual interaction relationships recommendation framework overcoming limitations conventional single modality approaches dynamic role sensitive solution thereby provided intelligent collaboration systems based educational environments offering significant theoretical practical implications digital transformation education
"Current energy system models have problems like supply-demand mismatch and an unreasonable energy structure. Clean energy is not economically competitive enough compared to fossil energy, which affects the sustainability and transformation of energy systems. This study introduces Renewable Energy Technology and uses Multi-Objective Programming to optimize the installed capacity. This reduces the overall cost of the energy system by improving the conversion efficiency and installed capacity of renewable energy. The proposed model reduces wastewater emissions by 89.64%, which is much higher than the other two models. It also has higher proportions of reclaimed water and waste treatment. The data show that the proposed model improves energy supply stability, is more economically competitive, and provides insights for sustainable energy system development.",current energy system models problems like supply demand mismatch unreasonable energy structure clean energy economically competitive enough compared fossil energy affects sustainability transformation energy systems study introduces renewable energy technology uses multi objective programming optimize installed capacity reduces overall cost energy system improving conversion efficiency installed capacity renewable energy proposed model reduces wastewater emissions much higher two models also higher proportions reclaimed water waste treatment data show proposed model improves energy supply stability economically competitive provides insights sustainable energy system development
"In the context of current tourism informatization, with the rapid development of Internet, artificial intelligence and other technologies, tourism data mining technology has been widely used. To solve the problem of insufficient depth in tourism data analysis, Apriori and strong association rule algorithms are studied to design tourism data mining techniques. Based on the traditional Apriori algorithm, this study analyzes its shortcomings in processing large amounts of data and proposes the use of correlation to fuse frequent itemsets in the Apriori algorithm, in order to improve the effectiveness of strong association rules. Meanwhile, parallelization is introduced to continue optimization, and the practical application value is analyzed and judged. The experiment outcomes indicate that when using the Apriori algorithm and strong association rule extraction for efficiency and accuracy experiments, the implementation time is reduced by 359%. The generated association rules account for 25.67% of the total, with an efficiency 2.6 times faster than other algorithms. The maximum and minimum correlation values are 1.3 and 1.1, respectively, with a range error of no more than 0.3. The changes in support and credibility are more stable and reliable than other algorithms. The data show that the optimized Apriori strong association rule mining algorithm designed has significantly improved computational efficiency and accuracy. The research results are of great significance for the data mining and development of tourism systems.",context current tourism informatization rapid development internet artificial intelligence technologies tourism data mining technology widely used solve problem insufficient depth tourism data analysis apriori strong association rule algorithms studied design tourism data mining techniques based traditional apriori algorithm study analyzes shortcomings processing large amounts data proposes use correlation fuse frequent itemsets apriori algorithm order improve effectiveness strong association rules meanwhile parallelization introduced continue optimization practical application value analyzed judged experiment outcomes indicate using apriori algorithm strong association rule extraction efficiency accuracy experiments implementation time reduced generated association rules account total efficiency times faster algorithms maximum minimum correlation values respectively range error changes support credibility stable reliable algorithms data show optimized apriori strong association rule mining algorithm designed significantly improved computational efficiency accuracy research results great significance data mining development tourism systems
"This study proposes an artificial intelligence-based automated information management and optimization model, combining genetic algorithm and fuzzy optimization algorithm, aimed at addressing the inefficiencies and communication barriers present in traditional information management systems. By integrating the genetic algorithm and fuzzy optimization algorithm fusion, the model strikes a balance between global search capabilities and local optimization abilities, effectively improving decision accuracy and resource allocation efficiency. Experimental validation demonstrates that the model performs exceptionally well in equipment fault prediction and production scheduling in the manufacturing industry, significantly improving equipment utilization and task execution efficiency. Compared to traditional optimization algorithms, the genetic algorithm and fuzzy optimization algorithm fusion offer clear advantages in convergence speed, computational accuracy, and global search capacity. This study provides an innovative solution for the field of information management, with significant practical implications, especially in smart manufacturing and industrial internet environments, offering broad application prospects.",study proposes artificial intelligence based automated information management optimization model combining genetic algorithm fuzzy optimization algorithm aimed addressing inefficiencies communication barriers present traditional information management systems integrating genetic algorithm fuzzy optimization algorithm fusion model strikes balance global search capabilities local optimization abilities effectively improving decision accuracy resource allocation efficiency experimental validation demonstrates model performs exceptionally well equipment fault prediction production scheduling manufacturing industry significantly improving equipment utilization task execution efficiency compared traditional optimization algorithms genetic algorithm fuzzy optimization algorithm fusion offer clear advantages convergence speed computational accuracy global search capacity study provides innovative solution field information management significant practical implications especially smart manufacturing industrial internet environments offering broad application prospects
"This study investigates how algorithmic technology influences consumers’ shopping intentions in the context of live-streaming bandwagon consumption. Grounded in the Theory of Planned Behavior (TPB) and the Technology Acceptance Model (TAM), it develops an integrated framework that combines psychological and technological dimensions. Based on data collected through an online questionnaire and analyzed via structural equation modeling, the study finds that algorithmic technology enhances consumers’ perceived ease of use by improving access to live-streaming interfaces and increases perceived enjoyment by strengthening anchor effectiveness. These factors jointly shape consumers’ attitudes toward live shopping. Additionally, algorithmic mechanisms such as data targeting, user labeling, scoring systems, and group-based recommendations significantly affect behavioral intentions through both direct and mediated pathways, including attitudes, perceived behavioral control, and promotional incentives. This research offers a novel empirical contribution by modeling how algorithmic personalization impacts live shopping behaviors, providing theoretical and practical insights for digital commerce applications.",study investigates algorithmic technology influences consumers shopping intentions context live streaming bandwagon consumption grounded theory planned behavior tpb technology acceptance model tam develops integrated framework combines psychological technological dimensions based data collected online questionnaire analyzed via structural equation modeling study finds algorithmic technology enhances consumers perceived ease use improving access live streaming interfaces increases perceived enjoyment strengthening anchor effectiveness factors jointly shape consumers attitudes toward live shopping additionally algorithmic mechanisms data targeting user labeling scoring systems group based recommendations significantly affect behavioral intentions direct mediated pathways including attitudes perceived behavioral control promotional incentives research offers novel empirical contribution modeling algorithmic personalization impacts live shopping behaviors providing theoretical practical insights digital commerce applications
"In education, it is crucial to comprehensively evaluate the physical fitness of students. The limitations of information processing and analysis efficiency make it difficult for traditional evaluation methods to reveal deep physical correlation patterns. Given this, this study will focus on innovative evaluation methods that combine frequent pattern growth algorithms. An Apriori association rule model based on transaction compression and hash optimization is proposed to address association classification between physical fitness indicators. Moreover, this study optimizes operational efficiency through preprocessing techniques and hash acceleration strategies. It introduces enhancement parameters to accurately identify and establish strong association rules to achieve efficient and accurate evaluation of student physical fitness. The results showed that by comparing the running time of K-means + FP-growth and improved FP-growth under different support levels, the improved FP-growth tended to stabilize after a support level of 0.2%. The optimized model improved the execution efficiency by 82.87%–88.4% compared to Apriori and FP-growth in physical measurement data processing. The effectiveness and reliability of the improved algorithm were verified by measuring strong association rules with the introduction of enhancement degree. This study is expected to better understand the physical fitness status of students, and provide new ideas for educational decision-making and practice, which has profound practical significance for promoting innovation in physical fitness assessment methods.",education crucial comprehensively evaluate physical fitness students limitations information processing analysis efficiency make difficult traditional evaluation methods reveal deep physical correlation patterns given study focus innovative evaluation methods combine frequent pattern growth algorithms apriori association rule model based transaction compression hash optimization proposed address association classification physical fitness indicators moreover study optimizes operational efficiency preprocessing techniques hash acceleration strategies introduces enhancement parameters accurately identify establish strong association rules achieve efficient accurate evaluation student physical fitness results showed comparing running time means growth improved growth different support levels improved growth tended stabilize support level optimized model improved execution efficiency compared apriori growth physical measurement data processing effectiveness reliability improved algorithm verified measuring strong association rules introduction enhancement degree study expected better understand physical fitness status students provide new ideas educational decision making practice profound practical significance promoting innovation physical fitness assessment methods
"Enterprise employees are confronted with the dual challenges of social competition and work pressure, and mental health problems are becoming increasingly prominent. Financial staff play a very important role in an enterprise, and their mental health issues can have negative impacts on the operational efficiency and productivity of the enterprise. However, current methods for analyzing mental health suffer from high costs, low accuracy, and low efficiency. In response to this, a financial staff mental health analysis model that combines biosensing technology and decision tree algorithm is proposed. This model is applied to enterprise management work. The study first takes the biomechanical feature of pulse waves as the analysis basis, collects pulse wave signals using photoelectric sensors, and analyzes the variability of pulse rate. Afterwards, the variability features are input into a deep forest model integrated based on decision tree algorithm for psychological state recognition. The study selected 100 employees as the research subjects. Through a 3-month follow-up monitoring period, data was collected and the validity of the model was verified. The F1 score, recognition accuracy, recognition time, and recall rate were 0.94, 95.28%, 2.31 seconds, and 93.84%, respectively. Compared with other models, its performance was significantly better. In addition, in practical application experiments, the psychological health analysis accuracy of the proposed model reached 95.35%. In practical applications, the turnover rate and performance of the financial staff in the corresponding enterprises decreased and increased by 8.09% and 10.20%, respectively. The proposed model can further assist companies in monitoring the mental health issues of the financial staff and enhance their management capabilities.",enterprise employees confronted dual challenges social competition work pressure mental health problems becoming increasingly prominent financial staff play important role enterprise mental health issues negative impacts operational efficiency productivity enterprise however current methods analyzing mental health suffer high costs low accuracy low efficiency response financial staff mental health analysis model combines biosensing technology decision tree algorithm proposed model applied enterprise management work study first takes biomechanical feature pulse waves analysis basis collects pulse wave signals using photoelectric sensors analyzes variability pulse rate afterwards variability features input deep forest model integrated based decision tree algorithm psychological state recognition study selected employees research subjects month follow monitoring period data collected validity model verified score recognition accuracy recognition time recall rate seconds respectively compared models performance significantly better addition practical application experiments psychological health analysis accuracy proposed model reached practical applications turnover rate performance financial staff corresponding enterprises decreased increased respectively proposed model assist companies monitoring mental health issues financial staff enhance management capabilities
"Artificial intelligence and deep learning allow for both opportunities and challenges in intellectual property protection. The technical tools of artificial intelligence and deep learning can be used for the protection and management of intellectual property rights to improve efficiency and reduce costs. The purpose of this paper is to propose a watermark extraction and embedding technique based on Transformer to facilitate the protection of intellectual property rights. This paper adopts the literature review method and theoretical analysis method to construct an intellectual property protection model based on AI and deep learning, which can realize the embedding and extraction of watermarking information in deep learning models, so as to realize the ownership authentication and traceability of deep learning models. In this paper, an experimental analysis was carried out through public datasets to compare the model with other similar models. The results show that the model can accurately extract the watermark information and is robust against various attacks. The Transformer-based watermark embedding and extraction model is an effective video watermarking method that can effectively protect the intellectual property rights of videos.",artificial intelligence deep learning allow opportunities challenges intellectual property protection technical tools artificial intelligence deep learning used protection management intellectual property rights improve efficiency reduce costs purpose paper propose watermark extraction embedding technique based transformer facilitate protection intellectual property rights paper adopts literature review method theoretical analysis method construct intellectual property protection model based deep learning realize embedding extraction watermarking information deep learning models realize ownership authentication traceability deep learning models paper experimental analysis carried public datasets compare model similar models results show model accurately extract watermark information robust various attacks transformer based watermark embedding extraction model effective video watermarking method effectively protect intellectual property rights videos
"With smart cities’ rapid development, Internet of Things (IoT) technology applications have become widespread. However, significant data gaps in smart city IoT systems often lead to inaccurate analytics and decision-making. This study proposes a novel imputation model using an improved Mahalanobis distance algorithm. The research first examines this improved algorithm, and then analyzes its performance in filling smart city IoT missing data. Results show that at a 10% missing rate, the proposed model achieves the lowest misjudgment rate (0.45), outperforming ARBI (0.90) and RFI (0.95) models. It also demonstrates the lowest error rate at 5% (0.32) and maintains superior performance at 30% missing data (error rate: 0.48). Compared to similar models, this approach shows highest accuracy in missing data imputation, proving its effectiveness and reliability for smart city IoT applications.",smart cities rapid development internet things iot technology applications become widespread however significant data gaps smart city iot systems often lead inaccurate analytics decision making study proposes novel imputation model using improved mahalanobis distance algorithm research first examines improved algorithm analyzes performance filling smart city iot missing data results show missing rate proposed model achieves lowest misjudgment rate outperforming arbi rfi models also demonstrates lowest error rate maintains superior performance missing data error rate compared similar models approach shows highest accuracy missing data imputation proving effectiveness reliability smart city iot applications
"The major of Fashion and Apparel Design is an applied major, and it is crucial to establish a sound practical teaching system. Currently, there is a lack of large-scale comprehensive training and experimental teaching in this major. This study is based on the background of the development of the clothing industry in the era of big data and the current situation of professional practical teaching. In response to practical course issues, we conducted research on the application of computer-aided technology in clothing design course teaching. Analyze how to start from the current situation of professional teaching, improve students’ design efficiency, strengthen professional practice, and achieve the full process of clothing style from creative design to product sales and operation. To this end, innovative teaching methods for practical courses are proposed, the scope of practice is expanded to enhance students' hands-on abilities, and a 3D virtual simulation model for analyzing college clothing design teaching assignments is constructed to improve the effectiveness of practical teaching and cultivate high-quality applied clothing professionals.",major fashion apparel design applied major crucial establish sound practical teaching system currently lack large scale comprehensive training experimental teaching major study based background development clothing industry big data current situation professional practical teaching response practical course issues conducted research application computer aided technology clothing design course teaching analyze start current situation professional teaching improve students design efficiency strengthen professional practice achieve full process clothing style creative design product sales operation end innovative teaching methods practical courses proposed scope practice expanded enhance students hands abilities virtual simulation model analyzing college clothing design teaching assignments constructed improve effectiveness practical teaching cultivate high quality applied clothing professionals
"To help enterprises detect potential financial risks in a timely manner, this study proposed a security assessment and early warning method for company financial systems based on grey correlation screening. This study evaluated the security status of a company financial system through grey correlation analysis and constructed a corresponding evaluation and early warning system. The results showed that the success rate of testing the system access function, query function, service function, and management function reached 100%. The usage rate of the system’s central processing unit was about 2%, memory usage rate was about 51%, and disk usage rate was about 33%, indicating that the system ran well. The average accuracy of the financial system security assessment and early warning system was 99.5%, and the overall application effect was good. In the long-term evaluation satisfaction survey, the satisfaction score before the system renovation was 63.5, and the satisfaction score after the renovation was 92.7. This study indicated that this method was effective and feasible in enterprises, providing new ideas and tools for enterprises to enhance their operational stability and risk control capabilities.",help enterprises detect potential financial risks timely manner study proposed security assessment early warning method company financial systems based grey correlation screening study evaluated security status company financial system grey correlation analysis constructed corresponding evaluation early warning system results showed success rate testing system access function query function service function management function reached usage rate system central processing unit memory usage rate disk usage rate indicating system ran well average accuracy financial system security assessment early warning system overall application effect good long term evaluation satisfaction survey satisfaction score system renovation satisfaction score renovation study indicated method effective feasible enterprises providing new ideas tools enterprises enhance operational stability risk control capabilities
"In response to key challenges in urban traffic management, especially in ensuring compliance with traffic regulations, this paper proposes an Packet Grey Wolf Optimization (PGWO) algorithm designed to improve prediction accuracy and enforcement efficiency in traffic violations. By introducing a momentum coefficient, a grouping position update strategy, and a reverse learning mechanism, the PGWO algorithm significantly improves global search capability and convergence speed, effectively avoiding early convergence to a local optimum. Taking the accurate identification of traffic violations as the core issue, this study applies PGWO algorithm to the traffic violation prediction model based on the Stanford Open Policing Project dataset. By comparing and analyzing the original Grey Wolf Optimization algorithm and other traditional optimization algorithms, PGWO showed excellent performance in improving the accuracy of traffic violation arrest. In addition, the PGWO algorithm has been integrated into the PNN regression prediction model, and its effectiveness and superiority in the field of traffic laws have been further verified through testing on the Kaggle dataset. The experimental results demonstrate that the PGWO algorithm not only achieves greater accuracy in predicting traffic violations but also enhances the model’s generalization ability, providing a new optimization strategy and decision support tool for intelligent traffic management and regulatory compliance.",response key challenges urban traffic management especially ensuring compliance traffic regulations paper proposes packet grey wolf optimization pgwo algorithm designed improve prediction accuracy enforcement efficiency traffic violations introducing momentum coefficient grouping position update strategy reverse learning mechanism pgwo algorithm significantly improves global search capability convergence speed effectively avoiding early convergence local optimum taking accurate identification traffic violations core issue study applies pgwo algorithm traffic violation prediction model based stanford open policing project dataset comparing analyzing original grey wolf optimization algorithm traditional optimization algorithms pgwo showed excellent performance improving accuracy traffic violation arrest addition pgwo algorithm integrated pnn regression prediction model effectiveness superiority field traffic laws verified testing kaggle dataset experimental results demonstrate pgwo algorithm achieves greater accuracy predicting traffic violations also enhances model generalization ability providing new optimization strategy decision support tool intelligent traffic management regulatory compliance
"Focusing on the security issues of medical archive information, a privacy protection technology for archive information that integrates blockchain, interstellar file system, and encryption algorithms is proposed. Among them, the research focuses on building an archive information sharing platform based on sharded blockchain, introducing reputation consensus algorithm to adjust computational pressure, and strengthening storage security and sharing stability. At the same time, research has proposed an encryption technique that integrates multiple prime asymmetric encryption algorithms to enhance data privacy protection through multi link encryption and decryption. In terms of data privacy protection, research proposes the integration of multiple encryption technologies. In performance testing, the average gas consumption of this technology in blockchain is only 1.12, far lower than other technologies, and the resource cost advantage is obvious. In the encryption time test, when the number of encryption attributes exceeds 45, the traditional encryption technology significantly increases the encryption time. However, the research technology is similar to reference, and when the number of attributes is between 5 and 15, there is a 7.25% improvement in encryption performance compared to reference. In addition, in government recorded scenarios, the interception rate reached 99.4% under high load (1500 attacks), with 1491 intercepted attacks, outperforming other technologies. In addition, in the delay and throughput testing of the encryption process, the maximum delay of the research technology was only 352 ms when the encryption attributes were 68, and the maximum throughput was 3584 TX/s, both of which were superior to similar technologies. The technology proposed by the research has excellent performance in the privacy management of medical institution archive information. The research content will provide technical support for the protection and management of archive information privacy in the medical field.",focusing security issues medical archive information privacy protection technology archive information integrates blockchain interstellar file system encryption algorithms proposed among research focuses building archive information sharing platform based sharded blockchain introducing reputation consensus algorithm adjust computational pressure strengthening storage security sharing stability time research proposed encryption technique integrates multiple prime asymmetric encryption algorithms enhance data privacy protection multi link encryption decryption terms data privacy protection research proposes integration multiple encryption technologies performance testing average gas consumption technology blockchain far lower technologies resource cost advantage obvious encryption time test number encryption attributes exceeds traditional encryption technology significantly increases encryption time however research technology similar reference number attributes improvement encryption performance compared reference addition government recorded scenarios interception rate reached high load attacks intercepted attacks outperforming technologies addition delay throughput testing encryption process maximum delay research technology encryption attributes maximum throughput superior similar technologies technology proposed research excellent performance privacy management medical institution archive information research content provide technical support protection management archive information privacy medical field
"In today’s era, English is exceptionally important to our work and life. The rapid development of Artificial Intelligence has driven the development of various industries, and the education industry is no exception. This study aims to construct a Transformer-based model and apply it to several teaching tasks to improve the quality and efficiency of English teaching. The model integrates the functions of vocabulary teaching, reading teaching, and grammar teaching into one and sets up the roles of teachers, students, and online question-answering platforms to provide students with a personalized and high-quality knowledge learning model. After experimental validation, the model has an advantage over similar benchmark models in terms of model generalization ability, training time, and correct rate.",today english exceptionally important work life rapid development artificial intelligence driven development various industries education industry exception study aims construct transformer based model apply several teaching tasks improve quality efficiency english teaching model integrates functions vocabulary teaching reading teaching grammar teaching one sets roles teachers students online question answering platforms provide students personalized high quality knowledge learning model experimental validation model advantage similar benchmark models terms model generalization ability training time correct rate
"With the increasing popularity of big data technology in the field of education, its application in college music education has become a new field of concern. The purpose of this study is to explore the application and effect of big data technology in personalized teaching and learning innovation in college music education. Through the comprehensive use of questionnaire survey, experiment design and big data analysis model, the research finds that big data technology can significantly improve the teaching quality of music education and the learning effect of students. The questionnaire revealed the cognition and attitude of teachers and students toward big data technology, while the experimental results showed that the teaching class using big data technology outperformed traditional teaching methods in terms of academic achievement and motivation. In addition, the constructed big data analysis model showed high accuracy in predicting students’ learning outcomes. However, the study also points out that there are still technical, privacy and adaptability challenges when it comes to practical application of big data technology. This study provides a scientific basis for the further application of big data technology in music education, and puts forward suggestions for future educational practice and research direction.",increasing popularity big data technology field education application college music education become new field concern purpose study explore application effect big data technology personalized teaching learning innovation college music education comprehensive use questionnaire survey experiment design big data analysis model research finds big data technology significantly improve teaching quality music education learning effect students questionnaire revealed cognition attitude teachers students toward big data technology experimental results showed teaching class using big data technology outperformed traditional teaching methods terms academic achievement motivation addition constructed big data analysis model showed high accuracy predicting students learning outcomes however study also points still technical privacy adaptability challenges comes practical application big data technology study provides scientific basis application big data technology music education puts forward suggestions future educational practice research direction
"Developing critical thinking skills (CTS) and reading comprehension ability (RCA) is crucial for English as a foreign language (EFL) learners, yet many struggle with these cognitive competencies. Although project-based learning (PBL) has been widely recognized for its pedagogical benefits, existing research has primarily focused on quantitative evidence of its effectiveness, with limited understanding of the underlying mechanisms through which PBL fosters these skills. Moreover, few studies have explored how PBL supports CTS and RCA development in Chinese college EFL contexts. To address these gaps, this study explores how PBL facilitates the development of CTS and RCA by employing a qualitative case study approach. Data were collected through in-depth interviews, classroom observations, and journals. Purposive sampling with maximum variation was employed, and three cases from high, medium, and low levels of CTS and RCA were selected as participants of the study. Thematic analysis was conducted within cases and across cases. The study reveals that PBL fosters CTS by engaging students in problem-solving, inquiry-based learning, and reflective evaluation. In addition, it enhances RCA by integrating explicit reading instruction, authentic material engagement, and collaborative discussions. Moreover, the study highlights differentiated development pathways in CTS and RCA across learners with varying proficiency levels, supported by adaptive scaffolding that facilitates their integrated growth. The findings contribute to the literature by proposing a conceptual framework that illustrates the cognitive and instructional mechanisms through which PBL supports CTS and RCA development in EFL contexts.",developing critical thinking skills cts reading comprehension ability rca crucial english foreign language efl learners yet many struggle cognitive competencies although project based learning pbl widely recognized pedagogical benefits existing research primarily focused quantitative evidence effectiveness limited understanding underlying mechanisms pbl fosters skills moreover studies explored pbl supports cts rca development chinese college efl contexts address gaps study explores pbl facilitates development cts rca employing qualitative case study approach data collected depth interviews classroom observations journals purposive sampling maximum variation employed three cases high medium low levels cts rca selected participants study thematic analysis conducted within cases across cases study reveals pbl fosters cts engaging students problem solving inquiry based learning reflective evaluation addition enhances rca integrating explicit reading instruction authentic material engagement collaborative discussions moreover study highlights differentiated development pathways cts rca across learners varying proficiency levels supported adaptive scaffolding facilitates integrated growth findings contribute literature proposing conceptual framework illustrates cognitive instructional mechanisms pbl supports cts rca development efl contexts
"Considering the high coupling between different variables and data characteristics, based on automatic coding machine, an improved RBF neural network model is proposed in this paper, and the adaptive data sample processing method is adopt to study the response mechanism of enterprise technological innovation. The new energy vehicle enterprises are taken as examples, the characteristics and mechanism variables are integrated to conduct sensitivity regulation on the response mechanism of enterprise technological innovation. The proposed model has strong universality in the study of enterprise technological innovation.",considering high coupling different variables data characteristics based automatic coding machine improved rbf neural network model proposed paper adaptive data sample processing method adopt study response mechanism enterprise technological innovation new energy vehicle enterprises taken examples characteristics mechanism variables integrated conduct sensitivity regulation response mechanism enterprise technological innovation proposed model strong universality study enterprise technological innovation
"Currently, there are a series of problems such as insufficient power, short range, and low operating efficiency of commercially available electric microtillers in mountainous and hilly areas, and this paper proposes a three-port power converter design for electric microtillers based on a hybrid energy storage system. The design adopts a hybrid energy storage structure of lithium batteries and supercapacitors, a dual closed-loop controller and a three-port power converter, which ensures the voltage stability of the system during load fluctuations. To further reduce the cost of tillage, a combination of photovoltaic power generation and three-port power converter is used to realize the efficient management of energy flow between the photovoltaic module, energy storage module, and motor. During the tillage process, the three-port power converter can provide feedback adjustment for different depths and changes in soil hardness based on field test data. Through simulation and experiment, the high efficiency and stability of the three-port power converter and its application feasibility and application prospect are verified.",currently series problems insufficient power short range low operating efficiency commercially available electric microtillers mountainous hilly areas paper proposes three port power converter design electric microtillers based hybrid energy storage system design adopts hybrid energy storage structure lithium batteries supercapacitors dual closed loop controller three port power converter ensures voltage stability system load fluctuations reduce cost tillage combination photovoltaic power generation three port power converter used realize efficient management energy flow photovoltaic module energy storage module motor tillage process three port power converter provide feedback adjustment different depths changes soil hardness based field test data simulation experiment high efficiency stability three port power converter application feasibility application prospect verified
"With the rapid development of information technology and artificial intelligence, the application of virtual reality (VR) and reinforcement learning (RL) in education and training has attracted more and more attention. First, this study systematically reviews the latest progress of VR and RL in the field of education and training through literature review. Then, a virtual reality training model based on reinforcement learning is constructed, and the model is verified and evaluated in detail. On the basis of the model training results, the model testing and performance evaluation are further carried out, and the performance of different models is compared. This study not only provides a highly adaptive and personalized training model combining VR and RL, but also verifies its effectiveness in improving education and training results through experiments.",rapid development information technology artificial intelligence application virtual reality reinforcement learning education training attracted attention first study systematically reviews latest progress field education training literature review virtual reality training model based reinforcement learning constructed model verified evaluated detail basis model training results model testing performance evaluation carried performance different models compared study provides highly adaptive personalized training model combining also verifies effectiveness improving education training results experiments
"(1) Medical image segmentation is crucial for disease diagnosis, surgical planning, and therapeutic monitoring, but existing methods face significant challenges due to the complex structures of human organs, including substantial size variations, indistinct boundaries, and low inter-tissue contrast. (2) To address this, we propose SDM-UNet, a hybrid network integrating CNN and Transformer modules to enhance segmentation performance. The architecture features a Multi-Attention Feature Refinement (MAFR) block replacing the Swin-UNet bottleneck, which combines adaptive kernel convolution, enhanced convolution, and channel attention to improve local feature extraction, and Multi-Fusion Dense Skip Connections that facilitate multi-scale feature fusion between the encoder and decoder to mitigate spatial information loss during downsampling. (3) Validated on the Synapse multi-organ CT and ACDC cardiac MRI datasets, SDM-UNet was trained using the PyTorch framework with ImageNet-pretrained weights and evaluated via Dice Similarity Coefficient (DSC) and 95th percentile Hausdorff Distance (HD95). (4) Experimental results show that SDM-UNet achieves an average DSC of 80.51% and HD95 of 22.09 mm on Synapse, and an average DSC of 90.58% and HD95 of 1.12 mm on ACDC, outperforming state-of-the-art methods like Swin-UNet and SCUNet++ and demonstrating its superiority in balancing global context understanding and local detail preservation.",medical image segmentation crucial disease diagnosis surgical planning therapeutic monitoring existing methods face significant challenges due complex structures human organs including substantial size variations indistinct boundaries low inter tissue contrast address propose sdm unet hybrid network integrating cnn transformer modules enhance segmentation performance architecture features multi attention feature refinement mafr block replacing swin unet bottleneck combines adaptive kernel convolution enhanced convolution channel attention improve local feature extraction multi fusion dense skip connections facilitate multi scale feature fusion encoder decoder mitigate spatial information loss downsampling validated synapse multi organ acdc cardiac mri datasets sdm unet trained using pytorch framework imagenet pretrained weights evaluated via dice similarity coefficient dsc percentile hausdorff distance experimental results show sdm unet achieves average dsc synapse average dsc acdc outperforming state art methods like swin unet scunet demonstrating superiority balancing global context understanding local detail preservation
"With the rapid development of information technology, Linux operating system, as an open source system, is widely used in servers, embedded devices, and cloud computing platforms. Because of its stability, flexibility, and efficiency, Linux systems play a role in many key areas. However, with the complexity of application environment and the expansion of system scale, the reliability and performance evaluation of Linux system have gradually become the focus of research. Based on fuzzy comprehensive evaluation method, this paper proposes a new reliability benchmarking and dynamic performance evaluation method of Linux system. By constructing a fuzzy evaluation model, the reliability of Linux system is comprehensively and quantitatively evaluated, and through comparative analysis, the dynamic performance optimization strategy of Linux system in different running environments is put forward. Based on experimental data and actual usage scenarios, this paper collects performance data of multiple Linux systems in terms of load, network transmission, IO operation, etc., covering mainstream versions such as Ubuntu, CentOS, and Debian. Through fuzzy comprehensive evaluation of these data, the reliability scores of different system versions are obtained, and their performance bottlenecks are deeply analyzed. The experimental results show that the reliability score of Ubuntu system under high load conditions is 82.5%, while the reliability score of CentOS system is slightly lower at 78.3%. Through dynamic performance evaluation, it is found that IO performance fluctuates significantly under high concurrency, which affects the stability and response speed of the system. The evaluation method proposed in this paper can not only accurately reflect the performance of Linux system in practical application but also provide theoretical basis and technical support for system optimization. The research results are of great significance to improve the reliability and performance of Linux systems, especially in large-scale applications and demanding industry environments.",rapid development information technology linux operating system open source system widely used servers embedded devices cloud computing platforms stability flexibility efficiency linux systems play role many key areas however complexity application environment expansion system scale reliability performance evaluation linux system gradually become focus research based fuzzy comprehensive evaluation method paper proposes new reliability benchmarking dynamic performance evaluation method linux system constructing fuzzy evaluation model reliability linux system comprehensively quantitatively evaluated comparative analysis dynamic performance optimization strategy linux system different running environments put forward based experimental data actual usage scenarios paper collects performance data multiple linux systems terms load network transmission operation etc covering mainstream versions ubuntu centos debian fuzzy comprehensive evaluation data reliability scores different system versions obtained performance bottlenecks deeply analyzed experimental results show reliability score ubuntu system high load conditions reliability score centos system slightly lower dynamic performance evaluation found performance fluctuates significantly high concurrency affects stability response speed system evaluation method proposed paper accurately reflect performance linux system practical application also provide theoretical basis technical support system optimization research results great significance improve reliability performance linux systems especially large scale applications demanding industry environments
"Chinese ceramic culture has a long history, and Chinese ceramics are closely related to the process of Chinese history. In each era, the dissemination and evolution of Chinese ceramics has formed its own characteristics in the dissemination background and channels, dissemination characteristics, and dissemination influences. Ceramic culture is the common artistic language of mankind, which exists in the form of a special cultural symbol. It has a long history and profound cultural heritage, and plays an important role in Sino-foreign exchanges. However, there are many difficulties and challenges in the process of communication and dissemination with cultures from all over the world. It is increasingly an important topic to study the propagation path of ceramic culture, so as to promote overseas spread for ceramic culture. This work uses complex networks to predict and analyze the propagation path of ceramic culture. First, this work selects the BP network as the basic framework to predict the propagation path of ceramic culture. Neural networks are complex networks that can perform such tasks efficiently. Second, because the BP algorithm has problems such as slow convergence and easy to be affected by the initial weight, this work uses the PSO algorithm to make up for the shortcomings of BP algorithm. To solve slow convergence in the later stage of the PSO algorithm and easy to fall into local optimum, this paper adopts improved strategy with dynamic inertia weight and dynamic learning factor. Third, this work integrates the improved PSO (IPSO) with the BP network to construct an IPSO-BP network, and realizes the prediction of the propagation path of ceramic culture based on IPSO-BP.",chinese ceramic culture long history chinese ceramics closely related process chinese history dissemination evolution chinese ceramics formed characteristics dissemination background channels dissemination characteristics dissemination influences ceramic culture common artistic language mankind exists form special cultural symbol long history profound cultural heritage plays important role sino foreign exchanges however many difficulties challenges process communication dissemination cultures world increasingly important topic study propagation path ceramic culture promote overseas spread ceramic culture work uses complex networks predict analyze propagation path ceramic culture first work selects network basic framework predict propagation path ceramic culture neural networks complex networks perform tasks efficiently second algorithm problems slow convergence easy affected initial weight work uses pso algorithm make shortcomings algorithm solve slow convergence later stage pso algorithm easy fall local optimum paper adopts improved strategy dynamic inertia weight dynamic learning factor third work integrates improved pso ipso network construct ipso network realizes prediction propagation path ceramic culture based ipso
"To address the issues of insufficient accuracy and significant impact of image noise on model performance in current facial recognition systems, this study proposes a new framework that integrates wavelet transform, an improved multi-task cascaded convolutional neural network (MTCNN), and genetic algorithms (GA). This framework utilizes wavelet transform for image denoising; optimizes MTCNN by introducing a hybrid threshold function and a confidence candidate box retention mechanism to effectively address information loss issues; and applies GA for feature compensation and optimization. Experimental results demonstrate that this method significantly improves the accuracy, recall rate, regression value, and model stability of facial recognition, effectively enhancing the system’s robustness under noise interference, and provides an effective solution for optimizing facial recognition performance.",address issues insufficient accuracy significant impact image noise model performance current facial recognition systems study proposes new framework integrates wavelet transform improved multi task cascaded convolutional neural network mtcnn genetic algorithms framework utilizes wavelet transform image denoising optimizes mtcnn introducing hybrid threshold function confidence candidate box retention mechanism effectively address information loss issues applies feature compensation optimization experimental results demonstrate method significantly improves accuracy recall rate regression value model stability facial recognition effectively enhancing system robustness noise interference provides effective solution optimizing facial recognition performance
"Joint event extraction aims at discovering event from texts and simultaneously identifying their corresponding event types, argument roles, which is an essential but challenging task in natural language processing. Although extensively studied, existing approaches still suffered from low accuracy. To solve this issue, this paper proposes a deep learning-based joint event extraction approach, JBGCN-MATT, which applies deep pre-trained language model to represent texts and combines syntactic graph convolution network and multi-attention mechanism to capture long-distance dependencies. Comprehensive experiments were conducted on the benchmark dataset ACE2005, results show that JBGCN-MATT achieves the F1 score of 74.2% and 60.5% for the trigger classification task and the argument classification task, respectively, and outperforms the state-of-the-art methods.",joint event extraction aims discovering event texts simultaneously identifying corresponding event types argument roles essential challenging task natural language processing although extensively studied existing approaches still suffered low accuracy solve issue paper proposes deep learning based joint event extraction approach jbgcn matt applies deep pre trained language model represent texts combines syntactic graph convolution network multi attention mechanism capture long distance dependencies comprehensive experiments conducted benchmark dataset ace results show jbgcn matt achieves score trigger classification task argument classification task respectively outperforms state art methods
"In the contemporary epoch of novel productive forces, the cultivation of high-quality, application-oriented talent is imperative for the advancement of the new chemical materials industry. The proposed model is a “three joints and five collaborations” university-enterprise collaborative talent training model. The “three joints” refer to joint team building, joint talent training, and joint science and industry, whereas the “five cooperations” include cooperation in the establishment of training programs, teaching of quality management, cultivation of high-level students, promotion of industrial scientific research, and sharing of achievements. This model addresses the challenges faced by traditional talent cultivation modes in meeting the demands of new productive forces. The Applied Chemistry Program at Putian University, in collaboration with several leading enterprises in Fujian Province, has established internship bases, restructured the curriculum system, and strengthened practical teaching. These efforts have yielded substantial advancements in students’ application and innovation abilities. Notable outcomes include a 99% employment rate, with 25% of graduates pursuing further education, and 83% of employed graduates adapting “seamlessly” to technical roles. Furthermore, students have published over 100 papers and obtained more than 10 patents. The model has also facilitated the transformation of scientific research achievements into productive forces, generating significant economic benefits for the enterprises involved. This model offers a promising approach for local universities to cultivate high-quality talent that meets the demands of new productive forces. This study provides a valuable reference for the transformation, upgrading, and high-quality development of China’s new chemical materials industry and broader manufacturing sector.",contemporary epoch novel productive forces cultivation high quality application oriented talent imperative advancement new chemical materials industry proposed model three joints five collaborations university enterprise collaborative talent training model three joints refer joint team building joint talent training joint science industry whereas five cooperations include cooperation establishment training programs teaching quality management cultivation high level students promotion industrial scientific research sharing achievements model addresses challenges faced traditional talent cultivation modes meeting demands new productive forces applied chemistry program putian university collaboration several leading enterprises fujian province established internship bases restructured curriculum system strengthened practical teaching efforts yielded substantial advancements students application innovation abilities notable outcomes include employment rate graduates pursuing education employed graduates adapting seamlessly technical roles furthermore students published papers obtained patents model also facilitated transformation scientific research achievements productive forces generating significant economic benefits enterprises involved model offers promising approach local universities cultivate high quality talent meets demands new productive forces study provides valuable reference transformation upgrading high quality development china new chemical materials industry broader manufacturing sector
"Whether tourism resources can be scientifically and effectively developed and utilized is directly related to the amount of economic benefits that tourism resources bring to developers, as well as the revenue data of the local tourism industry. Therefore, how to develop tourism resources has become the main issue that countless scenic area developers need to study today. When formulating a tourism resource development plan using traditional methods, it is usually necessary to arrange researchers to evaluate the quantity and quality of the tourism resources owned by the scenic area. This process often takes several months, and after obtaining the evaluation data, it will be submitted to the decision-making level for repeated and unpredictable meetings and discussions. The discussion mainly focuses on the proportion of service investment and specific measures for each resource in the scenic area, and finally a preliminary plan is obtained. In response to the drawbacks of traditional methods such as time-consuming and cumbersome steps, this study attempts to apply genetic algorithms to optimize the development of tourism resources, hoping to provide an intelligent and efficient method for formulating development plans. The process of using genetic algorithms to develop tourism resource development plans is as follows. Firstly, the optimization task was modeled, abstracted into a mathematical representation that the model can understand, and model parameters were set for subsequent iterative tasks; then, the population was randomly initialized to provide a richer gene pool for the entire population, allowing individuals in the population to be distributed throughout the solution space. Next, it is necessary to iterate the population, where individuals within the population undergo selection, crossover, and mutation in each iteration round, while adding randomness to evolve towards higher fitness values. When the iteration round ends, the highest fitness value of individuals in the population can converge, and this individual represents the best solution considered by the model. Five simulation experiments were conducted in this article. The initial population size was 100, 120, 140, 80, and 70, and the number of iteration rounds was 100, 80, 70, 110, and 130. Finally, the highest fitness values of the five experiments all converge to 208.9, and the X of the individual with the highest fitness values converges to [1,1,1,0,0]. Y converges approximately at [0.44, 0.41, 0.15, 0,0], and Z converges at [0.4, 0.3, 0.3]. Finally, this article also compares with examples of rural tourism development to verify the effectiveness and practicality of genetic algorithms in optimizing tourism resources. After calculation, the sample distance between excellent development cases and model generated solutions is 7.512, the sample distance between negative cases and model generated solutions is 31.836, and the sample distance between fuzzy cases and model generated solutions is 16.757. The experimental results demonstrate that the use of genetic algorithms can provide scientific decision support and methodological guidance for the development and utilization of tourism resources.",whether tourism resources scientifically effectively developed utilized directly related amount economic benefits tourism resources bring developers well revenue data local tourism industry therefore develop tourism resources become main issue countless scenic area developers need study today formulating tourism resource development plan using traditional methods usually necessary arrange researchers evaluate quantity quality tourism resources owned scenic area process often takes several months obtaining evaluation data submitted decision making level repeated unpredictable meetings discussions discussion mainly focuses proportion service investment specific measures resource scenic area finally preliminary plan obtained response drawbacks traditional methods time consuming cumbersome steps study attempts apply genetic algorithms optimize development tourism resources hoping provide intelligent efficient method formulating development plans process using genetic algorithms develop tourism resource development plans follows firstly optimization task modeled abstracted mathematical representation model understand model parameters set subsequent iterative tasks population randomly initialized provide richer gene pool entire population allowing individuals population distributed throughout solution space next necessary iterate population individuals within population undergo selection crossover mutation iteration round adding randomness evolve towards higher fitness values iteration round ends highest fitness value individuals population converge individual represents best solution considered model five simulation experiments conducted article initial population size number iteration rounds finally highest fitness values five experiments converge individual highest fitness values converges converges approximately converges finally article also compares examples rural tourism development verify effectiveness practicality genetic algorithms optimizing tourism resources calculation sample distance excellent development cases model generated solutions sample distance negative cases model generated solutions sample distance fuzzy cases model generated solutions experimental results demonstrate use genetic algorithms provide scientific decision support methodological guidance development utilization tourism resources
"With the widespread application of AI image generation technology in higher education design fields, traditional design education models face the necessity of reevaluation. This study aims to explore how design aesthetic features (as objective product attributes) influence designers’ creative thinking and design expression (as subjective capabilities), and accordingly reassess the educational value of foundational design courses in the AI era. Using a comparative experimental method, the research recruited 25 first-year and 25 third-year industrial design students to create product designs using Midjourney, with 36 industrial design experts systematically evaluating the works. Results indicate that design aesthetic features significantly impact design expression more than design thinking, and the two student groups demonstrate notable differences in design element application: novice design students primarily express creativity through intuitive visual elements such as product patterns and product appearance, while advanced students more effectively utilize professional design elements like form contours and material textures, reflecting how design education facilitates students' transition from perceptual cognition to rational analysis. Additionally, the positive correlation between creative thinking and design expression strengthens with deepening design education, indicating a mutually reinforcing relationship. Based on these findings, the paper suggests that foundational design courses in the AI-generated imagery era need repositioning: color and expression courses should shift from basic skill training to high-level theoretical education, creative thinking courses significantly increase in importance, form and material courses maintain core value but need content updates aligned with AI characteristics, while human-computer collaborative design should become a new curricular direction. This study provides an empirical foundation for design education reform in the AI era, emphasizing the importance of understanding design essentials and cultivating innovative thinking.",widespread application image generation technology higher education design fields traditional design education models face necessity reevaluation study aims explore design aesthetic features objective product attributes influence designers creative thinking design expression subjective capabilities accordingly reassess educational value foundational design courses using comparative experimental method research recruited first year third year industrial design students create product designs using midjourney industrial design experts systematically evaluating works results indicate design aesthetic features significantly impact design expression design thinking two student groups demonstrate notable differences design element application novice design students primarily express creativity intuitive visual elements product patterns product appearance advanced students effectively utilize professional design elements like form contours material textures reflecting design education facilitates students transition perceptual cognition rational analysis additionally positive correlation creative thinking design expression strengthens deepening design education indicating mutually reinforcing relationship based findings paper suggests foundational design courses generated imagery need repositioning color expression courses shift basic skill training high level theoretical education creative thinking courses significantly increase importance form material courses maintain core value need content updates aligned characteristics human computer collaborative design become new curricular direction study provides empirical foundation design education reform emphasizing importance understanding design essentials cultivating innovative thinking
"Art appreciation relies heavily on the imagery as the primary medium of information dissemination. It enables teachers and students alike to communicate a gamut of emotions through its visuals. Nevertheless, the abstract and subjective nature of emotions coupled with the intricate and nonlinear relationship between image characteristics and image sentiment presents a formidable challenge in image emotion classification. Thus, this study proposes a novel image emotion classification model that integrates the depth separable convolution technique. First, the RGB features of the image are extracted through the judicious use of yellow correction, brightness adjustment, and size scaling, while optimizing color video signal transmission with YCrCb. Second, the deep semantic features of the image are extracted through multi-scale fusion features using an improved FPN model, wherein pre-training parameters of ResNet101 are transferred to the model. Finally, the emotion semantics in art images are classified using a convolution block attention module to form a depth separable convolution. Experimental results reveal that the 33 image evaluation features obtained through training have a strong correlation with the expressed emotional semantics. The predictive capacity of the proposed model aligns with the true polarity of the sample with a remarkable accuracy of 93.31% in emotional classification.",art appreciation relies heavily imagery primary medium information dissemination enables teachers students alike communicate gamut emotions visuals nevertheless abstract subjective nature emotions coupled intricate nonlinear relationship image characteristics image sentiment presents formidable challenge image emotion classification thus study proposes novel image emotion classification model integrates depth separable convolution technique first rgb features image extracted judicious use yellow correction brightness adjustment size scaling optimizing color video signal transmission ycrcb second deep semantic features image extracted multi scale fusion features using improved fpn model wherein pre training parameters resnet transferred model finally emotion semantics art images classified using convolution block attention module form depth separable convolution experimental results reveal image evaluation features obtained training strong correlation expressed emotional semantics predictive capacity proposed model aligns true polarity sample remarkable accuracy emotional classification
"The primary aim of this research is to develop and implement a piano evaluation mechanism enhanced with an error correction feature utilizing AI. Precisely, the study discovers to overcome the limitations of traditional piano teaching evaluations which frequently suffer from poor convergence and a tendency to fall into local extremes. We propose a novel AI method called Selfish Herds Search-integrated Improved Probabilistic Neural Network (SHS-IPNN), for piano music evaluation. The method leverages the Improved Probabilistic Neural Network (IPNN) for its robustness in handling data and for improving convergence in learning. In addition to using the short-term energy difference (STED) technique to precisely determine the temporal assessment of every note in the audio of a piano performance. Additionally, the Discrete Wavelet Transform (DWT) is applied to assess pitch accuracy. The entire system is integrated within a Musical Instrument Digital Interface (MIDI) framework to facilitate detailed evaluation of piano performances. The piano performances are categorized as “Good,” “Fair,” or “Poor” in this examination, which is structured for a classification problem. Our findings emphasize the efficacy of the SHS-IPNN technique, as demonstrated by its overall performance in terms of recall (90.5%), accuracy (96%), F1-score (93.5%), and precision (95.5%). The experimental outcomes indicate that the SHS-IPNN model outperforms existing methods in terms of accurately detecting performance errors and evaluating piano performances. The model’s increased accuracy in providing expressive, rhythmic, and overall judgments is demonstrative of this development. The innovative application of the SHS-IPNN method in piano music education demonstrates a significant advancement in the field. This approach not only improves accuracy in performance evaluation but also improves the learning procedure by providing error correction, which is crucial for developing proficient piano skills.",primary aim research develop implement piano evaluation mechanism enhanced error correction feature utilizing precisely study discovers overcome limitations traditional piano teaching evaluations frequently suffer poor convergence tendency fall local extremes propose novel method called selfish herds search integrated improved probabilistic neural network shs ipnn piano music evaluation method leverages improved probabilistic neural network ipnn robustness handling data improving convergence learning addition using short term energy difference sted technique precisely determine temporal assessment every note audio piano performance additionally discrete wavelet transform dwt applied assess pitch accuracy entire system integrated within musical instrument digital interface midi framework facilitate detailed evaluation piano performances piano performances categorized good fair poor examination structured classification problem findings emphasize efficacy shs ipnn technique demonstrated overall performance terms recall accuracy score precision experimental outcomes indicate shs ipnn model outperforms existing methods terms accurately detecting performance errors evaluating piano performances model increased accuracy providing expressive rhythmic overall judgments demonstrative development innovative application shs ipnn method piano music education demonstrates significant advancement field approach improves accuracy performance evaluation also improves learning procedure providing error correction crucial developing proficient piano skills
"Portrait painting, as a traditional art form, has seen increasing research efforts aimed at automating its creation with the advancement of computer technology. Currently, research on computer-generated portrait painting primarily focuses on using deep learning models to generate high-quality artistic style works from input images. However, despite the application of advanced technologies such as generative adversarial networks (GANs) in this field, there are still shortcomings in terms of preserving image details and features. To address this, the study proposes a portrait design model based on dynamic receptive fields (DyRF), whose core innovation lies in introducing multi-scale convolutional modules and adaptive receptive field mechanisms to achieve dynamic perception of features and precise reconstruction of details in different image regions, significantly enhancing the image’s ability to retain details and artistic expression. Experimental results show that DyRF significantly outperforms comparison models on multiple key metrics: achieving a Fréchet depth distance (FID) of 12.21, approximately 30% lower than DyMo (17.65) and FSGAN (23.66); it achieves a feature similarity (FSIM) of 0.85, significantly higher than SCA-GAN (0.71) and FSGAN (0.68), indicating superior performance in terms of image realism and feature consistency. The research results validate the effectiveness of the DyRF model in detail reconstruction and feature retention, bringing new breakthroughs to the field of computer-generated art.",portrait painting traditional art form seen increasing research efforts aimed automating creation advancement computer technology currently research computer generated portrait painting primarily focuses using deep learning models generate high quality artistic style works input images however despite application advanced technologies generative adversarial networks gans field still shortcomings terms preserving image details features address study proposes portrait design model based dynamic receptive fields dyrf whose core innovation lies introducing multi scale convolutional modules adaptive receptive field mechanisms achieve dynamic perception features precise reconstruction details different image regions significantly enhancing image ability retain details artistic expression experimental results show dyrf significantly outperforms comparison models multiple key metrics achieving fréchet depth distance fid approximately lower dymo fsgan achieves feature similarity fsim significantly higher sca gan fsgan indicating superior performance terms image realism feature consistency research results validate effectiveness dyrf model detail reconstruction feature retention bringing new breakthroughs field computer generated art
"In tunnel pipe jacking construction, the complex and variable environment affects the stability and support of the lining structure, which hinders the normal progress of tunnel construction. Therefore, this study proposes a structural stability detection model that integrates the Grey Wolf Optimizer and Particle Swarm Optimization. This model utilizes the preprocessing capabilities of the cubic spline interpolation and wavelet denoising methods, combined with the fast learning ability of the Extreme Learning Machine neural network, to extract structural stability. The results of the study show that the model achieved an Area Under the Curve value of 0.8981, a prediction accuracy of 94%, and stabilizes within 20 iterations. In cumulative data prediction based on the time sequence superposition principle, the maximum offset is 0.055 mm, with the most concentrated absolute error points and a median absolute error of 0.11 μm, outperforming other algorithms. In actual detection, after data processing by this model, the average Signal-to-Noise Ratio increases by 18.57%, the correlation coefficient reaches 0.96, and the standard deviation is 0.019. These results indicate that the model balances computational efficiency and accuracy, demonstrating strong stability and practicality. It improves the efficiency and accuracy of structural stability detection in pipe jacking construction, ensuring smooth construction and safeguarding life and property.",tunnel pipe jacking construction complex variable environment affects stability support lining structure hinders normal progress tunnel construction therefore study proposes structural stability detection model integrates grey wolf optimizer particle swarm optimization model utilizes preprocessing capabilities cubic spline interpolation wavelet denoising methods combined fast learning ability extreme learning machine neural network extract structural stability results study show model achieved area curve value prediction accuracy stabilizes within iterations cumulative data prediction based time sequence superposition principle maximum offset concentrated absolute error points median absolute error outperforming algorithms actual detection data processing model average signal noise ratio increases correlation coefficient reaches standard deviation results indicate model balances computational efficiency accuracy demonstrating strong stability practicality improves efficiency accuracy structural stability detection pipe jacking construction ensuring smooth construction safeguarding life property
"Traditional swimming action recognition methods suffer from low accuracy, high latency, and poor robustness in complex underwater environments, failing to meet high-level training demands. This study proposes a novel framework combining phase-based motion segmentation, artificial neural networks, and hidden Markov models, enhanced by inertial sensors for real-time, high-precision recognition across multiple strokes. Testing showed the model achieved a 94.73% F1 score on public datasets with an average delay of just 25.98 ms, outperforming existing methods in stability under varying lighting. Additionally, its sensitivity and technical improvement scores for rhythmic strokes like butterfly and breaststroke exceeded 92 points in coach evaluations. The model excels in accuracy, temporal consistency, and environmental adaptability, demonstrating strong practical potential for competitive training, stroke correction, and performance analysis systems.",traditional swimming action recognition methods suffer low accuracy high latency poor robustness complex underwater environments failing meet high level training demands study proposes novel framework combining phase based motion segmentation artificial neural networks hidden markov models enhanced inertial sensors real time high precision recognition across multiple strokes testing showed model achieved score public datasets average delay outperforming existing methods stability varying lighting additionally sensitivity technical improvement scores rhythmic strokes like butterfly breaststroke exceeded points coach evaluations model excels accuracy temporal consistency environmental adaptability demonstrating strong practical potential competitive training stroke correction performance analysis systems
"This paper aims to solve the problems of single expression, insufficient interactivity, low design efficiency and limited public participation in traditional landscape design, and explore the innovative application of intelligent big data and virtual reality (VR) technology in landscape design. The study proposes a clustering visualization algorithm based on intelligent big data to achieve efficient processing and real-time analysis of massive multi-dimensional landscape design data, and intuitively presents data correlation and difference by establishing data field and radiation effect, providing designers with clearer design ideas. At the same time, VR technology is introduced to build an immersive virtual environment, so that designers and users can intuitively experience the landscape effect in three-dimensional space, breaking through the limitations of traditional plane or two-dimensional display methods. The experimental results show that compared with traditional methods, this method significantly improves the scientificity, interactivity, and design efficiency of landscape design schemes: the enthusiasm of designers is increased by 70%, and 66.3% of designers say that the design time is greatly shortened; public satisfaction is increased from 13.3% “very satisfied” under traditional design to 50%, and design risks and costs are effectively controlled. Research shows that the integration of intelligent big data and VR technology not only solves the pain points in the traditional design process but also promotes the transformation of design thinking from one-way expression to multi-party collaboration, expands the expressiveness and social acceptance of landscape design, and provides the industry with a new technical path and practice paradigm.",paper aims solve problems single expression insufficient interactivity low design efficiency limited public participation traditional landscape design explore innovative application intelligent big data virtual reality technology landscape design study proposes clustering visualization algorithm based intelligent big data achieve efficient processing real time analysis massive multi dimensional landscape design data intuitively presents data correlation difference establishing data field radiation effect providing designers clearer design ideas time technology introduced build immersive virtual environment designers users intuitively experience landscape effect three dimensional space breaking limitations traditional plane two dimensional display methods experimental results show compared traditional methods method significantly improves scientificity interactivity design efficiency landscape design schemes enthusiasm designers increased designers say design time greatly shortened public satisfaction increased satisfied traditional design design risks costs effectively controlled research shows integration intelligent big data technology solves pain points traditional design process also promotes transformation design thinking one way expression multi party collaboration expands expressiveness social acceptance landscape design provides industry new technical path practice paradigm
"Scenario perception enables personalized service recommendations by analyzing user contextual data. To enhance physical education evaluation, we integrate scenario-aware recommendation algorithms (e.g., TextRank with temporal weighting) to optimize real-time data filtering for teaching scenarios. Concurrently, we develop multi-object tracking models (e.g., twin networks with feature fusion) to address challenges in athlete movement analysis, creating a unified framework for data-driven teaching assessment. Second, the continuous word bag (model and user emotional tendency analysis algorithm on Baidu AI open platform), which can significantly reduce or eliminate the influence of the above data on the recommended content. Multi-objective tracking method based on the fusion of pedestrian derecognition and player number features. According to the problem of high target similarity, a feature dimension fusion strategy is proposed, which combines target characteristics from different dimensions (e.g., appearance, motion, and jersey numbers) to enhance the algorithm’s discriminative ability for similar targets. The experiments show that the proposed multi-feature fusion tracking method can still achieve a better tracking effect in the case of high target similarity, with direct applications in real-time analysis of athletes’ movement trajectories and tactical behaviors during physical education classes. Different IoU thresholds can ultimately affect the tracking performance of the algorithm to a large extent, providing quantitative support for optimizing teaching evaluation systems. It can be seen from the results that when the IoU threshold is 0.5, the algorithm achieves the best tracking performance, with the HOTA index improving by 3.8 points compared to the benchmark method, AssA index by 6.4 points compared to the benchmark method, and IDF 1 index by 7.1 points compared to the benchmark method. When the IoU threshold is 0.6 and 0.7, the HOTA index increased by 3.2 and 2.2 points compared to the benchmark method; the AssA index increased by 5.1 and 4.0 points with the benchmark method. To solve the problem of severe target occlusion, a twin network (a neural network architecture with two parallel branches for feature extraction and similarity calculation) is introduced to enhance anti-occlusion capabilities by modeling target motion and contextual information. The tracking method consists of three-level target associations, and the similarity needed to calculate the association is calculated using the information of twin network, multi-feature fusion, and spatial location. To solve the problem of severe target occlusion, the twin network method is introduced to use the algorithm in anti-occlusion. The experimental results show that the multilevel object tracking structure of the similarity calculation method of the twin network can effectively alleviate serious occlusion problems in sports events.",scenario perception enables personalized service recommendations analyzing user contextual data enhance physical education evaluation integrate scenario aware recommendation algorithms textrank temporal weighting optimize real time data filtering teaching scenarios concurrently develop multi object tracking models twin networks feature fusion address challenges athlete movement analysis creating unified framework data driven teaching assessment second continuous word bag model user emotional tendency analysis algorithm baidu open platform significantly reduce eliminate influence data recommended content multi objective tracking method based fusion pedestrian derecognition player number features according problem high target similarity feature dimension fusion strategy proposed combines target characteristics different dimensions appearance motion jersey numbers enhance algorithm discriminative ability similar targets experiments show proposed multi feature fusion tracking method still achieve better tracking effect case high target similarity direct applications real time analysis athletes movement trajectories tactical behaviors physical education classes different iou thresholds ultimately affect tracking performance algorithm large extent providing quantitative support optimizing teaching evaluation systems seen results iou threshold algorithm achieves best tracking performance hota index improving points compared benchmark method assa index points compared benchmark method idf index points compared benchmark method iou threshold hota index increased points compared benchmark method assa index increased points benchmark method solve problem severe target occlusion twin network neural network architecture two parallel branches feature extraction similarity calculation introduced enhance anti occlusion capabilities modeling target motion contextual information tracking method consists three level target associations similarity needed calculate association calculated using information twin network multi feature fusion spatial location solve problem severe target occlusion twin network method introduced use algorithm anti occlusion experimental results show multilevel object tracking structure similarity calculation method twin network effectively alleviate serious occlusion problems sports events
"To test the weld penetration situation, a new method based on deep learning has been researched. The welding technique in paper is TIG weld. The weld testing and experiment system has been firstly setup. Weld experiments can be performed. A CCD sensor is used to snap images during the welding procession. A neural filter and a narrow band filer are chosen to setup a composite system, which can filter weld arc light. The experiment system can capture several groups of weld pool images in real time. The pool images are processed by median filter and gray transformation operations. On this basis, the CNN(convolution neural network) is built up. The input layer contains 215×215 neurodes. The character extraction network is a single convolution layer composed by 20 convolutional filters, which are 9×9. The active function of convolution layer is the ReLU function. The 2×2 average pooling method is used for the pool layer. And the classifier network contains one hidden and a output layer. There are 100 neurodes in hidden layer, and the activated function of it is the ReLU function. The neurodes number in output layer is three, which stand for the unfused, fused and overfused condition of weldments. The Soft max function is determined to be activated function for output layer. Then 300 pool images are used to be sample data, which are captured by welding experiments. And the set up neural network can be trained by the data. So a visual CNN model can be setup for weld penetration predicting. In the end of the paper, experiment is performed to test the accuracy. Another 100 pool images are imported to the setup CNN model. The precision is up to 92%, which is showed that the setup network has certain accuracy rate.",test weld penetration situation new method based deep learning researched welding technique paper tig weld weld testing experiment system firstly setup weld experiments performed ccd sensor used snap images welding procession neural filter narrow band filer chosen setup composite system filter weld arc light experiment system capture several groups weld pool images real time pool images processed median filter gray transformation operations basis cnn convolution neural network built input layer contains neurodes character extraction network single convolution layer composed convolutional filters active function convolution layer relu function average pooling method used pool layer classifier network contains one hidden output layer neurodes hidden layer activated function relu function neurodes number output layer three stand unfused fused overfused condition weldments soft max function determined activated function output layer pool images used sample data captured welding experiments set neural network trained data visual cnn model setup weld penetration predicting end paper experiment performed test accuracy another pool images imported setup cnn model precision showed setup network certain accuracy rate
"Forklifts, as one of the most commonly used transportation tools in current engineering construction, have extremely high research value. With the increasing energy-saving requirements of the forklift industry, energy-saving improvement of the lifting system in the hydraulic lifting system of forklifts has become a research hotspot. To optimize the energy saving of the forklift hydraulic lifting system, this research proposes to use the secondary regulation technology to optimize it. The gravity potential energy generated in the lifting of the load is recovered through the secondary components, and stored in the energy accumulator to provide energy for the operation of the system, to reduce the energy consumption of the engine. The experimental results show that compared to traditional lifting systems, the proposed secondary regulation lifting system saves 29 kJ of energy consumption and can achieve an efficiency of 87.8%. In addition, the load weight, lifting speed, and energy storage capacity all have varying degrees of influence on the energy consumption of the improved secondary regulation system. When the load weight is 5 kg and 15 kg, the energy consumption difference between the two is 75 kJ, and the displacement difference is 0 m. When the lifting speed is 0.1 m/s and 0.3 m/s, the displacement distance difference between the two is 1.5 m, and the energy consumption difference is 123 kJ. When the energy storage capacity is 5 L and 11 L, the displacement distance difference is 0.1 m, and the energy consumption difference is 89 kJ. Therefore, the improved secondary regulation and lifting system proposed in the study has high energy-saving performance.",forklifts one commonly used transportation tools current engineering construction extremely high research value increasing energy saving requirements forklift industry energy saving improvement lifting system hydraulic lifting system forklifts become research hotspot optimize energy saving forklift hydraulic lifting system research proposes use secondary regulation technology optimize gravity potential energy generated lifting load recovered secondary components stored energy accumulator provide energy operation system reduce energy consumption engine experimental results show compared traditional lifting systems proposed secondary regulation lifting system saves energy consumption achieve efficiency addition load weight lifting speed energy storage capacity varying degrees influence energy consumption improved secondary regulation system load weight energy consumption difference two displacement difference lifting speed displacement distance difference two energy consumption difference energy storage capacity displacement distance difference energy consumption difference therefore improved secondary regulation lifting system proposed study high energy saving performance
"With the development of digital technologies, including artificial intelligence (AI), big data, and cloud computing, there has been a sweeping wave of digitalization across diverse fields and countries. Research indicates that digitalization has driven a surge in research and development (R&amp;D) activities and significantly enhanced R&amp;D efficiency. Cost review is a critical aspect of managing R&amp;D efficiency. However, traditional cost review methods no longer suffice in meeting the demands of the digital era. This raises the question of whether digital technologies, such as LLM, can facilitate an evolutionary shift towards a more automated and efficient cost review approach that synergizes human and machine efforts. This paper validated the positive impact of digital technology on R&amp;D efficiency using a Panel Vector Autoregression (PVAR) model and the impulse response analysis. Based on empirical results, we discussed the role of LLM in cost review and evaluated the potential risks associated with its use. To mitigate these risks, we recommended that integrating LLM with expert insights for analysis and interpretation, ensuring more reliable outcomes.",development digital technologies including artificial intelligence big data cloud computing sweeping wave digitalization across diverse fields countries research indicates digitalization driven surge research development amp activities significantly enhanced amp efficiency cost review critical aspect managing amp efficiency however traditional cost review methods longer suffice meeting demands digital raises question whether digital technologies llm facilitate evolutionary shift towards automated efficient cost review approach synergizes human machine efforts paper validated positive impact digital technology amp efficiency using panel vector autoregression pvar model impulse response analysis based empirical results discussed role llm cost review evaluated potential risks associated use mitigate risks recommended integrating llm expert insights analysis interpretation ensuring reliable outcomes
"Prefabricated buildings (PBs) represent a new type of construction method that take the path of sustainable development and are an important way to promote the transformation of China’s construction industry. In order to better promote the development of China’s PBs industry and help achieve the goal of “dual carbon” in the construction industry, this paper discusses the driving factors and driving paths for the development of PBs industry under the background of “dual carbon.” Using the literature review method and questionnaire survey method to construct a multi-dimensional driving index system for the development of the PBs industry, constructing a multi-dimensional driving model for the PBs industry based on the data, analyzing the driving path using the structural equation model, and calculating the comprehensive effect of each index. The results show that the development of PBs industry is driven by government support, economic level, technical specification, social environment and market environment, and the size of the driving force is in the following order: government support &gt; market environment &gt; economic level &gt; technical specification &gt; social environment.",prefabricated buildings pbs represent new type construction method take path sustainable development important way promote transformation china construction industry order better promote development china pbs industry help achieve goal dual carbon construction industry paper discusses driving factors driving paths development pbs industry background dual carbon using literature review method questionnaire survey method construct multi dimensional driving index system development pbs industry constructing multi dimensional driving model pbs industry based data analyzing driving path using structural equation model calculating comprehensive effect index results show development pbs industry driven government support economic level technical specification social environment market environment size driving force following order government support market environment economic level technical specification social environment
"To address the problem that traditional artificial recognition of oral English in the development of oral English is easy to be affected by its own factors leading to the recognition accuracy is not high, this article adopted the long short-term memory (LSTM) neural network in machine learning algorithms to construct a speech recognition model. Pre-emphasis, framing, windowing, Mel Frequency Cepstral Coefficient (MCFF) feature extraction and other preprocessing operations were performed on the speech data. Using MFCC parameters as model inputs to analyze and validate the performance of speech recognition models, multiple parameters such as intonation, speaking speed, and rhythm were utilized to evaluate oral speech. The findings showed that the model constructed in this article achieved a speech recognition accuracy of 95.21%, which can effectively enhance the accuracy of speech recognition.",address problem traditional artificial recognition oral english development oral english easy affected factors leading recognition accuracy high article adopted long short term memory lstm neural network machine learning algorithms construct speech recognition model pre emphasis framing windowing mel frequency cepstral coefficient mcff feature extraction preprocessing operations performed speech data using mfcc parameters model inputs analyze validate performance speech recognition models multiple parameters intonation speaking speed rhythm utilized evaluate oral speech findings showed model constructed article achieved speech recognition accuracy effectively enhance accuracy speech recognition
"As the growth of network technology, the network intrusion has become increasingly serious. An elephant herding optimization algorithm and support vector machine-based network intrusion detection method are proposed to address the difficulties and low detection accuracy of the detection. This method first uses an optimized elephant swarm optimization algorithm to select features from the intrusion data and then uses the elephant swarm optimization method to optimize the parameters of the support vector machine algorithm. Finally, a detection model is constructed based on support vector machines. The main contribution of the research is the proposal of a network intrusion detection method based on improved swarm optimization algorithm and support vector machine. By using an improved swarm optimization algorithm to optimize the parameters of the support vector machine classification algorithm, this method significantly improves the accuracy and stability of detection when dealing with the classification task of network intrusion detection. The experimental results show that the detection model has a stable average accuracy of around 94% in detecting four types of intrusion data, surpassing the performance of other commonly used algorithms. The results validate the effectiveness of introducing the improved elephant swarm optimization algorithm and demonstrate its superiority in intrusion detection tasks.",growth network technology network intrusion become increasingly serious elephant herding optimization algorithm support vector machine based network intrusion detection method proposed address difficulties low detection accuracy detection method first uses optimized elephant swarm optimization algorithm select features intrusion data uses elephant swarm optimization method optimize parameters support vector machine algorithm finally detection model constructed based support vector machines main contribution research proposal network intrusion detection method based improved swarm optimization algorithm support vector machine using improved swarm optimization algorithm optimize parameters support vector machine classification algorithm method significantly improves accuracy stability detection dealing classification task network intrusion detection experimental results show detection model stable average accuracy around detecting four types intrusion data surpassing performance commonly used algorithms results validate effectiveness introducing improved elephant swarm optimization algorithm demonstrate superiority intrusion detection tasks
"Aiming at the problems of low efficiency of traditional manual picking in smart agriculture and insufficient recognition accuracy of existing algorithms in complex scenarios, an intelligent recognition method for fruit and vegetable information integrating lightweight YOLOv5 and SENet attention mechanisms is studied and proposed, and an intelligent recognition system for fruit and vegetable information is constructed. The system designs the myCPS module to replace the traditional CSP structure, adopts the MobileNet-v2 backbone network and combines mySwin Transformer to optimize feature extraction, and integrates the SENet mechanism to enhance the weights of key features. The results show that on the same test set, mAP reaches 99.78%, which is 14.55% higher than that of YOLOv5. The model size is 203.89 MB, which is reduced by 4.5% compared with the benchmark model YOLOv5. The floating-point operation amount is 3.32GFLOPs, which is 9.3% lower than that of the benchmark YOLOv5 model. The single-frame inference time of the model is 38.49 ms, which is 15.7% lower than that of YOLOv5. In practical applications, the CPU usage rate of the system is only 26%, and the processing time for the same task is 81.9% shorter than that of YOLOv5X. The system proposed in this research can effectively improve the accuracy and real-time performance of fruit and vegetable recognition, reduce labor costs, optimize the sorting and quality inspection processes of fruits and vegetables, and is of great significance for promoting agricultural automation and intelligence.",aiming problems low efficiency traditional manual picking smart agriculture insufficient recognition accuracy existing algorithms complex scenarios intelligent recognition method fruit vegetable information integrating lightweight yolov senet attention mechanisms studied proposed intelligent recognition system fruit vegetable information constructed system designs mycps module replace traditional csp structure adopts mobilenet backbone network combines myswin transformer optimize feature extraction integrates senet mechanism enhance weights key features results show test set map reaches higher yolov model size reduced compared benchmark model yolov floating point operation amount gflops lower benchmark yolov model single frame inference time model lower yolov practical applications cpu usage rate system processing time task shorter yolov system proposed research effectively improve accuracy real time performance fruit vegetable recognition reduce labor costs optimize sorting quality inspection processes fruits vegetables great significance promoting agricultural automation intelligence
"With the increasing importance of safety management in chemical and bioengineering laboratories, how to quickly and accurately detect and warn dangerous behaviors is becoming a popular topic. In this study, a dangerous behavior detection method based on improved You Only Look Once v5s model and human posture estimation is proposed. It is applied to the safety management of chemical and bioengineering laboratories. First, by improving the You Only Look Once v5s model, the detection accuracy and speed are improved. Then, the detected dangerous behaviors are further analyzed and judged by combining the human posture estimation technique. Finally, an indoor personnel safety video surveillance system is developed by adopting a distributed architecture and modularized design strategy. The results validated that the precision of the improved model was 97.68%, the recall rate was 96.79%, the detection time consumed was 0.022 s, and the GPU performance was 10.1. The method can quickly and accurately detect and warn dangerous behaviors. The optimized model designed by the study has important practical application value.",increasing importance safety management chemical bioengineering laboratories quickly accurately detect warn dangerous behaviors becoming popular topic study dangerous behavior detection method based improved look model human posture estimation proposed applied safety management chemical bioengineering laboratories first improving look model detection accuracy speed improved detected dangerous behaviors analyzed judged combining human posture estimation technique finally indoor personnel safety video surveillance system developed adopting distributed architecture modularized design strategy results validated precision improved model recall rate detection time consumed gpu performance method quickly accurately detect warn dangerous behaviors optimized model designed study important practical application value
"This study focuses on the digital protection needs of ancient Chinese mural art and innovatively constructs an intelligent recognition system based on deep transfer learning and multi-modal feature fusion. In response to the challenges of scarce and uneven quality of mural image data, the system adopts a two-stage progressive training strategy. Firstly, a deep convolutional neural network is pre-trained on a large general image dataset to obtain the basic visual feature representation and then fine-tuned and optimized on a professional mural dataset through a domain adaptive method. The feature extraction stage innovatively integrates the high-level semantic features of deep neural networks with the low-level artistic features of traditional image processing. Among them, the deep features are extracted through an improved residual network architecture, while the artistic features blend the color distribution features based on the HSV color space with the improved rotational invariance LBP texture features. The system innovatively designs a feature fusion module based on the attention mechanism, achieving dynamic optimization and combination of multi-source features through learnable feature weights. Experimental verification shows that, compared with mainstream deep network models, this system has achieved a significant improvement in the accuracy of feature recognition in the era of murals. At the same time, it has demonstrated excellent generalization performance in cross-dataset tests, providing reliable technical support and new methodological guidance for the intelligent research of cultural heritage.",study focuses digital protection needs ancient chinese mural art innovatively constructs intelligent recognition system based deep transfer learning multi modal feature fusion response challenges scarce uneven quality mural image data system adopts two stage progressive training strategy firstly deep convolutional neural network pre trained large general image dataset obtain basic visual feature representation fine tuned optimized professional mural dataset domain adaptive method feature extraction stage innovatively integrates high level semantic features deep neural networks low level artistic features traditional image processing among deep features extracted improved residual network architecture artistic features blend color distribution features based hsv color space improved rotational invariance lbp texture features system innovatively designs feature fusion module based attention mechanism achieving dynamic optimization combination multi source features learnable feature weights experimental verification shows compared mainstream deep network models system achieved significant improvement accuracy feature recognition murals time demonstrated excellent generalization performance cross dataset tests providing reliable technical support new methodological guidance intelligent research cultural heritage
"To improve the efficiency of Chinese named entity recognition, this study optimizes existing models to address the insufficient context feature capture and difficulty in recognizing polysemous words when processing complex text information. Given the grid long short-term memory network model, text information memory perception module, text information adaptive fusion module, and conditional random field are added to enhance the model’s ability to capture contextual information and feature fusion effect. Finally, an improved lattice long short-term memory network model is designed for Chinese named entity recognition tasks. The results indicated that the new model had higher recognition accuracy than the conventional model in benchmark performance testing, with the highest recognition accuracy reaching 0.98 in both datasets. In practical applications, the model achieved recognition accuracy of over 95% in 10 different types of Chinese named entity recognition tasks, with the highest reaching 99.15%. In addition, the average recognition time of this optimized model was as low as 0.06 seconds, far less than the other three compared models. Therefore, the designed model can provide a more efficient and accurate technical means for Chinese named entity recognition tasks.",improve efficiency chinese named entity recognition study optimizes existing models address insufficient context feature capture difficulty recognizing polysemous words processing complex text information given grid long short term memory network model text information memory perception module text information adaptive fusion module conditional random field added enhance model ability capture contextual information feature fusion effect finally improved lattice long short term memory network model designed chinese named entity recognition tasks results indicated new model higher recognition accuracy conventional model benchmark performance testing highest recognition accuracy reaching datasets practical applications model achieved recognition accuracy different types chinese named entity recognition tasks highest reaching addition average recognition time optimized model low seconds far less three compared models therefore designed model provide efficient accurate technical means chinese named entity recognition tasks
"As globalization accelerates, the significance of English in international communication becomes increasingly prominent, making the effective learning of English vocabulary a pivotal aspect of language acquisition. This study aims to explore a personalized grading method for English vocabulary learning difficulty through machine learning technology, facilitating learners in more efficiently mastering English vocabulary. Initially, the paper analyzes the limitations of traditional methods for grading the difficulty of English vocabulary learning, highlighting the lack of dynamic adaptation to the differences among learners. Subsequently, it introduces a novel approach to predicting the difficulty of learning English vocabulary using machine learning, particularly through the application of transfer learning techniques at the edge. This method adjusts the prediction model based on the learner’s background knowledge and learning history, thus enhancing the accuracy and applicability of predictions. Finally, by predicting the English vocabulary learning difficulty levels of students at different stages (beginner, intermediate, and advanced), this study validates the effectiveness of the proposed method. The results indicate that the machine learning model employing transfer learning demonstrates significant advantages in grading the difficulty of English vocabulary learning, offering more personalized learning guidance to learners of varying levels. The findings of this research not only provide a new perspective for the field of English education but also offer technical support for designing personalized learning paths.",globalization accelerates significance english international communication becomes increasingly prominent making effective learning english vocabulary pivotal aspect language acquisition study aims explore personalized grading method english vocabulary learning difficulty machine learning technology facilitating learners efficiently mastering english vocabulary initially paper analyzes limitations traditional methods grading difficulty english vocabulary learning highlighting lack dynamic adaptation differences among learners subsequently introduces novel approach predicting difficulty learning english vocabulary using machine learning particularly application transfer learning techniques edge method adjusts prediction model based learner background knowledge learning history thus enhancing accuracy applicability predictions finally predicting english vocabulary learning difficulty levels students different stages beginner intermediate advanced study validates effectiveness proposed method results indicate machine learning model employing transfer learning demonstrates significant advantages grading difficulty english vocabulary learning offering personalized learning guidance learners varying levels findings research provide new perspective field english education also offer technical support designing personalized learning paths
"Urban renewal projects involve the regeneration and retrofitting of urban areas, which presents significant cost control challenges due to their scale, complexity and evolving requirements. This research proposes a full process cost control framework enabled by a digital design building information modeling (BIM) platform tailored for urban renewal projects. The platform integrates architectural design, structure, MEP, cost estimation, and 4D construction simulation, allowing continuous cost tracking and forecasting throughout the project lifecycle. The digital platform approach provides enhanced visibility and analytics into the cost profile across design, procurement, construction and operation, overcoming limitations of traditional cost control methods. Further work involves incorporating additional functionality like artificial intelligence, virtual reality and Internet of Things connectivity to the platform to enable data-driven cost management.",urban renewal projects involve regeneration retrofitting urban areas presents significant cost control challenges due scale complexity evolving requirements research proposes full process cost control framework enabled digital design building information modeling bim platform tailored urban renewal projects platform integrates architectural design structure mep cost estimation construction simulation allowing continuous cost tracking forecasting throughout project lifecycle digital platform approach provides enhanced visibility analytics cost profile across design procurement construction operation overcoming limitations traditional cost control methods work involves incorporating additional functionality like artificial intelligence virtual reality internet things connectivity platform enable data driven cost management
A scientific and reasonable indicator system is the key to measuring the level of data governance in manufacturing enterprises (DGME). The study analyzes documents and policy analysis to classify manufacturing enterprise data governance into five dimensions and constructs a manufacturing enterprise data governance indicator system with 5 primary indicators and 23 secondary indicators. Study 1 utilized Exploratory Factor Analysis (EFA) analysis to determine that the data governance indicator system of manufacturing companies presents a five-factor structure; Study 2 used Confirmatory Factor Analysis (CFA) and Average Variance Extracted (AVE) to determine that the overall fitness of the model and the reliability and validity of each level of the indicator measurement items were at the desired level. The proposed indicator system can provide a reference for data governance in manufacturing enterprises.,scientific reasonable indicator system key measuring level data governance manufacturing enterprises dgme study analyzes documents policy analysis classify manufacturing enterprise data governance five dimensions constructs manufacturing enterprise data governance indicator system primary indicators secondary indicators study utilized exploratory factor analysis efa analysis determine data governance indicator system manufacturing companies presents five factor structure study used confirmatory factor analysis cfa average variance extracted ave determine overall fitness model reliability validity level indicator measurement items desired level proposed indicator system provide reference data governance manufacturing enterprises
"Amid diverse market competition and evolving consumer demands, the construction of tea brand assets has gradually garnered significant attention from enterprises. However, existing literature investigations reveal a scarcity of theoretical research focusing on Chinese tea brand assets from a consumer perspective; this will inevitably lead to a lack of theoretical experience support for tea brands in the practical process, affecting the construction of brand assets. This study aims to clarify the constituent factors of tea brand assets and explore the interrelationships among these factors. Through a comprehensive analysis of existing literature, the constituent factors of tea brand assets were identified. By formulating theoretical hypotheses, a model for tea brand assets was proposed and an assessment scale was developed. Subsequently, a questionnaire survey was conducted based on the scale, followed by a reliability and validity test of the data and model using factor analysis. The results demonstrate that tea brand assets from a consumer perspective consist of Brand Awareness, Brand Association, Perceived Quality, and Brand Loyalty, with interconnections among these factors that influence brand assets. Notably, there is a significant correlation between Brand Awareness and Brand Association, while Brand Awareness does not significantly affect Perceived Quality. Brand Association positively impacts Perceived Quality and Brand Loyalty. Both Brand Awareness and Perceived Quality significantly influence Brand Loyalty.",amid diverse market competition evolving consumer demands construction tea brand assets gradually garnered significant attention enterprises however existing literature investigations reveal scarcity theoretical research focusing chinese tea brand assets consumer perspective inevitably lead lack theoretical experience support tea brands practical process affecting construction brand assets study aims clarify constituent factors tea brand assets explore interrelationships among factors comprehensive analysis existing literature constituent factors tea brand assets identified formulating theoretical hypotheses model tea brand assets proposed assessment scale developed subsequently questionnaire survey conducted based scale followed reliability validity test data model using factor analysis results demonstrate tea brand assets consumer perspective consist brand awareness brand association perceived quality brand loyalty interconnections among factors influence brand assets notably significant correlation brand awareness brand association brand awareness significantly affect perceived quality brand association positively impacts perceived quality brand loyalty brand awareness perceived quality significantly influence brand loyalty
"In the evolving landscape of scientific information technology and its pervasive influence on college students’ lives through the internet, concerns around cyber information security among this demographic have heightened. Particularly, deficiencies in college students’ abilities to discern online information and their insufficient knowledge and skills in information security have drawn attention. This study delves into the current state and challenges of information security among college students in the digital realm, outlining corresponding preventive strategies. This research is vital for understanding cybersecurity.",evolving landscape scientific information technology pervasive influence college students lives internet concerns around cyber information security among demographic heightened particularly deficiencies college students abilities discern online information insufficient knowledge skills information security drawn attention study delves current state challenges information security among college students digital realm outlining corresponding preventive strategies research vital understanding cybersecurity
"Customer engagement is a cornerstone of successful power marketing, especially as energy providers move towards digital transformation. However, traditional marketing strategies often fail to capture real-time customer sentiment and adapt dynamically to individual needs. The research introduces a data-driven method utilizing deep learning (DL)-based Natural Language Processing (NLP) techniques to interpret and respond to customer feedback, thereby enhancing engagement through personalized, timely, and context-aware communication. Data was collected from social media, customer service chat transcripts, and online feedback from energy company websites. The text was refined using preprocessing techniques such as lemmatization and stop-word removal. Word2Vec was used for feature extraction to capture the semantic meaning and context of customer expressions. The proposed method integrates Bidirectional Encoder Representations from Transformers (BERT) with an Attention-based Temporal Convolutional Neural Network (Att-TCNN) to capture contextual and temporal features in customer communication. The system uses BERT to understand customer language and track behavioral patterns by extracting contextual word representations and processing them through temporal convolution layers enhanced with attention, focusing on relevant text sequence parts. This hybrid BERT-Att-TCNN approach supports sentiment classification, topic identification, and engagement prediction, delivering personalized, adaptive, and real-time customer engagement in power marketing. Python was used to implement and train the model efficiently. Results from experimental evaluation demonstrate that the proposed BERT-Att-TCNN model achieved performance metrics ranging from 90 to 96%, highlighting the model’s robustness and reliability compared to traditional NLP models. This hybrid approach ensures scalable, intelligent, and real-time engagement in modern power marketing management.",customer engagement cornerstone successful power marketing especially energy providers move towards digital transformation however traditional marketing strategies often fail capture real time customer sentiment adapt dynamically individual needs research introduces data driven method utilizing deep learning based natural language processing nlp techniques interpret respond customer feedback thereby enhancing engagement personalized timely context aware communication data collected social media customer service chat transcripts online feedback energy company websites text refined using preprocessing techniques lemmatization stop word removal word vec used feature extraction capture semantic meaning context customer expressions proposed method integrates bidirectional encoder representations transformers bert attention based temporal convolutional neural network att tcnn capture contextual temporal features customer communication system uses bert understand customer language track behavioral patterns extracting contextual word representations processing temporal convolution layers enhanced attention focusing relevant text sequence parts hybrid bert att tcnn approach supports sentiment classification topic identification engagement prediction delivering personalized adaptive real time customer engagement power marketing python used implement train model efficiently results experimental evaluation demonstrate proposed bert att tcnn model achieved performance metrics ranging highlighting model robustness reliability compared traditional nlp models hybrid approach ensures scalable intelligent real time engagement modern power marketing management
"The traditional construction industry relies heavily on non-renewable energy sources, resulting in high energy consumption. However, traditional renewable energy system regulation models have the problem of weak adaptability. This study proposes a reinforcement learning based regulation model for green building renewable energy systems. The results validated that the reward value of the research model remained stable between −1.3 and −1.4, demonstrating good convergence. When the learning rate was 0.1, the annual cumulative expenditure cost of the proposed model remained stable at 8200–8300 yen, which was better than the comparative model. The similarity between the model and the basic rules reached 94.62%, which was higher than other models. The model had a minimum net load demand of 1.75 kWh in winter and 0.42 kWh in summer. In terms of battery utilization, the model achieved the highest cycle numbers of 2.81 and 3.42 in winter and summer, respectively. In terms of renewable energy utilization, the winter photovoltaic self-consumption rate of the model reached 81.2%, while the summer grid connection rate and self-sufficiency rate were 9.2% and 67.2%, which were better than other models. Overall, this study explores new methods in the field of green building energy regulation, enhances the flexibility and adaptability of energy regulation, and has certain research significance for achieving efficient utilization and sustainable development of building energy.",traditional construction industry relies heavily non renewable energy sources resulting high energy consumption however traditional renewable energy system regulation models problem weak adaptability study proposes reinforcement learning based regulation model green building renewable energy systems results validated reward value research model remained stable demonstrating good convergence learning rate annual cumulative expenditure cost proposed model remained stable yen better comparative model similarity model basic rules reached higher models model minimum net load demand kwh winter kwh summer terms battery utilization model achieved highest cycle numbers winter summer respectively terms renewable energy utilization winter photovoltaic self consumption rate model reached summer grid connection rate self sufficiency rate better models overall study explores new methods field green building energy regulation enhances flexibility adaptability energy regulation certain research significance achieving efficient utilization sustainable development building energy
"To enhance the efficiency and effectiveness of cultural heritage scene rendering, enabling dynamic interaction and immersive experiences for cultural inheritance. The study improves the grey wolf optimization algorithm, integrates it into deep learning oversampling training, and optimizes rendering based on color attribute theory. Performance is evaluated through function testing and image reconstruction, with examples from Dunhuang murals and the Forbidden City. The improved algorithm achieved fast convergence (50 iterations for uni-modal, 80 for multi-modal functions) and high robustness (98.6% success rate, 0.35 s runtime). Rendering optimization increased contrast and color saturation significantly, enhancing visual appeal and cultural conveyance. This research innovatively combines improved optimization algorithms with deep learning and color theory, offering an efficient technical solution for digital cultural heritage preservation and interactive display, promoting cultural diversity and heritage appreciation.",enhance efficiency effectiveness cultural heritage scene rendering enabling dynamic interaction immersive experiences cultural inheritance study improves grey wolf optimization algorithm integrates deep learning oversampling training optimizes rendering based color attribute theory performance evaluated function testing image reconstruction examples dunhuang murals forbidden city improved algorithm achieved fast convergence iterations uni modal multi modal functions high robustness success rate runtime rendering optimization increased contrast color saturation significantly enhancing visual appeal cultural conveyance research innovatively combines improved optimization algorithms deep learning color theory offering efficient technical solution digital cultural heritage preservation interactive display promoting cultural diversity heritage appreciation
"With the rapid development of artificial intelligence technology, large language models have become a key force in promoting the transformation of many industries. This study aims to explore the application of large language model in power industry training, especially how it can be used as an intelligent coaching tool to optimize training process and improve learning results. The research first reviews the current traditional methods of power industry training, and introduces the basic principles of the large language model and its application potential in the field of education. By designing and conducting a comparison experiment involving traditional and intelligent tutoring methods, this study assesses the effectiveness of large language models in improving learning efficiency and knowledge mastery. The experimental results show that the intelligent tutoring method is superior to the traditional method in knowledge transfer efficiency, skill mastery, and application effect. The optimization strategy based on the large language model is proposed and the feasibility of its implementation is analyzed. Finally, the application prospect of large language model in electric power industry training is summarized, and the future research direction is discussed. The hope provides a new perspective for the power industry training field and demonstrates the application potential of AI technology in professional training.",rapid development artificial intelligence technology large language models become key force promoting transformation many industries study aims explore application large language model power industry training especially used intelligent coaching tool optimize training process improve learning results research first reviews current traditional methods power industry training introduces basic principles large language model application potential field education designing conducting comparison experiment involving traditional intelligent tutoring methods study assesses effectiveness large language models improving learning efficiency knowledge mastery experimental results show intelligent tutoring method superior traditional method knowledge transfer efficiency skill mastery application effect optimization strategy based large language model proposed feasibility implementation analyzed finally application prospect large language model electric power industry training summarized future research direction discussed hope provides new perspective power industry training field demonstrates application potential technology professional training
"Against the backdrop of implementing the national carbon peaking and carbon neutrality strategy, this study proposes a standardized evaluation system for building low (zero) carbon industrial parks, which encompasses principles of construction, evaluation methods, indicators system, data collection, indicators calculation, evaluation results, and implementation steps. This proposal follows a deep understanding on the challenges facing industrial parks, such as trade barriers and discrepancies in technical standards, as well as opportunities under the context of high-quality development. Through quantitative analysis on the layout and distribution characteristics of government-approved industrial parks, it figures out the status quo of industrial parks in the development zone. Moreover, a case study of an economic development zone in the southeastern coastal area is made to validate the evaluation. Finally, this paper gives suggestions on the dynamic adjustment of indicators of the evaluation system and the collection of full-sample data.",backdrop implementing national carbon peaking carbon neutrality strategy study proposes standardized evaluation system building low zero carbon industrial parks encompasses principles construction evaluation methods indicators system data collection indicators calculation evaluation results implementation steps proposal follows deep understanding challenges facing industrial parks trade barriers discrepancies technical standards well opportunities context high quality development quantitative analysis layout distribution characteristics government approved industrial parks figures status quo industrial parks development zone moreover case study economic development zone southeastern coastal area made validate evaluation finally paper gives suggestions dynamic adjustment indicators evaluation system collection full sample data
"This study aims to explore the construction of an English teaching evaluation system based on artificial intelligence and its application effect. By designing a questionnaire survey and employing various data analysis methods, including Pearson correlation coefficient and Spear man rank correlation coefficient, the paper examines the relationship between learners’ frequency of using AI English learning platform functions and their satisfaction, the background of using teaching tools, and their views on the teaching evaluation system. The results indicate that younger learners use the AI learning platform more frequently. High satisfaction with personalized learning plans and real-time feedback features is positively correlated with increased learner engagement. Integrating AI tools into classroom teaching can significantly enhance learning efficiency. The study underscores the critical role of personalized instruction and immediate feedback in improving teaching quality and learning outcomes, providing valuable insights for the future deep application of AI in education.",study aims explore construction english teaching evaluation system based artificial intelligence application effect designing questionnaire survey employing various data analysis methods including pearson correlation coefficient spear man rank correlation coefficient paper examines relationship learners frequency using english learning platform functions satisfaction background using teaching tools views teaching evaluation system results indicate younger learners use learning platform frequently high satisfaction personalized learning plans real time feedback features positively correlated increased learner engagement integrating tools classroom teaching significantly enhance learning efficiency study underscores critical role personalized instruction immediate feedback improving teaching quality learning outcomes providing valuable insights future deep application education
"As the digital age progresses, gamified learning emerges as a transformative force in education, especially in the realm of environmental studies. This research delves into enhancing environmental education by integrating gaming principles with educational objectives, aiming to create immersive learning experiences. However, traditional gamification approaches in environmental education often overlook the intricate group dynamics and individual learning trajectories of students. Addressing these gaps, this study focuses on optimizing task recommendations and game progression in gamified learning environments through the implementation of advanced attention mechanisms and bidirectional long-short term memory (Bi-LSTM) neural networks. These technologies enable precise predictions of students’ evolving preferences and facilitate the customization of learning tasks, thereby enriching the educational experience. Furthermore, the study explores adaptive strategies for modifying game progression based on real-time learning outcomes, ensuring that educational content remains challenging yet attainable. The insights gained from this research provide a robust theoretical framework and practical tools for effectively employing gamification strategies in environmental education, thereby fostering deeper student engagement and a profound understanding of environmental issues. This study explores the adaptive adjustment of progress plans in gamified learning by focusing on identifying deviation intensity, processing priorities, and determining the appropriate range of deviation values. These findings not only optimize the gamified learning structure but also enhance the personalized instructional effectiveness of educational games.",digital age progresses gamified learning emerges transformative force education especially realm environmental studies research delves enhancing environmental education integrating gaming principles educational objectives aiming create immersive learning experiences however traditional gamification approaches environmental education often overlook intricate group dynamics individual learning trajectories students addressing gaps study focuses optimizing task recommendations game progression gamified learning environments implementation advanced attention mechanisms bidirectional long short term memory lstm neural networks technologies enable precise predictions students evolving preferences facilitate customization learning tasks thereby enriching educational experience furthermore study explores adaptive strategies modifying game progression based real time learning outcomes ensuring educational content remains challenging yet attainable insights gained research provide robust theoretical framework practical tools effectively employing gamification strategies environmental education thereby fostering deeper student engagement profound understanding environmental issues study explores adaptive adjustment progress plans gamified learning focusing identifying deviation intensity processing priorities determining appropriate range deviation values findings optimize gamified learning structure also enhance personalized instructional effectiveness educational games
"As a key component of heavy gantry CNC machine tools, the accuracy reliability of segmented beam has an important influence on the machining accuracy reliability of machine tools. First, the dynamic response analysis was carried out by using the finite element model of segmented beam considering the influence of joint surface. Then, based on the stress-strength interference theory, a processing reliability model of segmented beams was proposed. Finally, the above model was used to study the process reliability analysis under different cutting parameters and different tool parameters, and the engineering solution is given, which provides a basis for improving the machining accuracy and retention of heavy CNC machine tools.",key component heavy gantry cnc machine tools accuracy reliability segmented beam important influence machining accuracy reliability machine tools first dynamic response analysis carried using finite element model segmented beam considering influence joint surface based stress strength interference theory processing reliability model segmented beams proposed finally model used study process reliability analysis different cutting parameters different tool parameters engineering solution given provides basis improving machining accuracy retention heavy cnc machine tools
"In the era of complicated information, consumer behavior mining is of great significance to enterprises and markets, however, the current information processing technology is difficult to comprehensively mine and segment consumer data. The study aims to conduct an effective analysis of consumer behavior. And for the shortcomings of the current consumer behavior mining algorithms, the study proposes an improved consumer behavior data mining algorithm based on the map reduce model. After the experimental analysis, the results revealed that the research algorithm has the closest Mahalanobis Distances compared to the two algorithms, fuzzy C-means and density-based spatial clustering of application with noise, indicating that the research algorithm is more effective in clustering. The average clustering accuracy of K-means clustering algorithm (K-means) based on Andersori’s Iris data seto dataset was 93.2%, and the average clustering accuracy of the two datasets Glass and Wine was 94.3% and 93.8%, respectively. The research methodology categorized consumers into three classes based on their transaction frequency and transaction amount. Among the consumers in cluster 1, the total transaction amount was in the range of 0.62–0.82, the transaction frequency was between 0.41 and 0.72, and the number of transactions was between 0.72 and 0.94, which shows that the consumers in this cluster belonged to the group of moderately active and high consumption. The above data indicate that this method, through the collaborative optimization of MapReduce and K-means, has an accuracy rate of over 90% in cross-industry scenarios. It effectively solves the problems of low efficiency, poor accuracy and weak adaptability of traditional algorithms, providing a quantifiable technical solution for the research of consumer behavior.",complicated information consumer behavior mining great significance enterprises markets however current information processing technology difficult comprehensively mine segment consumer data study aims conduct effective analysis consumer behavior shortcomings current consumer behavior mining algorithms study proposes improved consumer behavior data mining algorithm based map reduce model experimental analysis results revealed research algorithm closest mahalanobis distances compared two algorithms fuzzy means density based spatial clustering application noise indicating research algorithm effective clustering average clustering accuracy means clustering algorithm means based andersori iris data seto dataset average clustering accuracy two datasets glass wine respectively research methodology categorized consumers three classes based transaction frequency transaction amount among consumers cluster total transaction amount range transaction frequency number transactions shows consumers cluster belonged group moderately active high consumption data indicate method collaborative optimization mapreduce means accuracy rate cross industry scenarios effectively solves problems low efficiency poor accuracy weak adaptability traditional algorithms providing quantifiable technical solution research consumer behavior
"The classification of performance skills of national vocal music is an existing problem in music research and education, and the traditional manual classification method is time-consuming and labor-intensive. To solve this problem, this article studied the automatic classification system of performance skills of national vocal music. Specifically, it is to select audio samples from the MTG-QBH dataset for audio preprocessing, feature extraction, and training and testing of classification models. In the audio preprocessing stage, spectrum subtraction is used for denoising, data standardization, mute processing and data enhancement to ensure the consistency and quality of audio data. MFCC method is used to extract audio features and convert audio signals into feature vectors for classification. Based on these feature vectors, SVM model is used for training and classification, and different vocal skills are accurately identified. The experimental results show that the system has excellent performance in classification accuracy, precision, recall rate and F1 score, all reaching about 0.95. The experiment shows that the system can effectively classify various ethnic vocal techniques, improving classification efficiency and accuracy.",classification performance skills national vocal music existing problem music research education traditional manual classification method time consuming labor intensive solve problem article studied automatic classification system performance skills national vocal music specifically select audio samples mtg qbh dataset audio preprocessing feature extraction training testing classification models audio preprocessing stage spectrum subtraction used denoising data standardization mute processing data enhancement ensure consistency quality audio data mfcc method used extract audio features convert audio signals feature vectors classification based feature vectors svm model used training classification different vocal skills accurately identified experimental results show system excellent performance classification accuracy precision recall rate score reaching experiment shows system effectively classify various ethnic vocal techniques improving classification efficiency accuracy
"As virtual reality (VR) technology is widely used in various fields, particularly in dance education and training, precise dance posture capture and correction techniques are key to enhancing the quality of teaching and learning efficiency. Although current dance posture estimation technologies have made some progress, there are still deficiencies in the accurate capture of three-dimensional spatial information and real-time analysis of dynamic postures. This study addresses the limitations of existing technologies and proposes a new method combining YOLO-V3 with a cascaded convolutional neural network (CNN), aimed at improving the accuracy and real-time performance of dance motion capture in VR environments. This paper proposes an innovative three-dimensional dance pose estimation algorithm that combines YOLO-V3-based human localization with three-dimensional cropping technology and a three-stage cascaded CNN system to significantly improve the precision and accuracy of dance poses in VR environments. The method demonstrates its superiority in generating accurate human models and handling real-world data by optimizing pre-trained weights and combining synthetic and real datasets. A series of experimental verifications demonstrate that both technologies significantly improve the accuracy and real-time feedback capabilities of dance motion analysis, showcasing potential applications in the fusion of art and technology. The findings of this paper not only provide technical support for dance training in VR but also offer new research ideas and methods for intelligent analysis of complex human movements, possessing high theoretical value and practical application prospects.",virtual reality technology widely used various fields particularly dance education training precise dance posture capture correction techniques key enhancing quality teaching learning efficiency although current dance posture estimation technologies made progress still deficiencies accurate capture three dimensional spatial information real time analysis dynamic postures study addresses limitations existing technologies proposes new method combining yolo cascaded convolutional neural network cnn aimed improving accuracy real time performance dance motion capture environments paper proposes innovative three dimensional dance pose estimation algorithm combines yolo based human localization three dimensional cropping technology three stage cascaded cnn system significantly improve precision accuracy dance poses environments method demonstrates superiority generating accurate human models handling real world data optimizing pre trained weights combining synthetic real datasets series experimental verifications demonstrate technologies significantly improve accuracy real time feedback capabilities dance motion analysis showcasing potential applications fusion art technology findings paper provide technical support dance training also offer new research ideas methods intelligent analysis complex human movements possessing high theoretical value practical application prospects
"To address the issue of insufficient positioning accuracy caused by faults in the transmission lines between substations, this study established a single-terminal travelling wave ranging model and proposed an optimized wavelet transform-based method for processing travelling wave signals. Using MATLAB as the simulation platform, we conducted simulations of single-terminal travelling wave ranging. The critical challenge in this method lies in accurately identifying the arrival time of the travelling wave signals. To address this, two different wavelet basis functions were selected to process the travelling wave signals, and their effectiveness in processing the same fault at different locations was compared and analyzed. The results show that the db4 wavelet better retains the original signal features. The error range for the Haar wavelet was 0.4289–0.6948 km, while for the db4 wavelet it was 0.0301–0.2960 km. Thus, the db4 wavelet provides more accurate ranging than the Haar wavelet.",address issue insufficient positioning accuracy caused faults transmission lines substations study established single terminal travelling wave ranging model proposed optimized wavelet transform based method processing travelling wave signals using matlab simulation platform conducted simulations single terminal travelling wave ranging critical challenge method lies accurately identifying arrival time travelling wave signals address two different wavelet basis functions selected process travelling wave signals effectiveness processing fault different locations compared analyzed results show wavelet better retains original signal features error range haar wavelet wavelet thus wavelet provides accurate ranging haar wavelet
"Property engineering project management is faced with increasingly complex risks and challenges. If these risks are not effectively controlled, it is easy to lead to cost overruns, time delays and even project failures. This study proposes a property engineering construction safety risk assessment method based on the IT2FS-MARCOS method to address the uncertainty of risk assessment caused by the subjectivity of expert decision-making and the problem of considering multiple risk factors under different risk parameter weights. A property engineering construction safety risk indicator evaluation system was constructed, and the combination of the G1 weighting method and the objective weighting method (CRITIC) was used to analyze the weights of each risk indicator. Interval type-2 fuzzy sets (IT2FS) were introduced to improve the compromise-based alternative ranking method (MARCOS) in multicriteria decision-making. The utility function values of each risk factor were calculated, and the construction safety risk factors of property engineering projects were analyzed and evaluated based on the utility function values. The findings reveal the dynamic process of construction risks in property engineering projects. The model not only considers the fuzziness of expert evaluation but also adopts a multicriteria decision-making method to avoid simple weighting of risk parameters. This method has a high degree of fit with engineering practice and provides a new approach of handling risk assessment in uncertain information of property engineering.",property engineering project management faced increasingly complex risks challenges risks effectively controlled easy lead cost overruns time delays even project failures study proposes property engineering construction safety risk assessment method based marcos method address uncertainty risk assessment caused subjectivity expert decision making problem considering multiple risk factors different risk parameter weights property engineering construction safety risk indicator evaluation system constructed combination weighting method objective weighting method critic used analyze weights risk indicator interval type fuzzy sets introduced improve compromise based alternative ranking method marcos multicriteria decision making utility function values risk factor calculated construction safety risk factors property engineering projects analyzed evaluated based utility function values findings reveal dynamic process construction risks property engineering projects model considers fuzziness expert evaluation also adopts multicriteria decision making method avoid simple weighting risk parameters method high degree fit engineering practice provides new approach handling risk assessment uncertain information property engineering
"Under corporate crises, the formation of credit relationships based on group-level credit reciprocity strategies is a multi-factorial phenomenon. From the perspective of a fully mixed corporate group, the research explores the inadequacy of traditional snowdrift game models in adapting to the structural elements of credit relationship formation. Firstly, it improves the model by introducing inter-temporal costs, benefits, and crisis randomness of credit reciprocity as decision parameters. Secondly, it introduces inter-temporal credit reciprocity as a condition to allow the snowdrift game model to express the evolution between corporate groups. Finally, based on credit reciprocity, the mechanism of credit relationship formation is verified through numerical simulations. The results show that under the crisis probability, the formation of credit relationships exhibits a “threshold trigger,” and the inter-firm assistance and crisis probability play an important role in establishing credit relationships between enterprises.",corporate crises formation credit relationships based group level credit reciprocity strategies multi factorial phenomenon perspective fully mixed corporate group research explores inadequacy traditional snowdrift game models adapting structural elements credit relationship formation firstly improves model introducing inter temporal costs benefits crisis randomness credit reciprocity decision parameters secondly introduces inter temporal credit reciprocity condition allow snowdrift game model express evolution corporate groups finally based credit reciprocity mechanism credit relationship formation verified numerical simulations results show crisis probability formation credit relationships exhibits threshold trigger inter firm assistance crisis probability play important role establishing credit relationships enterprises
"There is often a latent period before the outbreak of financial crisis in enterprises, and reliable financial crisis warning is crucial for the stable development of enterprises. At present, significant achievements have been made in the field of financial crisis prediction, but the systematic research on the mechanism of financial crisis formation is still insufficient. This study aims to deeply analyze the formation mechanism of chronic financial crises, improve the accuracy of financial crisis warning by introducing a penalty logistic regression model, and provide a basis for enterprises to prevent, control, and overcome difficulties. This study uses the financial statement data of 2155 listed companies in Shanghai and Shenzhen from 2017 to 2024, and combines the three penalty functions of Lasso, SCAD and MCP to analyze these data. Through cross-validation, this paper selects the optimal adjustment parameters, constructs a penalized logistic regression model, and compares it with the standard logistic regression model. Research has shown that the MCP penalty logistic regression model performs the best in accuracy (89.6 ± 0.8%), F1 score (0.852), AUC value (0.918), and other aspects, outperforming the standard logistic regression model and other penalty logistic regression models. The MCP model identified 15 important financial indicators that have a significant predictive effect on the company’s financial crisis. Overall, the financial crisis warning model combined with MCP penalty logistic regression performs well in prediction accuracy and variable selection ability, providing an effective financial crisis warning tool for enterprises.",often latent period outbreak financial crisis enterprises reliable financial crisis warning crucial stable development enterprises present significant achievements made field financial crisis prediction systematic research mechanism financial crisis formation still insufficient study aims deeply analyze formation mechanism chronic financial crises improve accuracy financial crisis warning introducing penalty logistic regression model provide basis enterprises prevent control overcome difficulties study uses financial statement data listed companies shanghai shenzhen combines three penalty functions lasso scad mcp analyze data cross validation paper selects optimal adjustment parameters constructs penalized logistic regression model compares standard logistic regression model research shown mcp penalty logistic regression model performs best accuracy score auc value aspects outperforming standard logistic regression model penalty logistic regression models mcp model identified important financial indicators significant predictive effect company financial crisis overall financial crisis warning model combined mcp penalty logistic regression performs well prediction accuracy variable selection ability providing effective financial crisis warning tool enterprises
"Reliable grid management is critical for the efficiency and stability of electrical transmission and distribution systems. As grid systems become more sophisticated and complex, there is an increased need for digital asset management and automation to make operations more efficient. A lot of standard grid management techniques include manual physical inspection and repair of assets. These methods tend to take too long and raise operational costs. The proposed project will improve the grid management methods by employing both digital asset feature identification and automation. The intention is to promote greater operational efficiency through automation identified through real-time monitoring of decision-making processes. The proposed process will include identifying digital maintenance features on grid assets, including transformers, electricity lines, and meters, using IoT sensors. To standardize the input values, the data is pre-processed with procedures such as z-score normalization. Features including voltage, temperature, and current are extracted using the Discrete Wavelet Transform (DWT). Attention-based Bidirectional Gated Recurrent Units with Grid Search Optimization (AttenBi-GRU-GSO) method are used to analyze the processed data to predict the faults and optimize performance. The AttenBi-GRU-GSO combines the attention mechanism to focus on critical features, Bi-GRU to capture sequential dependencies from both past and future, and GSO to fine-tune the model’s hyperparameters for optimal performance, ensuring efficient and accurate fault detection and prediction in grid management. Experimental results demonstrate that the AttenBi-GRU-GSO method achieves superior performance, with an accuracy of 95.31%, an MAE loss of 0.11, a total loss of 0.07, and an RMSE loss of 0.03. Automated systems provide control of detected problems by responding with remedial actions, including rerouting power and organizing maintenance. The automation increases the accuracy and speed of fault detection, as well as reduces downtime and maintenance costs. Digital asset feature identification and automation enhances grid management, lowers operating costs, and increases total electrical network reliability. The proposed technology provides a scalable platform for current grid operations.",reliable grid management critical efficiency stability electrical transmission distribution systems grid systems become sophisticated complex increased need digital asset management automation make operations efficient lot standard grid management techniques include manual physical inspection repair assets methods tend take long raise operational costs proposed project improve grid management methods employing digital asset feature identification automation intention promote greater operational efficiency automation identified real time monitoring decision making processes proposed process include identifying digital maintenance features grid assets including transformers electricity lines meters using iot sensors standardize input values data pre processed procedures score normalization features including voltage temperature current extracted using discrete wavelet transform dwt attention based bidirectional gated recurrent units grid search optimization attenbi gru gso method used analyze processed data predict faults optimize performance attenbi gru gso combines attention mechanism focus critical features gru capture sequential dependencies past future gso fine tune model hyperparameters optimal performance ensuring efficient accurate fault detection prediction grid management experimental results demonstrate attenbi gru gso method achieves superior performance accuracy mae loss total loss rmse loss automated systems provide control detected problems responding remedial actions including rerouting power organizing maintenance automation increases accuracy speed fault detection well reduces downtime maintenance costs digital asset feature identification automation enhances grid management lowers operating costs increases total electrical network reliability proposed technology provides scalable platform current grid operations
"With the booming development of e-commerce, personalized recommendation systems have become the key to attracting users. However, traditional recommendation systems face user diversity and information overload, driving the search for more intelligent and adaptable solutions. Therefore, a multi-recommendation model for e-commerce is proposed by integrating backpropagation algorithm and genetic algorithm. The backpropagation algorithm is used to learn user historical behavior data and establish an initial recommendation model. Then, the genetic algorithm is applied to evolve and optimize the model to meet the personalized needs. Through continuous iteration and evolution, the recommendation model can better capture the potential interests and behavioral patterns of users. The proposed method reached the target value after 51 iterations, which was significantly faster than that of the pre-optimized model. The latter reached the standard after 100 iterations. In terms of error, the highest value was 43, with an average of about 15, while the highest value of the optimized network was only 40, with an average of about 12. Therefore, the optimized network performed significantly better than the pre-optimized network. The multi-recommendation mode based on optimized backpropagation algorithm performs better than traditional methods. This study provides a new approach for the design and optimization of e-commerce recommendation systems.",booming development commerce personalized recommendation systems become key attracting users however traditional recommendation systems face user diversity information overload driving search intelligent adaptable solutions therefore multi recommendation model commerce proposed integrating backpropagation algorithm genetic algorithm backpropagation algorithm used learn user historical behavior data establish initial recommendation model genetic algorithm applied evolve optimize model meet personalized needs continuous iteration evolution recommendation model better capture potential interests behavioral patterns users proposed method reached target value iterations significantly faster pre optimized model latter reached standard iterations terms error highest value average highest value optimized network average therefore optimized network performed significantly better pre optimized network multi recommendation mode based optimized backpropagation algorithm performs better traditional methods study provides new approach design optimization commerce recommendation systems
"The methods used in sports training are essential for bridging sports theory and practical application, playing a vital role in achieving effective training outcomes. Enhancing training efficiency through the most effective methods has long been a core focus in physical conditioning. With advancements in computer technology, virtual reality (VR), known for its immersive human–computer interaction capabilities, is increasingly being applied across various fields. This study aims to explore how VR can be leveraged to optimize sports training methods, potentially transforming traditional practices and improving training effectiveness. This article is based on virtual reality technology to the national, one of the most popular sport of badminton, for example, to talk about how to use virtual reality technology of machine vision in the pre-match practice auxiliary optimization control to help athletes, through the human limbs and head of the visual control and motion capture algorithm optimization, the final result shows that the presented training method. Compared with the traditional training method, the training efficiency is increased by 7%, and the training result accuracy is increased by 13%.",methods used sports training essential bridging sports theory practical application playing vital role achieving effective training outcomes enhancing training efficiency effective methods long core focus physical conditioning advancements computer technology virtual reality known immersive human computer interaction capabilities increasingly applied across various fields study aims explore leveraged optimize sports training methods potentially transforming traditional practices improving training effectiveness article based virtual reality technology national one popular sport badminton example talk use virtual reality technology machine vision pre match practice auxiliary optimization control help athletes human limbs head visual control motion capture algorithm optimization final result shows presented training method compared traditional training method training efficiency increased training result accuracy increased
"With the development of information technology, the hotel industry needs to utilize advanced technologies and methods to improve the accuracy and efficiency of room forecasting and pricing in order to adapt to the changes in the market and the pressure of competition. This paper adopts a combination of quantitative and qualitative methods, following two phases: (1) data analysis phase, using statistical analysis and visualization techniques to clean, process, describe, and explore the collected data related to hotel rooms; (2) model building phase, using machine learning techniques to construct and train a deep neural network model to achieve the prediction of hotel rooms, and using reinforcement learning techniques that realizes the pricing of hotel rooms. In this paper, theoretical and practical experiments are conducted to compare and evaluate with existing research methods and models from different perspectives and levels. The results show that the model proposed in this paper outperforms other models in all indicators, with higher prediction accuracy and pricing efficiency, as well as good generalization and adaptation capabilities. The research in this paper has important theoretical and practical significance for revenue management and competitive strategies in the hotel industry.",development information technology hotel industry needs utilize advanced technologies methods improve accuracy efficiency room forecasting pricing order adapt changes market pressure competition paper adopts combination quantitative qualitative methods following two phases data analysis phase using statistical analysis visualization techniques clean process describe explore collected data related hotel rooms model building phase using machine learning techniques construct train deep neural network model achieve prediction hotel rooms using reinforcement learning techniques realizes pricing hotel rooms paper theoretical practical experiments conducted compare evaluate existing research methods models different perspectives levels results show model proposed paper outperforms models indicators higher prediction accuracy pricing efficiency well good generalization adaptation capabilities research paper important theoretical practical significance revenue management competitive strategies hotel industry
"In an era marked by the rapid ascension of the global digital economy, the digital innovation ecosystem has emerged as a vital conduit for businesses to pursue digital innovation endeavors and enhance their core competitiveness. This paper marries the theoretical discourse and empirical analysis of digital innovation ecosystem theory with modular theory, systematically expounding the concepts of digital innovation ecosystems and modularity, as well as elucidating the design principles of a modular digital innovation ecosystem. It delves into the characteristics and design processes of modularity and dissects the collaborative mechanisms underpinning the Apollo digital innovation ecosystem. Moreover, employing a modular perspective, it empirically scrutinizes the Apollo digital innovation ecosystem to clarify the framework constituted by modularity, the symbiotic operation among the ecosystem modules, and the collaboration within the system’s internal modules. This study embarks from the vantage point of digital innovation ecosystems and modularity to offer an in-depth explanation of the collaborative mechanisms within the Apollo digital innovation ecosystem. It not only broadens the methodological horizons for research on digital innovation ecosystem theory but also delineates, from a practical application standpoint, the modes of collaboration between the ecosystem’s modules. In the process of exploring the collaborative mechanisms of the Apollo digital innovation ecosystem, this research integrates global autonomous driving technology with the digital innovation ecosystem in a modular analysis. The innovative contribution of this paper lies in the construction of a collaborative mechanism for the Apollo digital innovation ecosystem from a modular viewpoint.",marked rapid ascension global digital economy digital innovation ecosystem emerged vital conduit businesses pursue digital innovation endeavors enhance core competitiveness paper marries theoretical discourse empirical analysis digital innovation ecosystem theory modular theory systematically expounding concepts digital innovation ecosystems modularity well elucidating design principles modular digital innovation ecosystem delves characteristics design processes modularity dissects collaborative mechanisms underpinning apollo digital innovation ecosystem moreover employing modular perspective empirically scrutinizes apollo digital innovation ecosystem clarify framework constituted modularity symbiotic operation among ecosystem modules collaboration within system internal modules study embarks vantage point digital innovation ecosystems modularity offer depth explanation collaborative mechanisms within apollo digital innovation ecosystem broadens methodological horizons research digital innovation ecosystem theory also delineates practical application standpoint modes collaboration ecosystem modules process exploring collaborative mechanisms apollo digital innovation ecosystem research integrates global autonomous driving technology digital innovation ecosystem modular analysis innovative contribution paper lies construction collaborative mechanism apollo digital innovation ecosystem modular viewpoint
"This study examines the effects of Virtual Reality (VR) technology on the safety psychological empowerment and risk perception as well as avoidance behavior of cyber-physical system monitoring high-risk mining personnel. We conducted a randomized controlled experiment with 72 mining professionals to assess the effectiveness of a comprehensive VR training system featuring network security elements on behavioral awareness, contrasted with traditional training approaches. The study specifically examines underground mining personnel across four high-risk operational roles, employing a randomized controlled trial design with validated psychological empowerment measures based on established empowerment and social cognitive theories. The results showed that the experimental group outperformed the control group in all safety metrics evaluated, with the greatest gains in correct identification of threats (experimental group: 82.5% vs control group: 52.6%) and response times to integrated physical-digital threats (41.5% faster). The data indicate that psychological empowerment, particularly strong competence perception (r = 0.74) and impact (r = 0.65) perceptions, is an essential mediator between VR training and safety outcomes. The Security-Safety Risk Integration model developed in this research aids in explaining the interplay of the multiple theoretical elements such as human perception, technical vulnerabilities, and psychological empowerment elements within a complex mining environment. These findings enhance the understanding of risk perception from a theoretical perspective and practical measures for safety-security training in advanced industrial systems.",study examines effects virtual reality technology safety psychological empowerment risk perception well avoidance behavior cyber physical system monitoring high risk mining personnel conducted randomized controlled experiment mining professionals assess effectiveness comprehensive training system featuring network security elements behavioral awareness contrasted traditional training approaches study specifically examines underground mining personnel across four high risk operational roles employing randomized controlled trial design validated psychological empowerment measures based established empowerment social cognitive theories results showed experimental group outperformed control group safety metrics evaluated greatest gains correct identification threats experimental group control group response times integrated physical digital threats faster data indicate psychological empowerment particularly strong competence perception impact perceptions essential mediator training safety outcomes security safety risk integration model developed research aids explaining interplay multiple theoretical elements human perception technical vulnerabilities psychological empowerment elements within complex mining environment findings enhance understanding risk perception theoretical perspective practical measures safety security training advanced industrial systems
"With the rapid advancement of educational technology, the complexity of attention allocation among university students in information-rich learning environments has significantly increased. Effective attention allocation is crucial for learning outcomes; however, traditional research methods face limitations in capturing and analyzing dynamically changing cognitive processes. This study aims to utilize neural network technology, particularly the model based on Time-series generative adversarial network (TimeGAN), to identify and analyze the patterns of attention allocation in university students’ cognitive processes. The TimeGAN model, selected for its proficiency in handling time-series data, is employed to reveal the dynamic shifts of attention across various tasks and states. The second part introduces a novel framework that combines a Customized Gate Control (CGC) model with a progressive hierarchical extraction model, aiming to more accurately simulate the management of attention switching in a multitasking environment. These approaches enhance the model’s adaptability to individual differences, offering support for the formulation of personalized learning strategies. Not only does this research expand the application of computational models in the field of educational psychology, but it also provides new theoretical foundations and practical tools for optimizing teaching designs and promoting cognitive development among students.",rapid advancement educational technology complexity attention allocation among university students information rich learning environments significantly increased effective attention allocation crucial learning outcomes however traditional research methods face limitations capturing analyzing dynamically changing cognitive processes study aims utilize neural network technology particularly model based time series generative adversarial network timegan identify analyze patterns attention allocation university students cognitive processes timegan model selected proficiency handling time series data employed reveal dynamic shifts attention across various tasks states second part introduces novel framework combines customized gate control cgc model progressive hierarchical extraction model aiming accurately simulate management attention switching multitasking environment approaches enhance model adaptability individual differences offering support formulation personalized learning strategies research expand application computational models field educational psychology also provides new theoretical foundations practical tools optimizing teaching designs promoting cognitive development among students
"In response to the problem of insufficient adaptability of parameterized learning in local observation graph signal scenes in multi-task learning scenarios, a graph signal processing technique for multi-task scenarios is studied and introduced. In this technology, the research is based on the improved diffusion least mean square algorithm for clustering sampling, using different graph filters to train parameters and a clustering strategy to complete sampling. In addition, a multi-task power grid voltage graph signal detection technique with improved high pass filtering is proposed, and power grid data security detection is achieved through threshold setting and projection detection. In the analysis of graph signal processing, the research model has better steady-state learning performance. For example, in the transient mean square deviation test of multitasking scenarios, its convergence value is −31.25 dB, which is better than similar models. In the single-task transient mean square deviation test, the research model is closer to the reference standard, with a convergence value of −34.25 dB and better learning ability. In power application analysis, the research model performs better in graph signal detection, such as in voltage amplitude scenarios where the maximum detection probability is 100%. In the voltage angle scenario, when the voltage angle is higher than ±−2°, the detection probability of the research model is higher than 92.25%. It can be seen that the technology proposed by the research has excellent application effects. This study will provide technical support for the improvement of graph signal technology and data security.",response problem insufficient adaptability parameterized learning local observation graph signal scenes multi task learning scenarios graph signal processing technique multi task scenarios studied introduced technology research based improved diffusion least mean square algorithm clustering sampling using different graph filters train parameters clustering strategy complete sampling addition multi task power grid voltage graph signal detection technique improved high pass filtering proposed power grid data security detection achieved threshold setting projection detection analysis graph signal processing research model better steady state learning performance example transient mean square deviation test multitasking scenarios convergence value better similar models single task transient mean square deviation test research model closer reference standard convergence value better learning ability power application analysis research model performs better graph signal detection voltage amplitude scenarios maximum detection probability voltage angle scenario voltage angle higher detection probability research model higher seen technology proposed research excellent application effects study provide technical support improvement graph signal technology data security
"Student behavior recognition is of great significance in the intelligent classroom environment for improving teaching quality, achieving personalized learning, and optimizing classroom management. However, the accuracy and real-time performance of existing technologies in complex scenarios still have limitations. To improve the accuracy and real-time performance of student behavior recognition, the study improved the eighth-generation model of “You Only Look Once” based on the squeezing—incentive attention mechanism, feature pyramid network structure, and anchor box mechanism, significantly enhancing the accuracy and robustness of student behavior recognition. The SE attention mechanism improves the efficiency of feature extraction by enhancing the dependency relationship between feature channels. The FPN structure enhances the multi-scale feature fusion ability by fusing features at different levels. Meanwhile, combined with the DeepSort real-time multi-object tracking algorithm based on deep learning, the problems of identity switching and trajectory loss in object tracking have been effectively solved, continuous tracking of students’ behaviors has been achieved, and the real-time performance has been significantly improved. In the experimental results, the average accuracies of the improved You Only Look Once eighth-generation model on the SCB-dataset3 and CampusGuard datasets reached 82.3% and 83.0%, respectively, which was significantly better than that of the control models. The multi-target tracking accuracy of the DeepSort algorithm is 84.2% and 83.7% respectively, and it also performs well in terms of robustness and real-time performance. The results show that the improved You Only Look Once eighth-generation model and the real-time multi-object tracking algorithm based on deep learning can effectively improve the performance of student behavior recognition and tracking, providing strong technical support for teaching management and personalized learning in the intelligent classroom environment.",student behavior recognition great significance intelligent classroom environment improving teaching quality achieving personalized learning optimizing classroom management however accuracy real time performance existing technologies complex scenarios still limitations improve accuracy real time performance student behavior recognition study improved eighth generation model look based squeezing incentive attention mechanism feature pyramid network structure anchor box mechanism significantly enhancing accuracy robustness student behavior recognition attention mechanism improves efficiency feature extraction enhancing dependency relationship feature channels fpn structure enhances multi scale feature fusion ability fusing features different levels meanwhile combined deepsort real time multi object tracking algorithm based deep learning problems identity switching trajectory loss object tracking effectively solved continuous tracking students behaviors achieved real time performance significantly improved experimental results average accuracies improved look eighth generation model scb dataset campusguard datasets reached respectively significantly better control models multi target tracking accuracy deepsort algorithm respectively also performs well terms robustness real time performance results show improved look eighth generation model real time multi object tracking algorithm based deep learning effectively improve performance student behavior recognition tracking providing strong technical support teaching management personalized learning intelligent classroom environment
"This study addresses the pivotal role of university physics courses in the curriculum of non-physics science and engineering majors, proposing a novel deep learning-driven approach to enhance the instructional design and thereby improve learning efficacy and student engagement. The investigation stems from prevalent issues in contemporary physics education, notably the deficiency in practicality, innovation, and applicability, alongside waning student interest and motivation. The core of the research comprises the meticulous design, meticulous implementation, and real-world application of a transformer-based neural network architecture. Transformers, known for their prowess in handling sequential data, enable a sophisticated understanding and processing of complex physical concepts, marking a significant advancement in educational methodology. Findings reveal that the proposed model excels in delivering swift, precise, and interpretable resolutions to physics challenges, demonstrating exceptional generalization capabilities and adaptability surpassing conventional teaching methodologies. Its performance underscores the potential for transforming the learning landscape by fostering a deeper comprehension of physical phenomena. The overarching significance of this work lies in the establishment of a robust framework for university physics course design. By leveraging the model’s capabilities, it facilitates not only the acquisition of fundamental knowledge but also nurtures critical thinking, problem-solving, and innovative application among students. Consequently, this research pioneers a new paradigm for integrating deep learning technologies into physics pedagogy, offering a blueprint for enhancing educational practices and enriching the learning journey for aspiring scientists and engineers.",study addresses pivotal role university physics courses curriculum non physics science engineering majors proposing novel deep learning driven approach enhance instructional design thereby improve learning efficacy student engagement investigation stems prevalent issues contemporary physics education notably deficiency practicality innovation applicability alongside waning student interest motivation core research comprises meticulous design meticulous implementation real world application transformer based neural network architecture transformers known prowess handling sequential data enable sophisticated understanding processing complex physical concepts marking significant advancement educational methodology findings reveal proposed model excels delivering swift precise interpretable resolutions physics challenges demonstrating exceptional generalization capabilities adaptability surpassing conventional teaching methodologies performance underscores potential transforming learning landscape fostering deeper comprehension physical phenomena overarching significance work lies establishment robust framework university physics course design leveraging model capabilities facilitates acquisition fundamental knowledge also nurtures critical thinking problem solving innovative application among students consequently research pioneers new paradigm integrating deep learning technologies physics pedagogy offering blueprint enhancing educational practices enriching learning journey aspiring scientists engineers
"With the widespread application of deep learning technology in image analysis, visual-based data processing and action recognition have become current research hotspots. To improve the accuracy and real-time evaluation of the quality of aerobics actions, a new lightweight OpenPose network is developed to extract key points of athletes’ skeletons. At the same time, the adaptive multi-scale optimization is carried out on the spatiotemporal graph convolutional network to grasp the temporal characteristics of actions. Finally, a new aerobics action quality evaluation model is designed by integrating Siamese network. The accuracy of the proposed model reached 96.8%, which was 5.3% higher than existing methods. The highest action evaluation matching degree was 89.7%, the highest action improvement rate was 21.34%, the highest action similarity evaluation was 92.17%, and the shortest evaluation time was 0.71 seconds. Its efficiency and effectiveness in the detection and evaluation process are significantly higher than other advanced models. From this, the proposed method for evaluating the quality of aerobics actions has strong application potential while ensuring efficiency and accuracy, and can provide certain technical support for aerobics competitions and actions training.",widespread application deep learning technology image analysis visual based data processing action recognition become current research hotspots improve accuracy real time evaluation quality aerobics actions new lightweight openpose network developed extract key points athletes skeletons time adaptive multi scale optimization carried spatiotemporal graph convolutional network grasp temporal characteristics actions finally new aerobics action quality evaluation model designed integrating siamese network accuracy proposed model reached higher existing methods highest action evaluation matching degree highest action improvement rate highest action similarity evaluation shortest evaluation time seconds efficiency effectiveness detection evaluation process significantly higher advanced models proposed method evaluating quality aerobics actions strong application potential ensuring efficiency accuracy provide certain technical support aerobics competitions actions training
"With the rapid development of artificial intelligence technology in the field of education, it has become an important research field to explore its application and influence in classroom interaction. This study evaluated the effect of artificial intelligence AIDS on teaching effectiveness by comparing the experimental group and the control group’s academic performance, class participation and satisfaction. Studies have found that students who use AI tools perform better in terms of learning outcomes and classroom engagement. In addition, through linear regression models and statistical significance tests, the results show that AI tools have a significant effect in increasing students’ learning interest and satisfaction. However, the study also faced the limitations of sample size and lack of long-term effect analysis. Future studies could expand the sample size and conduct long-term follow-up to fully assess the potential of AI tools for educational applications. This study provides a new perspective for understanding the application of artificial intelligence in the field of education and points the way for future educational practice and technology development.",rapid development artificial intelligence technology field education become important research field explore application influence classroom interaction study evaluated effect artificial intelligence aids teaching effectiveness comparing experimental group control group academic performance class participation satisfaction studies found students use tools perform better terms learning outcomes classroom engagement addition linear regression models statistical significance tests results show tools significant effect increasing students learning interest satisfaction however study also faced limitations sample size lack long term effect analysis future studies could expand sample size conduct long term follow fully assess potential tools educational applications study provides new perspective understanding application artificial intelligence field education points way future educational practice technology development
"With the rapid development of China’s economy, the scale of enterprises is growing, but it is followed by the problem of rapidly expanding accounts receivable. The huge stock of enterprise accounts receivable is bound to occupy a large amount of monetary funds of enterprises. As an indispensable part of China’s multi-level capital market, asset securitization can effectively revitalize enterprise accounts receivable, improve capital utilization and reduce financing costs. However, there is no unified and standardized pricing mechanism and an effective pricing model for securitization products of accounts receivable assets for Chinese enterprises at present. This study introduces the option binomial tree model and simulates the price change of the underlying assets before the option expires by constructing a price tree. So as to calculate the theoretical price of accounts receivable. In order to seek a new idea of risk pricing of enterprise accounts receivable asset securitization under the current unfavorable economic environment.",rapid development china economy scale enterprises growing followed problem rapidly expanding accounts receivable huge stock enterprise accounts receivable bound occupy large amount monetary funds enterprises indispensable part china multi level capital market asset securitization effectively revitalize enterprise accounts receivable improve capital utilization reduce financing costs however unified standardized pricing mechanism effective pricing model securitization products accounts receivable assets chinese enterprises present study introduces option binomial tree model simulates price change underlying assets option expires constructing price tree calculate theoretical price accounts receivable order seek new idea risk pricing enterprise accounts receivable asset securitization current unfavorable economic environment
"This paper econometrically analyzed 5868 Internet insurance-related literature published between 2004 and 2024, examining annual publication trends, countries studied, and research institutions. It found that the number of Internet insurance literature increased by 37% in 2018, reaching its highest level in nearly a decade in 2022, highlighting the booming global Internet insurance sector. Research organizations in various countries have invested their efforts to provide strong support for the healthy development of the sector. Additionally, a systematic review of 30 selected papers was conducted, comprehensively defining Internet insurance and its related concepts, and highlighting its virtual, interactive, convenient, and economic advantages over traditional offline insurance. The development of the Internet has promoted the deep integration of Internet insurance and traditional insurance, accelerated the provision of universal insurance services, and promoted fair competition in the insurance market. However, the paper also revealed the product and operational risks as well as moral hazard challenges faced by Internet insurance. To promote sustainable development, the study proposed optimizing products, strengthening credit assessment mechanisms, and establishing risk early warning mechanisms. This paper is the first to explore the impact and sustainability of Internet insurance using econometric analyses and systematic review methodology, aiming to inspire further in-depth research to promote the sustainable development of Internet insurance and provide greater value and protection for consumers, the industry, and society.",paper econometrically analyzed internet insurance related literature published examining annual publication trends countries studied research institutions found number internet insurance literature increased reaching highest level nearly decade highlighting booming global internet insurance sector research organizations various countries invested efforts provide strong support healthy development sector additionally systematic review selected papers conducted comprehensively defining internet insurance related concepts highlighting virtual interactive convenient economic advantages traditional offline insurance development internet promoted deep integration internet insurance traditional insurance accelerated provision universal insurance services promoted fair competition insurance market however paper also revealed product operational risks well moral hazard challenges faced internet insurance promote sustainable development study proposed optimizing products strengthening credit assessment mechanisms establishing risk early warning mechanisms paper first explore impact sustainability internet insurance using econometric analyses systematic review methodology aiming inspire depth research promote sustainable development internet insurance provide greater value protection consumers industry society
"Smart grid integration and deregulated electricity markets have complicated power marketing ecosystems, necessitating flexible solutions to ensure grid stability, economic competitiveness, and operational efficiency through precise forecasting, real-time decision-making, and enhanced service delivery. Conventional forecasting models struggle with accuracy, scalability, and robust optimization, especially when dealing with non-linear, high-dimensional energy datasets. Moreover, they lack adaptability to volatile demand–supply dynamics and real-time operational constraints. This research presents a next-generation intelligent platform that integrates a Weighted Golden Eagle Optimized Light Gradient Boosting Machine (WGEO-LightGBM) for predictive modeling and operational optimization in electricity marketing and service delivery. Data is collected from IoT-based smart meters and supervisory control and data acquisition (SCADA) systems, encompassing load demand, pricing signals, and historical consumption trends. The collected data were preprocessed using missing value imputation, Min-Max scaling, and Recursive Feature Elimination (RFE) to enhance model input quality and reduce feature redundancy. The WGE algorithm enhances the search efficiency and solution diversity by adaptively adjusting flight dynamics and prey targeting mechanisms. LightGBM, a fast, scalable regression model, is used for short-term load forecasting and price prediction, enabling real-time pricing, demand-side management, efficient service delivery, and resource allocation. The WGEO-LightGBM model predicted load values ranging from approximately 120 MW–160 MW across multiple hourly intervals, closely aligning with actual observations and demonstrating its precision in dynamic grid environments. Its integration into intelligent platforms offers scalable, data-driven solutions for real-time energy management and improved service delivery.",smart grid integration deregulated electricity markets complicated power marketing ecosystems necessitating flexible solutions ensure grid stability economic competitiveness operational efficiency precise forecasting real time decision making enhanced service delivery conventional forecasting models struggle accuracy scalability robust optimization especially dealing non linear high dimensional energy datasets moreover lack adaptability volatile demand supply dynamics real time operational constraints research presents next generation intelligent platform integrates weighted golden eagle optimized light gradient boosting machine wgeo lightgbm predictive modeling operational optimization electricity marketing service delivery data collected iot based smart meters supervisory control data acquisition scada systems encompassing load demand pricing signals historical consumption trends collected data preprocessed using missing value imputation min max scaling recursive feature elimination rfe enhance model input quality reduce feature redundancy wge algorithm enhances search efficiency solution diversity adaptively adjusting flight dynamics prey targeting mechanisms lightgbm fast scalable regression model used short term load forecasting price prediction enabling real time pricing demand side management efficient service delivery resource allocation wgeo lightgbm model predicted load values ranging approximately across multiple hourly intervals closely aligning actual observations demonstrating precision dynamic grid environments integration intelligent platforms offers scalable data driven solutions real time energy management improved service delivery
"Interior design is the art and science of upgrading interior spaces to create a practical and aesthetically pleasant environment. It entails envisioning, planning and implementing designs that take into consideration spatial layout, color schemes, vases and pottery choices, lighting, along with decorative components, while balancing functionality with the client’s preferences and the space’s purpose. The purpose of this research is to develop an innovative VR technology integrated interior design method for living space experience. We propose a novel sea horse optimized-versatile deep convolutional neural network (SHO-VDCNN) algorithm for recognizing innovative interior designs in living spaces. At first we gathered a dataset, to train our proposed recognition model. Image standardization is employed to pre-process the gathered data. In this research, we utilize a VR interior design layout mechanism to improve the potential of autonomous layout interior while enhancing interactions of machines with virtualization. Furthermore, the optimal placements (states) for these internal model components in simulated environments could be spontaneously identified by employing the deep Q-learning network (DQN) method. The proposed model is implemented in Python software. We assessed our suggested framework with various evaluation metrics. We also conducted a comparison analysis to examine the effectiveness of the suggested paradigm. The experimental findings demonstrate that the approached recognition model performed better than other traditional learning models for recognizing interior design frameworks in living spaces.",interior design art science upgrading interior spaces create practical aesthetically pleasant environment entails envisioning planning implementing designs take consideration spatial layout color schemes vases pottery choices lighting along decorative components balancing functionality client preferences space purpose purpose research develop innovative technology integrated interior design method living space experience propose novel horse optimized versatile deep convolutional neural network sho vdcnn algorithm recognizing innovative interior designs living spaces first gathered dataset train proposed recognition model image standardization employed pre process gathered data research utilize interior design layout mechanism improve potential autonomous layout interior enhancing interactions machines virtualization furthermore optimal placements states internal model components simulated environments could spontaneously identified employing deep learning network dqn method proposed model implemented python software assessed suggested framework various evaluation metrics also conducted comparison analysis examine effectiveness suggested paradigm experimental findings demonstrate approached recognition model performed better traditional learning models recognizing interior design frameworks living spaces
"With the progress of information technology, e-commerce has gradually expanded to rural areas, providing new impetus for rural economic growth and transformation. However, the challenges facing rural e-commerce are also relatively complex. This study focuses on how cloud platforms and artificial intelligence can optimize resource management in rural e-commerce. First of all, through the in-depth analysis of the characteristics and development trend of rural e-commerce, the interaction between it and cloud platform is understood. Then, clarify the application of artificial intelligence on the cloud platform, and build the corresponding model for verification. The research results show that cloud platform combined with artificial intelligence technology can significantly optimize the resource allocation of rural e-commerce, achieve rapid resource matching and intelligent management, and greatly improve the efficiency and accuracy of rural e-commerce resource management. In addition, this technology integration also helps rural e-commerce to be more resilient and adaptable to complex market environments and changes, bringing more economic opportunities and social value to rural areas in the future.",progress information technology commerce gradually expanded rural areas providing new impetus rural economic growth transformation however challenges facing rural commerce also relatively complex study focuses cloud platforms artificial intelligence optimize resource management rural commerce first depth analysis characteristics development trend rural commerce interaction cloud platform understood clarify application artificial intelligence cloud platform build corresponding model verification research results show cloud platform combined artificial intelligence technology significantly optimize resource allocation rural commerce achieve rapid resource matching intelligent management greatly improve efficiency accuracy rural commerce resource management addition technology integration also helps rural commerce resilient adaptable complex market environments changes bringing economic opportunities social value rural areas future
"In today’s era of widespread education, there is a complex correlation between students’ academic performance and their behavior. In order to find the potential relationship between students’ learning behavior and academic performance from complex teaching data and provide better teaching solutions for educators, this paper used the Apriori algorithm to mine association rules in educational data. This study collected learning data from 5000 students in their first to third year of high school from a key higher education institution and a non-key higher education institution. Through data preprocessing and transformation, the optimized Apriori algorithm was used for frequent itemset mining, and four main learning behavior patterns were identified: online discussion, completing assignments on time, classroom participation, and additional reading. The experimental results show that the optimized algorithm reduces the execution time by 32% from 12.5 seconds before optimization to 8.5 seconds after optimization when processing the same amount of data. The performance of the optimized algorithm is stable when processing large-scale data. As the amount of data increases, the running time of the algorithm steadily increases from 2.4 seconds to 15.0 seconds, and the number of frequent itemsets gradually increases from 48 to 270. Research has found a significant correlation between frequent participation in online discussions and timely completion of assignments with student performance, particularly among students who attend regularly and have a higher Grade Point Average (GPA), with support and confidence levels of 0.22 and 0.85, respectively. The results of this study have been successfully applied to the development of targeted tutoring and personalized learning plans in a certain higher education institution, which significantly improved students’ academic performance and classroom participation.",today widespread education complex correlation students academic performance behavior order find potential relationship students learning behavior academic performance complex teaching data provide better teaching solutions educators paper used apriori algorithm mine association rules educational data study collected learning data students first third year high school key higher education institution non key higher education institution data preprocessing transformation optimized apriori algorithm used frequent itemset mining four main learning behavior patterns identified online discussion completing assignments time classroom participation additional reading experimental results show optimized algorithm reduces execution time seconds optimization seconds optimization processing amount data performance optimized algorithm stable processing large scale data amount data increases running time algorithm steadily increases seconds seconds number frequent itemsets gradually increases research found significant correlation frequent participation online discussions timely completion assignments student performance particularly among students attend regularly higher grade point average gpa support confidence levels respectively results study successfully applied development targeted tutoring personalized learning plans certain higher education institution significantly improved students academic performance classroom participation
"The rapid advancement in educational technology has ushered in an era where multimodal learning environments are increasingly recognized as crucial for fostering students’ cognitive and emotional development. This research explores the dual impact of these environments, integrating diverse sensory information, including visual and auditory inputs, thus offering enriched learning experiences with potential influences on students’ cognitive and emotional states. A notable gap in current research is the lack of a comprehensive understanding of how multimodal learning environments distinctly affect these aspects of student development. Traditional methodologies in this field often fail to account for the dynamic nature and individual variances inherent in the learning process, thereby limiting the depth of insights into the influence mechanisms of these environments. To bridge these gaps, this study employs a dual-methodological approach. Firstly, a sophisticated correlation matrix is developed to diagnose cognitive development within multimodal learning environments, effectively identifying key factors in cognitive processes. Secondly, an innovative emotion recognition model is utilized, incorporating sensor feature embedding and temporal feature embedding techniques, to adeptly capture students’ emotional fluctuations within these settings. The findings from this study provide substantial implications for educational practice. They enable educators to enhance the design and optimization of learning environments, promoting not only cognitive growth but also emotional well-being among students. This approach underscores the importance of addressing both cognitive and emotional domains in educational settings to foster holistic development. This study innovatively establishes a matrix correlating students, learning items, and knowledge points, facilitating a joint analysis to diagnose the cognitive development mechanism within multimodal learning environments. Additionally, an emotion recognition model, combining both sensor and temporal feature embedding networks, has been constructed. This model accurately identifies and analyzes students’ emotional states. The primary contribution lies in providing a comprehensive method for evaluating both cognitive and emotional states of students, which aids in optimizing personalized learning strategies and enhancing educational outcomes.",rapid advancement educational technology ushered multimodal learning environments increasingly recognized crucial fostering students cognitive emotional development research explores dual impact environments integrating diverse sensory information including visual auditory inputs thus offering enriched learning experiences potential influences students cognitive emotional states notable gap current research lack comprehensive understanding multimodal learning environments distinctly affect aspects student development traditional methodologies field often fail account dynamic nature individual variances inherent learning process thereby limiting depth insights influence mechanisms environments bridge gaps study employs dual methodological approach firstly sophisticated correlation matrix developed diagnose cognitive development within multimodal learning environments effectively identifying key factors cognitive processes secondly innovative emotion recognition model utilized incorporating sensor feature embedding temporal feature embedding techniques adeptly capture students emotional fluctuations within settings findings study provide substantial implications educational practice enable educators enhance design optimization learning environments promoting cognitive growth also emotional well among students approach underscores importance addressing cognitive emotional domains educational settings foster holistic development study innovatively establishes matrix correlating students learning items knowledge points facilitating joint analysis diagnose cognitive development mechanism within multimodal learning environments additionally emotion recognition model combining sensor temporal feature embedding networks constructed model accurately identifies analyzes students emotional states primary contribution lies providing comprehensive method evaluating cognitive emotional states students aids optimizing personalized learning strategies enhancing educational outcomes
"The inventory taking process in the archives is complex and error-prone, and the traditional manual method is inefficient. To this end, this study proposes a real-time inventory count model for archives that integrates QR code recognition and fingerprint encryption algorithms. This model adopts the Adaptive Median Filtering Algorithm (AMFA) to suppress salt-and-pepper noise, and combines the improved Otsu threshold segmentation and Hough transform to correct image distortion. The fingerprint feature encryption algorithm is introduced. Dynamic keys are generated through the translation, rotation, and scaling transformations of biometric feature points, forming double-layer encryption with the improved RC4 algorithm to achieve secure storage and access control of data. Experiments show that in the multi-source archive image test, the average MSE value of the model is 28.56 and the PSNR value is 32.45, which improves the noise suppression ability by 15.3% compared with the traditional median filtering algorithm. Facing complex scenarios, the decoding rates reached 87.33% and 92.11%, respectively, and the positioning accuracy rate reached 97.38%. Under the FGSM attack, the decoding rate of the model remained at 100%, and the EER value was only 0.92%. The above results prove that the proposed model has better encryption performance. This model effectively enhances the overall efficiency and intelligence level of archive management, providing technical support for promoting the automation of archive management.",inventory taking process archives complex error prone traditional manual method inefficient end study proposes real time inventory count model archives integrates code recognition fingerprint encryption algorithms model adopts adaptive median filtering algorithm amfa suppress salt pepper noise combines improved otsu threshold segmentation hough transform correct image distortion fingerprint feature encryption algorithm introduced dynamic keys generated translation rotation scaling transformations biometric feature points forming double layer encryption improved algorithm achieve secure storage access control data experiments show multi source archive image test average mse value model psnr value improves noise suppression ability compared traditional median filtering algorithm facing complex scenarios decoding rates reached respectively positioning accuracy rate reached fgsm attack decoding rate model remained eer value results prove proposed model better encryption performance model effectively enhances overall efficiency intelligence level archive management providing technical support promoting automation archive management
"Integrating Augmented Reality (AR) into financial reporting and analysis transforms traditional data interpretation methods. Traditional financial statements often present challenges in interpreting complex data sets, leading to potential misinterpretations. AR offers immersive, interactive visualizations that can enhance comprehension and decision-making processes in financial contexts. This research aims to investigate the effectiveness of AR-enhanced data visualization in financial reporting and identify the barriers to its widespread adoption. The investigation provides a mixed-methods approach, integrating quantitative data from financial survey responses with qualitative insights from case studies of firms adopting AR in their financial reporting procedures. Data were collected over 6 months from 120 participants, focusing on user experience, comprehension levels, visual appeal, overall satisfaction, confidence in interpretation, and decision-making efficiency. Quantitative survey data were analyzed using descriptive statistics to summarize the respondents. Inferential statistical techniques, including t-tests and ANOVA, were employed to assess differences in comprehension and decision-making efficiency between users of AR-enhanced reporting tools and traditional methods utilizing SPSS software. Regression analysis was conducted to identify predictors of successful AR adoption in financial reporting. Qualitative data were analyzed using thematic analysis. Results show that AR-augmented financial reports greatly enhance user participation and comprehension of intricate financial information. Users provided feedback on greater efficiency in decision-making and increased confidence in the interpretation of financial information. The study concludes that AR can revolutionize financial reporting by making data more accessible and interactive, but solutions to implementation issues are critical.",integrating augmented reality financial reporting analysis transforms traditional data interpretation methods traditional financial statements often present challenges interpreting complex data sets leading potential misinterpretations offers immersive interactive visualizations enhance comprehension decision making processes financial contexts research aims investigate effectiveness enhanced data visualization financial reporting identify barriers widespread adoption investigation provides mixed methods approach integrating quantitative data financial survey responses qualitative insights case studies firms adopting financial reporting procedures data collected months participants focusing user experience comprehension levels visual appeal overall satisfaction confidence interpretation decision making efficiency quantitative survey data analyzed using descriptive statistics summarize respondents inferential statistical techniques including tests anova employed assess differences comprehension decision making efficiency users enhanced reporting tools traditional methods utilizing spss software regression analysis conducted identify predictors successful adoption financial reporting qualitative data analyzed using thematic analysis results show augmented financial reports greatly enhance user participation comprehension intricate financial information users provided feedback greater efficiency decision making increased confidence interpretation financial information study concludes revolutionize financial reporting making data accessible interactive solutions implementation issues critical
"This study aims to enhance the recognition performance of children’s furniture pattern recognition by improving a multi-scale image enhancement algorithm using genetic algorithms, addressing the limitations of current recognition methods. The study integrates genetic algorithms with a multi-scale image enhancement approach to process edge details and restores pattern damage factors. The performance of the improved algorithm is evaluated using accuracy, recall, precision, and mean absolute error as key metrics, and compared with other algorithms. The improved algorithm achieved an average accuracy of 0.986, significantly outperforming other algorithms. Its mean absolute error was 2.635, lower than the comparison methods. The proposed algorithm also demonstrated superior recognition accuracy across different categories of children’s furniture patterns, confirming its effectiveness in pattern classification. This study introduces a novel combination of genetic algorithms and multi-scale image enhancement for children’s furniture pattern recognition, offering a substantial improvement in recognition accuracy. The algorithm’s high performance in handling diverse patterns provides a significant advancement in the field of image recognition.",study aims enhance recognition performance children furniture pattern recognition improving multi scale image enhancement algorithm using genetic algorithms addressing limitations current recognition methods study integrates genetic algorithms multi scale image enhancement approach process edge details restores pattern damage factors performance improved algorithm evaluated using accuracy recall precision mean absolute error key metrics compared algorithms improved algorithm achieved average accuracy significantly outperforming algorithms mean absolute error lower comparison methods proposed algorithm also demonstrated superior recognition accuracy across different categories children furniture patterns confirming effectiveness pattern classification study introduces novel combination genetic algorithms multi scale image enhancement children furniture pattern recognition offering substantial improvement recognition accuracy algorithm high performance handling diverse patterns provides significant advancement field image recognition
"With the arrival of the AI era, both students and teachers have an increasingly strong demand for various digital and intelligent software in teaching. How to better integrate intelligent software into curriculum teaching to meet the needs of teachers and students and improve the teaching effectiveness is an issue that urgently needs to be addressed. Therefore, this article takes the teachers and students who have participated in the curriculum reform of “Marketing” as the research objects, applies multi-dimensional intelligent software to the “Marketing” course, uses SPSS software and AMOS software to complete the correlation analysis, regression analysis, and structural equation model analysis of relevant factors, and forms a genuine large model + curriculum reform mode. The results show that 74% of the people support the use of multi-dimensional intelligent software in curriculum reform. However, 40% of them believe that they do not have enough technical proficiency in familiarizing themselves with the software itself. As a result, their willingness to recommend the digital software they have used to others is not high. The research findings indicate that the key to enhancing the effectiveness of curriculum reform is to improve the teachers’ and students’ own proficiency in using digital software, especially the intelligent components of digital and intelligent software. This paper suggests that more software training programs should be carried out in curriculum teaching, and more policy support related to intelligent software should be introduced, so as to better promote the improvement of the effectiveness of curriculum reform.",arrival students teachers increasingly strong demand various digital intelligent software teaching better integrate intelligent software curriculum teaching meet needs teachers students improve teaching effectiveness issue urgently needs addressed therefore article takes teachers students participated curriculum reform marketing research objects applies multi dimensional intelligent software marketing course uses spss software amos software complete correlation analysis regression analysis structural equation model analysis relevant factors forms genuine large model curriculum reform mode results show people support use multi dimensional intelligent software curriculum reform however believe enough technical proficiency familiarizing software result willingness recommend digital software used others high research findings indicate key enhancing effectiveness curriculum reform improve teachers students proficiency using digital software especially intelligent components digital intelligent software paper suggests software training programs carried curriculum teaching policy support related intelligent software introduced better promote improvement effectiveness curriculum reform
"Traditional equipment manufacturing training methods face many problems, such as high resource consumption, high safety risks, time and space limitations, difficulty in personalized teaching, and incomplete evaluation. This article aims to develop an intelligent international equipment manufacturing training platform by combining deep learning and virtual reality (VR) technology to address these issues. Firstly, the virtual reality technology was utilized to create immersive virtual environments, reducing reliance on physical equipment and venues, lowering costs and resource consumption, while eliminating security risks in real operations. Students could conduct practical training anytime and anywhere in a virtual environment, overcoming the limitations of traditional training in terms of time and space, and enhancing the flexibility and convenience of learning. Then, through deep learning algorithms CNN (convolutional neural network) and RNN (recurrent neural network), the platform could analyze students’ operational data in real-time and provide personalized feedback and guidance. The teaching content and difficulty were adjusted according to the learning progress and performance of the students, ensuring that each student can have a suitable learning experience. In addition, deep learning algorithms could automatically evaluate students’ operational skills and learning effectiveness, and generate detailed evaluation reports, making the evaluation process more objective and fair, and avoiding the influence of subjective factors. The training platform developed in this article could also simulate complex equipment manufacturing scenarios and troubleshooting processes, allowing students to significantly improve their scores in operational accuracy, efficiency, fault recognition ability, and problem-solving ability. The average improvement percentage of students in various evaluation indicators was about 43.83%. Through the development and evaluation of this platform, this article aims to significantly improve the efficiency, safety, and learning effectiveness of equipment manufacturing training, providing an innovative solution for international equipment manufacturing training.",traditional equipment manufacturing training methods face many problems high resource consumption high safety risks time space limitations difficulty personalized teaching incomplete evaluation article aims develop intelligent international equipment manufacturing training platform combining deep learning virtual reality technology address issues firstly virtual reality technology utilized create immersive virtual environments reducing reliance physical equipment venues lowering costs resource consumption eliminating security risks real operations students could conduct practical training anytime anywhere virtual environment overcoming limitations traditional training terms time space enhancing flexibility convenience learning deep learning algorithms cnn convolutional neural network rnn recurrent neural network platform could analyze students operational data real time provide personalized feedback guidance teaching content difficulty adjusted according learning progress performance students ensuring student suitable learning experience addition deep learning algorithms could automatically evaluate students operational skills learning effectiveness generate detailed evaluation reports making evaluation process objective fair avoiding influence subjective factors training platform developed article could also simulate complex equipment manufacturing scenarios troubleshooting processes allowing students significantly improve scores operational accuracy efficiency fault recognition ability problem solving ability average improvement percentage students various evaluation indicators development evaluation platform article aims significantly improve efficiency safety learning effectiveness equipment manufacturing training providing innovative solution international equipment manufacturing training
"This study investigates big data-driven collaborative development models between low-altitude economy and emergency logistics across China. Through comprehensive empirical analysis of multi-source operational data from 2018 to 2024, we identify significant regional disparities in collaborative development indices (eastern: 0.86; western: 0.50 on a 0–1 scale). This study proposes and validates three collaborative models—traffic coordination, intelligent resource allocation, and regional service networks—each addressing distinct operational challenges. Path analysis reveals data integration quality as the primary determinant of collaborative efficiency (47.3%), followed by infrastructure readiness (26.8%). The findings add to complex systems theory by using a multi-dimensional assessment framework with dynamic weighing mechanisms. We suggest differentiated policy prescriptions to remove regional bottlenecks while expediting infrastructure development in under-developed areas.",study investigates big data driven collaborative development models low altitude economy emergency logistics across china comprehensive empirical analysis multi source operational data identify significant regional disparities collaborative development indices eastern western scale study proposes validates three collaborative models traffic coordination intelligent resource allocation regional service networks addressing distinct operational challenges path analysis reveals data integration quality primary determinant collaborative efficiency followed infrastructure readiness findings add complex systems theory using multi dimensional assessment framework dynamic weighing mechanisms suggest differentiated policy prescriptions remove regional bottlenecks expediting infrastructure development developed areas
"The current inquiry maps the digital resilience tactics embedded in China’s and Russia’s customary cultures as they circulate across social media. Mixed-methods scrutiny of roughly 2400 posts drawn from eight service providers forms the evidential core of the project. China’s social environments lean heavily towards visual artistry—more than 42% of the data, by one estimate—and produce formal novelties at an impressive 63% clip. Russian spaces, in contrast, foreground century-old stories, with nearly 41% of the corpus devoted to such materials, and score 54% on an informal semantic-adaptation metric. Hybrid cultural mixes are on the rise: China’s proportion jumped from 42 to 69% between 2020 and early 2024, while Russia’s climbed from 38 to just over 52% in the same window. Content rooted in tradition outlasts its contemporary cousins by a wide margin, attracting average user attention for just over 7 days in Chinese streams and for roughly 9 days in Russian ones; the shorter lifespan of modern posts hovers around 3 days in both contexts. Engagement styles diverge, too: Chinese users tend to share posts at a 39% rate, whereas Russian cohorts favor commenting, which they do at close to 43%. Taken together, the observations chart culturally specific pathways of digital adaptation that preserve core identities and introduce a Cultural Resilience Index as a new methodological benchmark.",current inquiry maps digital resilience tactics embedded china russia customary cultures circulate across social media mixed methods scrutiny roughly posts drawn eight service providers forms evidential core project china social environments lean heavily towards visual artistry data one estimate produce formal novelties impressive clip russian spaces contrast foreground century old stories nearly corpus devoted materials score informal semantic adaptation metric hybrid cultural mixes rise china proportion jumped early russia climbed window content rooted tradition outlasts contemporary cousins wide margin attracting average user attention days chinese streams roughly days russian ones shorter lifespan modern posts hovers around days contexts engagement styles diverge chinese users tend share posts rate whereas russian cohorts favor commenting close taken together observations chart culturally specific pathways digital adaptation preserve core identities introduce cultural resilience index new methodological benchmark
"This study proposes a secure food quality traceability system based on a multi-chain blockchain architecture to improve both data protection and the efficiency of food quality management. The system leverages blockchain technology to monitor and safeguard the entire supply chain—including production, processing, and transportation. To ensure confidentiality and integrity of data, the system integrates the Paillier encryption algorithm and secure hash algorithms for encrypted storage and secure sharing (the Paillier algorithm supports addition and multiplication operations, making it suitable for handling data aggregation and analysis tasks in the supply chain. It has high computational efficiency and can quickly complete data encryption and decryption). Hyperledger Fabric is employed to manage permissioned access, enforce data query control, and mitigate unauthorized operations. Security tests indicate that the system achieves high encryption efficiency—encrypting 2500 kb of data in 22 ms with a 32-bit key and achieving the fastest decryption at 443 ms with a 128-bit key. Compared to traditional systems, it significantly improves throughput (412 TPS vs 305 TPS) and risk interception (92 vs 52). These results demonstrate that the proposed system enhances cybersecurity in traceability applications by offering strong encryption, fine-grained access control, and high performance. It provides a robust and scalable solution for securing sensitive data in food supply chains, with broader implications for blockchain-based cybersecurity applications in industrial networks.",study proposes secure food quality traceability system based multi chain blockchain architecture improve data protection efficiency food quality management system leverages blockchain technology monitor safeguard entire supply chain including production processing transportation ensure confidentiality integrity data system integrates paillier encryption algorithm secure hash algorithms encrypted storage secure sharing paillier algorithm supports addition multiplication operations making suitable handling data aggregation analysis tasks supply chain high computational efficiency quickly complete data encryption decryption hyperledger fabric employed manage permissioned access enforce data query control mitigate unauthorized operations security tests indicate system achieves high encryption efficiency encrypting data bit key achieving fastest decryption bit key compared traditional systems significantly improves throughput tps tps risk interception results demonstrate proposed system enhances cybersecurity traceability applications offering strong encryption fine grained access control high performance provides robust scalable solution securing sensitive data food supply chains broader implications blockchain based cybersecurity applications industrial networks
"This research carried out the durability assessment of manufactured sand concrete under the combined effects of stray current and chloride ions. Experiments demonstrated that manufactured sand concrete resists chloride ion penetration 15%–25% more than natural sand concrete does, with the best performance registered at 6.4% stone powder content. Stray current causes the electromigration effect which causes chloride ion migration to be more rapid. The depth of chloride penetration is about 30% greater when 10 V is supplied compared to 40 V. COMSOL Multiphysics-based numerical simulations (a finite element analysis software) showed that the irregular morphology of manufactured sand particles increases the transport paths of chloride ions, creating a “tortuosity effect” with a value of 1.35. This value is higher than that of natural sand, which is 1.22, and the circular aggregate models, which is 1.15. At the same time, the stone powder filling effect reduces the porosity of concrete. The simulation results accurately matched the experimental data by more than 90% for the early stage of corrosion. This research offers theoretical evidence for the feasibility of manufactured sand concrete applications in coastal metro engineering and shows its best performance under severe conditions of stray current and chloride exposure. The use of manufactured sand provides significant sustainability benefits by reducing natural sand depletion and environmental impact while maintaining superior durability performance.",research carried durability assessment manufactured sand concrete combined effects stray current chloride ions experiments demonstrated manufactured sand concrete resists chloride ion penetration natural sand concrete best performance registered stone powder content stray current causes electromigration effect causes chloride ion migration rapid depth chloride penetration greater supplied compared comsol multiphysics based numerical simulations finite element analysis software showed irregular morphology manufactured sand particles increases transport paths chloride ions creating tortuosity effect value value higher natural sand circular aggregate models time stone powder filling effect reduces porosity concrete simulation results accurately matched experimental data early stage corrosion research offers theoretical evidence feasibility manufactured sand concrete applications coastal metro engineering shows best performance severe conditions stray current chloride exposure use manufactured sand provides significant sustainability benefits reducing natural sand depletion environmental impact maintaining superior durability performance
"With the popularity of portable electronic devices, the demand for 12-bit low-power analog-to-digital converter (ADC) chips continues to grow. Considering that these devices typically rely on battery power, high precision and stability are crucial. In order to improve the voltage resistance of ADC chips under high voltage, an innovative bootstrap sampling switch design is proposed, which utilizes the capacitance characteristics of NMOS and PMOS transistors to enhance the voltage resistance of the chip. At the same time, considering factors such as power fluctuations, time domain and voltage interleaving, a two-stage hybrid architecture based on successive approximation registers (SARs) and time to digital converters (TDCs) was studied and designed to optimize the balance between low power consumption and high voltage resistance performance. The results showed that the power consumption of the chip design was only 9.5 mW. The voltage compensation measures significantly reduced the error value from 7.01% to 1.97%, optimizing 5.04%. In summary, this design not only improves the accuracy and speed of ADC chips, but also successfully controls the exponential growth of power consumption, which has strong practical value. The proposed solution by the research institute has broad potential in multiple application fields, especially suitable for situations with high requirements for power consumption and stability, such as medical imaging and aerospace. This design provides new ideas and directions for the development of high-performance and low-power ADC chips in the future.",popularity portable electronic devices demand bit low power analog digital converter adc chips continues grow considering devices typically rely battery power high precision stability crucial order improve voltage resistance adc chips high voltage innovative bootstrap sampling switch design proposed utilizes capacitance characteristics nmos pmos transistors enhance voltage resistance chip time considering factors power fluctuations time domain voltage interleaving two stage hybrid architecture based successive approximation registers sars time digital converters tdcs studied designed optimize balance low power consumption high voltage resistance performance results showed power consumption chip design voltage compensation measures significantly reduced error value optimizing summary design improves accuracy speed adc chips also successfully controls exponential growth power consumption strong practical value proposed solution research institute broad potential multiple application fields especially suitable situations high requirements power consumption stability medical imaging aerospace design provides new ideas directions development high performance low power adc chips future
"Amidst profound transformations witnessed in the contemporary occupational landscape, precipitated by persistent forces of globalization and digitalization, an urgent emergence of the need for a comprehensive array of adaptive skills has been observed. Addressing this imperative, the merging of vocational education and lifelong learning is deemed essential, emerging as a cornerstone in contemporary educational discourse. Though the import of adaptive skills is universally acknowledged, the means to impart such training effectively remains elusive. Present technological strategies reveal shortcomings in provisioning individualized learning experiences and in extracting semantic information from textual content. Through the current investigation, a strategy is introduced whereby adaptive skills are fortified via intelligent recommendations, in tandem with an optimized Long-Short Term Memory (LSTM) network structure, aiming for an incisive classification of adaptive skill training texts. Such strategies not only delineate a fresh theoretical paradigm for adaptive skill instruction but also proffer cutting-edge technical reinforcement for the intertwined realms of vocational education and lifelong learning. The innovation of this research lies in the proposal of an adaptive skill enhancement method based on intelligent recommendation and the design of an intelligent test item recommendation algorithm for adaptive skill enhancement. Additionally, the research introduces innovative improvements to the LSTM network structure by incorporating thematic semantic information, enabling the model to generate more accurate text representations, thereby significantly improving classification performance and the accuracy of personalized recommendations. These contributions provide effective technical support for enhancing the effectiveness of adaptive skill training.",amidst profound transformations witnessed contemporary occupational landscape precipitated persistent forces globalization digitalization urgent emergence need comprehensive array adaptive skills observed addressing imperative merging vocational education lifelong learning deemed essential emerging cornerstone contemporary educational discourse though import adaptive skills universally acknowledged means impart training effectively remains elusive present technological strategies reveal shortcomings provisioning individualized learning experiences extracting semantic information textual content current investigation strategy introduced whereby adaptive skills fortified via intelligent recommendations tandem optimized long short term memory lstm network structure aiming incisive classification adaptive skill training texts strategies delineate fresh theoretical paradigm adaptive skill instruction also proffer cutting edge technical reinforcement intertwined realms vocational education lifelong learning innovation research lies proposal adaptive skill enhancement method based intelligent recommendation design intelligent test item recommendation algorithm adaptive skill enhancement additionally research introduces innovative improvements lstm network structure incorporating thematic semantic information enabling model generate accurate text representations thereby significantly improving classification performance accuracy personalized recommendations contributions provide effective technical support enhancing effectiveness adaptive skill training
"Using ethnographic methods in community-based participatory research (CBPR) fosters mutual trust and collaboration between researchers and community members. However, scaling these methods is challenged by limited researcher resources and community member involvement. This paper introduces computational ethnography (CE), which uses computational assists to deliver rich insights. We argue that traditional ethnography, even with computational techniques, is insufficient for scaling effectively. Our Machine–Readable Co-design (MaRC) toolkit, a CE method, leverages human–machine teaming to address the collection and analysis of community stories at scale in CBPR. MaRC integrates AI/ML technologies to support effective community co-planning. In this study, two researchers used MaRC to capture and analyze over 100 community stories. While researchers spent over 120 hr across 2 months manually measuring and recording data, MaRC computed each story within 8 to 10 minutes, achieving an 84% concordance rate with our manual, baseline analysis. MaRC demonstrates significant potential for scaling future CBPR efforts.",using ethnographic methods community based participatory research cbpr fosters mutual trust collaboration researchers community members however scaling methods challenged limited researcher resources community member involvement paper introduces computational ethnography uses computational assists deliver rich insights argue traditional ethnography even computational techniques insufficient scaling effectively machine readable design marc toolkit method leverages human machine teaming address collection analysis community stories scale cbpr marc integrates technologies support effective community planning study two researchers used marc capture analyze community stories researchers spent across months manually measuring recording data marc computed story within minutes achieving concordance rate manual baseline analysis marc demonstrates significant potential scaling future cbpr efforts
"From the cross-cultural communication, this study applies interactive educational psychology (a branch of educational psychology that focuses on student cognitive and cognitive interaction, emphasizing the promotion of ability development through two-way feedback) to the reform of the mobile English literature translation teaching mode. A translation ability development model is proposed, which is divided into cognitive and thinking skills. New teaching methods are designed and experiments are carried out. A total of 120 students majoring in English in a certain university were selected for the experiment and randomly divided into the intervention group (60 students) and the control group (60 students). The intervention period was 8 weeks (T1-T8 were the test time points from the 1st to the 8th week). The results showed that at T8, the innovative thinking (78.9 points), critical thinking (80.2 points), and decision-making thinking (79.8 points) of the intervention group were significantly higher than those of the control group, and their cognitive abilities (orientation, analysis, and evaluation) also improved significantly. This confirms that the mobile teaching model can effectively enhance students’ practical translation abilities.",cross cultural communication study applies interactive educational psychology branch educational psychology focuses student cognitive cognitive interaction emphasizing promotion ability development two way feedback reform mobile english literature translation teaching mode translation ability development model proposed divided cognitive thinking skills new teaching methods designed experiments carried total students majoring english certain university selected experiment randomly divided intervention group students control group students intervention period weeks test time points week results showed innovative thinking points critical thinking points decision making thinking points intervention group significantly higher control group cognitive abilities orientation analysis evaluation also improved significantly confirms mobile teaching model effectively enhance students practical translation abilities
"With the rise of intelligent algorithms, automated motion recognition in sports like volleyball—known for its complex movements—remains challenging. To enhance accuracy, this study introduces the Human Posture and Spatiotemporal Graph Convolution (HP-SGC) model. It uses skeletal keypoints to create a 2D coordinate system, combines object detection and pose estimation for action recognition, and applies spatiotemporal graph convolution for classification. The tests showed over 95% accuracy for continuous actions, with recognition time as low as 2.2 seconds. The model successfully identified 8 foul actions, 7 basic moves, and 4 skillful actions, averaging &gt;90% accuracy. These results demonstrate HP-SGC’s strong performance in volleyball action recognition, offering valuable tools for match analysis and statistics.",rise intelligent algorithms automated motion recognition sports like volleyball known complex movements remains challenging enhance accuracy study introduces human posture spatiotemporal graph convolution sgc model uses skeletal keypoints create coordinate system combines object detection pose estimation action recognition applies spatiotemporal graph convolution classification tests showed accuracy continuous actions recognition time low seconds model successfully identified foul actions basic moves skillful actions averaging accuracy results demonstrate sgc strong performance volleyball action recognition offering valuable tools match analysis statistics
"With the growth of the film market, users are often overwhelmed by irrelevant information, making it difficult to make accurate movie choices. To address this, we propose an improved deep structured semantic model (DSSM) movie recommendation algorithm based on Spark technology. The algorithm employs two DSSMs: one to extract users’ long-term preferences and another for final movie recommendations. The preference model integrates explicit and implicit user interaction features, while the recommendation model utilizes recent viewing history and search behavior. Experiments were conducted on the MovieLens 100K and MovieLens 1M datasets, using metrics such as accuracy, recall, mean reciprocal ranking (MRR), diversity, and normalized discounted cumulative return (NDCG). The results show that the improved algorithm outperforms others in recall, with rates of 36.0% and 42.1% for the top 40 recommendations on the MovieLens 100K and 1M datasets, respectively. Additionally, the algorithm’s NDCG scores for the top 10 recommendations were 0.525 and 0.554, higher than those of competing algorithms. The system, built on Spark technology, passes all performance tests, with an average response time of under 600 ms. These results demonstrate that the proposed DSSM-based movie recommendation algorithm can provide accurate recommendations while overcoming the challenge of data sparsity. The research not only provides an efficient and accurate solution for the field of movie recommendation, but also shows the significant advantages of Spark technology in processing large-scale data sets, which has a wide range of application value and promotion prospects.",growth film market users often overwhelmed irrelevant information making difficult make accurate movie choices address propose improved deep structured semantic model dssm movie recommendation algorithm based spark technology algorithm employs two dssms one extract users long term preferences another final movie recommendations preference model integrates explicit implicit user interaction features recommendation model utilizes recent viewing history search behavior experiments conducted movielens movielens datasets using metrics accuracy recall mean reciprocal ranking mrr diversity normalized discounted cumulative return ndcg results show improved algorithm outperforms others recall rates top recommendations movielens datasets respectively additionally algorithm ndcg scores top recommendations higher competing algorithms system built spark technology passes performance tests average response time results demonstrate proposed dssm based movie recommendation algorithm provide accurate recommendations overcoming challenge data sparsity research provides efficient accurate solution field movie recommendation also shows significant advantages spark technology processing large scale data sets wide range application value promotion prospects
"Football matches not only showcase athletes’ skills and spirit but also foster sports culture and social cohesion. This study designs an improved Faster Region-based Convolutional Neural Network-based target detection model to extract football players’ movement routes. The model enhances feature extraction using residual and feature pyramid networks and optimizes Anchor boxes with binary K-means clustering. A similarity matrix integrating motion and appearance features is then used for movement route extraction. The results show that the accuracy and recall of the detection model are excellent, and the intersection over union ratio and precision mean are also better than the comparison model. Meanwhile, in various scenarios, the accuracy of the mobile route extraction algorithm is at a high level, and the average frame rate performs well. The results demonstrate the effectiveness of the detection model and extraction algorithm in analyzing player movement, providing valuable insights for tactical analysis.",football matches showcase athletes skills spirit also foster sports culture social cohesion study designs improved faster region based convolutional neural network based target detection model extract football players movement routes model enhances feature extraction using residual feature pyramid networks optimizes anchor boxes binary means clustering similarity matrix integrating motion appearance features used movement route extraction results show accuracy recall detection model excellent intersection union ratio precision mean also better comparison model meanwhile various scenarios accuracy mobile route extraction algorithm high level average frame rate performs well results demonstrate effectiveness detection model extraction algorithm analyzing player movement providing valuable insights tactical analysis
"With the rapid advancement of large language models (LLMs) and computer vision technologies, multimodal large language models (MLLMs) have demonstrated remarkable potential in sentiment analysis. Traditional sentiment analysis methods often rely on unimodal data (e.g., text or images), making it difficult to comprehensively capture complex emotional expressions. This paper proposes a multimodal sentiment analysis framework based on MLLMs, integrating both visual and textual information to enhance sentiment classification accuracy. Experiments on a high-profile social media event show that Qwen2-VL-Adpter model outperforms conventional methods in multiple evaluation metrics, validating the effectiveness of multimodal information fusion. This study provides a robust technical framework for sentiment analysis in public opinion monitoring and offers valuable data support for crisis management. However, the model’s performance is influenced by the specificity of the dataset and computational demands, which may limit its application in resource-constrained environments.",rapid advancement large language models llms computer vision technologies multimodal large language models mllms demonstrated remarkable potential sentiment analysis traditional sentiment analysis methods often rely unimodal data text images making difficult comprehensively capture complex emotional expressions paper proposes multimodal sentiment analysis framework based mllms integrating visual textual information enhance sentiment classification accuracy experiments high profile social media event show qwen adpter model outperforms conventional methods multiple evaluation metrics validating effectiveness multimodal information fusion study provides robust technical framework sentiment analysis public opinion monitoring offers valuable data support crisis management however model performance influenced specificity dataset computational demands may limit application resource constrained environments
"The traditional automatic driving system has the problems of low perception accuracy and low driving intelligence. To solve this problem, the research proposes an autonomous driving system based on the Internet of Things and visual cognition. Based on the traditional system, a symmetrical visual cognition sensor is added and the internal module of the sensor is designed. Then, the road information is obtained by combining visual cognition technology, and the driving route is planned by the sparrow optimization algorithm. In the process of automatic driving, dynamic sensing optimization is realized through dynamic detection and path control navigation. The experimental results showed that the highest automatic driving perception accuracy obtained by the proposed automatic driving system was 100%, and the perception time was within 7.62 seconds. Compared with other automatic driving methods, it had higher perceptual accuracy and faster response speed. This shows that the proposed system can improve the comfort and safety of autonomous driving, providing a positive research path for autonomous driving technology in the automotive industry.",traditional automatic driving system problems low perception accuracy low driving intelligence solve problem research proposes autonomous driving system based internet things visual cognition based traditional system symmetrical visual cognition sensor added internal module sensor designed road information obtained combining visual cognition technology driving route planned sparrow optimization algorithm process automatic driving dynamic sensing optimization realized dynamic detection path control navigation experimental results showed highest automatic driving perception accuracy obtained proposed automatic driving system perception time within seconds compared automatic driving methods higher perceptual accuracy faster response speed shows proposed system improve comfort safety autonomous driving providing positive research path autonomous driving technology automotive industry
"As the demand for e-voting grows, it has become particularly important to ensure the security and fairness of the voting system. Therefore, an e-voting system is studied and designed to ensure the non-tamperability of voting results. The system utilizes zero-knowledge proof to verify the identity of the voter, while ensuring the tamperability of the voting data through blockchain technology. The experimental results indicated that the system outperformed the existing schemes in terms of processing speed and verification efficiency. Specifically, when the number of voters was 200, the time consuming and single verification time of this system were 3.7 s and 6.4 ms, respectively. When the number of voters increased to 600, the time consuming and single verification time were 8.3 s and 13.3 ms, respectively. In the number of candidates/voters was 5/80, none of the system’s gas consumption exceeded the maximum limit of a single transaction in Ether. Among them, the gas consumption of Vote Control contract was 5577485, and the gas consumption of non-interactive zero knowledge contract was 3826753. Furthermore, the more candidates there were, the longer it took the system to operate, although the number of voters had less of an effect on the cost of operating the voter system. The above outcomes reveal that the e-voting system proposed in the study provides a secure and efficient solution for small-scale voting activities and provides a basis for future optimization of large-scale voting scenarios.",demand voting grows become particularly important ensure security fairness voting system therefore voting system studied designed ensure non tamperability voting results system utilizes zero knowledge proof verify identity voter ensuring tamperability voting data blockchain technology experimental results indicated system outperformed existing schemes terms processing speed verification efficiency specifically number voters time consuming single verification time system respectively number voters increased time consuming single verification time respectively number candidates voters none system gas consumption exceeded maximum limit single transaction ether among gas consumption vote control contract gas consumption non interactive zero knowledge contract furthermore candidates longer took system operate although number voters less effect cost operating voter system outcomes reveal voting system proposed study provides secure efficient solution small scale voting activities provides basis future optimization large scale voting scenarios
"This study developed a computer-assisted language learning system based on English syntactic structure using long short-term memory (LSTM) network. Through dynamic learning models and adaptive learning paths, personalized and efficient learning experiences were provided, aiming to address the shortcomings of traditional systems and improve learners’ mastery of syntactic structure. The system processed language sequence data by designing an LSTM model, combined with a dynamic adjustment mechanism and real-time feedback system, to achieve dynamic adjustment of personalized learning content and difficulty. Based on data-driven teaching strategies, teaching methods were continuously optimized and learning outcomes were improved by collecting and analyzing learning data. The experimental results of this study indicated that the system had significant effects in improving learners’ mastery of syntactic structures, overcoming the limitations of traditional computer-assisted language learning systems. In the experiment, 120 English learners were selected and divided into an experimental group and a control group, using the system developed in this study and the traditional system, respectively. After the experiment, learners in the experimental group significantly improved their mastery of syntactic structures, with their scores jumping from 70 to 94, while those in the control group increased from 68 to 85. The percentage of correct learning increased from 75% to 94% in the experimental group and from 73% to 86% in the control group. The study shows that the computer-assisted language learning system is effective in improving the mastery of syntactic structures.",study developed computer assisted language learning system based english syntactic structure using long short term memory lstm network dynamic learning models adaptive learning paths personalized efficient learning experiences provided aiming address shortcomings traditional systems improve learners mastery syntactic structure system processed language sequence data designing lstm model combined dynamic adjustment mechanism real time feedback system achieve dynamic adjustment personalized learning content difficulty based data driven teaching strategies teaching methods continuously optimized learning outcomes improved collecting analyzing learning data experimental results study indicated system significant effects improving learners mastery syntactic structures overcoming limitations traditional computer assisted language learning systems experiment english learners selected divided experimental group control group using system developed study traditional system respectively experiment learners experimental group significantly improved mastery syntactic structures scores jumping control group increased percentage correct learning increased experimental group control group study shows computer assisted language learning system effective improving mastery syntactic structures
"With the widespread application of wireless sensor networks in complex environments, the traditional DV-Hop localization algorithm exhibits low accuracy under conditions such as uneven node density and obstacle interference. This study proposes an improved DV-Hop algorithm (IOWDV-Hop) to address this issue. The IOWDV-Hop integrates an optimal weighting function with an anchor node selection strategy. This method improves the accuracy of node localization by introducing error-minimizing weights and a constraint-based mechanism for selecting anchor nodes. Experiments were conducted using different irregular network structures for comparative testing. The results showed that the proposed algorithm performed best in a “Tˮ-shaped network. It achieved an average positioning error of only 19%, which was nearly a 60% reduction compared to the traditional DV-Hop algorithm. The study demonstrates that this method possesses stronger robustness and practicality. It provides both theoretical support and engineering value for high-precision positioning in wireless sensor networks.",widespread application wireless sensor networks complex environments traditional hop localization algorithm exhibits low accuracy conditions uneven node density obstacle interference study proposes improved hop algorithm iowdv hop address issue iowdv hop integrates optimal weighting function anchor node selection strategy method improves accuracy node localization introducing error minimizing weights constraint based mechanism selecting anchor nodes experiments conducted using different irregular network structures comparative testing results showed proposed algorithm performed best shaped network achieved average positioning error nearly reduction compared traditional hop algorithm study demonstrates method possesses stronger robustness practicality provides theoretical support engineering value high precision positioning wireless sensor networks
"The uniformity of website style is one aspect of improving network user interface design, while the size of images and the uniformity of information are also aspects of website style uniformity. In response to the phenomenon of inconsistent product image styles on shopping websites, this article used the image recognition algorithm of Mask R-CNN (Mask Region-based Convolutional Neural Network) to recognize and annotate product images, and extract the information contained in the product images, facilitating the standardization of information, thus improving network user interface design. This article improved the Mask R-CNN model network: in the region proposal network, more pre-selected boxes of different sizes were added; in the feature pyramid, a new feature fusion path was added; the loss function was optimized; the shopping website product image dataset obtained by the crawler was pre-trained into the Mask R-CNN model network to obtain the final Mask R-CNN image recognition algorithm model. Through experiments, it was known that compared with other model algorithms, the final algorithm model had a response time of 2s/per picture, an average precision of 95.22, and a recall rate of 96.32%, which was higher than known image recognition models.",uniformity website style one aspect improving network user interface design size images uniformity information also aspects website style uniformity response phenomenon inconsistent product image styles shopping websites article used image recognition algorithm mask cnn mask region based convolutional neural network recognize annotate product images extract information contained product images facilitating standardization information thus improving network user interface design article improved mask cnn model network region proposal network pre selected boxes different sizes added feature pyramid new feature fusion path added loss function optimized shopping website product image dataset obtained crawler pre trained mask cnn model network obtain final mask cnn image recognition algorithm model experiments known compared model algorithms final algorithm model response time per picture average precision recall rate higher known image recognition models
"The dynamic field of fashion design increasingly demands innovative approaches to streamline style creation and enhance designer creativity. Traditional methods relying on manual sketching and coloring are time-consuming and limit rapid exploration of styles. Recognizing the challenges in automatic fashion style generation, particularly the lack of high-fidelity image recognition and realistic style synthesis, the research proposes a novel deep learning (DL) framework named the automated fashion style generation model based on image recognition techniques. To address the identified research gap, the research introduces a novel Vision Transformer-driven Style Generative Adversarial Network (ViT-StyleGAN), aiming to transform raw fashion imagery into diverse, high-quality styles. For this research, a comprehensive dataset comprising fashion items was assembled to ensure rich stylistic diversity. Data preprocessing involved noise reduction and clothing region segmentation using a Mask R-CNN variant, ensuring clean inputs for the model. Feature extraction was performed using an enhanced Vision Transformer (ViT) encoder to capture detailed spatial representations. The proposed framework processes the gathered data by first extracting robust features; it is subsequently used to create innovative fashion designs using the StyleGAN-based generator. ViT-StyleGAN effectively leverages global visual context and fine-grained texture generation, optimizing outputs for realism and creativity. The ViT-StyleGAN was evaluated with the YOLOv5s models that were trained for 100 and 300 epochs. A mAP (98.8%), precision (98.5%), recall (98%), F1-score (98.3%), and FPS of 42 were attained in 100 epochs. With a mAP (99.3%), a precision (98.2%), a recall (98.8 %), an F1-score (98.5%), and FPS of 42 attained in 300 epochs, the ViT-StyleGAN fared better than both in terms of style realism, recognition accuracy, and speed. This framework establishes a new direction in automated fashion style generation, providing a powerful tool for designers to accelerate creativity while preserving personal esthetic preferences.",dynamic field fashion design increasingly demands innovative approaches streamline style creation enhance designer creativity traditional methods relying manual sketching coloring time consuming limit rapid exploration styles recognizing challenges automatic fashion style generation particularly lack high fidelity image recognition realistic style synthesis research proposes novel deep learning framework named automated fashion style generation model based image recognition techniques address identified research gap research introduces novel vision transformer driven style generative adversarial network vit stylegan aiming transform raw fashion imagery diverse high quality styles research comprehensive dataset comprising fashion items assembled ensure rich stylistic diversity data preprocessing involved noise reduction clothing region segmentation using mask cnn variant ensuring clean inputs model feature extraction performed using enhanced vision transformer vit encoder capture detailed spatial representations proposed framework processes gathered data first extracting robust features subsequently used create innovative fashion designs using stylegan based generator vit stylegan effectively leverages global visual context fine grained texture generation optimizing outputs realism creativity vit stylegan evaluated yolov models trained epochs map precision recall score fps attained epochs map precision recall score fps attained epochs vit stylegan fared better terms style realism recognition accuracy speed framework establishes new direction automated fashion style generation providing powerful tool designers accelerate creativity preserving personal esthetic preferences
"Fuel cell air compressors play a crucial role in proton exchange membrane fuel cell systems, and their performance has a direct impact on the stable operation and energy efficiency of the system. In practical applications, the air compressor of the fuel cell system will be affected by the frequency and amplitude of changes in the air load. To effectively solve this problem, this study proposes and evaluates the control algorithm of fuel cell air compressors considering the characteristics of air load. Firstly, a control model of the fuel cell air compressor system including air load and back pressure valve model is established. Secondly, the input–output feedback linearization (IOFL) theory is adopted to globally linearize the control model of the fuel cell air compressor system. Finally, the Q-axis current of the improved speed loop is generated through the feedback of the air state quantity to correct the output torque of the air compressor. The simulation results show that this method ensures the accuracy of the model while achieving precise control of the rotational speed, effectively improving the adaptability and dynamic response performance of the air compressor, and guaranteeing the stability and efficiency of the fuel cell system.",fuel cell air compressors play crucial role proton exchange membrane fuel cell systems performance direct impact stable operation energy efficiency system practical applications air compressor fuel cell system affected frequency amplitude changes air load effectively solve problem study proposes evaluates control algorithm fuel cell air compressors considering characteristics air load firstly control model fuel cell air compressor system including air load back pressure valve model established secondly input output feedback linearization iofl theory adopted globally linearize control model fuel cell air compressor system finally axis current improved speed loop generated feedback air state quantity correct output torque air compressor simulation results show method ensures accuracy model achieving precise control rotational speed effectively improving adaptability dynamic response performance air compressor guaranteeing stability efficiency fuel cell system
"Traditional online advertising evaluation indicators (e.g., click-through rate and conversion rate) are characterized by limitations in scope and delayed feedback, rendering them inadequate for real-time and comprehensive evaluation. The existing models have insufficient accuracy when fusing multiple features. Therefore, the research proposes an evaluation framework that integrates multimodal features (MF) with attribute integration for user classification and constructs a cascading model based on MF and timing features (TC). The classification of user types is achieved through the attribute integration method, which utilizes multi-source data, including cognitive styles and emotional states. Quantification of features is facilitated by employing membership functions. The cascade model involves combining Markov chains and the PrefixSpan algorithm to mine temporal patterns. The results showed that when the user type was 2, the cluster center distances were (1.3, 0.9), and the type differences were significant. Under the activation of ReLU, the Auc of the cascade model was 0.803 and the logloss was 0.45. Under tanh activation, Auc = 0.795 and logloss = 0.452. The evaluation accuracy rate of the cascade model from 18:00 to 22:00 can reach up to 0.99 at most. The studies show that this method improves the evaluation accuracy and provides technical references for the industry.",traditional online advertising evaluation indicators click rate conversion rate characterized limitations scope delayed feedback rendering inadequate real time comprehensive evaluation existing models insufficient accuracy fusing multiple features therefore research proposes evaluation framework integrates multimodal features attribute integration user classification constructs cascading model based timing features classification user types achieved attribute integration method utilizes multi source data including cognitive styles emotional states quantification features facilitated employing membership functions cascade model involves combining markov chains prefixspan algorithm mine temporal patterns results showed user type cluster center distances type differences significant activation relu auc cascade model logloss tanh activation auc logloss evaluation accuracy rate cascade model reach studies show method improves evaluation accuracy provides technical references industry
"Lane line boundary detection algorithms primarily rely on camera-acquired image data for accurate recognition. However, during image acquisition, distortions caused by glass refraction and the inherent imaging principles of the camera can compromise the precision of lane line detection. To enhance the precision of road environment perception, this study first conducts a comprehensive camera calibration to determine the intrinsic and extrinsic parameters, which serve as the basis for subsequent geometric distortion correction of the acquired images. Subsequently, Otsu’s method is employed for binary segmentation of lane images, and Inverse Perspective Mapping (IPM) is utilized to transform the perspective and extract precise lane line boundary coordinates. The eight-neighborhood boundary tracking algorithm is then employed to track lane lines, improving image processing speed while reducing false positives and false negatives. Finally, the Least Squares Method is utilized to fit the extracted boundary points of the lane lines, thereby constructing an accurate mathematical model for lane representation. The experiment was conducted using multi-element lane lines as the test platform. The experimental results demonstrate that the proposed algorithm achieves nearly a fivefold improvement in detection speed compared to the method without inverse perspective transformation. The Intersection over Union (IoU) between the extracted lane lines and the ground truth reaches 0.88, approaching the ideal value of 1.0. The pixel-level accuracy of lane line extraction is 93.7%, and the recall rate, representing the proportion of correctly extracted lane line pixels, is 95.2%. Furthermore, the balanced F1-score, which evaluates the trade-off between precision and recall, reaches 0.98, indicating excellent overall performance. Additionally, it exhibited superior robustness. These findings suggest that this method can significantly enhance the accuracy and efficiency of lane line detection in complex road environments, thereby providing reliable technical support for intelligent driving and Advanced Driver Assistance Systems (ADAS).",lane line boundary detection algorithms primarily rely camera acquired image data accurate recognition however image acquisition distortions caused glass refraction inherent imaging principles camera compromise precision lane line detection enhance precision road environment perception study first conducts comprehensive camera calibration determine intrinsic extrinsic parameters serve basis subsequent geometric distortion correction acquired images subsequently otsu method employed binary segmentation lane images inverse perspective mapping ipm utilized transform perspective extract precise lane line boundary coordinates eight neighborhood boundary tracking algorithm employed track lane lines improving image processing speed reducing false positives false negatives finally least squares method utilized fit extracted boundary points lane lines thereby constructing accurate mathematical model lane representation experiment conducted using multi element lane lines test platform experimental results demonstrate proposed algorithm achieves nearly fivefold improvement detection speed compared method without inverse perspective transformation intersection union iou extracted lane lines ground truth reaches approaching ideal value pixel level accuracy lane line extraction recall rate representing proportion correctly extracted lane line pixels furthermore balanced score evaluates trade precision recall reaches indicating excellent overall performance additionally exhibited superior robustness findings suggest method significantly enhance accuracy efficiency lane line detection complex road environments thereby providing reliable technical support intelligent driving advanced driver assistance systems adas
"At present, it is difficult to fully cover the actual needs of use in the actual renovation planning of old residential areas, which affects the implementation of the renovation. In order to improve the quality of renovation planning for old residential areas, a method based on the demand priority model was studied and designed. The process of establishing a model includes two aspects: content system and requirement priority decision-making method system. A specific list of transformation contents was constructed, expert interviews were conducted to improve the rationality of the transformation contents, and a decision-making system was proposed. The corresponding original decision matrix calculation method was established. When carrying out renovation planning, first identify the requirements and collect data, then build a model and classify it, and calculate the score to rank the requirements. The experimental results show that in the four quadrants established by the model, when the importance score is below 3.75, the various indicators are mainly located in quadrant 2. When the importance score exceeds 3.75, most indicators are located in the priority improvement zone, and a few indicators are located in the over concern zone and the lower value zone. The maximum distance between the transformation requirement and the positive ideal solution is 0.677 and 0.691, the minimum is 0.069 and 0.062, and the final score is 0.909. The research method can effectively assist in the planning of community renovation, and theoretically has high rationality and practicality in practical applications.",present difficult fully cover actual needs use actual renovation planning old residential areas affects implementation renovation order improve quality renovation planning old residential areas method based demand priority model studied designed process establishing model includes two aspects content system requirement priority decision making method system specific list transformation contents constructed expert interviews conducted improve rationality transformation contents decision making system proposed corresponding original decision matrix calculation method established carrying renovation planning first identify requirements collect data build model classify calculate score rank requirements experimental results show four quadrants established model importance score various indicators mainly located quadrant importance score exceeds indicators located priority improvement zone indicators located concern zone lower value zone maximum distance transformation requirement positive ideal solution minimum final score research method effectively assist planning community renovation theoretically high rationality practicality practical applications
"In physical education teaching, precise posture differentiation is very important for improving the teaching effect. To improve students’ skill mastery level and physical fitness, this study propostures an optimization method for physical education teaching based on the characteristics of different human postures. This method mainly adopts posture matching algorithm to more accurately process and respond to various human body differences and posture features. The results showed that the optimization methods of physical education teaching have improved students’ skill mastery level and physical fitness level. The skill mastery level of students using research methods reached 95.25%, with learning satisfaction and efficiency scores of 96.39 and 95.78, respectively. This method can effectively integrate human differences and posture characteristics and improve students’ skill acquisition and physical health. This study improves the overall effectiveness of physical education teaching and provides new ideas and references for subsequent related research and practice.",physical education teaching precise posture differentiation important improving teaching effect improve students skill mastery level physical fitness study propostures optimization method physical education teaching based characteristics different human postures method mainly adopts posture matching algorithm accurately process respond various human body differences posture features results showed optimization methods physical education teaching improved students skill mastery level physical fitness level skill mastery level students using research methods reached learning satisfaction efficiency scores respectively method effectively integrate human differences posture characteristics improve students skill acquisition physical health study improves overall effectiveness physical education teaching provides new ideas references subsequent related research practice
"Virtual Reality (VR) transforms second language acquisition by immersing learners in interactive, context-rich environments that foster cognitive engagement and enhance phonemic competence. This research explores the neurocognitive enhancements induced by VR-based English phoneme learning through multimodal bio-signal analysis and advanced machine learning (ML) techniques. A total of 450 English learners participated in immersive VR scenarios designed to target challenging English phonemes within authentic conversational tasks. Two types of datasets were collected: one in the form of a CSV file containing EEG signals and eye-tracking data, and the other comprising audio signal data. EEG and eye-tracking data were preprocessed using Z-score normalization to ensure consistency. Audio data were denoised using the Savitzky–Golay filter, which effectively preserves phonetic information while removing environmental noise. The cleaned data were fed into the feature extraction process. For the EEG and eye-tracking data, feature extraction was carried out using Independent Component Analysis (ICA), while Mel-frequency cepstral coefficients (MFCCs) were extracted from the audio data to capture detailed phonetic features essential for phoneme classification. This approach ensures accurate classification of phonemic performance and prediction of neurocognitive load in immersive VR-based phoneme learning. A feature-level fusion technique was employed to integrate the normalized event-log features and audio-based MFCCs into a unified, high-dimensional feature space, enabling comprehensive multimodal analysis. The Manta Ray Foraging Optimized Light Gradient-Boosting Machine (MRFO-LGBM) was introduced to optimize the LGBM model, enabling accurate classification of phonemic performance and prediction of neurocognitive load. The proposed method was implemented using Python 3.10.1. Experiments demonstrate that the proposed VR-enhanced cognitive phoneme recognition framework significantly outperforms other models, achieving superior results in terms of accuracy, F1-score, precision, and recall, with all metrics ranging from 95% to 96% in predicting neurocognitive states during immersive language acquisition. This research introduces a novel, scalable VR-based system that integrates bio-signal fusion and intelligent modeling to deliver personalized, measurable improvements in phonemic competence.",virtual reality transforms second language acquisition immersing learners interactive context rich environments foster cognitive engagement enhance phonemic competence research explores neurocognitive enhancements induced based english phoneme learning multimodal bio signal analysis advanced machine learning techniques total english learners participated immersive scenarios designed target challenging english phonemes within authentic conversational tasks two types datasets collected one form csv file containing eeg signals eye tracking data comprising audio signal data eeg eye tracking data preprocessed using score normalization ensure consistency audio data denoised using savitzky golay filter effectively preserves phonetic information removing environmental noise cleaned data fed feature extraction process eeg eye tracking data feature extraction carried using independent component analysis ica mel frequency cepstral coefficients mfccs extracted audio data capture detailed phonetic features essential phoneme classification approach ensures accurate classification phonemic performance prediction neurocognitive load immersive based phoneme learning feature level fusion technique employed integrate normalized event log features audio based mfccs unified high dimensional feature space enabling comprehensive multimodal analysis manta ray foraging optimized light gradient boosting machine mrfo lgbm introduced optimize lgbm model enabling accurate classification phonemic performance prediction neurocognitive load proposed method implemented using python experiments demonstrate proposed enhanced cognitive phoneme recognition framework significantly outperforms models achieving superior results terms accuracy score precision recall metrics ranging predicting neurocognitive states immersive language acquisition research introduces novel scalable based system integrates bio signal fusion intelligent modeling deliver personalized measurable improvements phonemic competence
"In today’s rapidly evolving global economy, the skills required by employers are constantly changing, placing increasing pressure on educational institutions to keep their curriculum relevant and responsive. Yet, academic programs comprising structured offerings like degrees, diplomas, and certificates frequently face challenges in adapting promptly to the changing demands of the labor market. This lag often leads to a sustained disconnect between the skills students acquire and those sought by employers. To address this challenge, this research introduces an innovative Artificial Intelligence (AI)-driven framework that leverages a Spatially-Aware Attention Long Short-Term Memory (SA-LSTM) architecture to harmonize academic curriculum with labor market trends. The framework processes two primary datasets: academic data, including curriculum outlines, course descriptions, student performance records, and labor market data, derived from online job postings, government employment reports, and industry white papers. The data undergoes a rigorous pre-processing pipeline, including cleaning, tokenization, and embedding using advanced Natural Language Processing (NLP) techniques. Term Frequency-Inverse Document Frequency (TF-IDF) is used to extract relevant features and feature-level fusion combines the features. The SA-LSTM model is the core of the system, incorporating spatial attention mechanisms to identify and model curriculum content and shifting skill requirements in the labor market. By leveraging the use of the attention mechanism, the model identifies contextual constraints in academic content and labor market trends to enable the identification of gaps in skills and new industry requirements. The SA-LSTM model generates curriculum recommendations from these results, suggesting new course modules, course modifications, and has better performance in accuracy with 96.2%, in comparison to baseline methods. This SA-LSTM offers a dynamic, data-driven solution to curriculum engineering, enabling schools to continuously adapt to the evolving needs of the job market.",today rapidly evolving global economy skills required employers constantly changing placing increasing pressure educational institutions keep curriculum relevant responsive yet academic programs comprising structured offerings like degrees diplomas certificates frequently face challenges adapting promptly changing demands labor market lag often leads sustained disconnect skills students acquire sought employers address challenge research introduces innovative artificial intelligence driven framework leverages spatially aware attention long short term memory lstm architecture harmonize academic curriculum labor market trends framework processes two primary datasets academic data including curriculum outlines course descriptions student performance records labor market data derived online job postings government employment reports industry white papers data undergoes rigorous pre processing pipeline including cleaning tokenization embedding using advanced natural language processing nlp techniques term frequency inverse document frequency idf used extract relevant features feature level fusion combines features lstm model core system incorporating spatial attention mechanisms identify model curriculum content shifting skill requirements labor market leveraging use attention mechanism model identifies contextual constraints academic content labor market trends enable identification gaps skills new industry requirements lstm model generates curriculum recommendations results suggesting new course modules course modifications better performance accuracy comparison baseline methods lstm offers dynamic data driven solution curriculum engineering enabling schools continuously adapt evolving needs job market
"As public power grids expand, utility providers face increasing challenges in managing vast inventories of infrastructure assets. Effective asset prioritization and disposal planning are critical to maintaining system reliability, minimizing operational risks, and optimizing financial performance. This research introduces a robust data-driven framework for asset prioritization and disposal planning by integrating advanced analytics and optimization techniques. Diverse datasets are collected, including asset condition reports, maintenance logs, operational performance data, failure records, and environmental parameters. Data preprocessing uses Z-score normalization to scale numerical features and handle anomalies, while missing values are treated using appropriate imputation methods to maintain dataset integrity. Asset prioritization is assessed using the Analytic Hierarchy Process (AHP), enabling structured multi-criteria decision-making based on factors such as failure impact, maintenance cost, and system importance. Natural Gradient Boosting (NGB) is employed to forecast asset degradation and estimate failure probability and Remaining Useful Life (RUL) with high accuracy. These predictive insights formed the basis for prioritizing assets for replacement or disposal. A Parallel Genetic Algorithm (PGA) is developed to optimize disposal schedules and support long-term planning within operational and financial constraints. This approach balances budget limitations with criticality and RUL outputs to generate efficient replacement strategies. Integrating AHP, NGB, and PGA within a unified decision-support system highlights the potential of hybrid analytical models in modernizing public infrastructure management. The results show that the hybrid approach performs significantly better than traditional approaches in terms of System Average Interruption Frequency Index (SAIFI) and System Average Interruption Duration Index (SAIDI). This approach supports proactive maintenance, reduces systemic risks, and enhances the overall resilience and performance of power distribution networks.",public power grids expand utility providers face increasing challenges managing vast inventories infrastructure assets effective asset prioritization disposal planning critical maintaining system reliability minimizing operational risks optimizing financial performance research introduces robust data driven framework asset prioritization disposal planning integrating advanced analytics optimization techniques diverse datasets collected including asset condition reports maintenance logs operational performance data failure records environmental parameters data preprocessing uses score normalization scale numerical features handle anomalies missing values treated using appropriate imputation methods maintain dataset integrity asset prioritization assessed using analytic hierarchy process ahp enabling structured multi criteria decision making based factors failure impact maintenance cost system importance natural gradient boosting ngb employed forecast asset degradation estimate failure probability remaining useful life rul high accuracy predictive insights formed basis prioritizing assets replacement disposal parallel genetic algorithm pga developed optimize disposal schedules support long term planning within operational financial constraints approach balances budget limitations criticality rul outputs generate efficient replacement strategies integrating ahp ngb pga within unified decision support system highlights potential hybrid analytical models modernizing public infrastructure management results show hybrid approach performs significantly better traditional approaches terms system average interruption frequency index saifi system average interruption duration index saidi approach supports proactive maintenance reduces systemic risks enhances overall resilience performance power distribution networks
"Aiming at the uncertain perturbation behaviors of collaborative innovation subjects under the influence of the external environment and internal organizational structure, which lead to abnormal operation of collaborative innovation network, this paper proposes a new collaborative innovation network vulnerability evaluation method based on the anti-entropy weight method and cloud model. Firstly, combining the attribute characteristics of collaborative innovation network, the method constructs collaborative innovation network vulnerability evaluation index from four dimensions, environmental vulnerability, network structure vulnerability, innovation subject collaborative vulnerability, and network governance vulnerability, and the anti-entropy weight method is then used to calculate the weights of evaluation indexes. Secondly, to effectively cope with the randomness, fuzzy, and uncertainty of the evaluation information in the process of comprehensive evaluation, and to enhance the accuracy of the evaluation results, this paper fully utilizes the advantages of the cloud model in transforming fuzzy qualitative information to quantitative information. It constructs a comprehensive evaluation model of the vulnerability of collaborative innovation network based on the cloud model. Finally, the proposed method’s rationality, validity, and scientific nature are verified through the specific enterprise case. The case study reveals that the key factors affecting the vulnerability of collaborative innovation network are partner selection, benefit distribution mechanism, and risk prevention mechanism. Based on the findings, corresponding management countermeasures and suggestions are put forward. The aim is to provide technical support for enterprises to carry out collaborative innovation.",aiming uncertain perturbation behaviors collaborative innovation subjects influence external environment internal organizational structure lead abnormal operation collaborative innovation network paper proposes new collaborative innovation network vulnerability evaluation method based anti entropy weight method cloud model firstly combining attribute characteristics collaborative innovation network method constructs collaborative innovation network vulnerability evaluation index four dimensions environmental vulnerability network structure vulnerability innovation subject collaborative vulnerability network governance vulnerability anti entropy weight method used calculate weights evaluation indexes secondly effectively cope randomness fuzzy uncertainty evaluation information process comprehensive evaluation enhance accuracy evaluation results paper fully utilizes advantages cloud model transforming fuzzy qualitative information quantitative information constructs comprehensive evaluation model vulnerability collaborative innovation network based cloud model finally proposed method rationality validity scientific nature verified specific enterprise case case study reveals key factors affecting vulnerability collaborative innovation network partner selection benefit distribution mechanism risk prevention mechanism based findings corresponding management countermeasures suggestions put forward aim provide technical support enterprises carry collaborative innovation
"In higher education, efficient feedback systems are essential for preserving and improving educational quality. However, higher education institutions continuously struggle to recover quality assessment processes by incorporating student feedback and academic data, also in delays, bias, and inadequacies in gaining valuable insights are common problems with traditional feedback methods. The primary concern lies in the manual processing of large, unstructured feedback datasets, leading to unnoticed issues and delayed interventions. To address this problem, the research goal is to design an Artificial intelligence (AI)-driven framework to automate, improve, and optimize feedback analysis for educational quality assessment. The data was collected through learning management system (LMS) activity logs, including student responses, participation rates, and academic performance records. Z-score normalization and missing value imputation were used as data preparation approaches to manage missing values and scale features consistently. Term Frequency-Inverse Document Frequency (TF-IDF) was used to process text data to extract features efficiently. The research employs an Adaptive Butterfly-Attention Mechanism Generative Adversarial Network (AB-Att-GAN). The AB and Attention module uses a GAN design to produce refined feedback patterns, focusing on significant feedback features across scales. This method helps interpret student concerns, aligns teaching practices dynamically, and allows early intervention strategies, ensuring quality and relevance. Experimental outcomes using Python showed that the AB-Att-GAN framework attains superior performance when compared to conventional deep learning (DL) models, achieving 98.5% accuracy. Additionally, the technology provides tailored feedback loops and forecast insights for early academic intervention. The AB-Att-GAN architecture displays a greater capacity for evolving high quality, real-time educational feedback systems and promoting continuous institutional development.",higher education efficient feedback systems essential preserving improving educational quality however higher education institutions continuously struggle recover quality assessment processes incorporating student feedback academic data also delays bias inadequacies gaining valuable insights common problems traditional feedback methods primary concern lies manual processing large unstructured feedback datasets leading unnoticed issues delayed interventions address problem research goal design artificial intelligence driven framework automate improve optimize feedback analysis educational quality assessment data collected learning management system lms activity logs including student responses participation rates academic performance records score normalization missing value imputation used data preparation approaches manage missing values scale features consistently term frequency inverse document frequency idf used process text data extract features efficiently research employs adaptive butterfly attention mechanism generative adversarial network att gan attention module uses gan design produce refined feedback patterns focusing significant feedback features across scales method helps interpret student concerns aligns teaching practices dynamically allows early intervention strategies ensuring quality relevance experimental outcomes using python showed att gan framework attains superior performance compared conventional deep learning models achieving accuracy additionally technology provides tailored feedback loops forecast insights early academic intervention att gan architecture displays greater capacity evolving high quality real time educational feedback systems promoting continuous institutional development
"Choral music, as a collective art form, achieves harmonious and rich musical effects through multi-voice coordination and cooperation. However, sound balance issues often affect the quality of choir singing. Traditional methods mainly rely on subjective adjustments by conductors and vocal teachers, lacking consistency, objectivity, and adaptability. To address this issue, this article proposed a choir sound balance optimization model based on simulated annealing (SA) algorithm. The experiment included sound data collection, environmental characteristic measurement, and evaluation index setting, which verified the effectiveness of this method in dealing with different voice parts and complex environmental factors. The outcomes demonstrated that the simulated annealing algorithm significantly reduced the standard deviation of the volume and frequency of each voice and improved the balance of the volume and frequency. In all scenarios, the volume standard deviation of the simulated annealing adjustment method was smaller or equal to the standard deviation of the conventional adjustment method. The standard deviation of the simulated annealing adjustment method was always the lowest among all voice parts and time windows. The average frequency standard deviation of the four voice parts decreased by 51.21% and 31.70% in indoor environments, 52.10% and 30.96% in semi-open environments, and 49.25% and 29.49% in outdoor environments, respectively. The subjective evaluation results of users further verified the effectiveness of the model, and various listeners gave high ratings to the adjusted volume balance and overall sound quality. This indicates that the method has broad application potential and can cope with diverse practical performance environments. The research results of this article provide a scientific sound balance optimization tool for choir groups, which can help choir conductors and sound engineers achieve higher levels of sound balance and harmony in actual performances.",choral music collective art form achieves harmonious rich musical effects multi voice coordination cooperation however sound balance issues often affect quality choir singing traditional methods mainly rely subjective adjustments conductors vocal teachers lacking consistency objectivity adaptability address issue article proposed choir sound balance optimization model based simulated annealing algorithm experiment included sound data collection environmental characteristic measurement evaluation index setting verified effectiveness method dealing different voice parts complex environmental factors outcomes demonstrated simulated annealing algorithm significantly reduced standard deviation volume frequency voice improved balance volume frequency scenarios volume standard deviation simulated annealing adjustment method smaller equal standard deviation conventional adjustment method standard deviation simulated annealing adjustment method always lowest among voice parts time windows average frequency standard deviation four voice parts decreased indoor environments semi open environments outdoor environments respectively subjective evaluation results users verified effectiveness model various listeners gave high ratings adjusted volume balance overall sound quality indicates method broad application potential cope diverse practical performance environments research results article provide scientific sound balance optimization tool choir groups help choir conductors sound engineers achieve higher levels sound balance harmony actual performances
"Image emotion classification technology has been widely applied in fields such as social media. How to accurately extract emotional information from numerous images has become an essential issue in computer vision. To achieve high-precision image emotion classification, this study proposes a polarity-aware attention mechanism for both negative and positive coarse-grained sentiment classifications. Among them, polarity perception refers to the perception and classification of the binary polarity (positive/negative) of image emotions, while supporting fine-grained emotional intensity evaluation. At the same time, a classification network combining polarity-aware attention module and Transformer is introduced to achieve fine-grained emotion classification containing eight emotions. The results showed that the proposed polarity-aware attention mechanism classification method had an average accuracy of 95.88%, an average recall rate of 94.94%, and an average F1-value of 95.62% on the Twitter I binary sentiment classification dataset. In the ArtPhoto eight emotion classification datasets, the fusion of polarity-aware attention module and Transformer classification network achieved the highest classification accuracy of 98.23% and the lowest accuracy of 96.14% for the eight emotions. The highest classification accuracy of the weighted spatial context network was only 89.01%. The results indicate that the proposed image sentiment classification technique has achieved significant performance advantages in both coarse-grained and fine-grained sentiment classification tasks. The study provides important methodological support for applications in multiple fields including social media and sentiment computing.",image emotion classification technology widely applied fields social media accurately extract emotional information numerous images become essential issue computer vision achieve high precision image emotion classification study proposes polarity aware attention mechanism negative positive coarse grained sentiment classifications among polarity perception refers perception classification binary polarity positive negative image emotions supporting fine grained emotional intensity evaluation time classification network combining polarity aware attention module transformer introduced achieve fine grained emotion classification containing eight emotions results showed proposed polarity aware attention mechanism classification method average accuracy average recall rate average value twitter binary sentiment classification dataset artphoto eight emotion classification datasets fusion polarity aware attention module transformer classification network achieved highest classification accuracy lowest accuracy eight emotions highest classification accuracy weighted spatial context network results indicate proposed image sentiment classification technique achieved significant performance advantages coarse grained fine grained sentiment classification tasks study provides important methodological support applications multiple fields including social media sentiment computing
"Aiming at the problems of large amount of calculation and slow detection speed of the existing soccer pose estimation model, this paper proposes a lightweight improved algorithm based on YOLOv8-Pose model and applies it to the recognition task of soccer player’s foul behavior. Firstly, RepConv and DBB reparameterization modules are introduced to enhance the ability of multi-scale feature fusion and solve the problem of large scale difference of visual objects. A centralized intra-layer adjustment feature pyramid network is designed, which combines the deformable attention mechanism and CASPPF in a parallel way. The pyramid network is globally centralized adjusted in a top–down manner, and the spatial weight of the global representation in the network is increased, so that the improved algorithm can obtain comprehensive and discriminative feature representation. The cross-entropy loss function is replaced by the exponential sliding sample weighting function (EMA‐SlideLoss) to enhance the classification ability of the model and improve the stability of training. Using these methods, we propose an improved model RDE-YOLOv8-Pose model. The experimental results show that compared with the YOLOv8-Pose model, this research method improves mAP@0.5 by 0.142 and achieves a detection speed of 47.4 frames/s. This study can effectively improve the motion capture ability of artificial intelligence methods for soccer players, and provide more efficient and accurate technical support for intelligent monitoring of sports events and auxiliary referee decision-making.",aiming problems large amount calculation slow detection speed existing soccer pose estimation model paper proposes lightweight improved algorithm based yolov pose model applies recognition task soccer player foul behavior firstly repconv dbb reparameterization modules introduced enhance ability multi scale feature fusion solve problem large scale difference visual objects centralized intra layer adjustment feature pyramid network designed combines deformable attention mechanism casppf parallel way pyramid network globally centralized adjusted top manner spatial weight global representation network increased improved algorithm obtain comprehensive discriminative feature representation cross entropy loss function replaced exponential sliding sample weighting function ema slideloss enhance classification ability model improve stability training using methods propose improved model rde yolov pose model experimental results show compared yolov pose model research method improves map achieves detection speed frames study effectively improve motion capture ability artificial intelligence methods soccer players provide efficient accurate technical support intelligent monitoring sports events auxiliary referee decision making
"With a profound legacy of historical and cultural heritage, China has been consistently committed to preserving its rich heritage. This study scrutinizes China’s approaches and effectiveness in heritage conservation planning by employing advanced text analysis methods on a corpus of 99 conservation plans for historical and cultural cities, towns, and villages. Utilizing Python for data extraction and VOSviewer for keyword analysis, the research categorizes different types of heritage conservation plans, revealing the nuances and focal points of each category. Additionally, this paper compares the focuses of China’s conservation planning with the six steps of Historic Urban Landscape (HUL) and assesses the deficiencies in the planning compilation and differences with the HUL approach. The main conclusions of this study include: (1) Despite detailed surveys in heritage conservation planning, China’s heritage conservation methods exhibit significant gaps, especially in public participation and vulnerability assessment of heritage sites, compared to international heritage conservation concepts. These shortcomings limit the effectiveness of the plans in achieving balanced and sustainable development in heritage conservation. (2) The analysis reveals significant variations in planning strategies across different types and regions, indicating the need for more tailored and region-specific approaches in heritage conservation planning to address the diverse contexts within China. This paper provides an important empirical analysis sample for quantitative research on China’s heritage conservation planning, offering insights for achieving a more balanced and sustainable development between heritage conservation and urban development.",profound legacy historical cultural heritage china consistently committed preserving rich heritage study scrutinizes china approaches effectiveness heritage conservation planning employing advanced text analysis methods corpus conservation plans historical cultural cities towns villages utilizing python data extraction vosviewer keyword analysis research categorizes different types heritage conservation plans revealing nuances focal points category additionally paper compares focuses china conservation planning six steps historic urban landscape hul assesses deficiencies planning compilation differences hul approach main conclusions study include despite detailed surveys heritage conservation planning china heritage conservation methods exhibit significant gaps especially public participation vulnerability assessment heritage sites compared international heritage conservation concepts shortcomings limit effectiveness plans achieving balanced sustainable development heritage conservation analysis reveals significant variations planning strategies across different types regions indicating need tailored region specific approaches heritage conservation planning address diverse contexts within china paper provides important empirical analysis sample quantitative research china heritage conservation planning offering insights achieving balanced sustainable development heritage conservation urban development
"The development of e-commerce has brought the public into the era of e-commerce logistics 4.0. Therefore, optimizing logistics network location and path planning is important to improve logistics efficiency and reduce operating costs. To deal with the increasingly complex market demand and the changing distribution environment, the diversity enhancement mechanism and neighborhood search strategy are introduced into the particle swarm optimization algorithm. This optimized algorithm is combined with the density peak clustering algorithm. Then, the e-commerce logistics network location and path planning algorithm is proposed. The optimal path on the grid map only produced 11 inflection points and passed 24 grids. It was the best in the comparison model. Finally, the total route length in modeling and testing was 32.51 km, the average loading rate was 97.85%, the total cost was 668.18 ¥, and the calculation time was 25.34s. The proposed model performs well in many key performance indicators, which can realize the efficient operation of logistics network. The research aims to provide new ideas and methods for network location and path planning of e-commerce logistics 4.0, which has theoretical and practical application value.",development commerce brought public commerce logistics therefore optimizing logistics network location path planning important improve logistics efficiency reduce operating costs deal increasingly complex market demand changing distribution environment diversity enhancement mechanism neighborhood search strategy introduced particle swarm optimization algorithm optimized algorithm combined density peak clustering algorithm commerce logistics network location path planning algorithm proposed optimal path grid map produced inflection points passed grids best comparison model finally total route length modeling testing average loading rate total cost calculation time proposed model performs well many key performance indicators realize efficient operation logistics network research aims provide new ideas methods network location path planning commerce logistics theoretical practical application value
"With the growing complexity of enterprise financial data, traditional financial warning models face limitations in handling large datasets and outliers. This study proposes a novel financial warning model integrating ensemble learning and stacked generalization. A two-layer fusion model is constructed using stacking generalization, while SMOTETomek addresses data imbalance. Model parameters are optimized via grid search and five-fold cross validation. Experimental results demonstrate superior performance, with an average accuracy of over 90%. The accuracy on the training and testing sets reach 0.93 and 0.95, respectively. The model achieves a low false positive rate (3.8%) and false negative rate (3.2%) in the low debt category, outperforming comparison models. It also exhibits high resource efficiency and low time costs, making it an ideal tool for enterprise financial early warning. The model aids in identifying financial risks, enabling proactive response strategies, promoting healthy financial management, and enhancing market stability.",growing complexity enterprise financial data traditional financial warning models face limitations handling large datasets outliers study proposes novel financial warning model integrating ensemble learning stacked generalization two layer fusion model constructed using stacking generalization smotetomek addresses data imbalance model parameters optimized via grid search five fold cross validation experimental results demonstrate superior performance average accuracy accuracy training testing sets reach respectively model achieves low false positive rate false negative rate low debt category outperforming comparison models also exhibits high resource efficiency low time costs making ideal tool enterprise financial early warning model aids identifying financial risks enabling proactive response strategies promoting healthy financial management enhancing market stability
"With the growth of the commodity economy, packaging has evolved beyond its original function of protecting goods to play a crucial role in capturing consumer attention, influencing purchasing decisions, and building brand identity. From a consumer perspective, emotional design in packaging addresses both their emotional aspirations and desire for a quality life. This study focuses on the emotional design of product packaging, aiming to meet consumers’ material needs while also fulfilling their deeper, spiritual expectations, ultimately enhancing the consumer experience and strengthening brand connection. This topic is based on MEC(Mobile Edge Computing), the research of commodity packaging design and emotion perception recognition. Design an algorithm framework of trademark pattern integrity detection and a model for image emotion classification and recognition. The binary image of trademark pattern is obtained by color image segmentation method, then the shape feature descriptor of trademark pattern is calculated, and the feature dimension is reduced by PCA(principal component analysis) method to reduce irrelevant features and feature data redundancy. The results show that the performance of this method in accuracy, recall, and F1 value are improved by 7.1%, 19.02%, and 11.20%, respectively. It provides a theoretical basis for how to choose the right starting point for packaging design and broadens the horizon for the future research of packaging design.",growth commodity economy packaging evolved beyond original function protecting goods play crucial role capturing consumer attention influencing purchasing decisions building brand identity consumer perspective emotional design packaging addresses emotional aspirations desire quality life study focuses emotional design product packaging aiming meet consumers material needs also fulfilling deeper spiritual expectations ultimately enhancing consumer experience strengthening brand connection topic based mec mobile edge computing research commodity packaging design emotion perception recognition design algorithm framework trademark pattern integrity detection model image emotion classification recognition binary image trademark pattern obtained color image segmentation method shape feature descriptor trademark pattern calculated feature dimension reduced pca principal component analysis method reduce irrelevant features feature data redundancy results show performance method accuracy recall value improved respectively provides theoretical basis choose right starting point packaging design broadens horizon future research packaging design
"When safety hazards such as tunnel collapse or water inflow occur, it not only increases construction cost and difficulty but also causes personal injury and death to construction personnel, which has adverse effects on society. A tunnel structural safety monitoring device system based on Internet of Things low-power communication is suggested to increase the stability and security of the tunnel building process. It monitors construction conditions by combining ambient sensors and gathering tunnel data using phase laser ranging. The results showed that compared with other methods, the accuracy, recall, F1 value, and mean square error of tunnel structure safety monitoring based on remote radio technology were 87.51%, 86.17%, 0.891, and 0.42, respectively. When the temperature was between 5 and 25°C, the reading difference was between -1 mm and 1 mm, and the fluctuation range of the reading difference was not large. The temperature had a small impact on the accuracy of laser ranging. When the air humidity was greater than 60% or the dust concentration was greater than 60ppm, the reading difference significantly increased, and the air humidity and dust concentration had an impact on the accuracy of laser ranging. The proposed method has strong reliability in monitoring the safety of tunnel structures, effectively shortening measurement time and improving monitoring efficiency.",safety hazards tunnel collapse water inflow occur increases construction cost difficulty also causes personal injury death construction personnel adverse effects society tunnel structural safety monitoring device system based internet things low power communication suggested increase stability security tunnel building process monitors construction conditions combining ambient sensors gathering tunnel data using phase laser ranging results showed compared methods accuracy recall value mean square error tunnel structure safety monitoring based remote radio technology respectively temperature reading difference fluctuation range reading difference large temperature small impact accuracy laser ranging air humidity greater dust concentration greater ppm reading difference significantly increased air humidity dust concentration impact accuracy laser ranging proposed method strong reliability monitoring safety tunnel structures effectively shortening measurement time improving monitoring efficiency
"To realize the fault early warning function of healthcare medical equipment, this study constructs an equipment fault early warning model and combines particle swarm optimization and long-short term memory network to test the performance. The study obtains the optimal value of data feature vectors through particle swarm optimization algorithm and uses long-short term memory prediction model to predict and classify feature signals. In addition, the study uses the binning method to denoise the collected data and normalizes the denoised data so that each feature data was distributed between 0 and 1. The results showed that the fitting between actual values and predicted values was good. The maximum values of Precision, Recall, and F1 of the designed warning model were 97.98%, 97.82%, and 97.68%, respectively, which were significantly better than the control model. This indicated that the warning model designed by the research had good performance. The combination of the particle swarm optimization algorithm and the long short-term memory network model offered unique advantages in the medical field. The particle swarm optimization algorithm could efficiently identify key features, avoid local optima, and improve the model’s generalization ability. Long short-term memory networks could accurately capture the dynamic trends of faults and adapt to the temporal nature of medical data. Combining the two could meet the high-precision, real-time, and adaptive requirements of medical equipment fault warnings, effectively improving their accuracy.",realize fault early warning function healthcare medical equipment study constructs equipment fault early warning model combines particle swarm optimization long short term memory network test performance study obtains optimal value data feature vectors particle swarm optimization algorithm uses long short term memory prediction model predict classify feature signals addition study uses binning method denoise collected data normalizes denoised data feature data distributed results showed fitting actual values predicted values good maximum values precision recall designed warning model respectively significantly better control model indicated warning model designed research good performance combination particle swarm optimization algorithm long short term memory network model offered unique advantages medical field particle swarm optimization algorithm could efficiently identify key features avoid local optima improve model generalization ability long short term memory networks could accurately capture dynamic trends faults adapt temporal nature medical data combining two could meet high precision real time adaptive requirements medical equipment fault warnings effectively improving accuracy
"Artificial Intelligence (AI) virtual coaches are currently a research hotspot in the field of sports and health. To improve the effectiveness of fitness training, this article proposes an intelligent virtual coaching system based on reinforcement learning and motion capture technology. Specifically, by collecting user motion data and utilizing two-dimensional human pose estimation methods for motion capture technology, a training model is constructed. Secondly, a reinforcement learning reward function combining imitation rewards and adaptive penalties was designed to balance the needs of action imitation and diversity adaptation, thereby generating a natural and stable training action sequence. Then, to alleviate the training difficulty caused by the initial posture, a dynamic weight adjustment mechanism was proposed. Finally, the system was experimentally validated on COCO, MPII, and self-built datasets, and the experimental results showed that the system performed excellently in terms of action accuracy, adaptability, and user satisfaction.",artificial intelligence virtual coaches currently research hotspot field sports health improve effectiveness fitness training article proposes intelligent virtual coaching system based reinforcement learning motion capture technology specifically collecting user motion data utilizing two dimensional human pose estimation methods motion capture technology training model constructed secondly reinforcement learning reward function combining imitation rewards adaptive penalties designed balance needs action imitation diversity adaptation thereby generating natural stable training action sequence alleviate training difficulty caused initial posture dynamic weight adjustment mechanism proposed finally system experimentally validated coco mpii self built datasets experimental results showed system performed excellently terms action accuracy adaptability user satisfaction
"Current recommendation systems for tourism struggle to capture the dynamic changes in user preferences. Therefore, this study proposes a knowledge graph embedding technique that combines dynamic mapping matrices to construct a tourism recommendation model. Meanwhile, bidirectional long short-term memory networks and node-level attention mechanisms are introduced to enhance the modeling ability for dynamic changes in user behavior. The experimental results on the YAGO11k dataset showed that the accuracy of the training set reached 99.4%, and the model had excellent training performance and generalization ability. In the evaluation indicators of the knowledge graph, the average ranking and average reciprocal ranking were 452 and 0.430, significantly better than the baseline, with a hit rate of up to 29.1%. This model provides an effective solution for personalized travel recommendations.",current recommendation systems tourism struggle capture dynamic changes user preferences therefore study proposes knowledge graph embedding technique combines dynamic mapping matrices construct tourism recommendation model meanwhile bidirectional long short term memory networks node level attention mechanisms introduced enhance modeling ability dynamic changes user behavior experimental results yago dataset showed accuracy training set reached model excellent training performance generalization ability evaluation indicators knowledge graph average ranking average reciprocal ranking significantly better baseline hit rate model provides effective solution personalized travel recommendations
"Given the high dimensionality and intricate characteristics of power measurement data, conventional attack identification methodologies struggle to effectively detect faults and false attacks within power systems, thereby jeopardizing the secure and stable operation of the power grid. This study introduces a novel recognition framework, termed CNN BiLSTM + QAN, designed specifically for the identification of minor faults and attacks in power systems. The proposed network employs Convolutional Neural Networks (CNN) to extract salient features from fault signals and integrates the Quantum Attention Network (QAN) into a Bidirectional Long Short-Term Memory (BiLSTM) architecture, thereby constructing a BiLSTM + QAN recognition network that significantly enhances the characterization and recognition capabilities of localized fault signal features. To mitigate the influence of anomalous noise, the loss function is formulated using Cross Entropy Loss, and gradient-based optimization techniques are employed to achieve global convergence of the network, further augmenting its efficacy in detecting minor faults. Utilizing simulated data of minor faults from the IEEE 14-bus system, comparative experiments demonstrate that the CNN BiLSTM + QAN framework developed in this study effectively recognizes minor faults, achieving an impressive recognition accuracy of 98.34%.",given high dimensionality intricate characteristics power measurement data conventional attack identification methodologies struggle effectively detect faults false attacks within power systems thereby jeopardizing secure stable operation power grid study introduces novel recognition framework termed cnn bilstm qan designed specifically identification minor faults attacks power systems proposed network employs convolutional neural networks cnn extract salient features fault signals integrates quantum attention network qan bidirectional long short term memory bilstm architecture thereby constructing bilstm qan recognition network significantly enhances characterization recognition capabilities localized fault signal features mitigate influence anomalous noise loss function formulated using cross entropy loss gradient based optimization techniques employed achieve global convergence network augmenting efficacy detecting minor faults utilizing simulated data minor faults ieee bus system comparative experiments demonstrate cnn bilstm qan framework developed study effectively recognizes minor faults achieving impressive recognition accuracy
"With the development of artificial intelligence technology and deep learning, the application of related network algorithm models in human motion posture recognition is becoming increasingly widespread. To address low accuracy and complex networks in traditional algorithms for human motion posture recognition, this study proposes a method based on particle swarm optimization to improve the backpropagation neural network for recognizing and analyzing human motion posture. The model first extracts key nodes from image data through improved OpenPose, performs viewpoint transformation, and uses an optimized backpropagation neural network to recognize relevant human postures. The experiments showed that the improved OpenPose algorithm had high accuracy in extracting key nodes. When the viewing angle was 108°, the error value generated by viewing angle conversion was the smallest and the accuracy was the highest. The Schaffer function of the backpropagation neural network model optimized by particle swarm optimization and small batch gradient descent converged after 60 and 99 iterations, respectively. The Griebank function converged after 78 and 98 iterations. The particle swarm optimization-based backpropagation neural network achieved an average improvement of 20%, 22%, 16%, and 12% in recognition rates for four different human motion postures compared to other algorithms. The results show that the particle swarm algorithm-improved backpropagation neural network has higher computational efficiency and better accuracy in human motion posture recognition.",development artificial intelligence technology deep learning application related network algorithm models human motion posture recognition becoming increasingly widespread address low accuracy complex networks traditional algorithms human motion posture recognition study proposes method based particle swarm optimization improve backpropagation neural network recognizing analyzing human motion posture model first extracts key nodes image data improved openpose performs viewpoint transformation uses optimized backpropagation neural network recognize relevant human postures experiments showed improved openpose algorithm high accuracy extracting key nodes viewing angle error value generated viewing angle conversion smallest accuracy highest schaffer function backpropagation neural network model optimized particle swarm optimization small batch gradient descent converged iterations respectively griebank function converged iterations particle swarm optimization based backpropagation neural network achieved average improvement recognition rates four different human motion postures compared algorithms results show particle swarm algorithm improved backpropagation neural network higher computational efficiency better accuracy human motion posture recognition
"To achieve high-quality development of rural landscapes, this study quantitatively analyzes and identifies the landscape morphology of polder fields through an improved self-organizing mapping neural network (SOM NN). The study first uses landscape morphology index to quantitatively translate the polder landscape, and then preprocesses the data through factor analysis to reduce dimensionality. On this basis, the K-means algorithm is introduced for pre-classification, and the SOM NN is improved by combining the confidence matrix. The clustering performance of the algorithm is first tested on the Iris data set, which contains measurements of three different species of iris (Iris), including calyx length and width, and petal length and width. The experimental outcomes denote that the average contour score and average adjusted Rand index of the improved SOM NN are 0.549 and 0.978, respectively. Its clustering accuracy and recall are 0.934 and 0.923, respectively, which are higher than other algorithms. In computation time, the average computation time of the improved SOM NN is only 0.022 ms, far lower than other algorithms. Moreover, the effectiveness of the improved SOM NN in identifying different types of polder landscape forms is verified through practical application analysis of Tangpu polder and Taixing polder in Jiangsu Province. The above results indicate that the improved SOM NN has great potential for application in landscape morphology quantification research, and can provide scientific basis for the management and protection of polder landscapes.",achieve high quality development rural landscapes study quantitatively analyzes identifies landscape morphology polder fields improved self organizing mapping neural network som study first uses landscape morphology index quantitatively translate polder landscape preprocesses data factor analysis reduce dimensionality basis means algorithm introduced pre classification som improved combining confidence matrix clustering performance algorithm first tested iris data set contains measurements three different species iris iris including calyx length width petal length width experimental outcomes denote average contour score average adjusted rand index improved som respectively clustering accuracy recall respectively higher algorithms computation time average computation time improved som far lower algorithms moreover effectiveness improved som identifying different types polder landscape forms verified practical application analysis tangpu polder taixing polder jiangsu province results indicate improved som great potential application landscape morphology quantification research provide scientific basis management protection polder landscapes
"This study proposes an interior design display model based on a neural style transfer algorithm and augmented reality technology to address the current time-consuming, single-function, and high information requirements of interior design display methods. This model combines virtual reality overlay technology to enhance users’ visual and auditory interaction experience, reasonably allocates the weights of the style loss function and content loss function, and reduces spatial loss. In the experiment, the model training achieved a loss value of 162,091.33 and a space loss rate of more than 1%. In practical applications, a reality reproduction score of 8.7 was achieved, with a peak signal-to-noise ratio higher than 50 dB, indicating a relatively small loss of image quality. Furthermore, the click-through rate of users has increased by 0.115, and the purchase conversion rate has increased by 0.270, indicating that this model has significant effectiveness and advancement in interior design displays.",study proposes interior design display model based neural style transfer algorithm augmented reality technology address current time consuming single function high information requirements interior design display methods model combines virtual reality overlay technology enhance users visual auditory interaction experience reasonably allocates weights style loss function content loss function reduces spatial loss experiment model training achieved loss value space loss rate practical applications reality reproduction score achieved peak signal noise ratio higher indicating relatively small loss image quality furthermore click rate users increased purchase conversion rate increased indicating model significant effectiveness advancement interior design displays
"Torsional effects in planar irregular buildings can lead to structural damage, making torsional response analysis of buildings a critical aspect of structural design. This study conducts numerical simulations on L-shaped frames subjected to frequent earthquakes to investigate the influence of low-frequency content on the torsional responses of planar irregular frame structures (PIFSs). Five L-shaped frame models with varying length-to-width ratios (L/B) of the protruding limbs-1:1, 1.5:1, 2:1, 2.5:1, and 3:1 were developed using ABAQUS. Columns were modeled using solid elements, reinforcing steel with truss elements, and floor slabs with shell elements. The peak shear forces and bending moments in corner columns were compared under various conditions of low-frequency content to analyze the effects of low-frequency content and L/B ratios on wave passage effect in frames. The results indicate that when the low-frequency content of seismic waves is sufficient, wave passage effect occurs in outer corner columns, whereas inner corner columns remain unaffected. At apparent wave velocity of 2000 m/s, increasing L/B ratios amplify peak shear forces and bending moments in wave-facing outer corner columns by 28.3% and 43.8%, respectively. Conversely, when the low-frequency content of seismic waves is insufficient, neither outer nor inner corner columns exhibit wave passage effect.",torsional effects planar irregular buildings lead structural damage making torsional response analysis buildings critical aspect structural design study conducts numerical simulations shaped frames subjected frequent earthquakes investigate influence low frequency content torsional responses planar irregular frame structures pifss five shaped frame models varying length width ratios protruding limbs developed using abaqus columns modeled using solid elements reinforcing steel truss elements floor slabs shell elements peak shear forces bending moments corner columns compared various conditions low frequency content analyze effects low frequency content ratios wave passage effect frames results indicate low frequency content seismic waves sufficient wave passage effect occurs outer corner columns whereas inner corner columns remain unaffected apparent wave velocity increasing ratios amplify peak shear forces bending moments wave facing outer corner columns respectively conversely low frequency content seismic waves insufficient neither outer inner corner columns exhibit wave passage effect
"The traditional methods of English oral evaluation and pronunciation optimization suffer from high subjectivity and delayed feedback. In response to this issue, this article aims to use speech recognition technology to provide objective and real-time oral evaluation and personalized pronunciation optimization guidance. Firstly, this article establishes a corpus containing rich phonetic data, including English oral samples with different accents and pronunciation characteristics. Secondly, this article adopts a DNN (deep neural network)-HMM (hidden Markov model) hybrid model for speech recognition, recognizing learners’ oral expressions and analyzing pronunciation errors. Then, personalized pronunciation optimization guidance is provided based on the recognition results. Finally, this article conducts experiments to verify the effectiveness of the algorithm. The experimental results show that the algorithm has made significant progress in spoken language assessment and pronunciation optimization, and the DNN-HMM model shows better performance in terms of the phonological error rate, word error rate and grammatical error rate, with the average phonological error rate, average word error rate and average grammatical error rate reaching 7.9%, 13.3% and 5.1% in multiple corpora, which significantly outperforms GMM (Gaussian mixture model)-HMM model, RNNs (recurrent neural networks), LSTM (Long Short-Term Memory), and Transformer. Moreover, participants demonstrated noticeable improvements in speaking speed, and their overall scores rose significantly, reaching a fluent level after utilizing the system for pronunciation training. The integration of DNN-HMM offers a novel approach for English speaking assessment. Through precise error detection and personalized optimization recommendations, learners’ pronunciation skills and overall speaking proficiency were significantly enhanced.",traditional methods english oral evaluation pronunciation optimization suffer high subjectivity delayed feedback response issue article aims use speech recognition technology provide objective real time oral evaluation personalized pronunciation optimization guidance firstly article establishes corpus containing rich phonetic data including english oral samples different accents pronunciation characteristics secondly article adopts dnn deep neural network hmm hidden markov model hybrid model speech recognition recognizing learners oral expressions analyzing pronunciation errors personalized pronunciation optimization guidance provided based recognition results finally article conducts experiments verify effectiveness algorithm experimental results show algorithm made significant progress spoken language assessment pronunciation optimization dnn hmm model shows better performance terms phonological error rate word error rate grammatical error rate average phonological error rate average word error rate average grammatical error rate reaching multiple corpora significantly outperforms gmm gaussian mixture model hmm model rnns recurrent neural networks lstm long short term memory transformer moreover participants demonstrated noticeable improvements speaking speed overall scores rose significantly reaching fluent level utilizing system pronunciation training integration dnn hmm offers novel approach english speaking assessment precise error detection personalized optimization recommendations learners pronunciation skills overall speaking proficiency significantly enhanced
"To provide tourists with more personalized scenic spot route planning scheme, this study combines multi-dimensional information and A* algorithm to construct an improved intelligent scenic spot route planning model. The model first introduces multi-dimensional environmental semantic information such as terrain undulation, tourist density, emergency events, and popularity of scenic spots, and then constructs an improved dynamic road network data model. Then, the A* algorithm is improved using the lightning search algorithm to improve the dynamic path planning capability. Empirical analysis is conducted on the intelligent planning model for tourist attraction routes constructed. The planned scenic spot route had 5 turning points, with a total length of 0.9 km. It was smoother than that of other models and better meets the planning requirements for the shortest path. When the iteration was 15, it entered the convergence stage and the execution time was 2.8 s, which was significantly lower than the execution time of other models. The tourist satisfaction score was 9.4 points, and the expert satisfaction score was 8.9 points. It was higher than the satisfaction scores of other models and better meets the actual needs. Based on the above results, the intelligent planning model for tourist attractions based on heuristic search algorithms has better performance than other comparative methods, providing tourists with a more personalized travel experience and promoting the tourism industry towards a more intelligent and sustainable direction.",provide tourists personalized scenic spot route planning scheme study combines multi dimensional information algorithm construct improved intelligent scenic spot route planning model model first introduces multi dimensional environmental semantic information terrain undulation tourist density emergency events popularity scenic spots constructs improved dynamic road network data model algorithm improved using lightning search algorithm improve dynamic path planning capability empirical analysis conducted intelligent planning model tourist attraction routes constructed planned scenic spot route turning points total length smoother models better meets planning requirements shortest path iteration entered convergence stage execution time significantly lower execution time models tourist satisfaction score points expert satisfaction score points higher satisfaction scores models better meets actual needs based results intelligent planning model tourist attractions based heuristic search algorithms better performance comparative methods providing tourists personalized travel experience promoting tourism industry towards intelligent sustainable direction
"In this study, we present a new multimodal subtitle translating system integrating a Generalized Regression Neural Network (GRNN) based syntactic error correcting mechanism. To produce accurate and fluid subtitles, our system aggregates text, video, and audio inputs. The grammar error correction system based on GRNN finds and fixes syntactic mistakes in the translated subtitles, therefore raising their general quality. Our testing findings reveal notable increases in subtitle translating accuracy and fluency of the proposed method. With a translation accuracy of 92.5%, the proposed method beats the baseline by 10.2%. By means of a 95.1% syntax error correction accuracy, the GRNN-based syntax error correction system lowers the syntax error rate by 70.5% Our method achieves a fluency score of 4.8/5.0, compared to 4.2/5.0 for the baseline technique, therefore improving the fluency of the translated subtitles. With a BLEU score of 0.85, our method shows great degree of similarity between the reference and translated subtitles. In all measures—including BLEU score, translation accuracy, syntax error correction accuracy, and fluency score—the DE-GRNN-based technique beats the GRNN-based method. The results show 8.2% increase in BLEU score, indicating improved subtitle quality, 2.9% increase in translation accuracy, showing better correctness, 3.6% increase in syntax error correction accuracy, indicating improved subtitle accuracy and 2.1% increase in fluency score, so indicating naturalism and readability. The findings show how well the proposed method generates correct, fluid, syntactic error-free subtitles.",study present new multimodal subtitle translating system integrating generalized regression neural network grnn based syntactic error correcting mechanism produce accurate fluid subtitles system aggregates text video audio inputs grammar error correction system based grnn finds fixes syntactic mistakes translated subtitles therefore raising general quality testing findings reveal notable increases subtitle translating accuracy fluency proposed method translation accuracy proposed method beats baseline means syntax error correction accuracy grnn based syntax error correction system lowers syntax error rate method achieves fluency score compared baseline technique therefore improving fluency translated subtitles bleu score method shows great degree similarity reference translated subtitles measures including bleu score translation accuracy syntax error correction accuracy fluency score grnn based technique beats grnn based method results show increase bleu score indicating improved subtitle quality increase translation accuracy showing better correctness increase syntax error correction accuracy indicating improved subtitle accuracy increase fluency score indicating naturalism readability findings show well proposed method generates correct fluid syntactic error free subtitles
"The details of facial expressions can effectively improve the realism of virtual animated characters. However, many expression generation algorithms suffer from problems such as inaccurate facial feature recognition. Therefore, based on the virtual reality technology, this study proposes a real-time simulation model for animated character facial expressions that integrates improved multi-task cascaded convolutional networks, and conducts experiments on the effectiveness and superiority of the model. The experiment outcomes showed that the lowest normalized average error for feature point recognition in the dataset was only 2.48, and the accuracy of facial action unit extraction was basically over 95%. The highest average in different datasets reached 97.68%, showing high intra-group correlation coefficient and low average absolute error. In addition, the rigid pose estimation algorithm that introduced head eye coordination had the smallest absolute deviation of the mean line of sight estimation, only 4.98°. Finally, the model simulated normal and exaggerated expressions in real-time, with an average frame rate of 54 frames and 38 frames, and a matching degree of 96.48% and 92.74%. Overall, this research can promote the advancement of virtual reality technology, which has significant importance for the metaverse, enhancing virtual interaction experience, and enhancing the realism of animated characters.",details facial expressions effectively improve realism virtual animated characters however many expression generation algorithms suffer problems inaccurate facial feature recognition therefore based virtual reality technology study proposes real time simulation model animated character facial expressions integrates improved multi task cascaded convolutional networks conducts experiments effectiveness superiority model experiment outcomes showed lowest normalized average error feature point recognition dataset accuracy facial action unit extraction basically highest average different datasets reached showing high intra group correlation coefficient low average absolute error addition rigid pose estimation algorithm introduced head eye coordination smallest absolute deviation mean line sight estimation finally model simulated normal exaggerated expressions real time average frame rate frames frames matching degree overall research promote advancement virtual reality technology significant importance metaverse enhancing virtual interaction experience enhancing realism animated characters
"In martial arts action recognition, complex poses, occlusions, and dynamic changes can lead to insufficient estimation accuracy, and traditional methods suffer from inaccurate joint localization and poor recognition of continuous pose jumps. In response to this, the research proposes the Dilated Convolution-Attention-Stacked Hourglass Network Pose Estimation (MS-DConv-Att-SHN) to achieve martial arts action recognition. Firstly, multiple dilated convolutions and mixed attention are proposed to improve the computational complexity and loss of joint information in stacked hourglass networks, enhancing the ability to capture subtle joint displacements. This is crucial for accurate estimation of complex poses such as movement, flicker, and virtual real transformations in martial arts. Secondly, in response to the strong coherence of martial arts movements, local feature refinement and channel fusion techniques are used to enhance the correlation analysis between consecutive action frames, solving the problem of traditional methods’ fragmented recognition of action chains such as “exertion contraction.” A martial arts action recognition system based on the improved MS-DConv-Att-SHN method has been developed to better identify individual movements and capture the intrinsic relationships between movements in routines. This provides key technical support for the digital inheritance, intelligent evaluation, and standardized promotion of martial arts, making it more closely aligned with the movement characteristics of martial arts that combine form and spirit. The results indicate that structural improvements to the stacked hourglass network can effectively increase its percentage of correct keypoints (PCK) and mean average precision (mAP) for both datasets, with PCK and mAP values exceeding 92% and 85%, respectively. The average recognition accuracy of attitude keypoints in the MS-DConv-Att-SHN model is superior to other comparison models, with a difference of over 1.2% compared to other models. The improved MS-DConv-Att-SHN model achieves recognition accuracy of over 90% for different martial arts movements, showing smaller parameter counts and PCK values compared to other comparative models. The research method can effectively provide technical support for the automation analysis of martial arts movements, sports training assistance systems, and intelligent martial arts teaching.",martial arts action recognition complex poses occlusions dynamic changes lead insufficient estimation accuracy traditional methods suffer inaccurate joint localization poor recognition continuous pose jumps response research proposes dilated convolution attention stacked hourglass network pose estimation dconv att shn achieve martial arts action recognition firstly multiple dilated convolutions mixed attention proposed improve computational complexity loss joint information stacked hourglass networks enhancing ability capture subtle joint displacements crucial accurate estimation complex poses movement flicker virtual real transformations martial arts secondly response strong coherence martial arts movements local feature refinement channel fusion techniques used enhance correlation analysis consecutive action frames solving problem traditional methods fragmented recognition action chains exertion contraction martial arts action recognition system based improved dconv att shn method developed better identify individual movements capture intrinsic relationships movements routines provides key technical support digital inheritance intelligent evaluation standardized promotion martial arts making closely aligned movement characteristics martial arts combine form spirit results indicate structural improvements stacked hourglass network effectively increase percentage correct keypoints pck mean average precision map datasets pck map values exceeding respectively average recognition accuracy attitude keypoints dconv att shn model superior comparison models difference compared models improved dconv att shn model achieves recognition accuracy different martial arts movements showing smaller parameter counts pck values compared comparative models research method effectively provide technical support automation analysis martial arts movements sports training assistance systems intelligent martial arts teaching
"The spread of false news has hurt both individual practitioners and the media. To enhance the efficiency of false news detection, this study constructs a multi-modal news detection model. The model includes a text encoding module, a contextual semantic encoder, a news propagation encoder, and a false news detection model that integrates semantic features and image recognition. In the results, the multi-modal model showed significantly higher accuracy and F1 score in detecting false news than the unimodal model. Its accuracy and F1 score improved by an average of 7.57% and 7.34% on the POL and GOS datasets, and 7.20% and 6.38% on the WEIBO and TWITTER datasets. In addition, hyperparameter analysis showed that the model performance reached its optimum when the parameters r and k were adjusted to their optimal values. The ablation experiment further validated the importance of the channel attention mechanism and graph comparison method in improving model performance. The results indicate that multi-modal models have significant advantages in detecting false news and can effectively utilize information from different modalities to improve detection accuracy. This study is meaningful for evaluating the reliability of false news information and the media’s credibility in society. Although certain achievements have been made in the research, there are still some limitations. For example, the model may have generalization issues when tested on specific datasets, and the complexity of the model may make deployment difficult in resource-constrained environments. Future work will explore simplified versions of the model and conduct tests on more diverse datasets to enhance the model’s generalization ability and practicability.",spread false news hurt individual practitioners media enhance efficiency false news detection study constructs multi modal news detection model model includes text encoding module contextual semantic encoder news propagation encoder false news detection model integrates semantic features image recognition results multi modal model showed significantly higher accuracy score detecting false news unimodal model accuracy score improved average pol gos datasets weibo twitter datasets addition hyperparameter analysis showed model performance reached optimum parameters adjusted optimal values ablation experiment validated importance channel attention mechanism graph comparison method improving model performance results indicate multi modal models significant advantages detecting false news effectively utilize information different modalities improve detection accuracy study meaningful evaluating reliability false news information media credibility society although certain achievements made research still limitations example model may generalization issues tested specific datasets complexity model may make deployment difficult resource constrained environments future work explore simplified versions model conduct tests diverse datasets enhance model generalization ability practicability
"To address the difficulties of traditional 2D image processing methods in segmenting crops and backgrounds in complex natural environments, and the inability to accurately obtain 3D phenotype information, this study proposes a 3D image processing technology that integrates depth information. Firstly, a dual branch multi-scale feature encoder was designed to process RGB images and depth images separately, and the feature extraction and fusion capabilities were enhanced by improving the residual module. Then, based on this encoder, a deep image completion network, a deep image super-resolution network, and an image edge detection algorithm were constructed. Finally, the effectiveness of the proposed method was validated on multiple datasets such as BSDS500 and KITTI. The results show that in the ablative experiment, the dual branch multi-scale feature encoder performs the best in accuracy, recall, F1 score, and mean square error, with values of 0.98, 0.82, 0.83, and 0.12, respectively. In the task of completing 200 incomplete image samples, the signal-to-noise ratio of the deep image completion network is mainly distributed between 6 dB and 8 dB. Research outcomes show that the raised method performs well in computer deep image processing, providing strong support for fields such as precision agriculture.",address difficulties traditional image processing methods segmenting crops backgrounds complex natural environments inability accurately obtain phenotype information study proposes image processing technology integrates depth information firstly dual branch multi scale feature encoder designed process rgb images depth images separately feature extraction fusion capabilities enhanced improving residual module based encoder deep image completion network deep image super resolution network image edge detection algorithm constructed finally effectiveness proposed method validated multiple datasets bsds kitti results show ablative experiment dual branch multi scale feature encoder performs best accuracy recall score mean square error values respectively task completing incomplete image samples signal noise ratio deep image completion network mainly distributed research outcomes show raised method performs well computer deep image processing providing strong support fields precision agriculture
"With the continuous development of human production and life, single type remote sensors are constrained by the external environment or their own factors, and the remote sensing image data obtained can no longer meet current needs. The research aims to propose a fusion model to address this issue. A fusion model of the NMF algorithm based on the projection gradient optimization rule is proposed by combining the sensor observation model, non-negative matrix factorization (NMF, a method of decomposing a non-negative matrix into the product of two non-negative matrices) algorithm, and linear spectral hybrid model. Among them, endmembers (referring to the pure spectral components that make up the mixed pixels in hyperspectral unmixing) participate in the construction of linear spectral mixing models. Simulation data experiments show that the NMF fusion algorithm results in high spatial resolution and minimal distortion of spectral information. Compared with the least squares algorithm, its peak signal-to-noise ratio average is improved by 1.2–4.5 dB, the general image quality index average is improved by 0.0–0.01, the spectral value error mean difference range is 0.02–0.68, and the root mean square error mean difference range is 7.64–46.64, overall better. In the real data fusion experiment, the information entropy value range of the fusion result of this algorithm is 5.0–6.4, and the image clarity improvement value range is 3.6–6.4. The algorithm proposed in this study combines high spatial resolution and high spectral resolution in remote sensing image data, which is of great significance for the widespread application of remote sensing technology in various industries.",continuous development human production life single type remote sensors constrained external environment factors remote sensing image data obtained longer meet current needs research aims propose fusion model address issue fusion model nmf algorithm based projection gradient optimization rule proposed combining sensor observation model non negative matrix factorization nmf method decomposing non negative matrix product two non negative matrices algorithm linear spectral hybrid model among endmembers referring pure spectral components make mixed pixels hyperspectral unmixing participate construction linear spectral mixing models simulation data experiments show nmf fusion algorithm results high spatial resolution minimal distortion spectral information compared least squares algorithm peak signal noise ratio average improved general image quality index average improved spectral value error mean difference range root mean square error mean difference range overall better real data fusion experiment information entropy value range fusion result algorithm image clarity improvement value range algorithm proposed study combines high spatial resolution high spectral resolution remote sensing image data great significance widespread application remote sensing technology various industries
"Grain storage security is a crucial component of food security, and adopting advanced technical methods is essential for effective supervision. In recent years, radar technology has become a prominent focus in grain storage monitoring. This study utilizes microwave transmission technology to measure the complex dielectric constant of grain and establishes a calculation model linking grain density and dielectric constant, offering a feasible approach for detecting the average density of grain piles. To address the “underdetermination” issue in identifying anomalies using ground-penetrating radar (GPR) echoes, a finite-difference time-domain (FDTD) simulation model of the granary detection environment is developed. By integrating simulation results with actual 2D radar echo images, high-precision positioning of anomalies within the granary is achieved. Electromagnetic detection experiments further validate the feasibility and practicality of cross-hole radar technology for identifying and imaging foreign objects in grain bins, resulting in a practical mathematical model. The model achieves a positioning accuracy of 0.3 m, meeting engineering application requirements. This research offers a scientific and reliable method for non-destructive detection of foreign matter in grain bins, providing new insights and practical solutions for improving grain storage monitoring.",grain storage security crucial component food security adopting advanced technical methods essential effective supervision recent years radar technology become prominent focus grain storage monitoring study utilizes microwave transmission technology measure complex dielectric constant grain establishes calculation model linking grain density dielectric constant offering feasible approach detecting average density grain piles address underdetermination issue identifying anomalies using ground penetrating radar gpr echoes finite difference time domain fdtd simulation model granary detection environment developed integrating simulation results actual radar echo images high precision positioning anomalies within granary achieved electromagnetic detection experiments validate feasibility practicality cross hole radar technology identifying imaging foreign objects grain bins resulting practical mathematical model model achieves positioning accuracy meeting engineering application requirements research offers scientific reliable method non destructive detection foreign matter grain bins providing new insights practical solutions improving grain storage monitoring
"This study proposes a hybrid Physical Education (PE) teaching quality evaluation system that integrates a K-medoids clustering algorithm with an enhanced CNN-LSTM neural network. Traditional evaluation methods are often subjective and inconsistent, failing to capture the complex, time-varying nature of student behavior in PE classes. The proposed model preprocesses historical classroom data through feature correlation analysis and PCA-based dimensionality reduction, followed by K-medoids clustering to improve data structure and training efficiency. It then takes Ensemble Empirical Mode Decomposition (EEMD) to enhance the input representation of the LSTM model. Experimental results demonstrated that the improved CNN-LSTM achieved an F1 value of 0.98 and an RMSE of 0.11 with 1000 training samples, significantly outperforming baseline models including CNN, LSTM, and GRNN. The model showed peak accuracy of 97.6% during 10:00–12:00 time slots, with an average recall of 90.4% across-varied student states. User evaluation by PE teachers indicated an average satisfaction score of 93.7. This model has been proven to be effective in handling nonlinear and non-stationary classroom data and can achieve real-time, objective, and personalized evaluation. Future improvements include expanding the dataset with classroom sensor data and incorporating cognitive and emotional engagement indicators.",study proposes hybrid physical education teaching quality evaluation system integrates medoids clustering algorithm enhanced cnn lstm neural network traditional evaluation methods often subjective inconsistent failing capture complex time varying nature student behavior classes proposed model preprocesses historical classroom data feature correlation analysis pca based dimensionality reduction followed medoids clustering improve data structure training efficiency takes ensemble empirical mode decomposition eemd enhance input representation lstm model experimental results demonstrated improved cnn lstm achieved value rmse training samples significantly outperforming baseline models including cnn lstm grnn model showed peak accuracy time slots average recall across varied student states user evaluation teachers indicated average satisfaction score model proven effective handling nonlinear non stationary classroom data achieve real time objective personalized evaluation future improvements include expanding dataset classroom sensor data incorporating cognitive emotional engagement indicators
"River pollution not only damages the environment but also hinders the overall development of cities. With the development of the economy, river management has gradually become more important. An image processing method based on principal component analysis is proposed, which preprocesses the data through principal component analysis algorithm, and standardizes the data. Then, a YOLOv5 algorithm detection model for river scenes is proposed. Attention mechanism is introduced to detect floating objects in the river through the YOLOv5 algorithm model. The experimental results showed that when the training set size was 800, the intersection over union ratios of factor analysis, independent component analysis, and principal component analysis models were 0.85, 0.92, and 0.98, respectively. The structural information loss was 0.12, 0.08, and 0.04, respectively. The fuzziness was 0.21, 0.18, and 0.12, respectively, and the signal-to-noise ratios were 0.85, 0.92, and 0.98, respectively. The research results indicate that the proposed improved model has excellent performance, which can promote the improvement of river and lake conditions and provide technical support for long-term effective river and lake supervision, providing new ideas for assisting river inspection and supervision.",river pollution damages environment also hinders overall development cities development economy river management gradually become important image processing method based principal component analysis proposed preprocesses data principal component analysis algorithm standardizes data yolov algorithm detection model river scenes proposed attention mechanism introduced detect floating objects river yolov algorithm model experimental results showed training set size intersection union ratios factor analysis independent component analysis principal component analysis models respectively structural information loss respectively fuzziness respectively signal noise ratios respectively research results indicate proposed improved model excellent performance promote improvement river lake conditions provide technical support long term effective river lake supervision providing new ideas assisting river inspection supervision
"With the rapid development of the Internet of Things and big data technology, its application in rural sports projects has provided new impetus for rural revitalization. This study explores the role of rural sports projects based on the Internet of Things and big Data in promoting rural revitalization by implementing related projects in Region A and conducting systematic evaluations. The research results show that the implementation of the project has significantly improved the effectiveness of governance, ecological livability, rural civilization and life prosperity. By optimizing resource allocation, enhancing community participation, and improving residents’ health and quality of life, sports programs have a significant positive impact on rural society. The study also puts forward future development suggestions in terms of technology application, privacy protection and policy support, aiming to further promote the process of rural revitalization through scientific and technological innovation.",rapid development internet things big data technology application rural sports projects provided new impetus rural revitalization study explores role rural sports projects based internet things big data promoting rural revitalization implementing related projects region conducting systematic evaluations research results show implementation project significantly improved effectiveness governance ecological livability rural civilization life prosperity optimizing resource allocation enhancing community participation improving residents health quality life sports programs significant positive impact rural society study also puts forward future development suggestions terms technology application privacy protection policy support aiming promote process rural revitalization scientific technological innovation
"As society develops and the economy changes, the traditional methods of driving rural revitalization do not adapt well to the growth needs of the times. Digital fusion technology has provided new development paths and opportunities for rural revitalization. This paper first introduced the background and characteristics of digital fusion technology, the current situation and problems of rural revitalization, and the application cases of digital fusion technology in rural revitalization. The study explored the role of digital fusion technology in rural revitalization through four key aspects: development planning, infrastructure, agricultural integration, and talent support. By comparing M Village, which adopted traditional rural revitalization, with N Village, which utilized digital fusion technology, it was found that N Village outperformed in economic growth, with faster and higher annual increases in economic value. Additionally, N Village demonstrated superior social, environmental, and farmers’ benefits, including higher employment rates, better education and healthcare coverage, improved resource utilization, and a significantly higher average per capita disposable income. The above findings indicate that the research on the development strategy of choosing digital fusion technology to drive rural revitalization in the information age is valuable.",society develops economy changes traditional methods driving rural revitalization adapt well growth needs times digital fusion technology provided new development paths opportunities rural revitalization paper first introduced background characteristics digital fusion technology current situation problems rural revitalization application cases digital fusion technology rural revitalization study explored role digital fusion technology rural revitalization four key aspects development planning infrastructure agricultural integration talent support comparing village adopted traditional rural revitalization village utilized digital fusion technology found village outperformed economic growth faster higher annual increases economic value additionally village demonstrated superior social environmental farmers benefits including higher employment rates better education healthcare coverage improved resource utilization significantly higher average per capita disposable income findings indicate research development strategy choosing digital fusion technology drive rural revitalization information age valuable
"Smart elderly care systems often struggle with accurate anomaly detection and health trend prediction due to the multi-source heterogeneity and high noise characteristics of health data. Existing methods face limitations in handling structural differences across data sources and capturing task correlations. To address these challenges, this study proposes a novel framework that integrates multi-task learning (MTL) with support vector machines (SVMs). Health data are collected from smart wearable devices, electronic medical records, and environmental monitoring sensors. Following data cleaning and normalization, an MTL framework is constructed to extract shared representations between anomaly detection and health trend prediction tasks. An SVM model with a radial basis function (RBF) kernel is employed for robust anomaly detection in high-dimensional data. Additionally, a hierarchical prediction mechanism is developed to dynamically forecast health trends using shared features and classification boundaries. Experimental evaluations on a real-world smart elderly care dataset demonstrate that the proposed method achieves an anomaly detection accuracy of 97% and a mean squared error (MSE) of 0.12 in health trend prediction. These results confirm the effectiveness of the approach in enhancing the analysis of complex health data, offering a promising solution for intelligent data processing in elderly care applications.",smart elderly care systems often struggle accurate anomaly detection health trend prediction due multi source heterogeneity high noise characteristics health data existing methods face limitations handling structural differences across data sources capturing task correlations address challenges study proposes novel framework integrates multi task learning mtl support vector machines svms health data collected smart wearable devices electronic medical records environmental monitoring sensors following data cleaning normalization mtl framework constructed extract shared representations anomaly detection health trend prediction tasks svm model radial basis function rbf kernel employed robust anomaly detection high dimensional data additionally hierarchical prediction mechanism developed dynamically forecast health trends using shared features classification boundaries experimental evaluations real world smart elderly care dataset demonstrate proposed method achieves anomaly detection accuracy mean squared error mse health trend prediction results confirm effectiveness approach enhancing analysis complex health data offering promising solution intelligent data processing elderly care applications
"The platform needs to design a reasonable resource recommendation mechanism to push learning resources and services that are selected, suitable, and satisfactory to users based on their personalized information. In this paper, we aim to build an AI-based personalized English learning recommendation platform, which adopts a self-attention mechanism to capture long-term dependencies in user learning data from a dialogue-based perspective, and uses position encoding and residual connection to enhance the expression of the model. Connection to enhance the expressive ability of the model. The system as a whole adopts the B/S architecture and uses Mysql and mongodb databases to build a front-end and back-end separated database. The final experimental results show that the new system significantly outperforms the old system in terms of click rate, recall rate, learning efficiency, user experience, user satisfaction, and user participation, which proves the effectiveness of this paper in combining AI algorithms to optimize the English learning recommendation system.",platform needs design reasonable resource recommendation mechanism push learning resources services selected suitable satisfactory users based personalized information paper aim build based personalized english learning recommendation platform adopts self attention mechanism capture long term dependencies user learning data dialogue based perspective uses position encoding residual connection enhance expression model connection enhance expressive ability model system whole adopts architecture uses mysql mongodb databases build front end back end separated database final experimental results show new system significantly outperforms old system terms click rate recall rate learning efficiency user experience user satisfaction user participation proves effectiveness paper combining algorithms optimize english learning recommendation system
"Accounting for finances is maintaining records of analyzing financial activities to produce financial declarations, income, and counting cash flow, and balance sheets. Verifying the completeness and correctness of financial accounts is the procedure known as auditing. Auditors guarantee the accuracy of financial data supplied by accounting and the integrity of accounting procedures. This study determines to establish an artificial intelligence model, named Dynamic Sea Lion Optimization (DSLO), for enhancing the efficiency of financial accounting and auditing processes. In this study, taxpayers functioning under an average tax regime, efficiency, and grow a fraud forecast method based on Dynamic Sea Lion Optimized Efficient Random Forest (DSLO-ERF). A comprehensive dataset is used to evaluate predictive models for detecting tax fraud. The data was preprocessed using normalization for the obtained data. Fraud, audit, administrative cost sharing, and external economic activity among the significant fraud predictors. The proposed framework is tested from 2018 to 2022 on the whole population of taxpayers and profit. In a comparative analysis, the proposed method performs various evaluation metrics. The result revealed that compared with other current methods, the efficacy of the suggested technique is better in terms of accuracy (98.621%), precision (99.517%), and recall (99.612%) in this condition, the proposed DSLO-ERF method also had the quickest training time of 0.10 seconds. The research focuses on an innovative approach to achieving greater operational efficiency and financial integrity, as well as the strategic advantages of AI adoption in the financial industry.",accounting finances maintaining records analyzing financial activities produce financial declarations income counting cash flow balance sheets verifying completeness correctness financial accounts procedure known auditing auditors guarantee accuracy financial data supplied accounting integrity accounting procedures study determines establish artificial intelligence model named dynamic lion optimization dslo enhancing efficiency financial accounting auditing processes study taxpayers functioning average tax regime efficiency grow fraud forecast method based dynamic lion optimized efficient random forest dslo erf comprehensive dataset used evaluate predictive models detecting tax fraud data preprocessed using normalization obtained data fraud audit administrative cost sharing external economic activity among significant fraud predictors proposed framework tested whole population taxpayers profit comparative analysis proposed method performs various evaluation metrics result revealed compared current methods efficacy suggested technique better terms accuracy precision recall condition proposed dslo erf method also quickest training time seconds research focuses innovative approach achieving greater operational efficiency financial integrity well strategic advantages adoption financial industry
"The structural design of fixed flexible fall protection devices often relies on single-objective optimization methods, leading to uneven stress distribution and local stress concentration, which compromise long-term safety and durability. To address these issues, this study proposes a multi-objective optimization framework integrating the Non-dominated Sorting Genetic Algorithm II (NSGA-II) with finite element analysis (FEA). Key structural parameters influencing load-bearing capacity, stress distribution, and deformation control are identified, forming the basis of a multi-objective optimization model aimed at maximizing bearing capacity, minimizing stress concentration, and optimizing deformation uniformity. To enhance computational efficiency, a Radial Basis Function (RBF)-based surrogate model is developed to approximate FEA results, while parallel computing accelerates population evaluation in NSGA-II. Candidate solutions are validated through FEA simulations, followed by further refinement using adaptive crossover-mutation and local search strategies. The optimized design demonstrates significant performance improvements: average bearing capacity reaches 172.88 kN, with node displacement reduced to 0.97 mm. Deformation gradient uniformity improves to 0.06, and residual deformation ratio decreases to 2.49%. These results confirm that the NSGA-II and FEA-integrated approach effectively enhances mechanical performance and structural reliability. This research not only ensures safer high-altitude operations but also provides a novel, efficient methodology for structural design optimization in related engineering fields.",structural design fixed flexible fall protection devices often relies single objective optimization methods leading uneven stress distribution local stress concentration compromise long term safety durability address issues study proposes multi objective optimization framework integrating non dominated sorting genetic algorithm nsga finite element analysis fea key structural parameters influencing load bearing capacity stress distribution deformation control identified forming basis multi objective optimization model aimed maximizing bearing capacity minimizing stress concentration optimizing deformation uniformity enhance computational efficiency radial basis function rbf based surrogate model developed approximate fea results parallel computing accelerates population evaluation nsga candidate solutions validated fea simulations followed refinement using adaptive crossover mutation local search strategies optimized design demonstrates significant performance improvements average bearing capacity reaches node displacement reduced deformation gradient uniformity improves residual deformation ratio decreases results confirm nsga fea integrated approach effectively enhances mechanical performance structural reliability research ensures safer high altitude operations also provides novel efficient methodology structural design optimization related engineering fields
"Traditional English text sentiment analysis models have limitations in handling complex texts and long-term dependency relationships, leading to an impact on the accuracy and deep understanding of sentiment analysis. This article aims to solve the above problems by applying the LSTM (long short-term memory) algorithm. First, the data is cleaned and preprocessed, including regular expressions, word segmentation, stop word filtering, and stem extraction. Next, a sentiment lexicon is utilized to annotat. Word2Vec and TF-IDF are combined for feature vectorization, and the positional encoding is applied. An LSTM network structure is then constructed. After training optimization, the model achieves an accuracy of up to 90% on the test set, outperforming other models. The LSTM algorithm effectively solves the limitations of traditional models and achieves higher accuracy. In summary, the application of LSTM algorithm in English text sentiment analysis models can effectively address existing limitations and make them more accurate.",traditional english text sentiment analysis models limitations handling complex texts long term dependency relationships leading impact accuracy deep understanding sentiment analysis article aims solve problems applying lstm long short term memory algorithm first data cleaned preprocessed including regular expressions word segmentation stop word filtering stem extraction next sentiment lexicon utilized annotat word vec idf combined feature vectorization positional encoding applied lstm network structure constructed training optimization model achieves accuracy test set outperforming models lstm algorithm effectively solves limitations traditional models achieves higher accuracy summary application lstm algorithm english text sentiment analysis models effectively address existing limitations make accurate
"In this study, we analyze the behavior of a damped parametric oscillator, incorporating a damping factor to examine its influence on system dynamics. We derive the equation of motion using the Lagrangian formulation and the Euler–Lagrange equation, which leads to a nonlinear differential equation governing the system’s motion. The equation is solved numerically using the 4th-order Runge-Kutta method and analytically using the Multistep Differential Transformation Method (MS-DTM), which provides an efficient and accurate approximation. The results obtained from MS-DTM are in good agreement with the numerical solutions, demonstrating its capability to capture the system’s dynamics with reduced computational effort. By analyzing two distinct scenarios—one with a small damping coefficient (β = 0.01) and the other with a larger damping coefficient (β = 0.5)—we observe that the damping parameter significantly influences system behavior. Specifically, in the first scenario, the system exhibits stable damped harmonic motion with constant energy, while in the second scenario, the system experiences energy dissipation and becomes unstable. This study highlights the critical role of damping in determining the system’s stability and energy dissipation, showcasing the effectiveness of both MS-DTM and numerical methods in analyzing nonlinear oscillatory systems.",study analyze behavior damped parametric oscillator incorporating damping factor examine influence system dynamics derive equation motion using lagrangian formulation euler lagrange equation leads nonlinear differential equation governing system motion equation solved numerically using order runge kutta method analytically using multistep differential transformation method dtm provides efficient accurate approximation results obtained dtm good agreement numerical solutions demonstrating capability capture system dynamics reduced computational effort analyzing two distinct scenarios one small damping coefficient larger damping coefficient observe damping parameter significantly influences system behavior specifically first scenario system exhibits stable damped harmonic motion constant energy second scenario system experiences energy dissipation becomes unstable study highlights critical role damping determining system stability energy dissipation showcasing effectiveness dtm numerical methods analyzing nonlinear oscillatory systems
"In this paper, the regional double-layer indoor high-precision positioning system based on Ibeacon network is studied. 3 circle positioning and centroid positioning algorithms are commonly used indoor positioning algorithms. However, due to the wireless signal in the process of indoor transmission will have a certain attenuation. Therefore, there will be some errors in these positioning algorithms. In this paper, the positioning error is reduced by filtering out the equipment information with large error, and then an improved multi circle positioning algorithm is proposed based on the three circle positioning. At the same time, the weighted centroid algorithm is added to the improved positioning algorithm, which can effectively improve the positioning accuracy. Compared with the traditional positioning algorithm, the improved positioning algorithm can limit the positioning error within 4 m. Compared with the traditional location algorithm, this algorithm is more stable. Finally, the paper gives the experimental simulation of the improved algorithm. With the help of Ibeacon equipment, the system obtains the distance between the current position and Ibeacon anchor. Through the simulation experiment, the improved algorithm of direction control is added, which performs well in the area block.",paper regional double layer indoor high precision positioning system based ibeacon network studied circle positioning centroid positioning algorithms commonly used indoor positioning algorithms however due wireless signal process indoor transmission certain attenuation therefore errors positioning algorithms paper positioning error reduced filtering equipment information large error improved multi circle positioning algorithm proposed based three circle positioning time weighted centroid algorithm added improved positioning algorithm effectively improve positioning accuracy compared traditional positioning algorithm improved positioning algorithm limit positioning error within compared traditional location algorithm algorithm stable finally paper gives experimental simulation improved algorithm help ibeacon equipment system obtains distance current position ibeacon anchor simulation experiment improved algorithm direction control added performs well area block
"As the main representative of ancient Shu culture and one of the precious Chinese cultural heritages, Sanxingdui Ruins has gradually been favored by Chinese and foreign tourists in recent years. Due to the influence of time factors such as festival and weekends, the daily passenger flow of Sanxingdui shows a large fluctuation characteristic. The sudden increase in the number of tourists on festival compared with ordinary days has brought challenges to the daily management of the scenic spot. At the same time, the development of new generation information technology makes network search become one of the main behaviors of tourists before travel. Therefore, this paper analyzes the correlation between Baidu Index and Sanxingdui festival daily passenger flow, and establish prediction model by using SVR prediction method combined with Baidu Index and Sanxingdui festival daily passenger flow, the model has an accurate prediction of Sanxingdui holiday passenger flow. The prediction can provide a scientific reference for the management decision-making of scenic spots on festival.",main representative ancient shu culture one precious chinese cultural heritages sanxingdui ruins gradually favored chinese foreign tourists recent years due influence time factors festival weekends daily passenger flow sanxingdui shows large fluctuation characteristic sudden increase number tourists festival compared ordinary days brought challenges daily management scenic spot time development new generation information technology makes network search become one main behaviors tourists travel therefore paper analyzes correlation baidu index sanxingdui festival daily passenger flow establish prediction model using svr prediction method combined baidu index sanxingdui festival daily passenger flow model accurate prediction sanxingdui holiday passenger flow prediction provide scientific reference management decision making scenic spots festival
"With the rapid development of digital technology, its application in the field of intelligent packaging is gradually changing the face of the packaging industry. This research focuses on the application of digital technology in the innovative design of intelligent packaging, and carries out in-depth analysis through comprehensive methodology. Firstly, through literature review and questionnaire survey, we understand the application status and market demand of digital technology in intelligent packaging. Furthermore, the influence of technology integration and user interface design on the acceptance of intelligent packaging was analyzed by constructing the innovative design model of intelligent packaging. The results show that these two factors have a significant positive impact on the user acceptance of smart packaging. However, the study also points out the limitations of the model, particularly in terms of data representation and future technological trends. In general, this study not only provides theoretical and practical guidance for intelligent packaging design, but also provides a reference for future research direction.",rapid development digital technology application field intelligent packaging gradually changing face packaging industry research focuses application digital technology innovative design intelligent packaging carries depth analysis comprehensive methodology firstly literature review questionnaire survey understand application status market demand digital technology intelligent packaging furthermore influence technology integration user interface design acceptance intelligent packaging analyzed constructing innovative design model intelligent packaging results show two factors significant positive impact user acceptance smart packaging however study also points limitations model particularly terms data representation future technological trends general study provides theoretical practical guidance intelligent packaging design also provides reference future research direction
"This study investigated the mechanical response of composite materials based on polyester resin and fiberglass, doped with large silicon carbide (SiC) particles (100–250 µm). The research included bending tests (ASTM D7264), nanoindentation, and multiscale computational homogenization to describe the mechanical behavior across scales. SiC doping was applied at 1.5%, 3%, and 5% by weight, using two fiberglass configurations: continuous laminates and a mixed arrangement of continuous and discontinuous laminates, with 13 fiberglass layers per sample. Scanning electron microscopy (SEM) analyzed porosity and SiC distribution at the interface, while nanoindentation tests examined the matrix, fibers, SiC particles, and their interfaces. Long laminate stacking provided the best mechanical performance due to uniform reinforcement distribution at the fiber/matrix interface. Increasing the SiC content in long laminates improved flexural strength by 17.5% compared to undoped samples. In contrast, mixed laminates exhibited a 41% strength reduction due to SiC particle agglomeration and stress concentrations. Bending curves of SiC-doped samples showed three stiffness zones, attributed to hierarchical mechanical behavior with progressive reinforcement activation. The large SiC particle size and low fraction enhanced deformation capacity below 50 MPa while increasing flexural strength. This balance of strength and deformation represents a novel finding in composite performance.",study investigated mechanical response composite materials based polyester resin fiberglass doped large silicon carbide sic particles research included bending tests astm nanoindentation multiscale computational homogenization describe mechanical behavior across scales sic doping applied weight using two fiberglass configurations continuous laminates mixed arrangement continuous discontinuous laminates fiberglass layers per sample scanning electron microscopy sem analyzed porosity sic distribution interface nanoindentation tests examined matrix fibers sic particles interfaces long laminate stacking provided best mechanical performance due uniform reinforcement distribution fiber matrix interface increasing sic content long laminates improved flexural strength compared undoped samples contrast mixed laminates exhibited strength reduction due sic particle agglomeration stress concentrations bending curves sic doped samples showed three stiffness zones attributed hierarchical mechanical behavior progressive reinforcement activation large sic particle size low fraction enhanced deformation capacity mpa increasing flexural strength balance strength deformation represents novel finding composite performance
"Against the backdrop of rapid development of artificial intelligence technology, traditional art creation is facing a critical period of digital transformation. This study aims to explore the application value of augmented reality (AR) technology in traditional art creation, and enhance artistic expression through technological innovation. The research method adopts an image enhancement algorithm based on deep learning, focusing on solving the problem of uneven brightness in traditional art images under low-light conditions. The experimental results show that the proposed algorithm performs excellently in multiple indicators: the absolute average brightness error is reduced to 0.15 (a 42% improvement compared to traditional methods), the peak signal-to-noise ratio reaches 28.6 dB (a 23% improvement), the contrast quality index is 0.89 (a 31% improvement), and the structural similarity index reaches 0.92 (an 18% improvement). The research results indicate that AR technology can effectively enhance the expression forms of traditional art, provide new technological paths for artistic creation, and have important practical application value.",backdrop rapid development artificial intelligence technology traditional art creation facing critical period digital transformation study aims explore application value augmented reality technology traditional art creation enhance artistic expression technological innovation research method adopts image enhancement algorithm based deep learning focusing solving problem uneven brightness traditional art images low light conditions experimental results show proposed algorithm performs excellently multiple indicators absolute average brightness error reduced improvement compared traditional methods peak signal noise ratio reaches improvement contrast quality index improvement structural similarity index reaches improvement research results indicate technology effectively enhance expression forms traditional art provide new technological paths artistic creation important practical application value
"Characteristic town is an important measure to promote the construction of new urbanization, development of urban-rural integration, rural revitalization, industrial structure upgrading, and improve social and economic development. Constructing the development level index system and development level measurement of characteristic town has important practical significance for the sustainable and high-quality development of characteristic town. According to the characteristics of it, the paper constructs the evaluation index system of the development level of characteristic towns from five dimensions, such as industrial resources and construction foundation, the weight of the evaluation index is determined based by applying the entropy weight &amp; analytic hierarchy process method, based on fuzzy mathematical theory, establishes the fuzzy-entropy weight method-analytic hierarchy process (F-EWM-AHP) comprehensive measurement model for the development level of characteristic towns, and empirical test the characteristic town development level, the results show that the comprehensive measurement model is consistent with the actual situation. The results can provide scientific basis for the characteristic town development evaluation, provides reference for the sustainable and high-quality development of characteristic towns under the background of rural revitalization.",characteristic town important measure promote construction new urbanization development urban rural integration rural revitalization industrial structure upgrading improve social economic development constructing development level index system development level measurement characteristic town important practical significance sustainable high quality development characteristic town according characteristics paper constructs evaluation index system development level characteristic towns five dimensions industrial resources construction foundation weight evaluation index determined based applying entropy weight amp analytic hierarchy process method based fuzzy mathematical theory establishes fuzzy entropy weight method analytic hierarchy process ewm ahp comprehensive measurement model development level characteristic towns empirical test characteristic town development level results show comprehensive measurement model consistent actual situation results provide scientific basis characteristic town development evaluation provides reference sustainable high quality development characteristic towns background rural revitalization
"ProBot integrates a unique multiverse technique and utilizes a speaking-enabled chatbot, leveraging co-learning ontology to foster collaboration between students and robots. Operating on a fuzzy logic-based linear model within a multi-agent reinforcement learning framework (MARL), ProBot delivers personalized communication over high-speed networks, including future-generation networks (6G). Its ontology framework tailors discussions, tests, and exercises to individual learning objectives, enhancing conversational flow and proficiency evaluation through advanced voice creation and natural language comprehension. MARL facilitates adaptive feedback loops among ProBot agents, offering tailored recommendations and learning strategies. ProBot adjusts difficulty levels, identifies improvement areas, and ensures scalable feedback across all proficiency levels. Simulations demonstrate ProBot’s effectiveness in improving student proficiency through varied parameters such as language skills and engagement levels. Experimental results confirm significant proficiency gains, establishing ProBot as a pioneering educational tool that transforms student-robot interactions and supports diverse learning goals in English proficiency.",probot integrates unique multiverse technique utilizes speaking enabled chatbot leveraging learning ontology foster collaboration students robots operating fuzzy logic based linear model within multi agent reinforcement learning framework marl probot delivers personalized communication high speed networks including future generation networks ontology framework tailors discussions tests exercises individual learning objectives enhancing conversational flow proficiency evaluation advanced voice creation natural language comprehension marl facilitates adaptive feedback loops among probot agents offering tailored recommendations learning strategies probot adjusts difficulty levels identifies improvement areas ensures scalable feedback across proficiency levels simulations demonstrate probot effectiveness improving student proficiency varied parameters language skills engagement levels experimental results confirm significant proficiency gains establishing probot pioneering educational tool transforms student robot interactions supports diverse learning goals english proficiency
"This paper briefly introduces the total factor productivity (TFP) and then analyzes A-share listed companies between 2017 and 2022. The regression analysis method is used to study the investor factor and TFP. The results showed that investor research behavior, shareholding ratio, grouping behavior, and digital technology level were significantly positively correlated with TFP.",paper briefly introduces total factor productivity tfp analyzes share listed companies regression analysis method used study investor factor tfp results showed investor research behavior shareholding ratio grouping behavior digital technology level significantly positively correlated tfp
"The integration of wind power into extensive grid networks presents a confluence of challenges arising from the inherently intermittent nature of wind resources and transmission bottlenecks. To address these complexities and maximize wind energy utilization while meeting demand requirements, a holistic planning paradigm is indispensable. Such an approach entails the synergistic coordination of wind power capacity allocation and siting, expansion of transmission infrastructure, and integration of energy storage systems. Additionally, operational strategies for both generation assets and energy storage facilities play pivotal roles in optimizing system performance. A joint planning framework is formulated to minimize the aggregate costs associated with transmission network augmentation, energy storage system deployment and operation, conventional unit dispatch, wind curtailment, and penalties incurred during periods of high demand. This model incorporates linearized representations of constraints governing energy storage system planning and operation and quadratic generation cost functions, facilitated by Mixed-Integer Linear Programming methodologies. Rigorous evaluation of the proposed methodology is conducted utilizing representative test systems across diverse scenario settings. The empirical findings underscore the efficacy of the devised planning model in significantly bolstering load acceptance capacity and facilitating heightened levels of wind power integration within the grid infrastructure.",integration wind power extensive grid networks presents confluence challenges arising inherently intermittent nature wind resources transmission bottlenecks address complexities maximize wind energy utilization meeting demand requirements holistic planning paradigm indispensable approach entails synergistic coordination wind power capacity allocation siting expansion transmission infrastructure integration energy storage systems additionally operational strategies generation assets energy storage facilities play pivotal roles optimizing system performance joint planning framework formulated minimize aggregate costs associated transmission network augmentation energy storage system deployment operation conventional unit dispatch wind curtailment penalties incurred periods high demand model incorporates linearized representations constraints governing energy storage system planning operation quadratic generation cost functions facilitated mixed integer linear programming methodologies rigorous evaluation proposed methodology conducted utilizing representative test systems across diverse scenario settings empirical findings underscore efficacy devised planning model significantly bolstering load acceptance capacity facilitating heightened levels wind power integration within grid infrastructure
"In response to the problem that existing arrangement methods do not fully consider the characteristics of popular music, resulting in poor performance of popular music arrangement, this paper proposes and designs an intelligent composition method for popular music based on adaptive multimodal particle swarm optimization algorithm. First, the design completes the establishment of the pop music intelligent composition model based on rhythm and melody generation and multi-instrument arrangement, and then applies the adaptive multimodal particle swarm optimization algorithm to complete the composition model solution, realizing the intelligent composition function. Finally, the progressiveness of the proposed method is verified through experiments. The experimental results demonstrate that the proposed method can effectively achieve musical composition based on the specified style, yielding a rhythm, completeness, harmony, and overall effect that are highly satisfactory. With scores consistently above 90 points, this method outperforms comparative techniques. These findings highlight the potential of the proposed method as a valuable tool for music producers, offering a reliable reference for arranging music and enhancing the quality of compositions. This research underscores the method’s significance in advancing automated music composition and its practical applications in the music industry.",response problem existing arrangement methods fully consider characteristics popular music resulting poor performance popular music arrangement paper proposes designs intelligent composition method popular music based adaptive multimodal particle swarm optimization algorithm first design completes establishment pop music intelligent composition model based rhythm melody generation multi instrument arrangement applies adaptive multimodal particle swarm optimization algorithm complete composition model solution realizing intelligent composition function finally progressiveness proposed method verified experiments experimental results demonstrate proposed method effectively achieve musical composition based specified style yielding rhythm completeness harmony overall effect highly satisfactory scores consistently points method outperforms comparative techniques findings highlight potential proposed method valuable tool music producers offering reliable reference arranging music enhancing quality compositions research underscores method significance advancing automated music composition practical applications music industry
"In order to solve the problems of precocious convergence, poor fault tolerance, and slow convergence speed of MVO algorithm in fault location of active distribution network, a fault location method based on SMVO is proposed. Firstly, adaptive elite and mutation strategies are used to improve the population quality and global development performance of MVO algorithm. Secondly, WEP and TDR parameter updating mechanism based on nonlinear curve change is designed to balance the convergence process of MVO algorithm in the early and late iterations. Finally, SMA algorithm is introduced to further improve the solving ability and solution quality of the improved MVO algorithm. In addition, the adaptive multi-DG switching function and high fault tolerance evaluation function are constructed, and a dynamic partition processing method suitable for active distribution network is introduced, which effectively reduces the dimension of the algorithm solution. The comparison experimental results of MVO, IMVO, SMA, and SMVO algorithms in the simulation examples of single point, multi-point, and information distortion faults of IEEE33-node active distribution network show that compared with the other three algorithms, the SMVO algorithm has higher local exploration ability and iterative stability, and the positioning accuracy reaches 100%. The average iteration time is only about 0.37s, and precocious convergence is not easy to occur, which can well meet the needs of online fault location of active distribution network.",order solve problems precocious convergence poor fault tolerance slow convergence speed mvo algorithm fault location active distribution network fault location method based smvo proposed firstly adaptive elite mutation strategies used improve population quality global development performance mvo algorithm secondly wep tdr parameter updating mechanism based nonlinear curve change designed balance convergence process mvo algorithm early late iterations finally sma algorithm introduced improve solving ability solution quality improved mvo algorithm addition adaptive multi switching function high fault tolerance evaluation function constructed dynamic partition processing method suitable active distribution network introduced effectively reduces dimension algorithm solution comparison experimental results mvo imvo sma smvo algorithms simulation examples single point multi point information distortion faults ieee node active distribution network show compared three algorithms smvo algorithm higher local exploration ability iterative stability positioning accuracy reaches average iteration time precocious convergence easy occur well meet needs online fault location active distribution network
"Accurate prediction of electricity load time series is crucial for optimizing power system scheduling and ensuring reliable operation. However, existing approaches face challenges in effectively capturing long-term global trends while addressing short-term local fluctuations. This study proposes a hybrid strategy that integrates the Mamba algorithm for long-term predictions with the Transformer model for short-term forecasting, leveraging the complementary strengths of both methods. The Mamba algorithm captures global trends in long-term sequences, while the Transformer focuses on local variations in short-term data. This combination enhances both long-term trend accuracy and the detection of short-term fluctuations. Additionally, a robust sequence decomposition module is introduced to refine the sliding window mechanism, which segments long sequences for Transformer input, thereby reducing computational overhead and memory demands. Experimental comparisons demonstrate that the hybrid approach outperforms both standalone Mamba and Transformer models in prediction precision, computational efficiency, and memory optimization. Specifically, compared to Mamba, the hybrid model reduces mean squared error (MSE) by 37%, and compared to Transformer, it achieves an 11% reduction in MSE. Additionally, the training time is shortened by 34%, and GPU memory usage is reduced by 47%. These findings confirm that the proposed strategy effectively integrates the advantages of Mamba and Transformer, achieving reliable long-term load forecasting alongside robust short-term prediction capabilities. The model’s efficacy has been validated using real-world power load datasets from Sichuan Province, China, covering the period from 2022 to July 2024.",accurate prediction electricity load time series crucial optimizing power system scheduling ensuring reliable operation however existing approaches face challenges effectively capturing long term global trends addressing short term local fluctuations study proposes hybrid strategy integrates mamba algorithm long term predictions transformer model short term forecasting leveraging complementary strengths methods mamba algorithm captures global trends long term sequences transformer focuses local variations short term data combination enhances long term trend accuracy detection short term fluctuations additionally robust sequence decomposition module introduced refine sliding window mechanism segments long sequences transformer input thereby reducing computational overhead memory demands experimental comparisons demonstrate hybrid approach outperforms standalone mamba transformer models prediction precision computational efficiency memory optimization specifically compared mamba hybrid model reduces mean squared error mse compared transformer achieves reduction mse additionally training time shortened gpu memory usage reduced findings confirm proposed strategy effectively integrates advantages mamba transformer achieving reliable long term load forecasting alongside robust short term prediction capabilities model efficacy validated using real world power load datasets sichuan province china covering period july
"In today’s economic landscape, there are significant differences in regional economic development levels. The uneven development of renewable energy has become a prominent problem and an important factor restricting the rapid and healthy development of the national economy. To further improve the competitiveness of the regional economy, this study first assesses the level of economic development by clustering based on improved grey correlation clustering. The economic development of each cluster group is also analyzed using the improved projection-seeking model. The outcomes indicated that the proposed model had a high consistency between the clustering results of the model and the real categories in terms of purity and standard mutual information maximum values of 0.940 and 0.907, respectively. In the analysis of regional economic development examples, the proposed method achieved the highest ability to categorize the economic status quo. This method provides a thorough analysis of the development level of economic indicators. It is a realistic approach that is demonstrably more effective than manual evaluation. This study provides a scientific and reliable basis for regional economic development. It helps promote the coordinated and sustainable growth of the regional economy and contributes innovative ideas and methods for solving the problem of the uneven development of renewable energy.",today economic landscape significant differences regional economic development levels uneven development renewable energy become prominent problem important factor restricting rapid healthy development national economy improve competitiveness regional economy study first assesses level economic development clustering based improved grey correlation clustering economic development cluster group also analyzed using improved projection seeking model outcomes indicated proposed model high consistency clustering results model real categories terms purity standard mutual information maximum values respectively analysis regional economic development examples proposed method achieved highest ability categorize economic status quo method provides thorough analysis development level economic indicators realistic approach demonstrably effective manual evaluation study provides scientific reliable basis regional economic development helps promote coordinated sustainable growth regional economy contributes innovative ideas methods solving problem uneven development renewable energy
"The traditional cross-border e-commerce demand and inventory prediction model has poor real-time and efficiency, and is difficult to undertake complex tasks. To address the above problems, the study proposes a dynamic demand and inventory prediction model for cross-border e-commerce based on differential autoregressive integrated moving average model and long short-term memory. This can optimize the timing of demand prediction and reduce the cost of inventory management, and finally set up experiments to verify it. The experimental results show that in the simulation running experiments, the average demand forecasting efficiency of the proposed model before training is 50.70%, and it is improved to 97.56% after training; the average inventory control accuracy of the model after training reaches 0.94, and the average deviation of accuracy is 0.009. After the model loses the demand forecasting module the demand forecasting efficiency decreases to 84.43%, and the inventory control accuracy decreases to 0.84. After the model loses the inventory management module the average demand forecasting efficiency decreases to 93.86%, and the inventory control accuracy decreases to 0.933. After losing the inventory management module, the demand forecasting efficiency drops to 93.86%, and the inventory control accuracy drops to 0.933. In the actual model performance experiments, the average demand forecasting efficiency in the sales smooth phase is 90.02%, and the inventory control accuracy is 0.90; the two indexes in the non-smooth phase are 81.15% and 0.82, respectively. The trajectory of the demand coefficient predicted by the model is 0.009 (the lowest value among all compared methods). The study can improve the supply chain demand and inventory prediction efficiency and real-time, and enhance the market competitiveness of the cross-border e-commerce in a larger field.",traditional cross border commerce demand inventory prediction model poor real time efficiency difficult undertake complex tasks address problems study proposes dynamic demand inventory prediction model cross border commerce based differential autoregressive integrated moving average model long short term memory optimize timing demand prediction reduce cost inventory management finally set experiments verify experimental results show simulation running experiments average demand forecasting efficiency proposed model training improved training average inventory control accuracy model training reaches average deviation accuracy model loses demand forecasting module demand forecasting efficiency decreases inventory control accuracy decreases model loses inventory management module average demand forecasting efficiency decreases inventory control accuracy decreases losing inventory management module demand forecasting efficiency drops inventory control accuracy drops actual model performance experiments average demand forecasting efficiency sales smooth phase inventory control accuracy two indexes non smooth phase respectively trajectory demand coefficient predicted model lowest value among compared methods study improve supply chain demand inventory prediction efficiency real time enhance market competitiveness cross border commerce larger field
"To address practical challenges in exothermic reactor control systems, this paper thoroughly investigates the structure of the SMPT-1000 experimental platform and PCS7 process control technology. The hardware design, software design, communication module, PLC programming languages, WinCC monitoring module on the upper computer, and PCS7 software are analyzed in detail. A PID controller is designed to stabilize the system’s output variables within the desired range through proper tuning of PID parameters. This design not only achieves effective system control but also integrates PCS7 monitoring technology, enabling operator station visualization and remote monitoring via WinCC, the SMPT-1000 provides a safe and economical pre-verification platform for exothermic reaction control through high-precision simulation, while the PCS7 ensures the stable operation of the actual system with industrial-grade reliability and advanced control functions, thereby creating an intuitive human-machine interface. Finally, simulations conducted with the SMPTLab software of the SMPT-1000 platform demonstrate that the proposed reactor control system operates reliably and achieves stable performance, meeting the expected design objectives.",address practical challenges exothermic reactor control systems paper thoroughly investigates structure smpt experimental platform pcs process control technology hardware design software design communication module plc programming languages wincc monitoring module upper computer pcs software analyzed detail pid controller designed stabilize system output variables within desired range proper tuning pid parameters design achieves effective system control also integrates pcs monitoring technology enabling operator station visualization remote monitoring via wincc smpt provides safe economical pre verification platform exothermic reaction control high precision simulation pcs ensures stable operation actual system industrial grade reliability advanced control functions thereby creating intuitive human machine interface finally simulations conducted smptlab software smpt platform demonstrate proposed reactor control system operates reliably achieves stable performance meeting expected design objectives
"A crucial aspect of creating character images in digital media is ensuring that animated characters’ details and distinctive features are accurately aligned. Additionally, facial animation contour feature matching and seamless splicing are essential for conveying precise facial expressions, making them key technologies in the creation of animated characters. Traditional facial motion capture technology makes it difficult for the audience to experience the character’s expressions. Low matching accuracy, low stitching efficiency, and severe image distortion after stitching are just a few of the issues that need to be addressed. An animation-based face contour feature matching method is proposed in this paper to address the issues raised above. Training a neural network with images of faces and animation characters is the first step in matching the datasets. Then, a Gaussian low-pass filter is used to scale and denoise the complex animation image in the scene. By using the double threshold method, a smooth image’s contour is detected, and the contour feature is extracted and a matching relationship is built. Finally, the facial key point bone parameters are used in conjunction with the face geometry correction. The best matching pair of faces is selected for image contour stitching based on a qualitative analysis of character expressions. The results of the simulation show that it has better feature matching accuracy, shorter running time, and stronger anti-distortion ability than the traditional stitching method, which greatly helps to improve the efficiency of animation manufacturing and increase the output.",crucial aspect creating character images digital media ensuring animated characters details distinctive features accurately aligned additionally facial animation contour feature matching seamless splicing essential conveying precise facial expressions making key technologies creation animated characters traditional facial motion capture technology makes difficult audience experience character expressions low matching accuracy low stitching efficiency severe image distortion stitching issues need addressed animation based face contour feature matching method proposed paper address issues raised training neural network images faces animation characters first step matching datasets gaussian low pass filter used scale denoise complex animation image scene using double threshold method smooth image contour detected contour feature extracted matching relationship built finally facial key point bone parameters used conjunction face geometry correction best matching pair faces selected image contour stitching based qualitative analysis character expressions results simulation show better feature matching accuracy shorter running time stronger anti distortion ability traditional stitching method greatly helps improve efficiency animation manufacturing increase output
"To address the problem of anomaly detection and root cause tracing in cigarette production rate data, a novel machine learning algorithm was proposed. This study employs a variety of algorithms to detect anomalies in silk yield data, culminating in a final result derived through a weighted method. Following the identification of anomalous data, key factors are pinpointed using correlation and regression analysis algorithms. Experimental results demonstrate that this algorithm excels in identifying data anomalies and tracing their origins, significantly contributing to the enhancement of tobacco production yields. The application of machine learning in anomaly detection and root cause analysis within the tobacco industry represents a significant advancement in production efficiency and quality control. By accurately identifying anomalies and their underlying causes, this algorithm ensures higher precision in monitoring production processes and facilitates proactive adjustments to maintain optimal yield levels. The integration of multiple algorithms and a weighted method enhances the robustness and reliability of the anomaly detection process. Ultimately, this study provides a valuable tool for improving operational effectiveness and production outcomes in the tobacco industry, highlighting the broader potential of machine learning applications in industrial settings.",address problem anomaly detection root cause tracing cigarette production rate data novel machine learning algorithm proposed study employs variety algorithms detect anomalies silk yield data culminating final result derived weighted method following identification anomalous data key factors pinpointed using correlation regression analysis algorithms experimental results demonstrate algorithm excels identifying data anomalies tracing origins significantly contributing enhancement tobacco production yields application machine learning anomaly detection root cause analysis within tobacco industry represents significant advancement production efficiency quality control accurately identifying anomalies underlying causes algorithm ensures higher precision monitoring production processes facilitates proactive adjustments maintain optimal yield levels integration multiple algorithms weighted method enhances robustness reliability anomaly detection process ultimately study provides valuable tool improving operational effectiveness production outcomes tobacco industry highlighting broader potential machine learning applications industrial settings
"Low-altitude safety is key to the sustainable development of the low-altitude economy. Drone swarms pose greater risks than individual unmanned aerial vehicles due to their scale and coordination. This paper proposes a situation awareness strategy for the defense of drone swarm. Multi-scale drone target detection is achieved through an anchor-free structure, drone swarm formation recognition is realized by Graph Neural Networks, and the situation of drone swarm is calculated by constructing macroscopic quantitative descriptors. It breaks through the feature extraction and fusion algorithm for multi-scale drones, graph neural networks for intra-layer and inter-layer feature extraction, and macroscopic quantitative descriptors based on divergence and curl to construct scale-invariant and rotation-invariant features. It achieves the detection of whether it is a drone swarm, the identification of which drone swarm it is, and the calculation of the degree of the drone swarm, providing a basis for the classification and graded handling of drone swarm, and effectively promoting the modernization of the low-altitude safety governance capacity.",low altitude safety key sustainable development low altitude economy drone swarms pose greater risks individual unmanned aerial vehicles due scale coordination paper proposes situation awareness strategy defense drone swarm multi scale drone target detection achieved anchor free structure drone swarm formation recognition realized graph neural networks situation drone swarm calculated constructing macroscopic quantitative descriptors breaks feature extraction fusion algorithm multi scale drones graph neural networks intra layer inter layer feature extraction macroscopic quantitative descriptors based divergence curl construct scale invariant rotation invariant features achieves detection whether drone swarm identification drone swarm calculation degree drone swarm providing basis classification graded handling drone swarm effectively promoting modernization low altitude safety governance capacity
"Highly efficient thiourea-based organic inhibitors, 1-phenyl-3-pyridin-4-ylmethyl-thiourea (PPMTU), 1-phenyl-4-methyl-3-pyridin-4-ylmethyl-thiourea, and 1-phenethyl-3-pyridin-4-ylmethyl-thiourea were synthesised and used for the first time for the protection of mild steel in 1 M HCl medium. The evaluation of the inhibitors’ effectiveness for protecting mild steel was conducted using the weight loss method, electrochemical impedance spectroscopy, and the Tafel method. The surface characteristics of the metal coupons were characterised using surface characterisation techniques (scanning electron microscopy and atomic force microscopy). The hydrophobic properties of the inhibitors were evaluated by means of contact angle. Langmuir, Freundlich, Temkin, and Frumkin's isothermal models were used to study inhibitor adsorption properties. The best fit was obtained for the Langmuir adsorption isotherm, and the adsorption process is a mixed type rather than physisorption or chemisorption. The inhibitor PPMTU exhibits the maximum inhibition efficiency of 99% by the weight loss method and 98% by the Tafel method at a concentration of just 90 µM. Notably, this is the first work to attain this high inhibition efficiency at a remarkably very low concentration of inhibitor, indicating that a very small amount is sufficient to control the corrosion process by up to 99%. The experimental data are validated using computational methods. The results are promising.",highly efficient thiourea based organic inhibitors phenyl pyridin ylmethyl thiourea ppmtu phenyl methyl pyridin ylmethyl thiourea phenethyl pyridin ylmethyl thiourea synthesised used first time protection mild steel hcl medium evaluation inhibitors effectiveness protecting mild steel conducted using weight loss method electrochemical impedance spectroscopy tafel method surface characteristics metal coupons characterised using surface characterisation techniques scanning electron microscopy atomic force microscopy hydrophobic properties inhibitors evaluated means contact angle langmuir freundlich temkin frumkin isothermal models used study inhibitor adsorption properties best fit obtained langmuir adsorption isotherm adsorption process mixed type rather physisorption chemisorption inhibitor ppmtu exhibits maximum inhibition efficiency weight loss method tafel method concentration notably first work attain high inhibition efficiency remarkably low concentration inhibitor indicating small amount sufficient control corrosion process experimental data validated using computational methods results promising
"The value co-creation is the true success of PPP projects. However, the traditional static governance based on project performance cannot cope with the lengthy process of value co-creation in PPP projects; as a result, the strategic asset of partnership has not been fully developed and utilized. This article takes the partnership in PPP projects as the starting point, defines the new connotation of the multi-value of PPP projects, introduces the trust parameter and establishes a contract incentive model based on the game theory which is a two-dimensional integration of contract and trust, and comprehensively analyzes the impact of the optimal incentive coefficient, optimal effort level, and trust level on the creation of cooperation value and overall project benefits by both parties through the establishment of a dual incentive game model of contract and trust. It provides path selection for promoting effective cooperation between the government department and social capitals to co-create project value.",value creation true success ppp projects however traditional static governance based project performance cope lengthy process value creation ppp projects result strategic asset partnership fully developed utilized article takes partnership ppp projects starting point defines new connotation multi value ppp projects introduces trust parameter establishes contract incentive model based game theory two dimensional integration contract trust comprehensively analyzes impact optimal incentive coefficient optimal effort level trust level creation cooperation value overall project benefits parties establishment dual incentive game model contract trust provides path selection promoting effective cooperation government department social capitals create project value
"In order to address problems related to label dependency, imbalance, extensive text analysis, and multi-label classification in legal contexts, this paper presents the BERT-CNN framework. The model’s development and integration with actual court cases allowed for the identification of the ideal hyperparameter configuration for the BERT-CNN architecture. The gathered dataset can be divided into three separate sections for the purpose of conducting a thorough assessment and examination of the model: the training, validation, and testing sets. The evaluation data from the testing set is used to evaluate the model’s performance. The evaluation’s conclusions show that the BERT-CNN model obtains metrics for precision, recall, F1 score and AUC-PR of 0.83, 0.82, 0.83 and 0.88, in that order. The empirical results highlight the BERT-CNN framework’s applicability for classifying legal texts, demonstrating impressive resilience and precision in multi-label classification tasks pertaining to court cases.",order address problems related label dependency imbalance extensive text analysis multi label classification legal contexts paper presents bert cnn framework model development integration actual court cases allowed identification ideal hyperparameter configuration bert cnn architecture gathered dataset divided three separate sections purpose conducting thorough assessment examination model training validation testing sets evaluation data testing set used evaluate model performance evaluation conclusions show bert cnn model obtains metrics precision recall score auc order empirical results highlight bert cnn framework applicability classifying legal texts demonstrating impressive resilience precision multi label classification tasks pertaining court cases
"In the communication systems of intelligent micro-grids, the intrinsic security of the physical layer is insufficient, with high computational complexity and poor robustness. To address these issues, a beamforming algorithm based on deep learning and successive convex approximation (DL-SCA) is proposed. Initially, under the premise that the norm of channel estimation error is bounded, a secrecy rate maximization problem is formulated. A deep neural network is employed to learn the intricate mapping between channel state information (CSI) and the optimal beamforming matrix, facilitating the extraction of profound channel features. Subsequently, building upon the initial solution obtained, the beamforming precoding optimization problem is further solved using the successive convex approximation (SCA) algorithm. Ultimately, through the Monte Carlo experiment based on imperfect CSI, the security performance and convergence speed of different algorithms are compared. In comparison with the zero forcing (ZF) beamforming algorithm, there is an approximate 20% increase in the system secrecy rate; and when compared with the traditional SCA algorithm, there is a reduction of about 10% in the execution time. This substantiates that the DL-SCA algorithm can effectively enhance the intrinsic security performance of the micro-grid, with high robustness and rapid convergence.",communication systems intelligent micro grids intrinsic security physical layer insufficient high computational complexity poor robustness address issues beamforming algorithm based deep learning successive convex approximation sca proposed initially premise norm channel estimation error bounded secrecy rate maximization problem formulated deep neural network employed learn intricate mapping channel state information csi optimal beamforming matrix facilitating extraction profound channel features subsequently building upon initial solution obtained beamforming precoding optimization problem solved using successive convex approximation sca algorithm ultimately monte carlo experiment based imperfect csi security performance convergence speed different algorithms compared comparison zero forcing beamforming algorithm approximate increase system secrecy rate compared traditional sca algorithm reduction execution time substantiates sca algorithm effectively enhance intrinsic security performance micro grid high robustness rapid convergence
"With the rapid development of information technology and the Internet, the existing data query system suffers from poor performance, single structure and slow efficiency in processing data. In view of this, the study introduces the GmmCube multidimensional data intelligent query system and combines column-to-block structure and single-dimension matrix for structure optimization. On the basis of guaranteeing specific dimensions, it continues to introduce the sparse prefix sum for efficiency optimization. The experimental results showed that the maximum storage space consumption was 1050 MB in the training set and 1040 MB in the test set after comparing with three commonly used algorithms: prefix sum array, binary index tree, and segment tree. The maximum deviation in query time was 0.1 hours. When querying Weibo big data (10 million records), the lowest measured absolute error was 2%, the precision rate was 93.56%, and the recall rate was 93.74%. It can be concluded that the novel data query technique proposed by the study is superior in data query performance and efficiency, and can provide an effective solution for big data query analysis.",rapid development information technology internet existing data query system suffers poor performance single structure slow efficiency processing data view study introduces gmmcube multidimensional data intelligent query system combines column block structure single dimension matrix structure optimization basis guaranteeing specific dimensions continues introduce sparse prefix sum efficiency optimization experimental results showed maximum storage space consumption training set test set comparing three commonly used algorithms prefix sum array binary index tree segment tree maximum deviation query time hours querying weibo big data million records lowest measured absolute error precision rate recall rate concluded novel data query technique proposed study superior data query performance efficiency provide effective solution big data query analysis
"Drawing from Lyapunov stability theory, a single input single output model reference adaptive control (MRAC) system using a unified format is researched. A novel design approach is proposed, wherein the control system utilizes the measured value of the model output, thus circumventing issues associated with the unmeasured output differential value of the controlled plant. Because of taking state variables from the model, the system’s resilience against disturbances is notably strengthened. In contrast to the augmentation error signal design method, this approach allows for the derivation of the adaptive control law without the need for an augmentation error signal. Consequently, there is a reduction in the number of adjustable parameters, and the elimination of a set of filters significantly simplifies the system structure, rendering it more amenable to engineering implementation. This streamlined approach promises enhanced efficiency and ease of deployment in practical applications.",drawing lyapunov stability theory single input single output model reference adaptive control mrac system using unified format researched novel design approach proposed wherein control system utilizes measured value model output thus circumventing issues associated unmeasured output differential value controlled plant taking state variables model system resilience disturbances notably strengthened contrast augmentation error signal design method approach allows derivation adaptive control law without need augmentation error signal consequently reduction number adjustable parameters elimination set filters significantly simplifies system structure rendering amenable engineering implementation streamlined approach promises enhanced efficiency ease deployment practical applications
"With the swift progression of machine learning technology, its potential applications in education are vast. This study introduces a convolutional neural network (CNN)-based algorithm designed to evaluate the quality of reforms in physical education classroom teaching. The goal is to improve the effectiveness and impact of physical education instruction through technological advancements. The research initially scrutinizes the limitations associated with traditional methods of evaluating the quality of physical education classrooms and delves into the possibilities afforded by employing CNN for assessing teaching quality. By formulating and implementing a CNN model tailored to the context of physical education classes, this study adeptly analyzes real-time student movement patterns, participation levels, and interaction effects. The experimental outcomes demonstrate the algorithm’s effectiveness in discerning critical teaching elements within the classroom, including students’ enthusiasm for engagement in activities and teachers’ instructional methodologies. The algorithm enables a quantitative assessment of these components. Additionally, this research explores the practical implementation of the algorithm in teaching contexts, including providing real-time feedback to educators and adjusting teaching strategies based on assessment outcomes. The results offer a fresh perspective for enhancing the quality of physical education instruction and lay the foundation for the wider adoption of machine learning technologies in the education sector.",swift progression machine learning technology potential applications education vast study introduces convolutional neural network cnn based algorithm designed evaluate quality reforms physical education classroom teaching goal improve effectiveness impact physical education instruction technological advancements research initially scrutinizes limitations associated traditional methods evaluating quality physical education classrooms delves possibilities afforded employing cnn assessing teaching quality formulating implementing cnn model tailored context physical education classes study adeptly analyzes real time student movement patterns participation levels interaction effects experimental outcomes demonstrate algorithm effectiveness discerning critical teaching elements within classroom including students enthusiasm engagement activities teachers instructional methodologies algorithm enables quantitative assessment components additionally research explores practical implementation algorithm teaching contexts including providing real time feedback educators adjusting teaching strategies based assessment outcomes results offer fresh perspective enhancing quality physical education instruction lay foundation wider adoption machine learning technologies education sector
"In the face of increasingly complex and multi-phase construction projects, traditional scheduling methods struggle to account for uncertainties commonly encountered on construction sites—such as sudden weather changes, delayed material deliveries, and fluctuating labor availability. These unpredictable factors often lead to significant discrepancies between actual progress and the original schedule. To address this challenge, this study proposes a hybrid construction progress optimization scheduling model that combines the A* algorithm with the genetic algorithm (GA), referred to as the A*-GA model. This approach leverages the heuristic search capability of the A* algorithm and the global optimization power of GA. The A*-GA model first employs the A* algorithm to generate an initial construction schedule, prioritizing candidate solutions to identify the most efficient task execution path and ensure rational allocation of time and resources. This preliminary plan is then further refined using GA. Through customized encoding schemes, fitness functions, and genetic operations such as selection, crossover, and mutation, the model iteratively evolves better scheduling solutions. Experimental results demonstrate that the A*-GA model significantly improves project efficiency, reducing the overall construction period by 59 days and cutting costs by 1.501 million yuan. The proposed model proves effective in optimizing both time and cost, offering a more intelligent and adaptive solution for construction progress management in complex engineering environments.",face increasingly complex multi phase construction projects traditional scheduling methods struggle account uncertainties commonly encountered construction sites sudden weather changes delayed material deliveries fluctuating labor availability unpredictable factors often lead significant discrepancies actual progress original schedule address challenge study proposes hybrid construction progress optimization scheduling model combines algorithm genetic algorithm referred model approach leverages heuristic search capability algorithm global optimization power model first employs algorithm generate initial construction schedule prioritizing candidate solutions identify efficient task execution path ensure rational allocation time resources preliminary plan refined using customized encoding schemes fitness functions genetic operations selection crossover mutation model iteratively evolves better scheduling solutions experimental results demonstrate model significantly improves project efficiency reducing overall construction period days cutting costs million yuan proposed model proves effective optimizing time cost offering intelligent adaptive solution construction progress management complex engineering environments
"Uncertainty in the demand and supply of social public resources leads to increased competition, which increases the complexity and collaboration cost of resource allocation. Therefore, this paper introduces a multi-agent reinforcement learning (MARL) algorithm to optimize the dynamic allocation mechanism of social public resources. Firstly, considering the problem of partial loss of information due to environmental unobservability in a multi-intelligent body environment, a MARL algorithm based on prophet-guided (MADRLPG) is proposed. On this basis, a flexible overlapping organization framework under spatio-temporal constraints is constructed. In addition, a dynamic resource allocation algorithm under spatio-temporal constraints is proposed to solve the unbalanced use of resources; meanwhile, a collaborative strategy generation algorithm is proposed to reduce resource competition. Finally, the simulation results show that the resource utilization rate of the proposed method is 97%, which can effectively alleviate the resource constraint problem.",uncertainty demand supply social public resources leads increased competition increases complexity collaboration cost resource allocation therefore paper introduces multi agent reinforcement learning marl algorithm optimize dynamic allocation mechanism social public resources firstly considering problem partial loss information due environmental unobservability multi intelligent body environment marl algorithm based prophet guided madrlpg proposed basis flexible overlapping organization framework spatio temporal constraints constructed addition dynamic resource allocation algorithm spatio temporal constraints proposed solve unbalanced use resources meanwhile collaborative strategy generation algorithm proposed reduce resource competition finally simulation results show resource utilization rate proposed method effectively alleviate resource constraint problem
"The significance of instrument recognition based on audio signals lies in its broad potential across diverse fields such as music education, intelligent technology, and even medical diagnosis. In this study, we focus on tackling the challenges posed by the OpenMIC-2018 (Open Museum Identification Challenge-2018) dataset. Leveraging the powerful nonlinear modeling capabilities of convolutional neural networks (CNNs), the research aims to address common interference factors in instrument recognition, such as background noise, pitch variations, and volume fluctuations during performances. By effectively mitigating these challenges, this approach enhances the accuracy and precision of instrument recognition, offering valuable contributions to both academic research and practical applications. Firstly, FFT (fast Fourier transform) is used to extract MFCC (Mel-frequency cepstral coefficient), chroma, time-domain, and spectral energy from each audio frame. Initial CNN models with 10 different hyperparameters are trained, and the prediction results are merged with the original features. Then, one-dimensional convolutional layers are used for spectral feature convolution operations, and pooling layers are used for downsampling. ReLU (rectified linear unit) is used as the activation function and nonlinearity is applied. Dropout layers are added between convolutional layers and a portion of neurons are randomly set to zero. Next, the learning rate is adjusted based on cross-validation. For multi-label classification tasks, a binary cross-entropy loss is used and parameters are updated through backpropagation algorithm. Finally, a secondary CNN model is used. The merged features are input and the prediction results are obtained. The research results indicate that the innovative method used achieves an average accuracy of 0.92 on OpenMIC-2018, with an accuracy of 0.97 for accordion, flute, and organ, realizing relatively precise instrument recognition.",significance instrument recognition based audio signals lies broad potential across diverse fields music education intelligent technology even medical diagnosis study focus tackling challenges posed openmic open museum identification challenge dataset leveraging powerful nonlinear modeling capabilities convolutional neural networks cnns research aims address common interference factors instrument recognition background noise pitch variations volume fluctuations performances effectively mitigating challenges approach enhances accuracy precision instrument recognition offering valuable contributions academic research practical applications firstly fft fast fourier transform used extract mfcc mel frequency cepstral coefficient chroma time domain spectral energy audio frame initial cnn models different hyperparameters trained prediction results merged original features one dimensional convolutional layers used spectral feature convolution operations pooling layers used downsampling relu rectified linear unit used activation function nonlinearity applied dropout layers added convolutional layers portion neurons randomly set zero next learning rate adjusted based cross validation multi label classification tasks binary cross entropy loss used parameters updated backpropagation algorithm finally secondary cnn model used merged features input prediction results obtained research results indicate innovative method used achieves average accuracy openmic accuracy accordion flute organ realizing relatively precise instrument recognition
"In a career development environment that is very dynamic and complicated, standard career path planning approaches can’t really deal with the uncertainty of the environment in multi-stage decision-making because they rely on static models and expert expertise. This research suggests a reinforcement learning (RL)-based career path planning model that uses a Markov decision process (MDP) to simulate multi-stage decision optimisation and deep Q network (DQN) to perform adaptive iterative optimisation of strategies. A cumulative reward normalised score comparison experiment and a learning rate sensitivity analysis experiment show that the model works best at a medium learning rate (0.01) and that it reaches its highest cumulative reward score during the training period of 25,000 to 35,000 steps. In conclusion, the RL algorithm suggested in this research provides both theoretical and technical support for making an effective career planning system.",career development environment dynamic complicated standard career path planning approaches really deal uncertainty environment multi stage decision making rely static models expert expertise research suggests reinforcement learning based career path planning model uses markov decision process mdp simulate multi stage decision optimisation deep network dqn perform adaptive iterative optimisation strategies cumulative reward normalised score comparison experiment learning rate sensitivity analysis experiment show model works best medium learning rate reaches highest cumulative reward score training period steps conclusion algorithm suggested research provides theoretical technical support making effective career planning system
"With the rapid development of educational technology, personalized learning and intelligent teaching have become the key to improving students’ learning efficiency and motivation. However, traditional teaching methods are often difficult to adapt to the learning needs of different students, resulting in low student participation and poor learning results. In this paper, from these two main indicators, based on deep learning technology, we construct a classroom learning model based on Transformer. Our model has three major modules, namely, student module, course module, and optimization module, which provide students and teachers with personalized classroom design. Through experimental validation, we have conducted experiments in English and math groups, and the performance is good. Specifically, our model outperforms the traditional model in both English and math teaching. This study not only helps to promote the application and development of educational technology but also has important significance for improving teaching practice and promoting the all-round development of students.",rapid development educational technology personalized learning intelligent teaching become key improving students learning efficiency motivation however traditional teaching methods often difficult adapt learning needs different students resulting low student participation poor learning results paper two main indicators based deep learning technology construct classroom learning model based transformer model three major modules namely student module course module optimization module provide students teachers personalized classroom design experimental validation conducted experiments english math groups performance good specifically model outperforms traditional model english math teaching study helps promote application development educational technology also important significance improving teaching practice promoting round development students
"Campus data is increasingly gaining the attention of college student management staff; they hope that through the use of machine learning and big data technology theory, analysis of students during the period of school data finds students growth law, timely find hidden dangers and properly handle, for students to develop more scientific and humanized management plan, truly in accordance with their individual aptitude. For Grade 13 students, the course management optimization success rates were 62.90% for Algorithm 1 and 69.35% for Algorithm 2. For grade 15 students, the success rate of course management optimization was 64.41% for algorithm 1 and 89.83% for algorithm 2. By studying the relevant techniques of student group behavior pattern analysis, we explore how to classify the behavior patterns of college students by combining machine learning K-means algorithm and NMF algorithm. Using the emotion polarity classification algorithm based on dependence, we paper how to classify the emotion polarity of student network comments and classify some real student comments in a university forum. The paper also completed the analysis of student group behavior patterns based on machine learning K-means algorithm and NMF algorithm, and obtained the classification results and characteristics of group behavior patterns, classified the real comments of some students based on the forum based on dependency, and determined the related theory based on support vector machine (SVM) algorithm. The characteristics of students’ behavior in school were analyzed from three aspects, and the K-means algorithm in the cluster analysis was used to mine the data of these three aspects, which obtained the distribution of students’ behavior characteristics of five types of consumption habits, three types of living habits and four types of learning habits. Students’ long Internet time, low number of books borrowing, and provide targeted management for teachers and schools according to the characteristics of these problems propose.",campus data increasingly gaining attention college student management staff hope use machine learning big data technology theory analysis students period school data finds students growth law timely find hidden dangers properly handle students develop scientific humanized management plan truly accordance individual aptitude grade students course management optimization success rates algorithm algorithm grade students success rate course management optimization algorithm algorithm studying relevant techniques student group behavior pattern analysis explore classify behavior patterns college students combining machine learning means algorithm nmf algorithm using emotion polarity classification algorithm based dependence paper classify emotion polarity student network comments classify real student comments university forum paper also completed analysis student group behavior patterns based machine learning means algorithm nmf algorithm obtained classification results characteristics group behavior patterns classified real comments students based forum based dependency determined related theory based support vector machine svm algorithm characteristics students behavior school analyzed three aspects means algorithm cluster analysis used mine data three aspects obtained distribution students behavior characteristics five types consumption habits three types living habits four types learning habits students long internet time low number books borrowing provide targeted management teachers schools according characteristics problems propose
"Handwritten signature authentication, a biometric authentication technology based on gesture behavior characteristics, is widely used for identity verification in fields like e-commerce, electronic contracts, finance, and legal sectors. However, it faces challenges such as high error rates, security vulnerabilities, and privacy concerns. To address these, this study designs a digital trajectory authentication method that leverages behavioral feature recognition. Utilizing the MediaPipe hand detection model, the method captures the air-writing trajectory through video analysis. Then, the model generates joint temporal feature descriptors from both time and frequency domains. Furthermore, a weighted probability matching strategy is adopted to construct a digital trajectory authentication model. Experiment results show that our method has an average authentication error rate (EER) of 3.04% on edge devices, which fully meets the accuracy of authentication recognition and is of great significance for the development of identity verification technology.",handwritten signature authentication biometric authentication technology based gesture behavior characteristics widely used identity verification fields like commerce electronic contracts finance legal sectors however faces challenges high error rates security vulnerabilities privacy concerns address study designs digital trajectory authentication method leverages behavioral feature recognition utilizing mediapipe hand detection model method captures air writing trajectory video analysis model generates joint temporal feature descriptors time frequency domains furthermore weighted probability matching strategy adopted construct digital trajectory authentication model experiment results show method average authentication error rate eer edge devices fully meets accuracy authentication recognition great significance development identity verification technology
"Aiming at issues such as slow convergence of algorithms and low utilization of overall resources in the current research of path optimization design of teaching resource sharing platform, this article applied the path optimization method of teaching resource sharing platform on the basis of ant colony optimization (ACO) to address the issue. By transforming the path optimization problem of the teaching resource sharing platform into a graph theory problem and adjusting the parameter configuration of ACO, the slow convergence speed of the algorithm in the path optimization of the teaching resource sharing platform was optimized, thereby improving the overall resource utilization rate. The experimental findings showed that after 300 iterations, the transmission delay of the path optimization method for the ACO-based teaching resource sharing platform was within 10 ms; the bandwidth utilization rate was over 90%; the path reliability was over 85%; the computation time was within 0.1 s. Moreover, by comparing with other algorithms, it was found that the ACO algorithm was at its best in all data metrics. The monitoring and survey results of the teaching resource sharing platform show that the distribution of teaching resources on the platform has been effectively optimized after the optimization of ACO, and user satisfaction and ratings have significantly improved.",aiming issues slow convergence algorithms low utilization overall resources current research path optimization design teaching resource sharing platform article applied path optimization method teaching resource sharing platform basis ant colony optimization aco address issue transforming path optimization problem teaching resource sharing platform graph theory problem adjusting parameter configuration aco slow convergence speed algorithm path optimization teaching resource sharing platform optimized thereby improving overall resource utilization rate experimental findings showed iterations transmission delay path optimization method aco based teaching resource sharing platform within bandwidth utilization rate path reliability computation time within moreover comparing algorithms found aco algorithm best data metrics monitoring survey results teaching resource sharing platform show distribution teaching resources platform effectively optimized optimization aco user satisfaction ratings significantly improved
"The social sciences and humanities have increasingly adopted computational terminology as the organizing categories for inquiry. We argue that by organizing research around vernacular computational objects (e.g. data, algorithms, or AI) and divided worldly domains (e.g. finance, health, and governance), scholars risk obscuring the universalizing practices and ambitions of computation. These practices seek to establish new relationalities at unprecedented scales, connecting disparate domains, circulating resources across boundaries, and positioning computational interventions as universally applicable. Drawing on intellectual traditions that inspect the fixity of universalizing claims, we problematize the easy adoption of computational categories and argue that they serve as epistemic traps that naturalize the expanding reach of computational universalism. Instead of accepting the hardened categories of our interlocutors, we propose attending to the partial, effortful, and often contested work of translation and commensuration that enables computational actors to position themselves as obligatory passage points across all domains. This approach reveals not only the remarkable achievements of computational relationalities at scale but also their exclusions, betrayals, and partialities. Our intervention aims to spur perspectives that examine how computational actors parse both technical objects and social worlds to advance universalizing ambitions while simultaneously obscuring the enormous labor required to maintain these divisions and connections.",social sciences humanities increasingly adopted computational terminology organizing categories inquiry argue organizing research around vernacular computational objects data algorithms divided worldly domains finance health governance scholars risk obscuring universalizing practices ambitions computation practices seek establish new relationalities unprecedented scales connecting disparate domains circulating resources across boundaries positioning computational interventions universally applicable drawing intellectual traditions inspect fixity universalizing claims problematize easy adoption computational categories argue serve epistemic traps naturalize expanding reach computational universalism instead accepting hardened categories interlocutors propose attending partial effortful often contested work translation commensuration enables computational actors position obligatory passage points across domains approach reveals remarkable achievements computational relationalities scale also exclusions betrayals partialities intervention aims spur perspectives examine computational actors parse technical objects social worlds advance universalizing ambitions simultaneously obscuring enormous labor required maintain divisions connections
"Pipelines serve as a crucial means for transporting hydrocarbon fluids across vast distances worldwide. These pipeline structures are meticulously designed to withstand various environmental loads, thereby ensuring the secure and dependable distribution of fluids from production points to distribution depots or coastal areas. However, the occurrence of leaks within pipeline networks poses a significant challenge, leading to substantial losses for pipeline operators and the environment. Pipeline failures can lead to serious ecological disasters, loss of life and economic setbacks. To mitigate these risks and maintain the safety and reliability of pipeline facilities, a significant amount of research has been devoted to implementing leakage detection and location systems employing diverse approaches. Based on the influence of corrosion and leakage on pipeline circumferential strain, this paper proposes a method to detect corrosion and leakage by using Optical Fiber Strain Sensor (OFSS) to monitor pipeline circumferential strain. The experimental results are verified. It provides a new idea and method for pipeline health monitoring.",pipelines serve crucial means transporting hydrocarbon fluids across vast distances worldwide pipeline structures meticulously designed withstand various environmental loads thereby ensuring secure dependable distribution fluids production points distribution depots coastal areas however occurrence leaks within pipeline networks poses significant challenge leading substantial losses pipeline operators environment pipeline failures lead serious ecological disasters loss life economic setbacks mitigate risks maintain safety reliability pipeline facilities significant amount research devoted implementing leakage detection location systems employing diverse approaches based influence corrosion leakage pipeline circumferential strain paper proposes method detect corrosion leakage using optical fiber strain sensor ofss monitor pipeline circumferential strain experimental results verified provides new idea method pipeline health monitoring
"This paper proposes an intelligent prevention and control framework. Combining SSL correlation analysis and graph convolutional network (GCN), it realizes efficient semantic restoration of HTTPS encrypted traffic and multi-hop behavior identification; designs a streaming computing engine based on Kafka-Flink, which supports millisecond anomaly detection and dynamic model updating; and constructs a group portrait model under the heterogeneous information network, which accurately locates vulnerable nodes to fraud. In addition, federal learning is introduced to optimize the virtual base station positioning algorithm, combined with particle filtering and improved Chan-Taylor parameter optimization, to improve the positioning accuracy in non-line-of-sight environments, and the Deep Reinforcement Learning (DRL) framework is used to achieve dynamic reasoning and adaptive defense of fraudulent intent. The framework provides theoretical support and technical breakthroughs at the algorithmic level for telecom fraud prevention and control.",paper proposes intelligent prevention control framework combining ssl correlation analysis graph convolutional network gcn realizes efficient semantic restoration https encrypted traffic multi hop behavior identification designs streaming computing engine based kafka flink supports millisecond anomaly detection dynamic model updating constructs group portrait model heterogeneous information network accurately locates vulnerable nodes fraud addition federal learning introduced optimize virtual base station positioning algorithm combined particle filtering improved chan taylor parameter optimization improve positioning accuracy non line sight environments deep reinforcement learning drl framework used achieve dynamic reasoning adaptive defense fraudulent intent framework provides theoretical support technical breakthroughs algorithmic level telecom fraud prevention control
"The Gaussian Process Regression (GPR) framework offers a robust and intuitively probabilistic approach, presenting advantages over the less interpretable neural network models, particularly in the context of image super-resolution. Traditional applications of GPR in super-resolution have primarily focused on the posterior mean for generating predictions, neglecting the insightful predictive variance. This variance quantifies the confidence level of each prediction, a critical aspect for ensuring the reliability of the reconstructed images. In this study, we leverage this underutilized metric by incorporating a confidence regularization term into the iterative back projection process. This innovation aims to enhance the reconstruction fidelity by guiding the enhancement process in a more informed manner. Our extensive experimental analysis on established benchmark datasets confirms the superior performance and efficiency of our proposed approach, marking a significant advancement in super-resolution methodologies.",gaussian process regression gpr framework offers robust intuitively probabilistic approach presenting advantages less interpretable neural network models particularly context image super resolution traditional applications gpr super resolution primarily focused posterior mean generating predictions neglecting insightful predictive variance variance quantifies confidence level prediction critical aspect ensuring reliability reconstructed images study leverage underutilized metric incorporating confidence regularization term iterative back projection process innovation aims enhance reconstruction fidelity guiding enhancement process informed manner extensive experimental analysis established benchmark datasets confirms superior performance efficiency proposed approach marking significant advancement super resolution methodologies
"Aiming at the inefficiency of retrieval and insufficient recommendation caused by the dispersion of dance multimodal resources, this study proposes a knowledge graph construction method based on improved GCNs. Integrating public dance libraries and teaching resources, the first dance knowledge graph DanceKG is constructed, covering entity relationships such as movements and styles. DRA-GCN is innovatively proposed to quantify the interaction frequency of nodes through dynamic weighting module and strengthen the characterization of long-tailed entities associated with complex movements by combining multi-head attention. Experiments show that DRA-GCN has a link prediction MRR of 0.72 (9% higher than traditional GCN) on 50,000 triad datasets, and the F1-value of movement classification is improved by 5%, which supports the movement recommendation and error correction of intelligent teaching system and promotes the digital development of dance education.",aiming inefficiency retrieval insufficient recommendation caused dispersion dance multimodal resources study proposes knowledge graph construction method based improved gcns integrating public dance libraries teaching resources first dance knowledge graph dancekg constructed covering entity relationships movements styles dra gcn innovatively proposed quantify interaction frequency nodes dynamic weighting module strengthen characterization long tailed entities associated complex movements combining multi head attention experiments show dra gcn link prediction mrr higher traditional gcn triad datasets value movement classification improved supports movement recommendation error correction intelligent teaching system promotes digital development dance education
"The construction industry faces significant challenges due to changes in the economic environment, making high-quality development a crucial area of research. This study examines the impact of economic policy uncertainty on corporate performance using financial data from the construction industry collected between 2000 and 2020. By constructing a multiple linear regression model, it explores the moderating and mediating effects of financial slack on this relationship. The results show a significant negative correlation between economic policy uncertainty and corporate performance. However, financial slack can reduce the negative impact of this uncertainty on corporate performance. By effectively managing financial slack, companies can mitigate the adverse effects of economic policy uncertainty. These findings provide a theoretical basis for businesses to optimize management decisions, improve resource allocation efficiency, and enhance corporate performance.",construction industry faces significant challenges due changes economic environment making high quality development crucial area research study examines impact economic policy uncertainty corporate performance using financial data construction industry collected constructing multiple linear regression model explores moderating mediating effects financial slack relationship results show significant negative correlation economic policy uncertainty corporate performance however financial slack reduce negative impact uncertainty corporate performance effectively managing financial slack companies mitigate adverse effects economic policy uncertainty findings provide theoretical basis businesses optimize management decisions improve resource allocation efficiency enhance corporate performance
"With the rapid growth of online education, an increasing number of people are turning to digital platforms for learning. However, due to the relatively recent development of online education, the quality of available resources varies significantly. To address this issue, this study explores the application of machine learning in online education, focusing specifically on how to design and optimize personalized learning paths. The goal is to enhance learning effectiveness by tailoring educational resources to individual needs. In this approach, student learning data is collected and analyzed using the K-means clustering algorithm to categorize students into distinct learning groups. Based on each group’s learning objectives and focus areas, customized resources are recommended to provide a more targeted and optimized learning experience. Through user collaborative filtering, users were asked to rate the recommended courses and improve resource recommendation. Then, based on the Term Frequency-Inverse Document Frequency (TF-IDF) algorithm, the content features of learning resources were analyzed and targeted learning path recommendations were made. Finally, through deep Q-learning network (DQN), personalized learning path planning was further strengthened and improved, achieving functions such as personalized learning path recommendation, intelligent tutoring system, dynamic difficulty adjustment, and optimized resource allocation. By combining machine learning methods to optimize the learning path of online education, the learning efficiency of students was significantly improved; their learning experience was optimized; the coherence of learning was greatly improved. The satisfaction of students with the online education platform improved by using machine learning under high-frequency usage ranged from 4.6 to 4.9, which was considered very satisfactory. After learning, the learning performance score was as high as 4.7 to 4.9, which was considered excellent. Machine learning has enormous application value in personalized learning path planning and optimization.",rapid growth online education increasing number people turning digital platforms learning however due relatively recent development online education quality available resources varies significantly address issue study explores application machine learning online education focusing specifically design optimize personalized learning paths goal enhance learning effectiveness tailoring educational resources individual needs approach student learning data collected analyzed using means clustering algorithm categorize students distinct learning groups based group learning objectives focus areas customized resources recommended provide targeted optimized learning experience user collaborative filtering users asked rate recommended courses improve resource recommendation based term frequency inverse document frequency idf algorithm content features learning resources analyzed targeted learning path recommendations made finally deep learning network dqn personalized learning path planning strengthened improved achieving functions personalized learning path recommendation intelligent tutoring system dynamic difficulty adjustment optimized resource allocation combining machine learning methods optimize learning path online education learning efficiency students significantly improved learning experience optimized coherence learning greatly improved satisfaction students online education platform improved using machine learning high frequency usage ranged considered satisfactory learning learning performance score high considered excellent machine learning enormous application value personalized learning path planning optimization
"Students can use the network music library to understand the connotation of works well, and their ability to perceive music can also be strengthened. The students’ music imagination space also becomes larger and larger with the enrichment of acoustic conditions. Applying big data information to the assessment of music courses forms a huge data collection in terms. Then professionalize the meaningful data to make it have power and insight. From the perspective of subject attributes, information management major, as one of the majors most related to the application of big data, focuses on information, information technology, and information systems. For comprehensive institution of higher learning, the diversification and multi-layering of big data results make it difficult to manage big data in professional music colleges. Despite the existing challenges in college music teaching, the big data era provides opportunities for teachers to enhance and optimize the educational experience. These advancements hold significant value for both students and educators. Focusing on the objectives, unique features, and limiting factors of big data in educational information management—particularly within music colleges—this paper proposes a model designed to support high-quality decision-making in information management.",students use network music library understand connotation works well ability perceive music also strengthened students music imagination space also becomes larger larger enrichment acoustic conditions applying big data information assessment music courses forms huge data collection terms professionalize meaningful data make power insight perspective subject attributes information management major one majors related application big data focuses information information technology information systems comprehensive institution higher learning diversification multi layering big data results make difficult manage big data professional music colleges despite existing challenges college music teaching big data provides opportunities teachers enhance optimize educational experience advancements hold significant value students educators focusing objectives unique features limiting factors big data educational information management particularly within music colleges paper proposes model designed support high quality decision making information management
"Traditional pose resolution methods face the problem of insufficient accuracy when dealing with complex table tennis sports scenes. For this reason, this paper firstly extracts the foreground of table tennis movement based on the improved AlphaPose algorithm. Then a three-dimensional stacked convolutional neural network (3D-SCNN) is designed to extract the spatio-temporal features of the motion pose. A 3D convolutional block attention mechanism is proposed to enhance the important spatio-temporal features and output the final motion pose estimation results through fully connected layers. Finally, a similarity calculation method is used to calculate the cumulative distances of the key points in the standard and estimated movements, and scoring is performed, so as to realize the accurate analysis of the movement postures. The experimental outcome implies that the correct attitude estimation rate of 3D-SCNN is improved by at least 2.6%, which validates the effectiveness of the designed approach.",traditional pose resolution methods face problem insufficient accuracy dealing complex table tennis sports scenes reason paper firstly extracts foreground table tennis movement based improved alphapose algorithm three dimensional stacked convolutional neural network scnn designed extract spatio temporal features motion pose convolutional block attention mechanism proposed enhance important spatio temporal features output final motion pose estimation results fully connected layers finally similarity calculation method used calculate cumulative distances key points standard estimated movements scoring performed realize accurate analysis movement postures experimental outcome implies correct attitude estimation rate scnn improved least validates effectiveness designed approach
"The rapid development of electronic data processing and Internet technology has promoted the geometric growth of data and information. In the past 2 years, the data generated globally has reached 90% of the total human data, indicating that human beings have entered the era of big data. The application of big data technology in the field of teacher team building is also a new trend. In addition, the emergence of intelligent education has triggered an educational reform that subverts the tradition. People are no longer limited to the old-fashioned teaching mode. Props are not just blackboards and chalk characters. It is also a research trend to use artificial intelligence technology to improve the level of intelligent education for teachers. Vocational undergraduate education teachers shoulder the important mission of cultivating people through morality, and it is particularly important to build a team of high-quality and high-quality teachers in the new era. Through the investigation of the teaching team of vocational undergraduate education in a school, this paper finds that there are insufficient number of teachers, poor stability of the teaching team, professional fatigue of teachers, and limited self-improvement of teachers in the construction of the teaching team of a school. And in response to these problems, combined with big data and artificial intelligence technology, the strategy of emphasizing curriculum, promoting social participation, and strengthening the self-construction of vocational undergraduate education teachers is proposed. By solving these problems, it will help to implement the requirements for the emotional cultivation of vocational undergraduate education, help teachers’ self-growth, and help to implement the fundamental task of cultivating people through virtue.",rapid development electronic data processing internet technology promoted geometric growth data information past years data generated globally reached total human data indicating human beings entered big data application big data technology field teacher team building also new trend addition emergence intelligent education triggered educational reform subverts tradition people longer limited old fashioned teaching mode props blackboards chalk characters also research trend use artificial intelligence technology improve level intelligent education teachers vocational undergraduate education teachers shoulder important mission cultivating people morality particularly important build team high quality high quality teachers new investigation teaching team vocational undergraduate education school paper finds insufficient number teachers poor stability teaching team professional fatigue teachers limited self improvement teachers construction teaching team school response problems combined big data artificial intelligence technology strategy emphasizing curriculum promoting social participation strengthening self construction vocational undergraduate education teachers proposed solving problems help implement requirements emotional cultivation vocational undergraduate education help teachers self growth help implement fundamental task cultivating people virtue
"BIM Technology provides an important solution for the informatization of the CI (construction industry). BIM is to establish an information integrated system through certain data standards at a certain stage or in the whole process of decision-making, implementation, and operation of construction projects. So as to realize information sharing and information conversion and high-quality and efficient engineering construction. Engineering projects are often completed jointly by multiple project participants. Different project participants will inevitably have certain connections when applying BIM Technology in the project, thus forming a technological innovation network. Using the statistical analysis method, analyze the central data of the network, and the relationship between cross-industry cooperation and innovation performance. Based on the findings from social network analysis and statistical analysis, recommendations are proposed to optimize the network structure and boost technological innovation performance. The analysis reveals that the current network structure is relatively fragmented. To address this, new approaches for information management within the CI are suggested, focusing on modernizing and enhancing the traditional methods of information sharing and transmission.",bim technology provides important solution informatization construction industry bim establish information integrated system certain data standards certain stage whole process decision making implementation operation construction projects realize information sharing information conversion high quality efficient engineering construction engineering projects often completed jointly multiple project participants different project participants inevitably certain connections applying bim technology project thus forming technological innovation network using statistical analysis method analyze central data network relationship cross industry cooperation innovation performance based findings social network analysis statistical analysis recommendations proposed optimize network structure boost technological innovation performance analysis reveals current network structure relatively fragmented address new approaches information management within suggested focusing modernizing enhancing traditional methods information sharing transmission
"Under the background of modern education, the importance of innovation and entrepreneurship teaching has become increasingly prominent, and how to effectively evaluate the teaching effect a quality evaluation model of innovation and entrepreneurship teaching effect based on fuzzy clustering algorithm, so as to improve the evaluation efficiency and accuracy of teaching effect. The design of the model framework considers the integration of multi-dimensional data, combining information from different sources, ensuring a comprehensive analysis of teaching effectiveness. In the aspect of model parameter selection and ambiguity control strategy, the key parameters are optimized by cross-validation and heuristic search to improve the reliability of clustering effect. By dynamically adjusting the ambiguity coefficient, the fuzziness in the model is effectively controlled, and the representativeness of clustering results is improved. Dynamic ambiguity control for adaptive fuzzy clustering, improving noise resistance by 18%. According to the further analysis of innovation and entrepreneurship education, it is found that teachers’ recognition of teaching content is 64%. During the evaluation process, 38.62% of the students said that the classroom atmosphere was active, which could stimulate their interest in learning. Although 94% of teachers think that the curriculum design is reasonable, 66% of students still feel that the curriculum content is out of touch with the practical application, and only 54.39% of students think that the curriculum meets their expectations.",background modern education importance innovation entrepreneurship teaching become increasingly prominent effectively evaluate teaching effect quality evaluation model innovation entrepreneurship teaching effect based fuzzy clustering algorithm improve evaluation efficiency accuracy teaching effect design model framework considers integration multi dimensional data combining information different sources ensuring comprehensive analysis teaching effectiveness aspect model parameter selection ambiguity control strategy key parameters optimized cross validation heuristic search improve reliability clustering effect dynamically adjusting ambiguity coefficient fuzziness model effectively controlled representativeness clustering results improved dynamic ambiguity control adaptive fuzzy clustering improving noise resistance according analysis innovation entrepreneurship education found teachers recognition teaching content evaluation process students said classroom atmosphere active could stimulate interest learning although teachers think curriculum design reasonable students still feel curriculum content touch practical application students think curriculum meets expectations
"This article studied an optimization method for enterprise financial health assessment (FHA) model based on genetic algorithm (GA). In response to the problems of strong subjectivity, insufficient dynamism, and limited data processing capabilities in traditional evaluation models, this article optimized the weights of financial indicators through GAs to improve the scientific and accurate evaluation results. It used 11 financial indicators as the main data to collect financial data from multiple enterprises and form a dataset. GA can be used to optimize the weights of financial indicators. This article constructed a FHA model, evaluated the constructed model, conducted corresponding analysis based on experimental results, and drew conclusions. The results showed that the model optimized by GA had significant improvements in evaluation accuracy and adaptability. The maximum mean squared error (MSE) of the GA in all comparative experiments was 19.962, which was lower than the minimum MSE of 20.043 obtained by other comparative methods. The maximum Mean Absolute Error (MAE) of the GA in all comparisons was 4.988, which was lower than the minimum MAE of 5.031 obtained by other comparison methods. This indicates that the enterprise financial health evaluation model based on GA has sufficient accuracy, is more objective in determining the weights of financial indicators, and has higher scientific validity in the evaluation results. The optimized enterprise FHA model provides more effective services for enterprise financial management. By optimizing the enterprise FHA model based on GA, it provides scientific basis for enterprise decision-making, improves enterprise management level, and provides new ideas and methods for the application of GA in enterprise finance. Future research can further explore the combined application of GAs and other intelligent algorithms, explore more efficient optimization methods, and promote the development of FHA research for enterprises.",article studied optimization method enterprise financial health assessment fha model based genetic algorithm response problems strong subjectivity insufficient dynamism limited data processing capabilities traditional evaluation models article optimized weights financial indicators gas improve scientific accurate evaluation results used financial indicators main data collect financial data multiple enterprises form dataset used optimize weights financial indicators article constructed fha model evaluated constructed model conducted corresponding analysis based experimental results drew conclusions results showed model optimized significant improvements evaluation accuracy adaptability maximum mean squared error mse comparative experiments lower minimum mse obtained comparative methods maximum mean absolute error mae comparisons lower minimum mae obtained comparison methods indicates enterprise financial health evaluation model based sufficient accuracy objective determining weights financial indicators higher scientific validity evaluation results optimized enterprise fha model provides effective services enterprise financial management optimizing enterprise fha model based provides scientific basis enterprise decision making improves enterprise management level provides new ideas methods application enterprise finance future research explore combined application gas intelligent algorithms explore efficient optimization methods promote development fha research enterprises
"Real-time fault monitoring in cable networks with complex topologies is of great significance in improving the stability of power systems. In recent years, Time-Reversal Multiple Signal Classification (TR-MUSIC) has been proposed for soft fault location in cable networks. This method can use single-frequency data to calculate the pseudo-spectrum and the faults can be located by finding the maximum energy positions. However, due to the propagation of harmonic signals, there will be multiple peaks in the pseudo-spectrum in the high-frequency band, which are referred to as ghosting traces. This leads to an inability to accurately determine the location of the soft fault, causing the false peak to be referred to as a ghost fault. This paper analyses the frequency characteristics of the TR-MUSIC method and then introduces the idea of co-prime sampling for frequency sampling to eliminate the influence of ghost fault. The results show that co-prime sampling in TR-MUSIC can counteract the effects of ghosting traces and reduce the computational complexity.",real time fault monitoring cable networks complex topologies great significance improving stability power systems recent years time reversal multiple signal classification music proposed soft fault location cable networks method use single frequency data calculate pseudo spectrum faults located finding maximum energy positions however due propagation harmonic signals multiple peaks pseudo spectrum high frequency band referred ghosting traces leads inability accurately determine location soft fault causing false peak referred ghost fault paper analyses frequency characteristics music method introduces idea prime sampling frequency sampling eliminate influence ghost fault results show prime sampling music counteract effects ghosting traces reduce computational complexity
"In order to better understand and grasp the progress of social innovation design research, this paper combines quantitative and qualitative research methods and conducts a quantitative analysis of 4037 papers on ‘social innovation design’ in the Web of Science database from 2013 to 2023. According to the findings of this study, the policy environment and the technological environment are the main guiding factors for social innovation design research. This paper presents an academic ecological landscape that differs from previous ones in terms of publication volume, research hotspots, and evolutionary trends. In addition, this paper conducts a descriptive analysis on the time distribution and key themes of social innovation design research. Finally, it analyses the content of the literature from three aspects: essential characteristics, challenges and development paths, and reflection on social innovation design research.",order better understand grasp progress social innovation design research paper combines quantitative qualitative research methods conducts quantitative analysis papers social innovation design web science database according findings study policy environment technological environment main guiding factors social innovation design research paper presents academic ecological landscape differs previous ones terms publication volume research hotspots evolutionary trends addition paper conducts descriptive analysis time distribution key themes social innovation design research finally analyses content literature three aspects essential characteristics challenges development paths reflection social innovation design research
"Functional core stability training has been widely applied in various fields and has shown positive results. In order to study its application methods and effects in police physical training, we developed a mathematical model according to the characteristics of fighting of SWAT, and methods are designed to be more in line with the action form of the fighting of SWAT. Except for the traditional strength exercise, this research adds another 12 weeks of core stability exercise, and then conducted a study on seven indicators that reflect the strength of fighting with hands before and after this 12 weeks of exercise, using these mathematical data to construct a mode. The mathematical model features of the core stability training for special police combat are shown as follows: the effect of exercise in the experimental group is significantly better than the control group. However, the core stability exercise has a different degree of impact on those indicators that reflect the strength of fighting with hands.",functional core stability training widely applied various fields shown positive results order study application methods effects police physical training developed mathematical model according characteristics fighting swat methods designed line action form fighting swat except traditional strength exercise research adds another weeks core stability exercise conducted study seven indicators reflect strength fighting hands weeks exercise using mathematical data construct mode mathematical model features core stability training special police combat shown follows effect exercise experimental group significantly better control group however core stability exercise different degree impact indicators reflect strength fighting hands
"In the research of data-driven adaptive smart teaching in business English, existing algorithms are prone to overfitting when dealing with complex teaching scenarios, resulting in low accuracy in prediction and decision-making and difficulty in effectively generalizing to new contexts. This study integrates machine learning algorithms and data-driven methods to deeply mine massive learning data and construct an efficient adaptive teaching model. The model uses advanced machine learning techniques to identify each learner’s needs and learning paths accurately. Research data shows that the practical application of this model has increased the learning efficiency of business English learners by 30% and significantly improved their learning satisfaction. In terms of a data-driven approach, over 5 million pieces of learning data were collected and analyzed, covering learners’ learning habits, performance changes, feedback, and other aspects. These data provide a strong basis for optimizing teaching strategies, making teaching more accurate and personalized. This study utilizes machine learning algorithms to achieve immediate responses and personalized guidance for learners’ questions. This system improves learners’ learning efficiency and greatly enhances their learning experience. Not only does it provide strong technical support for business English teaching, but it also offers new ideas and directions for future research on intelligent teaching.",research data driven adaptive smart teaching business english existing algorithms prone overfitting dealing complex teaching scenarios resulting low accuracy prediction decision making difficulty effectively generalizing new contexts study integrates machine learning algorithms data driven methods deeply mine massive learning data construct efficient adaptive teaching model model uses advanced machine learning techniques identify learner needs learning paths accurately research data shows practical application model increased learning efficiency business english learners significantly improved learning satisfaction terms data driven approach million pieces learning data collected analyzed covering learners learning habits performance changes feedback aspects data provide strong basis optimizing teaching strategies making teaching accurate personalized study utilizes machine learning algorithms achieve immediate responses personalized guidance learners questions system improves learners learning efficiency greatly enhances learning experience provide strong technical support business english teaching also offers new ideas directions future research intelligent teaching
"Storyboard design has always played a crucial role in the film, television, and animation industries. In today’s interactive media landscape, the significance of storyboard design for animation is becoming increasingly prominent. This paper aims to propose an intelligent storyboard algorithm to enhance the design of interactive contexts in animation and advance the modernization of animation production. A basic framework for storyboard-based virtual assembly animation is established, integrating storyboard scripting with virtual assembly technology commonly used in film and television. The entire assembly process is realized through the continuous construction of scenes, providing a detailed approach to the information representation and modeling of the storyboard script. Experimental results demonstrate that the storyboard virtual assembly animation system proposed in this paper significantly improves the interactive effects of animation and enhances the intelligent processing capabilities of animation production. Consequently, this system can be employed in future intelligent storyboard animations to optimize the expression of interactive contexts.",storyboard design always played crucial role film television animation industries today interactive media landscape significance storyboard design animation becoming increasingly prominent paper aims propose intelligent storyboard algorithm enhance design interactive contexts animation advance modernization animation production basic framework storyboard based virtual assembly animation established integrating storyboard scripting virtual assembly technology commonly used film television entire assembly process realized continuous construction scenes providing detailed approach information representation modeling storyboard script experimental results demonstrate storyboard virtual assembly animation system proposed paper significantly improves interactive effects animation enhances intelligent processing capabilities animation production consequently system employed future intelligent storyboard animations optimize expression interactive contexts
"In the realm of contemporary education, the emergence of big data technology has underscored the significance of analyzing student emotional and cognitive patterns, a process crucial for unveiling individual traits and needs throughout the learning experience. This study introduces a novel diagnostic framework, leveraging big data to precisely evaluate these patterns and enhance the allocation of personalized teaching resources. Traditional analysis methodologies, often limited by their inability to effectively process complex data structures and provide operational utility, are thus addressed. The proposed framework incorporates an integrative approach for emotional and cognitive diagnosis, employing Bayesian networks’ probabilistic distribution methods. This approach not only augments the interpretability of diagnostic outcomes but also streamlines the estimation of student ability distributions. Additionally, a groundbreaking multi-objective optimization strategy, grounded in the analytic hierarchy process (AHP), is presented, complemented by a solution method devised using a hybrid adaptive ant colony algorithm. This innovative method facilitates the efficient allocation of personalized learning resources, thereby significantly enhancing the adaptability of teaching content to individual student needs. The findings from this methodological and empirical investigation contribute a pioneering perspective to the field of personalized teaching. They provide educational practitioners with effective decision-support tools and offer researchers novel insights and methodologies in related domains.",realm contemporary education emergence big data technology underscored significance analyzing student emotional cognitive patterns process crucial unveiling individual traits needs throughout learning experience study introduces novel diagnostic framework leveraging big data precisely evaluate patterns enhance allocation personalized teaching resources traditional analysis methodologies often limited inability effectively process complex data structures provide operational utility thus addressed proposed framework incorporates integrative approach emotional cognitive diagnosis employing bayesian networks probabilistic distribution methods approach augments interpretability diagnostic outcomes also streamlines estimation student ability distributions additionally groundbreaking multi objective optimization strategy grounded analytic hierarchy process ahp presented complemented solution method devised using hybrid adaptive ant colony algorithm innovative method facilitates efficient allocation personalized learning resources thereby significantly enhancing adaptability teaching content individual student needs findings methodological empirical investigation contribute pioneering perspective field personalized teaching provide educational practitioners effective decision support tools offer researchers novel insights methodologies related domains
"Auto-PLD is the automatic design algorithm framework of machine learning pipeline for the whole-process data analysis scenario. First, define a machine learning pipeline with five stages and can support the processing of continuous and discrete type features separately. Then, the automated pipeline design problem is decomposed into two sub problems: structure search and hyper parameter optimization, and an algorithm combining reinforcement learning and Bayesian optimization is proposed to alternately optimize the two subproblems. Finally, in order to improve the efficiency of automatic pipeline design, two parallelized pipeline construction methods are further proposed. Experimental results show that Auto-PLD outperforms auto-sklearn with most datasets. Moreover, with the increase of computing nodes, the parallelized Auto-PLD can further improve the pipeline building performance. The Auto-PLD-random method performed at 1, 4, and 8 hours respectively: 19, 22, 23, 27, 26, and 25. Automated Pipeline Design with Q-learning (Auto-PLD-Q) methods performed at 1, 4, and 8 hours: 18, 20, 26; 23, 27, and 27. The Auto-PLD-DeepC method performed at 1, 4, and 8 hours, respectively: 21,23,27; 22, 26, and 26. Auto-PLD-PG methods performed at 1, 4, and 8 hours, respectively: 19, 21, 27; 26, 26, and 25. Auto-LLE is an automated machine learning algorithm framework for lifelong learning scenarios. For the classification task based on concept drift and data imbalance, an algorithm based on weighted ensemble learning of adaptive model was proposed. Concept types are divided into “long-term concepts” and “short-term concepts,” and different types of concepts are handled separately using incremental learners and adaptive weight update methods, and automatically capture the concept drift and improve the model prediction performance. Based on Auto-PLD and Auto-LLE, we design and implement a system that supports both automated pipeline design and automated lifelong learning. In system design, high system accessibility and scalability are obtained by designing easy high-level programming interface and pluggable module integration. In task type, the common data analysis tasks such as classification, regression, and clustering are supported.",auto pld automatic design algorithm framework machine learning pipeline whole process data analysis scenario first define machine learning pipeline five stages support processing continuous discrete type features separately automated pipeline design problem decomposed two sub problems structure search hyper parameter optimization algorithm combining reinforcement learning bayesian optimization proposed alternately optimize two subproblems finally order improve efficiency automatic pipeline design two parallelized pipeline construction methods proposed experimental results show auto pld outperforms auto sklearn datasets moreover increase computing nodes parallelized auto pld improve pipeline building performance auto pld random method performed hours respectively automated pipeline design learning auto pld methods performed hours auto pld deepc method performed hours respectively auto pld methods performed hours respectively auto lle automated machine learning algorithm framework lifelong learning scenarios classification task based concept drift data imbalance algorithm based weighted ensemble learning adaptive model proposed concept types divided long term concepts short term concepts different types concepts handled separately using incremental learners adaptive weight update methods automatically capture concept drift improve model prediction performance based auto pld auto lle design implement system supports automated pipeline design automated lifelong learning system design high system accessibility scalability obtained designing easy high level programming interface pluggable module integration task type common data analysis tasks classification regression clustering supported
"Group Activity Recognition (GAR) is the task of recognizing an overall activity in a multi-individual scene. Most of the existing methods have achieved significant progress by incorporating the attributes and relations between individuals. However, these methods still suffer from the ability to automatically detect, recognize, and infer potential connections in group behavior. To address the issue, inspired by the role of latent spatial position present in video frames, we propose a novel method for learning graph structures by incorporating the distances between individuals. Specifically, we design a graph reasoning module based on Graph Convolutional Networks (GCNs) to learn the hierarchical relationship between individual behaviors and group intentions. To evaluate the feasibility and effectiveness of our proposed model, we conduct experiments on publicly available datasets. Through the experimental results, we validate the effectiveness of our approach, demonstrating its ability to accurately analyze and interpret group behavior.",group activity recognition gar task recognizing overall activity multi individual scene existing methods achieved significant progress incorporating attributes relations individuals however methods still suffer ability automatically detect recognize infer potential connections group behavior address issue inspired role latent spatial position present video frames propose novel method learning graph structures incorporating distances individuals specifically design graph reasoning module based graph convolutional networks gcns learn hierarchical relationship individual behaviors group intentions evaluate feasibility effectiveness proposed model conduct experiments publicly available datasets experimental results validate effectiveness approach demonstrating ability accurately analyze interpret group behavior
"This article presents the Unheard City project that uses an Android application to read Bluetooth Low Energy and Wi-Fi on an app supported data walk through a Connected Autonomous Vehicle testbed. We listen to the sonic aspects of the devices found and apply sonic thinking to determine the socio-ecologies within the area. Sound, either through reading the signal as data or sonification, is used as a technical practice to think about the layers within the street and their relationships, drawing on situational analytics and mixing qualitative and quantitative frames of interpretation. The emergent relations are derived from the heard signals that enable a view of the heard software that enables us to understand the road as an increasingly subdivided and specialised networks. These networks connect both humans and technical layers in subdivided areas of the road. Silences are explored as either redundant architecture, present but no longer required, or obfuscated data leading to the socio-ecology becoming less public. Constraints, such as measuring emissions levels, on the ecologies are also considered. Future work will consider other protocols and signal types to extend the analysis.",article presents unheard city project uses android application read bluetooth low energy app supported data walk connected autonomous vehicle testbed listen sonic aspects devices found apply sonic thinking determine socio ecologies within area sound either reading signal data sonification used technical practice think layers within street relationships drawing situational analytics mixing qualitative quantitative frames interpretation emergent relations derived heard signals enable view heard software enables understand road increasingly subdivided specialised networks networks connect humans technical layers subdivided areas road silences explored either redundant architecture present longer required obfuscated data leading socio ecology becoming less public constraints measuring emissions levels ecologies also considered future work consider protocols signal types extend analysis
"Evaluating the teaching quality level of university courses is a very meaningful teaching research work. In the context of big data, various artificial intelligence algorithms can effectively utilize massive data to construct intelligent models for evaluating the teaching quality of universities. Popular algorithms include genetic algorithm, BP (Back Propagation) neural network, and support vector machine. However, the modeling steps of these algorithms are usually cumbersome and time-consuming. In response to this, this article proposed a university teaching quality evaluation model based on the Naive Bayes classification algorithm. The model calculated the posterior probability of labels using Bayesian formula and the posterior probability of features by statistically analyzing the feature distribution in the sample. The label with the highest posterior probability was selected as the evaluation result for the feature vector. This article designed experiments to verify the performance advantages and disadvantages of this model compared to other models, and compared the modeling time, accuracy, mean square error, and disciplinary generality of the Naive Bayes model with the three aforementioned models. The results indicated that the modeling time of the Naive Bayes model was 0.12 seconds, with an accuracy of 0.895 and a mean square error of 0.1902. The accuracy difference between science and philosophy disciplines was 0.029, with a mean square error difference of 0.0190. Although the model in this article does not have an advantage in prediction accuracy compared to other models, it has good disciplinary universality, mainly due to its simple model structure and extremely short modeling time.",evaluating teaching quality level university courses meaningful teaching research work context big data various artificial intelligence algorithms effectively utilize massive data construct intelligent models evaluating teaching quality universities popular algorithms include genetic algorithm back propagation neural network support vector machine however modeling steps algorithms usually cumbersome time consuming response article proposed university teaching quality evaluation model based naive bayes classification algorithm model calculated posterior probability labels using bayesian formula posterior probability features statistically analyzing feature distribution sample label highest posterior probability selected evaluation result feature vector article designed experiments verify performance advantages disadvantages model compared models compared modeling time accuracy mean square error disciplinary generality naive bayes model three aforementioned models results indicated modeling time naive bayes model seconds accuracy mean square error accuracy difference science philosophy disciplines mean square error difference although model article advantage prediction accuracy compared models good disciplinary universality mainly due simple model structure extremely short modeling time
"This study filled a gap in the literature by examining whether and how “systems thinking”, a theoretical lens advocated by the World Business Council for Sustainable Development Vision 2050, can be implemented to realize Sustainable Development Goals (SDGs) within the context of China. The study first explained the concept of systems thinking, then provided a review of China’s Ecological Civilization movement and the three pillars of traditional Chinese culture, namely, Daoism, Buddhism, and Confucianism. The research aimed to investigate the extent to which holistic Chinese cultural thinking is applied to drive sustainable business practices in China. We drew empirical insights from multiple perspectives and 74 key informant interviews, ranging from Chinese government officials and practitioners to university academics. Using Foucault’s episteme to examine the possibility of a transition to systems thinking, our key findings reveal that the Chinese micro-economy and firm-level operations, whilst being aware of the importance of environmental issues from top–down, still focus on economic values in the short run. However, as an episteme change is recognized over time, it is likely that the gap between vision and practice will be narrowed down, and a sustainable China just might prevail.",study filled gap literature examining whether systems thinking theoretical lens advocated world business council sustainable development vision implemented realize sustainable development goals sdgs within context china study first explained concept systems thinking provided review china ecological civilization movement three pillars traditional chinese culture namely daoism buddhism confucianism research aimed investigate extent holistic chinese cultural thinking applied drive sustainable business practices china drew empirical insights multiple perspectives key informant interviews ranging chinese government officials practitioners university academics using foucault episteme examine possibility transition systems thinking key findings reveal chinese micro economy firm level operations whilst aware importance environmental issues top still focus economic values short run however episteme change recognized time likely gap vision practice narrowed sustainable china might prevail
"The incremental topic detection method in news topic detection relies too much on the order of document flow, which leads to the drift of clustered topics. The drift phenomenon of news will affect the recognition of topic detection. This article proposes a Dynamic Subtopic Detection Model for Temporal Text (DSDTT), which establishes both related and independent relationships between topics in time windows by designing leader documents for them. The proposal of “Inflection Point Analysis Method” and “CrossMountain” pattern solves the problems of scattered clustering and low discrimination caused by the minimum perplexity determining the number of topics; the model can automatically construct subtopic evolution scenarios based on time windows, effectively alleviating the phenomenon of topic drift. The experiments were conducted from the optimal number of topics and topic perplexity, dynamic subtopic detection, and subtopic evolution relationship scenarios, verifying that the Dynamic Subtopic Detection Model for Temporal Text outperforms the topic perplexity method in selecting the optimal number of topics and exhibits more accurate tracking performance in dynamic subtopic detection.",incremental topic detection method news topic detection relies much order document flow leads drift clustered topics drift phenomenon news affect recognition topic detection article proposes dynamic subtopic detection model temporal text dsdtt establishes related independent relationships topics time windows designing leader documents proposal inflection point analysis method crossmountain pattern solves problems scattered clustering low discrimination caused minimum perplexity determining number topics model automatically construct subtopic evolution scenarios based time windows effectively alleviating phenomenon topic drift experiments conducted optimal number topics topic perplexity dynamic subtopic detection subtopic evolution relationship scenarios verifying dynamic subtopic detection model temporal text outperforms topic perplexity method selecting optimal number topics exhibits accurate tracking performance dynamic subtopic detection
"Design thinking is a human-centered decision-making approach that emphasizes empathy, problem definition, ideation, prototyping, and testing. It is gradually gaining recognition for its potential to deepen the understanding of mathematical concepts through problem-solving from a user-oriented lens. Integrating design thinking into e-learning activities in mathematics for STEM students promotes a transformative strategy for delivering content and enhancing students’ engagement with mathematical concepts, which could influence their educational experiences and academic performance. To contribute to the growing body of literature on the application of design thinking in education, this qualitative case study examined how STEM senior high school students from a private institution in the Philippines engaged with design thinking integrated e-learning activities focused on conic sections. The investigation reveals that the e-learning activities motivated students to utilize design skills and apply mathematical concepts to solve problems. Students also developed a better appreciation for e-learning. The findings highlight the studentsʼ creativity and resourcefulness in problem-solving, which are skills that are crucial in STEM fields. Insights from this study could guide educators and instructional material developers in conceptualizing e-learning activities in mathematics and other disciplines. Furthermore, it offers strategies for effectively integrating design thinking into instructional practices in STEM education.",design thinking human centered decision making approach emphasizes empathy problem definition ideation prototyping testing gradually gaining recognition potential deepen understanding mathematical concepts problem solving user oriented lens integrating design thinking learning activities mathematics stem students promotes transformative strategy delivering content enhancing students engagement mathematical concepts could influence educational experiences academic performance contribute growing body literature application design thinking education qualitative case study examined stem senior high school students private institution philippines engaged design thinking integrated learning activities focused conic sections investigation reveals learning activities motivated students utilize design skills apply mathematical concepts solve problems students also developed better appreciation learning findings highlight students creativity resourcefulness problem solving skills crucial stem fields insights study could guide educators instructional material developers conceptualizing learning activities mathematics disciplines furthermore offers strategies effectively integrating design thinking instructional practices stem education
"In recent years, in order to deal with the frequent occurrence of disasters, master and predict the law of disaster occurrence, and explore the nature, disaster knowledge map construction and analysis and application technology have become an important research hotspot and direction. However, existing knowledge graph structures for natural disasters often focus only on temporal or spatial properties, lacking the integration of spatiotemporal properties. Therefore, this paper proposes a technique for constructing a typical disaster knowledge graph integrated with its spatial and temporal characteristics. First of all, through the analysis of typical disaster events, established the “event-space-time-causal” triplet model, the disaster three factors relationship, formed a comprehensive, systematic disaster knowledge framework, and then use deep learning technology of semantic analysis of large amounts of text data and relationship extraction, to automatically build a disaster knowledge map. In the test verification, this paper takes a natural disaster as an example to construct a typical disaster knowledge map integrating spatial and temporal characteristics, and conducts empirical analysis. The results show that the knowledge graph construction method can better express the spatial and temporal characteristics and influencing factors of some natural disasters, and provide more comprehensive and accurate disaster prevention and control and response strategies for decision-making agencies. In conclusion, the typical disaster knowledge map construction technology integrating spatial and temporal characteristics is a novel and effective disaster response method with great application potential.",recent years order deal frequent occurrence disasters master predict law disaster occurrence explore nature disaster knowledge map construction analysis application technology become important research hotspot direction however existing knowledge graph structures natural disasters often focus temporal spatial properties lacking integration spatiotemporal properties therefore paper proposes technique constructing typical disaster knowledge graph integrated spatial temporal characteristics first analysis typical disaster events established event space time causal triplet model disaster three factors relationship formed comprehensive systematic disaster knowledge framework use deep learning technology semantic analysis large amounts text data relationship extraction automatically build disaster knowledge map test verification paper takes natural disaster example construct typical disaster knowledge map integrating spatial temporal characteristics conducts empirical analysis results show knowledge graph construction method better express spatial temporal characteristics influencing factors natural disasters provide comprehensive accurate disaster prevention control response strategies decision making agencies conclusion typical disaster knowledge map construction technology integrating spatial temporal characteristics novel effective disaster response method great application potential
"The paper presents a novel methodology for applying AI-driven style transfer to complex 3D architectural models. By converting 3D models into 2D image sequences, the process integrates sequential slicing, training, video-guided diffusion and reconstruction to transform existing 3D models based on text, image, or video prompts into new stylised forms. This enables architects to explore diverse design concepts, focusing on spatial composition, visual appearance and tectonics through high-resolution outputs that capture both exterior and interior spatial relations. The results demonstrates the setups potential in enhancing early-stage design ideation through AI, by both outperforming existing video diffusion platform while also facilitating a fast exploration of different outcomes - capabilities which were validated in a design course. The study highlights an approach for utilising advanced 2D image-based AI models to generate intricate and meaningful 3D architectural transformations.",paper presents novel methodology applying driven style transfer complex architectural models converting models image sequences process integrates sequential slicing training video guided diffusion reconstruction transform existing models based text image video prompts new stylised forms enables architects explore diverse design concepts focusing spatial composition visual appearance tectonics high resolution outputs capture exterior interior spatial relations results demonstrates setups potential enhancing early stage design ideation outperforming existing video diffusion platform also facilitating fast exploration different outcomes capabilities validated design course study highlights approach utilising advanced image based models generate intricate meaningful architectural transformations
"Renowned for its scientific, artistic, and cultural innovations, the Han dynasty marked a significant turning point in Chinese history. The use of ink, brushstrokes, and thematic components influenced by Confucianism, Daoism, and other philosophical traditions are few of the creative styles and methods that were popular during this period in Han painting (HP). HP is distinguished by its delicate brushstrokes, subtle ink washes, and meticulous attention to detail. It frequently depicts themes of nature, landscapes, animals, and everyday life. As an integral part of China’s creative legacy, this form of art continues to be honored and it had a significant effect on later Chinese painting (CP) shape. HPs’ color qualities and Patterns would suffer significant deterioration with time. Research on the culture indicates that Han art is limited. In addition, there is a powerful correlation between changes in the reliability of CP and the environment. The conservation and study of traditional Chinese HP techniques by artificial intelligence (AI) offers a possible path forward for the preservation and knowledge of this historic art form. In this research, the link between safeguarding the environment and the patterns (P), color (C), and shapes (S) of CP was investigated using a novel earthworm tuned recurrent neural network (EW- TRNN). We gathered Hans CP datasets as experimental data. Image preprocessing using Gaussian blur filter for noise reduction. Feature extraction employs the Gray Level Co-occurrence Matrix (GLCM) to capture the textural P and brushwork style characteristics. The study’s findings demonstrate that the suggested approach is more authentic in predicting the P, S, and C features of Chinese artworks. Finally, we conclude by stating that we may improve preservation efforts, research, and public happiness of HPs among contemporary audiences by utilizing AI-driven methodologies. This work assurance the endurance and accessibility of HPs for future generations by highlighting the revolutionary possibility of AI in bridging the gap between tradition and technological advances.",renowned scientific artistic cultural innovations dynasty marked significant turning point chinese history use ink brushstrokes thematic components influenced confucianism daoism philosophical traditions creative styles methods popular period painting distinguished delicate brushstrokes subtle ink washes meticulous attention detail frequently depicts themes nature landscapes animals everyday life integral part china creative legacy form art continues honored significant effect later chinese painting shape hps color qualities patterns would suffer significant deterioration time research culture indicates art limited addition powerful correlation changes reliability environment conservation study traditional chinese techniques artificial intelligence offers possible path forward preservation knowledge historic art form research link safeguarding environment patterns color shapes investigated using novel earthworm tuned recurrent neural network trnn gathered hans datasets experimental data image preprocessing using gaussian blur filter noise reduction feature extraction employs gray level occurrence matrix glcm capture textural brushwork style characteristics study findings demonstrate suggested approach authentic predicting features chinese artworks finally conclude stating may improve preservation efforts research public happiness hps among contemporary audiences utilizing driven methodologies work assurance endurance accessibility hps future generations highlighting revolutionary possibility bridging gap tradition technological advances
"The complexity of information data makes strengthening information evaluation and screening an important research content. Traditional association analysis algorithms are difficult to perform strong analysis on association rules, and they do not sufficiently grasp data feature information. Therefore, based on the original association classification algorithm, multiple learning of training sets and setting of interest threshold are studied to improve the correlation between category labels and reduce the interference of redundant information. Subsequently, multiple tag feature selection algorithms and frequent pattern trees are introduced to measure data information using tag importance as a metric, improving information processing efficiency, and implementing algorithm processing through local connection and pruning operations. The information evaluation results of the improved algorithm proposed in the study show that the training error of the improved association classification (AC) algorithm is 2.36%, and the maximum training error difference and time consumption range between the improved AC algorithm and other algorithms reach 31.28% and 14.28%, greatly improving the operational efficiency and classification accuracy of the algorithm. Simultaneously improve the average classification error and Recall@K Increased to 10.37% and above 5%, effectively achieving information evaluation accuracy. The information evaluation algorithm can effectively provide practical tools and method reference value for data mining and related management work.",complexity information data makes strengthening information evaluation screening important research content traditional association analysis algorithms difficult perform strong analysis association rules sufficiently grasp data feature information therefore based original association classification algorithm multiple learning training sets setting interest threshold studied improve correlation category labels reduce interference redundant information subsequently multiple tag feature selection algorithms frequent pattern trees introduced measure data information using tag importance metric improving information processing efficiency implementing algorithm processing local connection pruning operations information evaluation results improved algorithm proposed study show training error improved association classification algorithm maximum training error difference time consumption range improved algorithm algorithms reach greatly improving operational efficiency classification accuracy algorithm simultaneously improve average classification error recall increased effectively achieving information evaluation accuracy information evaluation algorithm effectively provide practical tools method reference value data mining related management work
"In the context of the era of big data, this study explores how big data affects the innovative trends and future prospects of art design. Through integrated application of data collection, processing, model construction and validation, as well as trend prediction and analysis methods, this study reveals the application value and potential of big data technology in the field of art design. The study found that big data not only provides rich resources and inspiration for design but also promotes the improvement of design efficiency and innovation through data-driven decision support. At the same time, the predictive analysis pointed out that the future of art design will be more focused on digital, user experience, sustainability, and interdisciplinary collaboration. Despite the technical challenges of data processing and the ethical issues of user privacy protection, these challenges can be effectively addressed through education and training, interdisciplinary cooperation and policy support to open up new directions for art design. This study provides a new perspective and practical strategy for the integration of art design and big data, and has important theoretical and practical significance for designers, educators and policy makers.",context big data study explores big data affects innovative trends future prospects art design integrated application data collection processing model construction validation well trend prediction analysis methods study reveals application value potential big data technology field art design study found big data provides rich resources inspiration design also promotes improvement design efficiency innovation data driven decision support time predictive analysis pointed future art design focused digital user experience sustainability interdisciplinary collaboration despite technical challenges data processing ethical issues user privacy protection challenges effectively addressed education training interdisciplinary cooperation policy support open new directions art design study provides new perspective practical strategy integration art design big data important theoretical practical significance designers educators policy makers
"The training plan of athletes is crucial for their performance and health. However, due to the differences among athletes in many aspects, traditional training plans are difficult to adapt to the personalized needs of athletes. This article used gated recurrent unit (GRU) to conduct in-depth analysis and learning of athlete training data, and designed targeted training plans for athletes to achieve the goal of improving sports performance. Firstly, a set of athlete training data was collected, and wavelet transform and box plot methods were utilized to denoise and handle outliers in the data; then, the GRU model was used to analyze these data and extract the most valuable feature indicators, providing a basis for adjusting subsequent training schemes. Finally, taking 15 basketball players from A University as research subjects, a personalized training plan was designed to help them better train and improve training effectiveness. The experiment showed that 15 athletes were selected, and their average score on the Functional Movement Screen (FMS) test after the experiment was 14.73 points, which was 2.4 points higher than before the experiment. Before the experiment, only 4 people had an FMS score of 14 or above, accounting for 26.67%, which was very low. However, after the experiment, 12 people had an FMS score of 14 or above, accounting for as high as 80%. Based on the GRU model, personalized training plans were studied to improve athlete performance and health, providing new research ideas for athletes to develop personalized training plans. This can better adapt to the different needs of different types of athletes, thereby improving their training effectiveness and competitive level.",training plan athletes crucial performance health however due differences among athletes many aspects traditional training plans difficult adapt personalized needs athletes article used gated recurrent unit gru conduct depth analysis learning athlete training data designed targeted training plans athletes achieve goal improving sports performance firstly set athlete training data collected wavelet transform box plot methods utilized denoise handle outliers data gru model used analyze data extract valuable feature indicators providing basis adjusting subsequent training schemes finally taking basketball players university research subjects personalized training plan designed help better train improve training effectiveness experiment showed athletes selected average score functional movement screen fms test experiment points points higher experiment experiment people fms score accounting low however experiment people fms score accounting high based gru model personalized training plans studied improve athlete performance health providing new research ideas athletes develop personalized training plans better adapt different needs different types athletes thereby improving training effectiveness competitive level
"In the digital age, with the improvement of people’s living standards, people’s spiritual needs are increasing day by day. Flower arrangement art design can enrich people’s spiritual world, beautify the environment, and improve the quality of life. To achieve innovation in flower arrangement art design, a genetic algorithm was introduced to optimize the K-medoids algorithm, resulting in an improved GK algorithm, which was applied to the classification of raw material types in flower arrangement art. At the same time, selecting stem length, color, and straightness from a large number of features as the main features of flower materials to improve clustering effectiveness. Then, by improving the algorithm, we can deeply explore the creative inspiration of flower arrangement art and achieve intelligent flower arrangement art design. The research results show that the GK algorithm only needs 26 iterations to reach the objective function value, and its clustering accuracy rate is 97.16%. Based on the above content, it can be found that the intelligent flower arrangement art design based on the GK algorithm proposed in the study has excellent performance and accuracy, which effectively improves the level and efficiency of flower arrangement art design.",digital age improvement people living standards people spiritual needs increasing day day flower arrangement art design enrich people spiritual world beautify environment improve quality life achieve innovation flower arrangement art design genetic algorithm introduced optimize medoids algorithm resulting improved algorithm applied classification raw material types flower arrangement art time selecting stem length color straightness large number features main features flower materials improve clustering effectiveness improving algorithm deeply explore creative inspiration flower arrangement art achieve intelligent flower arrangement art design research results show algorithm needs iterations reach objective function value clustering accuracy rate based content found intelligent flower arrangement art design based algorithm proposed study excellent performance accuracy effectively improves level efficiency flower arrangement art design
"With the development and popularity of automated parking lots, the problem of high energy consumption generated by unreasonable scheduling of parking robots has become increasingly prominent. In order to solve the scheduling problem of automatic parking lot, an intelligent scheduling strategy based on deep Q-network algorithm is proposed to solve the problem of high energy consumption caused by unreasonable scheduling in automatic parking lot. By introducing double deep Q-network architecture and advantage function, the scheduling strategy of the robot is optimized. In this study, the time difference error is used as the weight of the sample cache and the negative energy consumption value is used as the reward function to evaluate the performance of the improved deep Q-network algorithm and the traditional algorithm. The simulation results show that the improved deep Q-network algorithm performs well in vehicle scheduling optimization with a reward value of 46.8. By analyzing the distribution of vehicles in the garage under different algorithms, the effectiveness of the improved deep Q-network algorithm in preferentially utilizing high-quality garage resources is further verified. It demonstrates how the enhanced deep Q-network algorithm can efficiently optimize the automated parking lot’s scheduling strategy while utilizing the premium garage to lower operational energy usage.",development popularity automated parking lots problem high energy consumption generated unreasonable scheduling parking robots become increasingly prominent order solve scheduling problem automatic parking lot intelligent scheduling strategy based deep network algorithm proposed solve problem high energy consumption caused unreasonable scheduling automatic parking lot introducing double deep network architecture advantage function scheduling strategy robot optimized study time difference error used weight sample cache negative energy consumption value used reward function evaluate performance improved deep network algorithm traditional algorithm simulation results show improved deep network algorithm performs well vehicle scheduling optimization reward value analyzing distribution vehicles garage different algorithms effectiveness improved deep network algorithm preferentially utilizing high quality garage resources verified demonstrates enhanced deep network algorithm efficiently optimize automated parking lot scheduling strategy utilizing premium garage lower operational energy usage
"With the application of complex structural well technology such as large inclination well, horizontal well and horizontal branch well in petroleum exploitation industry, the problems of pressure support and sticking are increasing. In order to solve these problems, a hydraulic oscillator is designed. When drilling fluid flows through the hydraulic oscillator, the hydraulic oscillator generates mild oscillating forces and is transmitted through the drill tool to the drill bit, thereby reducing the friction of the drill tool on the wall, reducing the possibility of pressure and sticking. In order to verify the rationality of the design, Simulink is used to simulate the dynamic characteristics of the screw motor in the oscillator. The results show that the speed of the screw motor fluctuates from 0 to 0.01 s, and the stable speed of 6.8 r/s will be reached after 0.01 s. Finally, the prototype is trial-produced and tested. The working pressure of the prototype is 3.8 MPa and the vibration frequency is 14.7 Hz, which meets the design requirements.",application complex structural well technology large inclination well horizontal well horizontal branch well petroleum exploitation industry problems pressure support sticking increasing order solve problems hydraulic oscillator designed drilling fluid flows hydraulic oscillator hydraulic oscillator generates mild oscillating forces transmitted drill tool drill bit thereby reducing friction drill tool wall reducing possibility pressure sticking order verify rationality design simulink used simulate dynamic characteristics screw motor oscillator results show speed screw motor fluctuates stable speed reached finally prototype trial produced tested working pressure prototype mpa vibration frequency meets design requirements
"The trend of market house prices is influenced by various factors, and house price prediction algorithm remains a very classic and challenging nonlinear problem in data analysis. Analyzing various factors that may affect market house prices can help to provide a more accurate assessment of future trends in house prices. Multiple nonlinear regression is suitable for analyzing data affected by many factors. It is more efficient and practical to predict the house price by multiple independent characteristics than only one independent variable. This article is based on a deep learning algorithm and uses the PaddlePaddle experimental platform to learn about the past period of house sale prices and other related data in Beijing. Multiple nonlinear regression methods are used to analyze the data and predict the future house price trend in this region.",trend market house prices influenced various factors house price prediction algorithm remains classic challenging nonlinear problem data analysis analyzing various factors may affect market house prices help provide accurate assessment future trends house prices multiple nonlinear regression suitable analyzing data affected many factors efficient practical predict house price multiple independent characteristics one independent variable article based deep learning algorithm uses paddlepaddle experimental platform learn past period house sale prices related data beijing multiple nonlinear regression methods used analyze data predict future house price trend region
"In the interactions between load aggregators (LAs) and customers within the electricity market, a non-cooperative Stackelberg game model involving multiple LAs and multiple customers is developed to investigate the incentive strategies of LAs and the response strategies of customers. This model allows customers to trade with multiple LAs, freely allocating response resources based on incentive prices to encourage benign competition among LAs. It constructs differentiated users from the perspective of dissatisfaction with electricity usage and differentiated LAs from the perspective of deviations in aggregated load response, with both parties aiming to maximize their utility functions through game play. The optimal decisions of the players are analyzed using backward induction method, and a distributed solution framework is proposed based on consensus algorithms and optimization algorithms to ensure the privacy of users and LAs during response process. Finally, simulations are conducted to validate the rationality of the proposed multi-LA-multi-customer transaction framework, as well as the effectiveness of our proposed consensus-based solution framework. The simulation results indicate that the proposed model is capable of providing optimal decisions for LAs and customers while ensuring privacy protection.",interactions load aggregators customers within electricity market non cooperative stackelberg game model involving multiple multiple customers developed investigate incentive strategies response strategies customers model allows customers trade multiple freely allocating response resources based incentive prices encourage benign competition among constructs differentiated users perspective dissatisfaction electricity usage differentiated perspective deviations aggregated load response parties aiming maximize utility functions game play optimal decisions players analyzed using backward induction method distributed solution framework proposed based consensus algorithms optimization algorithms ensure privacy users response process finally simulations conducted validate rationality proposed multi multi customer transaction framework well effectiveness proposed consensus based solution framework simulation results indicate proposed model capable providing optimal decisions customers ensuring privacy protection
"Due to the new crown pneumonia outbreak in 2019, many schools are unable to teach online, which stimulates the birth of a new teaching model in the education industry. Online teaching has developed by leaps and bounds with the rapid development of information technology. Information-based teaching is a way to enhance student learning from the learning environment. There are many factors that influence the learning environment, including the classroom teaching scenario, cooperation between teachers and students, and communication in the classroom. In the teaching process, the teaching mode has changed from the traditional teacher-teaching-student-learning fill-in-the-blank teaching to the independent acquisition of knowledge by students. In order to investigate the current status of teaching informatics on the Internet, we chose a university in city A as a research object to investigate the current status of teaching informatics in its online philosophy department. At present, information technology teaching in China is still in a backward state. The current situation of information-based teaching in China is (1) the lack of information-based teaching resources and the lack of standardization in the use of resources. (2) Lack of awareness of online information teaching. In order to promote the level of online information-based learning of our schoolchildren from a comprehensive perspective, and in response to the drawbacks of online information-based teaching in China found in the above experimental survey, we propose an online information-based learning model based on big data and artificial intelligence decision-making to enhance students’ learning interest through an interesting learning model. After experimental verification, it can be concluded that our proposed model can significantly improve students’ behavioral performance and learning ability, and has a certain role in enhancing students’ learning effectiveness. Finally, we make recommendations based on the current state of application of big data and artificial intelligence in online informatics teaching. The recommendations are to improve the current situation of information-based teaching on the one hand to improve the quality of information-based resources, which requires the joint gate-keeping of the government, schools and teachers. On the other hand, the information technology literacy of teachers should be improved.",due new crown pneumonia outbreak many schools unable teach online stimulates birth new teaching model education industry online teaching developed leaps bounds rapid development information technology information based teaching way enhance student learning learning environment many factors influence learning environment including classroom teaching scenario cooperation teachers students communication classroom teaching process teaching mode changed traditional teacher teaching student learning fill blank teaching independent acquisition knowledge students order investigate current status teaching informatics internet chose university city research object investigate current status teaching informatics online philosophy department present information technology teaching china still backward state current situation information based teaching china lack information based teaching resources lack standardization use resources lack awareness online information teaching order promote level online information based learning schoolchildren comprehensive perspective response drawbacks online information based teaching china found experimental survey propose online information based learning model based big data artificial intelligence decision making enhance students learning interest interesting learning model experimental verification concluded proposed model significantly improve students behavioral performance learning ability certain role enhancing students learning effectiveness finally make recommendations based current state application big data artificial intelligence online informatics teaching recommendations improve current situation information based teaching one hand improve quality information based resources requires joint gate keeping government schools teachers hand information technology literacy teachers improved
"Demand-side management is becoming increasingly pivotal in modern power systems. This paper introduces a highly practical distributed control strategy for managing clusters of thermostatically controlled loads (TCLs). The approach is designed to enhance the participation of TCL clusters in demand-side management, ensuring efficient and reliable power system operations. The proposed control strategy defines a priority list, where TCLs can locally reconstruct the global priority list through distributed information exchange. Based on this list, TCLs make local decisions regarding their operational states. Furthermore, the strategy employs an asynchronous communication protocol, which significantly reduces the cost of establishing communication networks compared to control strategies based on synchronous communication. Numerical simulations confirm that, under the asynchronous communication mechanism, the proposed strategy effectively achieves precise tracking of the cluster’s aggregated power relative to the reference power, while also minimizing occurrences of temperature exceedance.",demand side management becoming increasingly pivotal modern power systems paper introduces highly practical distributed control strategy managing clusters thermostatically controlled loads tcls approach designed enhance participation tcl clusters demand side management ensuring efficient reliable power system operations proposed control strategy defines priority list tcls locally reconstruct global priority list distributed information exchange based list tcls make local decisions regarding operational states furthermore strategy employs asynchronous communication protocol significantly reduces cost establishing communication networks compared control strategies based synchronous communication numerical simulations confirm asynchronous communication mechanism proposed strategy effectively achieves precise tracking cluster aggregated power relative reference power also minimizing occurrences temperature exceedance
"In the advertising industry, understanding consumers’ sentiment reactions and preferences towards advertising images is crucial. To gain a deeper understanding for the impact of advertising images on audience sentiments and preference prediction, a saliency prediction model and an advertising preference prediction model for text reinforcement learning are proposed based on eye movement data. The saliency prediction model enhanced the visual features of advertising images by analyzing the text saliency in advertisements. The advertising preference prediction model utilized eye movement data and graph convolutional neural networks to understand individual visual reactions and sentiment preferences towards advertising images. The performance test results indicated that the proposed significance prediction model performed excellently on the accuracy-recall curve and F-measure curve. The accuracy on the model prediction performance of the study using the method was 0.787, SROCC was 0.628, PLCC was 0.583, and EMD decreased to 0.064. The highest SROCC value between the model and the true value can reach 0.6169, the highest PLCC can reach 0.5722, and the lowest error is 0.1929. The average similarity of the advertising preference prediction model was about 0.55 when the liking score was 1. When the score was 7, the average similarity increased to about 0.65. It presents that the method can effectively predict different preferences and meet practical application needs. As a result, the study not only enriches the research framework of advertising sentiment analysis and preference prediction but also provides more practical tools and methods for advertisement design, personalized recommendation, and marketing strategy development. Sentiment analysis models can be further explored in future research and their practical applications can be developed to promote the digital and intelligent transformation of the advertising industry.",advertising industry understanding consumers sentiment reactions preferences towards advertising images crucial gain deeper understanding impact advertising images audience sentiments preference prediction saliency prediction model advertising preference prediction model text reinforcement learning proposed based eye movement data saliency prediction model enhanced visual features advertising images analyzing text saliency advertisements advertising preference prediction model utilized eye movement data graph convolutional neural networks understand individual visual reactions sentiment preferences towards advertising images performance test results indicated proposed significance prediction model performed excellently accuracy recall curve measure curve accuracy model prediction performance study using method srocc plcc emd decreased highest srocc value model true value reach highest plcc reach lowest error average similarity advertising preference prediction model liking score score average similarity increased presents method effectively predict different preferences meet practical application needs result study enriches research framework advertising sentiment analysis preference prediction also provides practical tools methods advertisement design personalized recommendation marketing strategy development sentiment analysis models explored future research practical applications developed promote digital intelligent transformation advertising industry
"With the rapid development of emerging enterprises, efficient information resource sharing has become crucial. Entrepreneurial information contains various types of data, usually presented in images and visual documents, with rich spatial and temporal features. However, traditional information sharing models face difficulties in handling unstructured data and heterogeneous data from multiple sources, which limits the depth and breadth of information sharing. To address this issue, this study proposes an enterprise information resource sharing model based on YOLO algorithm, which combines a gated recurrent unit with an improved YOLOv4, taking shipyard enterprises as an example for research. YOLO, as a powerful real-time object detection tool, can effectively process image and visual document data. The results indicated that the proposed improved YOLOv4 achieved optimal recall, precision, and accuracy at a batch size of 16 and 80. It performed better than CNN and RNN in key metrics. MSE decreased by 25.33% and 5.17%, respectively, RMSE decreased by 13.64% and 2.58%, respectively, and MAE decreased by 56.67% and 26.37%, respectively. The YOLO-based model significantly enhances the efficiency and quality of information sharing in shipyard enterprises. Its applicability extends to other industries, providing scalable solutions for optimizing resource management.",rapid development emerging enterprises efficient information resource sharing become crucial entrepreneurial information contains various types data usually presented images visual documents rich spatial temporal features however traditional information sharing models face difficulties handling unstructured data heterogeneous data multiple sources limits depth breadth information sharing address issue study proposes enterprise information resource sharing model based yolo algorithm combines gated recurrent unit improved yolov taking shipyard enterprises example research yolo powerful real time object detection tool effectively process image visual document data results indicated proposed improved yolov achieved optimal recall precision accuracy batch size performed better cnn rnn key metrics mse decreased respectively rmse decreased respectively mae decreased respectively yolo based model significantly enhances efficiency quality information sharing shipyard enterprises applicability extends industries providing scalable solutions optimizing resource management
"This article aims to enhance the effectiveness of modern accounting audits, conduct in-depth analysis of cloud accounting environments using cloud computing technology, and study big data audit techniques under the guidance of COBIT standards. The research uses probabilistic algorithms to analyze the parallel solution of data. Based on the COBIT5.0 dimension of contributing factors, the implementation framework of the “Internet plus + Audit” model based on cloud accounting is constructed from seven specific factors: principles and policies, cultural atmosphere, personnel requirements, organizational structure, information resources, infrastructure and process design. By combining quantitative and qualitative analysis methods, threats from enterprises and cloud providers are evaluated, and vulnerability analysis is conducted. Through simulation analysis, the effectiveness of the big data audit method based on COBIT standards proposed in this article in the cloud accounting environment has been verified. This method can significantly improve the effectiveness of accounting and auditing, achieving efficient data processing and rapid generation of audit decisions. For auditing units, especially small and medium-sized ones, this method helps to break down technical barriers and promote service optimization and innovation. At the same time, the integration and collaboration of cloud resources will promote the transformation of audit mode to joint audit mode, achieve the integration of high-quality resources, and improve the independence and efficiency of auditing.",article aims enhance effectiveness modern accounting audits conduct depth analysis cloud accounting environments using cloud computing technology study big data audit techniques guidance cobit standards research uses probabilistic algorithms analyze parallel solution data based cobit dimension contributing factors implementation framework internet plus audit model based cloud accounting constructed seven specific factors principles policies cultural atmosphere personnel requirements organizational structure information resources infrastructure process design combining quantitative qualitative analysis methods threats enterprises cloud providers evaluated vulnerability analysis conducted simulation analysis effectiveness big data audit method based cobit standards proposed article cloud accounting environment verified method significantly improve effectiveness accounting auditing achieving efficient data processing rapid generation audit decisions auditing units especially small medium sized ones method helps break technical barriers promote service optimization innovation time integration collaboration cloud resources promote transformation audit mode joint audit mode achieve integration high quality resources improve independence efficiency auditing
"Computational medicine promises significant advancements in healthcare, using physics-based simulations and artificial intelligence to optimise disease diagnosis, personalise treatment strategies and accelerate medical innovation. Biomedical research efforts are generating a growing number of computational models of human pathophysiology and medical treatments, with advanced applications in areas such as cardiovascular diseases, orthopaedics and cancer diagnosis. However, the widespread adoption of these models is hindered by technological and regulatory barriers. This article provides an overview of the potential impact, needs and challenges of the adoption of in silico medicine in the healthcare ecosystem, with a focus on initiatives to sustain this technology within the European Union's regulatory environment. The article introduces the concept of the ‘computational model lifecycle’ as a framework to describe the stages from academic research to pre-clinical and clinical applications, analysing key opportunities and challenges in translating these technologies at each stage. These challenges are associated with data management, standards for model credibility assessment, transparency of regulatory frameworks, and clinical integration. The article highlights European initiatives such as the European Health Data Space and the Virtual Human Twins Initiative, aimed at fostering the development and application of computational medicine in healthcare.",computational medicine promises significant advancements healthcare using physics based simulations artificial intelligence optimise disease diagnosis personalise treatment strategies accelerate medical innovation biomedical research efforts generating growing number computational models human pathophysiology medical treatments advanced applications areas cardiovascular diseases orthopaedics cancer diagnosis however widespread adoption models hindered technological regulatory barriers article provides overview potential impact needs challenges adoption silico medicine healthcare ecosystem focus initiatives sustain technology within european union regulatory environment article introduces concept computational model lifecycle framework describe stages academic research pre clinical clinical applications analysing key opportunities challenges translating technologies stage challenges associated data management standards model credibility assessment transparency regulatory frameworks clinical integration article highlights european initiatives european health data space virtual human twins initiative aimed fostering development application computational medicine healthcare
"How intractable is the security dilemma? The extent to which scholars believe the security dilemma’s pernicious effects can be ameliorated marks a key dividing line between theoretical approaches to international relations. However, questions about the intractability of the security dilemma have remained largely theoretical and not directly empirically tested. We take advantage of the fact that the security dilemma relies on individual-level mental processes. We use parallel survey experiments in the United States and China across two waves in early 2020 and mid-2023 to assess the extent to which threat perceptions consistent with security dilemma thinking exist among mass publics, change with political context, and can be ameliorated. Our findings are consistent with interpretations of the security dilemma emphasizing its relative intractability. At least in the U.S.-China context, threat perceptions consistent with security dilemma thinking do not appear easily ameliorated by priming respondents to consider factors believed to ameliorate the security dilemma: offense-defense distinguishability, economic interdependence, nuclear weapons, and the reciprocal nature of the security dilemma.",intractable security dilemma extent scholars believe security dilemma pernicious effects ameliorated marks key dividing line theoretical approaches international relations however questions intractability security dilemma remained largely theoretical directly empirically tested take advantage fact security dilemma relies individual level mental processes use parallel survey experiments united states china across two waves early mid assess extent threat perceptions consistent security dilemma thinking exist among mass publics change political context ameliorated findings consistent interpretations security dilemma emphasizing relative intractability least china context threat perceptions consistent security dilemma thinking appear easily ameliorated priming respondents consider factors believed ameliorate security dilemma offense defense distinguishability economic interdependence nuclear weapons reciprocal nature security dilemma
"Magnetic Resonance Imaging (MRI) is a non-invasive imaging method that utilizes radio waves and magnetic fields. This study focuses on reducing the acoustic noise produced inside the cylindrical shell of the scanner, where the patient is located. Vibration modes are generated by eddy currents in the cylindrical shell induced by gradient magnetic fields. Additionally, the scanner wall is typically joined to the gradient spiral cylinder, causing vibrations to be transmitted to the wall and thereby producing extra sound waves. The present study investigates methods for mitigating noise from the scanner wall and reducing the transmission noise from the spiral gradient cylinder. Numerical methods and practical solutions for lowering acoustic noise in MRI gradient coils are explored. A 20 mm uniform absorber is demonstrated as an effective design for significantly reducing acoustic noise in the frequency range 0 to 3 kHz. Finally, numerical analysis of gradient cycles yields solutions that lower both vibration and noise levels.",magnetic resonance imaging mri non invasive imaging method utilizes radio waves magnetic fields study focuses reducing acoustic noise produced inside cylindrical shell scanner patient located vibration modes generated eddy currents cylindrical shell induced gradient magnetic fields additionally scanner wall typically joined gradient spiral cylinder causing vibrations transmitted wall thereby producing extra sound waves present study investigates methods mitigating noise scanner wall reducing transmission noise spiral gradient cylinder numerical methods practical solutions lowering acoustic noise mri gradient coils explored uniform absorber demonstrated effective design significantly reducing acoustic noise frequency range khz finally numerical analysis gradient cycles yields solutions lower vibration noise levels
"Computer-supported collaborative learning (CSCL) is considered an effective strategy for knowledge construction, and research on CSCL in cross-cultural contexts is of great significance for improving the knowledge construction and learning outcomes in minority regions. This study focuses on the relationship between thinking styles and role selection tendencies among Uyghur and Han students in a CSCL environment in a cross-cultural context. Using the Thinking Style Scale, the study surveyed 82 Uyghur and Han students in the second year of junior high school at No. 23 Middle School in Urumqi, Xinjiang, and revealed significant differences and correlations in their thinking styles and role selection in CSCL learning. The study also explains the mechanisms behind the differences in thinking styles and role choices from the perspective of cultural adaptation theory. The findings are as follows: (1) There are significant differences in thinking styles between Uyghur and Han students in a cross-cultural context. (2) Uyghur and Han students show different tendencies in role selection during CSCL learning. (3) There is a significant correlation between specific thinking styles and specific role selections among Uyghur and Han students in a cross-cultural context. The results not only enrich the theoretical foundation of cross-cultural education but also provide new perspectives and strategies for role allocation and instructional design in CSCL practice, contributing to the enhancement of ethnic unity, mutual assistance, and integration.",computer supported collaborative learning cscl considered effective strategy knowledge construction research cscl cross cultural contexts great significance improving knowledge construction learning outcomes minority regions study focuses relationship thinking styles role selection tendencies among uyghur students cscl environment cross cultural context using thinking style scale study surveyed uyghur students second year junior high school middle school urumqi xinjiang revealed significant differences correlations thinking styles role selection cscl learning study also explains mechanisms behind differences thinking styles role choices perspective cultural adaptation theory findings follows significant differences thinking styles uyghur students cross cultural context uyghur students show different tendencies role selection cscl learning significant correlation specific thinking styles specific role selections among uyghur students cross cultural context results enrich theoretical foundation cross cultural education also provide new perspectives strategies role allocation instructional design cscl practice contributing enhancement ethnic unity mutual assistance integration
"In response to the problem of traditional third-party logistics distribution evaluation methods ignoring the interaction and interdependence between evaluation indicators, this study constructs a comprehensive evaluation index system using the analytic network process and selects the most suitable logistics service provider. First, the optimization of third-party logistics distribution for agricultural products and the construction of an evaluation index system are carried out. Then, the analytic network process is used to establish a comprehensive evaluation system that includes multiple levels and evaluation index. The results indicate that the analytic network process has the best performance. When the evaluation process is 100%, the complexity, adaptability to dynamic environments, decision consistency, and usability are 57.3%, 19.2%, 92.6%, and 93.1%, respectively. The example analysis results show that the second service provider can be the preferred target. The above results demonstrate the effectiveness of the proposed method, provide scientific basis for the selection of agricultural product logistics service providers, and provide direction for logistics service providers to improve and optimize their services.",response problem traditional third party logistics distribution evaluation methods ignoring interaction interdependence evaluation indicators study constructs comprehensive evaluation index system using analytic network process selects suitable logistics service provider first optimization third party logistics distribution agricultural products construction evaluation index system carried analytic network process used establish comprehensive evaluation system includes multiple levels evaluation index results indicate analytic network process best performance evaluation process complexity adaptability dynamic environments decision consistency usability respectively example analysis results show second service provider preferred target results demonstrate effectiveness proposed method provide scientific basis selection agricultural product logistics service providers provide direction logistics service providers improve optimize services
"In recent years, student management methods have evolved from traditional disciplinary approaches to comprehensive quality education, emphasizing holistic development and personalized guidance. The application of information technology has made management more intelligent, with smart systems and big data analytics providing services more closely aligned with student needs. With the continuous development of artificial intelligence and deep learning technologies, NLP techniques have also made significant strides. This paper introduces an innovative SC-BGRU-ATT sentiment polarity analysis framework for analyzing student comment data using NLP techniques. This framework begins with the extraction of textual data using word embedding techniques, followed by further feature processing through Text-CNN and BI-GRU with a self-attention mechanism. Finally, a classification module is employed to identify the sentiment polarity of the texts. The test results on public datasets show that the framework achieves over 80% accuracy in identifying two types of sentiment polarity, and over 90% accuracy on self-built datasets. This significant performance demonstrates the potential of our framework in advancing sentiment polarity analysis. Moreover, by accurately capturing and analyzing students’ sentiment and opinions, this approach supports more effective self-organization and management of students. The innovation of this study lies in the integration of multiple models and the enhancement of relevant modules’ performance, leading to an improvement in sentiment polarity analysis. This technological advancement not only refines sentiment polarity analysis but also provides a scalable and adaptable framework for analyzing student sentiment in educational settings. The improved model performance offers robust, data-driven support for decision-making in student management, enabling institutions to better understand and respond to student needs. Furthermore, the modular design of our approach allows for seamless future enhancements, including integration with transformer-based models and multimodal data processing, ensuring continued advancements in sentiment analysis applications.",recent years student management methods evolved traditional disciplinary approaches comprehensive quality education emphasizing holistic development personalized guidance application information technology made management intelligent smart systems big data analytics providing services closely aligned student needs continuous development artificial intelligence deep learning technologies nlp techniques also made significant strides paper introduces innovative bgru att sentiment polarity analysis framework analyzing student comment data using nlp techniques framework begins extraction textual data using word embedding techniques followed feature processing text cnn gru self attention mechanism finally classification module employed identify sentiment polarity texts test results public datasets show framework achieves accuracy identifying two types sentiment polarity accuracy self built datasets significant performance demonstrates potential framework advancing sentiment polarity analysis moreover accurately capturing analyzing students sentiment opinions approach supports effective self organization management students innovation study lies integration multiple models enhancement relevant modules performance leading improvement sentiment polarity analysis technological advancement refines sentiment polarity analysis also provides scalable adaptable framework analyzing student sentiment educational settings improved model performance offers robust data driven support decision making student management enabling institutions better understand respond student needs furthermore modular design approach allows seamless future enhancements including integration transformer based models multimodal data processing ensuring continued advancements sentiment analysis applications
"Artistic portraiture holds a significant position in the art world, with its stories of artistic personalities carrying significant educational value and holding offers much scope for research. Amidst the contemporary artistic education milieu, pedagogical practices have embraced an amalgam of online and offline intelligent instruction modes, thus broadening the art students’ access to artistic portraiture. However, the abundance of digital resources has created a concomitant conundrum regarding the classification and selection of artistic portraiture, emerging as a pressing issue. Emotion recognition, a mechanism that enables natural human-computer interaction, assumes critical importance in artistic portraiture classification, as character emotional expressions serve as a pivotal basis for the said classification. Nevertheless, the extensive costs incurred during the collection and annotation of emotional data related to artistic portraiture constitute a major bottleneck that inhibits the development of emotion recognition research. This paper endeavors to explore the application of knowledge transfer between cross-domains or cross-tasks to enhance the efficacy of emotion recognition in scenarios involving limited or no labeling. To achieve efficient classification of portraiture, the paper employs a blend of domain adaptive and transfer learning techniques to map features onto a shared space between domains. Finally, the effectiveness of this method is verified through unsupervised sentiment classification and labeling of artistic portraiture based on the collected dataset, which resulted in an improvement in performance, achieving a classification accuracy of 46.7% compared to the most relevant domain adaptive sentiment analysis methods.",artistic portraiture holds significant position art world stories artistic personalities carrying significant educational value holding offers much scope research amidst contemporary artistic education milieu pedagogical practices embraced amalgam online offline intelligent instruction modes thus broadening art students access artistic portraiture however abundance digital resources created concomitant conundrum regarding classification selection artistic portraiture emerging pressing issue emotion recognition mechanism enables natural human computer interaction assumes critical importance artistic portraiture classification character emotional expressions serve pivotal basis said classification nevertheless extensive costs incurred collection annotation emotional data related artistic portraiture constitute major bottleneck inhibits development emotion recognition research paper endeavors explore application knowledge transfer cross domains cross tasks enhance efficacy emotion recognition scenarios involving limited labeling achieve efficient classification portraiture paper employs blend domain adaptive transfer learning techniques map features onto shared space domains finally effectiveness method verified unsupervised sentiment classification labeling artistic portraiture based collected dataset resulted improvement performance achieving classification accuracy compared relevant domain adaptive sentiment analysis methods
"As global environmental problems become increasingly severe, the concept of green development has gradually become an important strategic direction for corporate development. As an important tool to measure the environmental performance of enterprises, green accounting is of great significance to promote the sustainable development of enterprises. Green accounting performance evaluation can not only help enterprises identify and improve their environmental management strategies, but also provide data support for the formulation of relevant policies. However, the existing performance evaluation methods mostly focus on quantitative analysis, lacking systematic and comprehensive evaluation. Therefore, this study puts forward a performance evaluation model of enterprise green accounting based on analytic hierarchy process and fuzzy comprehensive evaluation, in order to scientifically evaluate enterprise green accounting performance in a multi-factor and multi-level environment. First, through literature research and expert interviews, this study determines the key indicators that affect the performance of enterprises’ green accounting, including resource utilization efficiency, pollution emission control, and environmental protection investment return. These weights derived from AHP are then integrated into the fuzzy comprehensive evaluation method, where they are applied to weight the membership degrees of each performance indicator. This integration allows the model to combine the structural rigor of AHP with the flexibility of fuzzy logic, resulting in a more comprehensive and realistic evaluation of green accounting performance. The fuzzy comprehensive evaluation method is used to generate a weighted composite score that reflects the overall environmental performance of enterprises across multiple green accounting indicators. Through the case analysis of three typical enterprises, the evaluation model based on analytic hierarchy process and fuzzy comprehensive evaluation can accurately reflect the actual performance of enterprises in green accounting. It demonstrates high applicability and operability, particularly for medium to large-sized enterprises in resource-intensive sectors such as chemical manufacturing, energy, and metallurgy, where environmental management and green investment decisions are of strategic importance. The data analysis shows that the comprehensive score of green accounting performance of enterprises evaluated by this model has increased by about 12% to 18% compared with traditional evaluation methods that rely primarily on financial metrics or single-dimensional environmental indicators. The data analysis shows that the comprehensive score of green accounting performance of enterprises evaluated by this model has increased by about 12% to 18% compared with the traditional evaluation method. The model produces a composite score for each enterprise and also classifies performance into qualitative levels such as excellent, good, moderate, passed, and failed, based on defined thresholds. This dual output enables both precise quantification and intuitive interpretation of enterprise green performance, while also effectively identifying the influence of each indicator on the overall performance and providing specific directions for improvement.",global environmental problems become increasingly severe concept green development gradually become important strategic direction corporate development important tool measure environmental performance enterprises green accounting great significance promote sustainable development enterprises green accounting performance evaluation help enterprises identify improve environmental management strategies also provide data support formulation relevant policies however existing performance evaluation methods mostly focus quantitative analysis lacking systematic comprehensive evaluation therefore study puts forward performance evaluation model enterprise green accounting based analytic hierarchy process fuzzy comprehensive evaluation order scientifically evaluate enterprise green accounting performance multi factor multi level environment first literature research expert interviews study determines key indicators affect performance enterprises green accounting including resource utilization efficiency pollution emission control environmental protection investment return weights derived ahp integrated fuzzy comprehensive evaluation method applied weight membership degrees performance indicator integration allows model combine structural rigor ahp flexibility fuzzy logic resulting comprehensive realistic evaluation green accounting performance fuzzy comprehensive evaluation method used generate weighted composite score reflects overall environmental performance enterprises across multiple green accounting indicators case analysis three typical enterprises evaluation model based analytic hierarchy process fuzzy comprehensive evaluation accurately reflect actual performance enterprises green accounting demonstrates high applicability operability particularly medium large sized enterprises resource intensive sectors chemical manufacturing energy metallurgy environmental management green investment decisions strategic importance data analysis shows comprehensive score green accounting performance enterprises evaluated model increased compared traditional evaluation methods rely primarily financial metrics single dimensional environmental indicators data analysis shows comprehensive score green accounting performance enterprises evaluated model increased compared traditional evaluation method model produces composite score enterprise also classifies performance qualitative levels excellent good moderate passed failed based defined thresholds dual output enables precise quantification intuitive interpretation enterprise green performance also effectively identifying influence indicator overall performance providing specific directions improvement
"Sentiment classification and prediction are critical research areas in natural language processing (NLP), primarily focused on extracting sentiment tendencies and opinions from text. Previous studies have predominantly relied on sentiment dictionaries, machine learning, or deep learning methods, each of which presents two main challenges. First, sentiment dictionaries and machine learning methods heavily depend on manually constructed sentiment lexicons and features, resulting in relatively low accuracy. Second, deep learning models often suffer from a lack of interpretability and are frequently regarded as “black box” systems, making it difficult to understand their decision-making processes. To address these issues, we propose an optimization method for text sentiment classification that combines a large language model (LLM) with a deep learning BERT model. Specifically, building on the concept of ensemble learning, we perform dual sentiment determination by fine-tuning the BERT model in conjunction with the cue-based GPT-3.5-turbo LLM. This approach utilizes the LLM to provide a robust basis for sentiment judgment. Validation is conducted using two classical datasets: one from the entertainment domain and another from the marketing domain. The results demonstrate that the proposed method achieves an average accuracy exceeding 90%, outperforming the benchmark model by at least 2 percentage points. This improvement highlights the effectiveness of the proposed method in enhancing multi-domain sentiment classification performance.",sentiment classification prediction critical research areas natural language processing nlp primarily focused extracting sentiment tendencies opinions text previous studies predominantly relied sentiment dictionaries machine learning deep learning methods presents two main challenges first sentiment dictionaries machine learning methods heavily depend manually constructed sentiment lexicons features resulting relatively low accuracy second deep learning models often suffer lack interpretability frequently regarded black box systems making difficult understand decision making processes address issues propose optimization method text sentiment classification combines large language model llm deep learning bert model specifically building concept ensemble learning perform dual sentiment determination fine tuning bert model conjunction cue based gpt turbo llm approach utilizes llm provide robust basis sentiment judgment validation conducted using two classical datasets one entertainment domain another marketing domain results demonstrate proposed method achieves average accuracy exceeding outperforming benchmark model least percentage points improvement highlights effectiveness proposed method enhancing multi domain sentiment classification performance
"During COVID-19, a blended approach to online and offline interactive teaching and learning will be a primary mobile teaching methodology. The effectiveness of online and offline interactive blended learning models depends heavily on the evaluation and optimization of the quality of instruction. However, current evaluation methods often lack systematicity and accuracy, and cannot fully reflect the actual performance and learning effects of students in the interactive blended learning mode. To optimize student learning in this mobile mode, it is most important to adhere to the basic design principles, utilize micro-learning resources and establish a mobile online teaching platform. Therefore, a comprehensive evaluation system and a hybrid teaching quality evaluation model based on back propagation (BP) neural network were proposed in the study. The new model innovatively combined BP neural network and genetic algorithm (GA) to propose a GA-BP optimization model. Subsequently, the accuracy and dependability of teaching quality evaluation were successfully increased by the study’s establishment of a thorough hybrid mobile system. The outcomes revealed superior student performance in the hybrid teaching mode compared to traditional methods, with an increase in the number of students achieving high scores. Among the four evaluation models (GA, BSA, BP, and GA-BP), GA-BP demonstrated the closest alignment with original grades. It yielded mean error and mean relative error of 3.78 and 0.03, respectively, representing the smallest discrepancies. These findings underpinned the efficacy of the blended instructional model in enhancing student learning outcomes. Moreover, the GA-BP-based mobile evaluation model was more accurate in assessing the quality of instruction, thus providing a more effective evaluation.",covid blended approach online offline interactive teaching learning primary mobile teaching methodology effectiveness online offline interactive blended learning models depends heavily evaluation optimization quality instruction however current evaluation methods often lack systematicity accuracy fully reflect actual performance learning effects students interactive blended learning mode optimize student learning mobile mode important adhere basic design principles utilize micro learning resources establish mobile online teaching platform therefore comprehensive evaluation system hybrid teaching quality evaluation model based back propagation neural network proposed study new model innovatively combined neural network genetic algorithm propose optimization model subsequently accuracy dependability teaching quality evaluation successfully increased study establishment thorough hybrid mobile system outcomes revealed superior student performance hybrid teaching mode compared traditional methods increase number students achieving high scores among four evaluation models bsa demonstrated closest alignment original grades yielded mean error mean relative error respectively representing smallest discrepancies findings underpinned efficacy blended instructional model enhancing student learning outcomes moreover based mobile evaluation model accurate assessing quality instruction thus providing effective evaluation
"To achieve efficient sharing and integrated development of financial information resources, the study proposes a blockchain-based financial management system. The system combines a server-side architecture with a modular data processing integration framework for seamless data flow, a dynamic game framework with group analysis strategy adaptation for dynamically mitigating 51% ledger attacks, and a hybrid tree structure for secure and efficient key-value storage in blockchain, as well as high fidelity data fusion for state trees, to solve the key challenges of secure and efficient financial data integration in enterprises. The results showed that the attack success rate was reduced by 91.2%, and the consensus latency was lower than 90 seconds (more than 10% higher than other consensus mechanism algorithms). The average data storage accuracy was over 90%, which was at least 5% more than that of cloud/distributed systems. The memory occupation result was no more than 1500 kb when the number of datasets was 5000, which was much smaller than 1906 kb and 2341 kb of distributed storage and cloud storage, respectively. The operation efficiency of the core financial function under the method was over 90%. The financial information management system can effectively add, modify, delete, and query voucher management, cashier management, and accounting information management. The proposed method can provide a secure and scalable solution for real-time financial decision-making in dynamic market conditions.",achieve efficient sharing integrated development financial information resources study proposes blockchain based financial management system system combines server side architecture modular data processing integration framework seamless data flow dynamic game framework group analysis strategy adaptation dynamically mitigating ledger attacks hybrid tree structure secure efficient key value storage blockchain well high fidelity data fusion state trees solve key challenges secure efficient financial data integration enterprises results showed attack success rate reduced consensus latency lower seconds higher consensus mechanism algorithms average data storage accuracy least cloud distributed systems memory occupation result number datasets much smaller distributed storage cloud storage respectively operation efficiency core financial function method financial information management system effectively add modify delete query voucher management cashier management accounting information management proposed method provide secure scalable solution real time financial decision making dynamic market conditions
"According to the constructivist learning theory, learners need to actively construct a knowledge system in a practical situation, and deep learning, with intelligent data analysis, pattern recognition and predictive decision-making capabilities, just builds a practical platform for students to discover market opportunities and formulate innovative solutions, which effectively promotes the development of their innovation and entrepreneurship skills. At the same time, deep learning emphasizes the understanding, utilization and generation of knowledge, and further strengthens students’ innovation and entrepreneurship literacy by cultivating human-machine collaboration, which is in line with the concept of “learning by doing” in contextual cognitive theory. In addition, deep learning has the characteristics of thinking training, knowledge transfer and application, which echoes the requirements of higher-order thinking cultivation in Bloom’s educational goal classification theory, which can effectively improve the knowledge integration, transformation and application ability of college students, and enhance the ability of knowledge creation and innovation and entrepreneurship. In the application of practical education scenarios, taking the YOLOv5s network as an example, after the introduction of the SE module and the SIoU loss function, the average accuracy (mAP) of the feature detection and entrepreneurship scene recognition tasks of innovative projects is increased to 89.74% and 89.33%, respectively. This significant performance improvement intuitively demonstrates the ability of deep learning algorithms to accurately analyze education data. Through the efficient processing of educational data such as classroom teaching videos and project display images, the system can accurately identify students’ innovative thinking performance and entrepreneurial practice scenarios, and then provide data support for teachers to optimize teaching design and reform teaching methods, tailor personalized learning paths for students, truly realize data-driven educational innovation, and promote the deep integration of innovation and entrepreneurship education theory and practice.",according constructivist learning theory learners need actively construct knowledge system practical situation deep learning intelligent data analysis pattern recognition predictive decision making capabilities builds practical platform students discover market opportunities formulate innovative solutions effectively promotes development innovation entrepreneurship skills time deep learning emphasizes understanding utilization generation knowledge strengthens students innovation entrepreneurship literacy cultivating human machine collaboration line concept learning contextual cognitive theory addition deep learning characteristics thinking training knowledge transfer application echoes requirements higher order thinking cultivation bloom educational goal classification theory effectively improve knowledge integration transformation application ability college students enhance ability knowledge creation innovation entrepreneurship application practical education scenarios taking yolov network example introduction module siou loss function average accuracy map feature detection entrepreneurship scene recognition tasks innovative projects increased respectively significant performance improvement intuitively demonstrates ability deep learning algorithms accurately analyze education data efficient processing educational data classroom teaching videos project display images system accurately identify students innovative thinking performance entrepreneurial practice scenarios provide data support teachers optimize teaching design reform teaching methods tailor personalized learning paths students truly realize data driven educational innovation promote deep integration innovation entrepreneurship education theory practice
"The centralized storage and processing of big data increases the risk of communication data leakage, and the security of user communication transmission has become a current research hotspot. Therefore, to improve the efficiency of existing anonymous communication schemes, this study designs an anonymous identity-based encryption algorithm based on the Decisional Bilinear Diffie-Hellman. An anonymous communication model for small-scale users is further proposed. When the data size was 100 MB, the proposed algorithm had an upload time cost of approximately 827.3 ms and a download time cost of 208.4 ms. The proposed method reduced communication time by 37.86%, 30.43%, and 16.88% compared with other methods, respectively. Under 20 multiple server attacks, the average resistance time was as high as 22.78 s, which increased by 30.02%, 25.65%, 52.07%, 488.63%, and 78.67% compared with the other 5 methods, respectively. The anonymous identity-based encryption algorithm can improve the security of information transmission, and the small-scale anonymous communication model strengthens the privacy and security of user communication processes, which is significant for improving the security of big data servers.",centralized storage processing big data increases risk communication data leakage security user communication transmission become current research hotspot therefore improve efficiency existing anonymous communication schemes study designs anonymous identity based encryption algorithm based decisional bilinear diffie hellman anonymous communication model small scale users proposed data size proposed algorithm upload time cost approximately download time cost proposed method reduced communication time compared methods respectively multiple server attacks average resistance time high increased compared methods respectively anonymous identity based encryption algorithm improve security information transmission small scale anonymous communication model strengthens privacy security user communication processes significant improving security big data servers
"This article discusses a 6 week research project that took place in a sixth grade U.S. history classroom in the Southeast. For 6 weeks, sixth graders explored pivotal issues throughout U.S. history connected to voting rights issues for African Americans. First, they examined the 13th, 14th, and 15th Amendments and the creation of literacy tests in the South. Next, students explored how Bloody Sunday played an instrumental role in leading to the Voting Rights Act of 1965. Then, the focus of the project shifted to the Supreme Court verdict of Shelby versus Holder that weakened the protections for minorities’ voting rights. Next, the students analyzed voting restriction laws in Georgia after the 2020 U.S. presidential election. Finally, our project closed with students examining the recent potential legislation by Congress: The Freedom to Vote Act. The summative assessment for our study asked students to write an essay on whether they supported or opposed the Freedom to Vote Act. All of the activities over the course of the 6 week project were designed to develop students’ political thinking skills as they analyzed the impact public policies with legislation passed by Congress could have on African Americans’ voting rights.",article discusses week research project took place sixth grade history classroom southeast weeks sixth graders explored pivotal issues throughout history connected voting rights issues african americans first examined amendments creation literacy tests south next students explored bloody sunday played instrumental role leading voting rights act focus project shifted supreme court verdict shelby versus holder weakened protections minorities voting rights next students analyzed voting restriction laws georgia presidential election finally project closed students examining recent potential legislation congress freedom vote act summative assessment study asked students write essay whether supported opposed freedom vote act activities course week project designed develop students political thinking skills analyzed impact public policies legislation passed congress could african americans voting rights
"The study primarily focuses on the optimization of query scheduling within economic dynamic systems, which are characterized by vast, sensitive datasets that necessitate exceptional accuracy and reliability. As the digital age accelerates data proliferation, the complexity of data retrieval escalates, leading to critical challenges such as sluggish query speeds and inconsistent results in existing systems. Traditional query algorithms, which rely on basic search logic such as keyword matching and hyperlink analysis, have been proven unsuitable for enterprise-scale operations, often resulting in delays and inaccurate data. To address these deficiencies, an Improved Query Scheduling (IQS) algorithm is embedded within an economic dynamic system model. The IQS leverages continuous data reading capabilities to enable seamless algorithm execution, coupled with weighted screening of query node loads. This approach significantly reduces central processing unit strain while enhancing query efficiency. The experimental results demonstrated the effectiveness of the model, which processed multiple combinations of query features with a CPU execution time of less than 5 seconds and achieved near-perfect query accuracy (0.99). In real-world deployments, the model maintained stable load distribution, completing platform-level queries in 16.56 seconds at a spatial frequency of 1. These findings underscore the model’s superior query scheduling capabilities for economic dynamic systems, confirming its ability to efficiently handle routine economic data queries with precision and reliability. This advancement offers a robust solution to the persistent challenges of data-intensive economic environments.",study primarily focuses optimization query scheduling within economic dynamic systems characterized vast sensitive datasets necessitate exceptional accuracy reliability digital age accelerates data proliferation complexity data retrieval escalates leading critical challenges sluggish query speeds inconsistent results existing systems traditional query algorithms rely basic search logic keyword matching hyperlink analysis proven unsuitable enterprise scale operations often resulting delays inaccurate data address deficiencies improved query scheduling iqs algorithm embedded within economic dynamic system model iqs leverages continuous data reading capabilities enable seamless algorithm execution coupled weighted screening query node loads approach significantly reduces central processing unit strain enhancing query efficiency experimental results demonstrated effectiveness model processed multiple combinations query features cpu execution time less seconds achieved near perfect query accuracy real world deployments model maintained stable load distribution completing platform level queries seconds spatial frequency findings underscore model superior query scheduling capabilities economic dynamic systems confirming ability efficiently handle routine economic data queries precision reliability advancement offers robust solution persistent challenges data intensive economic environments
"The paper aims to discuss the contribution of transformative learning theory and post-critical pedagogy perspectives to the field of radicalization studies. In the first part, the article examines a theoretical framework based on transformative learning theory to interpret the phenomena of radicalization. Examined through this lens, radicalization is an individual or collective process emerging from precritical thinking that can generate distorted ideological assumptions and polarized perspectives, that are potentially reflected in variegated, yet stereotypic-related actions and behaviors. Consistent with those arguments, in the second section, the article presents an illustrative case of an empirical research project investigating radicalization processes: the F.O.R.w.A.R.D. project. In the third part, the paper delves into the post-critical pedagogy to reinterpret educational perspectives that have been adopted in seeking to prevent the occurrence of radicalization phenomena.",paper aims discuss contribution transformative learning theory post critical pedagogy perspectives field radicalization studies first part article examines theoretical framework based transformative learning theory interpret phenomena radicalization examined lens radicalization individual collective process emerging precritical thinking generate distorted ideological assumptions polarized perspectives potentially reflected variegated yet stereotypic related actions behaviors consistent arguments second section article presents illustrative case empirical research project investigating radicalization processes project third part paper delves post critical pedagogy reinterpret educational perspectives adopted seeking prevent occurrence radicalization phenomena
"As micro-teaching software advances, more and more teachers and students opt to use micro-classroom videos for learning. Traditional video analysis methods often perform poorly and have low accuracy when analyzing micro-classroom videos. To address this issue, the research introduces a novel approach for constructing an intelligent video analysis platform. This approach integrates a bidirectional feature pyramid network with the You Only Look Once version 5 small (YOLOv5s) algorithm. The bidirectional feature pyramid network classifies and integrates video image information. The research then utilizes the YOLOv5s algorithm to collect feature information from intelligent video images within micro-classrooms. The image feature information goes through an input-output process. Next, the research incorporates a fusion compression excitation attention module to improve the accuracy of model-based video analysis, ultimately building an intelligent video analysis platform for micro-classrooms. The results show that the proposed method achieves a recognition accuracy of 97.31% for various video image information, with a harmonic mean of 97.67%. Its average accuracy for video analysis reaches 0.91, significantly exceeding that of other algorithm models. In the practical effect analysis experiment of the video analysis platform, the micro-teaching software shows a memory occupancy rate of only 1.54% and a central processor occupancy rate of just 1.77%. Additionally, its response time is better than that of other video analysis platforms. These findings indicate that the proposed method provides high accuracy and strong practicality for intelligent video analysis in micro-classrooms.",micro teaching software advances teachers students opt use micro classroom videos learning traditional video analysis methods often perform poorly low accuracy analyzing micro classroom videos address issue research introduces novel approach constructing intelligent video analysis platform approach integrates bidirectional feature pyramid network look version small yolov algorithm bidirectional feature pyramid network classifies integrates video image information research utilizes yolov algorithm collect feature information intelligent video images within micro classrooms image feature information goes input output process next research incorporates fusion compression excitation attention module improve accuracy model based video analysis ultimately building intelligent video analysis platform micro classrooms results show proposed method achieves recognition accuracy various video image information harmonic mean average accuracy video analysis reaches significantly exceeding algorithm models practical effect analysis experiment video analysis platform micro teaching software shows memory occupancy rate central processor occupancy rate additionally response time better video analysis platforms findings indicate proposed method provides high accuracy strong practicality intelligent video analysis micro classrooms
"With the booming development of sports events, the demand for athlete performance evaluation has begun to increase. The visualization analysis of basketball player data is significant for the analysis of individual value and potential of athletes. However, existing data analysis methods have problems such as insufficient efficiency, low accuracy, and insufficient attention to the inherent attributes of sports events. Therefore, to better analyze the composition data of basketball team members, the study combines Synthetic Minority Over-Sampling Technique (SMOTE) and Random under-sampling techniques to handle the imbalance in the athlete dataset, and then uses Random Forest (RF) to extract data features. Based on this data processing, a ensemble learning method based on SMOTE, Random under-sampling, and RF (SRR)-Voting is proposed to predict athlete performance. The results demonstrated that in the player dataset analysis, the Precision, Recall, and F1 of the research method were 0.9486, 0.9588, and 0.9752, which were superior to comparative methods. The proposed method had a ROC value of 0.94 and a fitting value of 0.17, indicating better fitting performance. This indicates that the designed player composition data visualization analysis method based on ensemble learning can effectively analyze the various attribute data of competitive players and ordinary players, providing effective data support for the comprehensive quality analysis of competitive players.",booming development sports events demand athlete performance evaluation begun increase visualization analysis basketball player data significant analysis individual value potential athletes however existing data analysis methods problems insufficient efficiency low accuracy insufficient attention inherent attributes sports events therefore better analyze composition data basketball team members study combines synthetic minority sampling technique smote random sampling techniques handle imbalance athlete dataset uses random forest extract data features based data processing ensemble learning method based smote random sampling srr voting proposed predict athlete performance results demonstrated player dataset analysis precision recall research method superior comparative methods proposed method roc value fitting value indicating better fitting performance indicates designed player composition data visualization analysis method based ensemble learning effectively analyze various attribute data competitive players ordinary players providing effective data support comprehensive quality analysis competitive players
"Traditional language teaching methods often fall short in meeting modern educational demands. These approaches are further restricted by data processing delays, limited feedback mechanisms, and accessibility issues. To address these challenges, this study introduces Nex-G QSLO (Next Generation Quantum Synergetic Learning Optimizer), a novel intelligent English teaching optimization system that integrates the advantages of 6G-enabled wireless systems, quantum machine learning (QML), and immersive learning technologies. The proposed model utilizes Quantum K-means (QKM) for effective clustering and Quantum Support Vector Machine (QSVM) for precise prediction and personalized recommendation. By utilizing 6G Ultra Reliable Low Latency Communication (URLLC) and edge computing, the system ensures effective data transmission and real-time feedback. Interactive technologies like VR and AR further enhance student engagement and learning involvement. The model also highlights its ability in advanced data security through quantum-resistant encryption. Simulation is conducted using the UCI College English Teaching dataset. When compared to baseline models, the proposed Nex-G QSLO achieves a 4.32% improvement in accuracy, a 6.57% increase in F1-score, and a 5.46% enhancement in AUC and demonstrates its superiority in optimizing English language instruction.",traditional language teaching methods often fall short meeting modern educational demands approaches restricted data processing delays limited feedback mechanisms accessibility issues address challenges study introduces nex qslo next generation quantum synergetic learning optimizer novel intelligent english teaching optimization system integrates advantages enabled wireless systems quantum machine learning qml immersive learning technologies proposed model utilizes quantum means qkm effective clustering quantum support vector machine qsvm precise prediction personalized recommendation utilizing ultra reliable low latency communication urllc edge computing system ensures effective data transmission real time feedback interactive technologies like enhance student engagement learning involvement model also highlights ability advanced data security quantum resistant encryption simulation conducted using uci college english teaching dataset compared baseline models proposed nex qslo achieves improvement accuracy increase score enhancement auc demonstrates superiority optimizing english language instruction
"Ethnic costume patterns are important carriers of cultural heritage, containing rich historical and aesthetic values. Digitizing and preserving these patterns through segmentation techniques contributes to cultural education and resists the threat of cultural erosion brought about by globalization. To this end, this study introduces an intuitive fuzzy clustering optimization (IFCO) algorithm based on spatial neighborhood information (SNI) and membership constraint penalty term (MCPT) enhancement, which achieves robust segmentation of ethnic minority clothing patterns. By further integrating complementary spatial information (CSI) and membership connectivity mechanism (MDCM), a Minority Clothing Pattern Segmentation Combined with an Intuitive Fuzzy Clustering Optimization (MCPSC-IFCO) method is proposed. This method utilizes the Gaussian kernel distance metric (GKDM) to effectively suppress noise and capture complex patterns. The results showed that in mixed noise application environments of 5%, 8%, and 10%, the research method’s partition coefficient and partition entropy achieved excellent performance, with average values of 97.43% and 4.62%, respectively. In the segmentation environment of colored ethnic minority clothing patterns, the research method has greatly improved the segmentation performance compared to mainstream methods, with average segmentation coefficients and entropy of 97.04% and 5.29%, respectively. In the running efficiency results, the average running time was 36.84 ms. The above research results highlight the important significance of digital technology in protecting and promoting minority cultures, promoting cultural diversity and identity.",ethnic costume patterns important carriers cultural heritage containing rich historical aesthetic values digitizing preserving patterns segmentation techniques contributes cultural education resists threat cultural erosion brought globalization end study introduces intuitive fuzzy clustering optimization ifco algorithm based spatial neighborhood information sni membership constraint penalty term mcpt enhancement achieves robust segmentation ethnic minority clothing patterns integrating complementary spatial information csi membership connectivity mechanism mdcm minority clothing pattern segmentation combined intuitive fuzzy clustering optimization mcpsc ifco method proposed method utilizes gaussian kernel distance metric gkdm effectively suppress noise capture complex patterns results showed mixed noise application environments research method partition coefficient partition entropy achieved excellent performance average values respectively segmentation environment colored ethnic minority clothing patterns research method greatly improved segmentation performance compared mainstream methods average segmentation coefficients entropy respectively running efficiency results average running time research results highlight important significance digital technology protecting promoting minority cultures promoting cultural diversity identity
"Existing methods for evaluating and predicting the health status of ecosystems suffer from low efficiency and low prediction accuracy. Therefore, an urban ecosystem health evaluation system and simulation prediction method based on geographic information systems and improved Markov models is proposed. Firstly, the basic geographic data of the research area is obtained through the spatial analysis function of GIS and the interpretation technology of urban remote sensing images. Based on the pressure-state-response model framework, 15 quantitative indicators were selected from three dimensions: environmental pressure, ecological status quo, and human coping to construct a comprehensive evaluation index system. In the stage of health status assessment, the particle swarm optimization algorithm is adopted to intelligently optimize the penalty factor and kernel function parameters of the support vector machine. By adaptively adjusting the position and velocity of the particles to find the optimal parameter combination, an ecosystem health classification model with the best classification performance is finally established. The prediction module integrates the dual advantages of Markov chains and cellular automata, uses the historical state transition probability matrix to represent the evolution law of the time dimension, combines the neighborhood transformation rule of CA to describe the spatial diffusion effect, and generates future multi-scenario prediction results through explicit iterative calculations in space. The prediction accuracy of the proposed method reached 95.2%, with a root mean square error of only 0.08. After applying the proposed method to practical ecosystem health management, the probability of the health status of the corresponding regional ecosystem transitioning to a healthier state reached 0.45% and 0.30%, respectively. In addition, the health status recognition accuracy of the proposed method also reached over 98%. The urban ecological health assessment and simulation prediction method proposed in the research can effectively assist in the effective implementation of ecosystem maintenance work and provide a reliable basis for urban ecosystem management.",existing methods evaluating predicting health status ecosystems suffer low efficiency low prediction accuracy therefore urban ecosystem health evaluation system simulation prediction method based geographic information systems improved markov models proposed firstly basic geographic data research area obtained spatial analysis function gis interpretation technology urban remote sensing images based pressure state response model framework quantitative indicators selected three dimensions environmental pressure ecological status quo human coping construct comprehensive evaluation index system stage health status assessment particle swarm optimization algorithm adopted intelligently optimize penalty factor kernel function parameters support vector machine adaptively adjusting position velocity particles find optimal parameter combination ecosystem health classification model best classification performance finally established prediction module integrates dual advantages markov chains cellular automata uses historical state transition probability matrix represent evolution law time dimension combines neighborhood transformation rule describe spatial diffusion effect generates future multi scenario prediction results explicit iterative calculations space prediction accuracy proposed method reached root mean square error applying proposed method practical ecosystem health management probability health status corresponding regional ecosystem transitioning healthier state reached respectively addition health status recognition accuracy proposed method also reached urban ecological health assessment simulation prediction method proposed research effectively assist effective implementation ecosystem maintenance work provide reliable basis urban ecosystem management
"Millimeter-wave (mmWave) MIMO systems are pivotal for enabling high-capacity wireless communications in next-generation networks. However, effective beamforming remains a key challenge due to high propagation loss and the dynamic nature of mmWave channels. Traditional approaches relying on exhaustive search or static codebooks are inefficient in mobile and blocked environments. In this work, we propose a deep reinforcement learning-based adaptive beamforming algorithm that formulates beam selection as a Markov Decision Process and applies the Deep Deterministic Policy Gradient (DDPG) method to learn continuous-valued beamforming strategies. To enhance reliability, the framework integrates a hybrid policy mechanism that fuses learned actions with domain-aware heuristics for robustness under uncertainty. Simulation results show that our method achieves up to 35% higher spectral efficiency and 1.5 dB lower beam misalignment loss compared to DQN and heuristic baselines, while converging 40% faster during training. These results highlight the promise of actor–critic reinforcement learning in realizing intelligent, low-overhead beam control for dynamic mmWave MIMO environments.",millimeter wave mmwave mimo systems pivotal enabling high capacity wireless communications next generation networks however effective beamforming remains key challenge due high propagation loss dynamic nature mmwave channels traditional approaches relying exhaustive search static codebooks inefficient mobile blocked environments work propose deep reinforcement learning based adaptive beamforming algorithm formulates beam selection markov decision process applies deep deterministic policy gradient ddpg method learn continuous valued beamforming strategies enhance reliability framework integrates hybrid policy mechanism fuses learned actions domain aware heuristics robustness uncertainty simulation results show method achieves higher spectral efficiency lower beam misalignment loss compared dqn heuristic baselines converging faster training results highlight promise actor critic reinforcement learning realizing intelligent low overhead beam control dynamic mmwave mimo environments
"The growing influence of Artificial Intelligence (AI) in digital media has catalyzed the development of autonomous content creation, transforming the production and consumption of visual art, design, and cultural narratives. Despite advances in Generative Adversarial Networks (GANs), existing research often overlooks comprehensive frameworks that assess both the aesthetic quality and moral implications of AI-generated content. This research aims to bridge that gap by proposing a holistic evaluation framework tailored for GAN-generated visual media. The novelty lies in combining aesthetic feature analysis with moral assessment using deep learning and language-based evaluation tools. It utilizes the ArtBench-10 dataset, which consists of 10 diverse artistic styles. Preprocessing steps include z-score normalization, Contrast-Limited Adaptive Histogram Equalization (CLAHE), and resizing images to 256 × 256 pixels to standardize input and enhance visual clarity. For aesthetic analysis, key features such as color harmony, texture richness, symmetry, and compositional balance were extracted using the Histogram of Oriented Gradients (HOG) method. The proposed method employs a conditional GAN (cGAN) to autonomously generate artwork, while a GPT-based language model, along with a rule-based ethics filter, evaluates associated textual metadata to detect potential moral issues, such as bias, cultural appropriation, and offensive content. Implemented using TensorFlow 2.0, the model supports modularity and real-time evaluation. Results indicate superior classification performance, achieving 96.3% precision and a 95.2% F1-score, confirming the effectiveness of the proposed framework in selecting aesthetically coherent and ethically sound AI-generated visual outputs.",growing influence artificial intelligence digital media catalyzed development autonomous content creation transforming production consumption visual art design cultural narratives despite advances generative adversarial networks gans existing research often overlooks comprehensive frameworks assess aesthetic quality moral implications generated content research aims bridge gap proposing holistic evaluation framework tailored gan generated visual media novelty lies combining aesthetic feature analysis moral assessment using deep learning language based evaluation tools utilizes artbench dataset consists diverse artistic styles preprocessing steps include score normalization contrast limited adaptive histogram equalization clahe resizing images pixels standardize input enhance visual clarity aesthetic analysis key features color harmony texture richness symmetry compositional balance extracted using histogram oriented gradients hog method proposed method employs conditional gan cgan autonomously generate artwork gpt based language model along rule based ethics filter evaluates associated textual metadata detect potential moral issues bias cultural appropriation offensive content implemented using tensorflow model supports modularity real time evaluation results indicate superior classification performance achieving precision score confirming effectiveness proposed framework selecting aesthetically coherent ethically sound generated visual outputs
"The rapid evolution of financial fraud in digital finance applications—such as mobile banking, cryptocurrency transactions, and online payment gateways—has rendered traditional rule-based detection systems increasingly ineffective, leading to heightened financial losses and security vulnerabilities. These systems struggle to adapt to complex fraudulent schemes, resulting in inefficiencies in identifying such activities. This research introduces a Scalable Black Widow-driven Gradient Boosting Machine (SBW-GBM) framework designed to enhance fraud detection through real-time data analysis and machine learning (ML). The framework evaluates performance in both decentralized finance and traditional financial contexts, utilizing two separate datasets. The first dataset is based on an Ethereum Phishing Transaction Network; the second originates from Kaggle and pertains to Credit Risk Assessment. Data preparation involves addressing missing values, normalizing numerical features, and employing outlier detection techniques to improve data quality. For feature extraction, Principal Component Analysis (PCA) reduces data dimensionality while preserving critical information regarding transaction behaviors. The classification employs Gradient Boosting Machine (GBM) for high predictive accuracy, with the SBW algorithm dynamically fine-tuning GBM hyperparameters to enhance efficiency. Inspired by black widow spiders, the SBW algorithm optimizes hyperparameter selection by eliminating weak solutions and reinforcing stronger ones. This results in an adaptive fraud detection model trained on labeled transaction data. Experimental results confirm that SBW-GBM achieves an F1-score of 0.899, an accuracy of 0.948, an error rate of 0.128, and an experimental runtime of 0.40 seconds on Dataset 1, outperforming the baseline FFSVM. For Dataset 2, SBW-GBM attains a precision of 0.875, a recall of 0.660, an F1-score of 0.750, and an AUC of 0.932, surpassing benchmark traditional classifiers. By continuously learning from new transaction patterns, the proposed framework ensures adaptability to emerging threats and supports real-time fraud detection.",rapid evolution financial fraud digital finance applications mobile banking cryptocurrency transactions online payment gateways rendered traditional rule based detection systems increasingly ineffective leading heightened financial losses security vulnerabilities systems struggle adapt complex fraudulent schemes resulting inefficiencies identifying activities research introduces scalable black widow driven gradient boosting machine sbw gbm framework designed enhance fraud detection real time data analysis machine learning framework evaluates performance decentralized finance traditional financial contexts utilizing two separate datasets first dataset based ethereum phishing transaction network second originates kaggle pertains credit risk assessment data preparation involves addressing missing values normalizing numerical features employing outlier detection techniques improve data quality feature extraction principal component analysis pca reduces data dimensionality preserving critical information regarding transaction behaviors classification employs gradient boosting machine gbm high predictive accuracy sbw algorithm dynamically fine tuning gbm hyperparameters enhance efficiency inspired black widow spiders sbw algorithm optimizes hyperparameter selection eliminating weak solutions reinforcing stronger ones results adaptive fraud detection model trained labeled transaction data experimental results confirm sbw gbm achieves score accuracy error rate experimental runtime seconds dataset outperforming baseline ffsvm dataset sbw gbm attains precision recall score auc surpassing benchmark traditional classifiers continuously learning new transaction patterns proposed framework ensures adaptability emerging threats supports real time fraud detection
"Artificial intelligence (AI) has advanced algorithmic music composition; however, generating emotionally expressive music remains an unresolved challenge. The application of existing models in emotional computing is limited, and has difficulty matching musical characteristics with human-perceived emotions. The research proposes an Intelligent Golden Eagle-driven Scalable Reinforcement Learning (IGE-SRL) framework to optimize the emotional depth of AI-generated music compositions. Data collection involves compiling a dataset of emotionally labeled music pieces from open source. The dataset includes diverse genres and emotional contexts, ensuring balanced representation. Preprocessing includes noise reduction into fixed-length sequences. Mel-frequency cepstral coefficients (MFCCs) are extracted as key features to capture timbral and spectral characteristics relevant to emotional perception. The IGE-SRL framework uses a policy network to generate melodies and harmonies with an adaptive reward function, integrating emotion recognition models, listener feedback, and sentiment analysis. The IGE algorithm improves exploration-exploitation balance by dynamically adjusting learning parameters, while the SRL agent refines music sequences through policy gradient updates, ensuring emotionally coherent pieces. When compared to traditional methods and rule-based approaches, the suggested IGE-SRL framework greatly improves the emotional expressiveness of AI-generated music. The performance of the suggested IGE-SRL method was evaluated in terms of precision (95.2%), F1-score (97.3%), accuracy (95.13%), recall (96.4%), and entropy (6.124). A greater sense of musical diversity and emotional coherence is confirmed by listener evaluations. By optimizing SRL parameters, the IGE approach enhances composition quality and convergence speed. With an emphasis on multimodal emotion modeling and real-time adaptive composition, the research advances affective computing and AI-driven music production.",artificial intelligence advanced algorithmic music composition however generating emotionally expressive music remains unresolved challenge application existing models emotional computing limited difficulty matching musical characteristics human perceived emotions research proposes intelligent golden eagle driven scalable reinforcement learning ige srl framework optimize emotional depth generated music compositions data collection involves compiling dataset emotionally labeled music pieces open source dataset includes diverse genres emotional contexts ensuring balanced representation preprocessing includes noise reduction fixed length sequences mel frequency cepstral coefficients mfccs extracted key features capture timbral spectral characteristics relevant emotional perception ige srl framework uses policy network generate melodies harmonies adaptive reward function integrating emotion recognition models listener feedback sentiment analysis ige algorithm improves exploration exploitation balance dynamically adjusting learning parameters srl agent refines music sequences policy gradient updates ensuring emotionally coherent pieces compared traditional methods rule based approaches suggested ige srl framework greatly improves emotional expressiveness generated music performance suggested ige srl method evaluated terms precision score accuracy recall entropy greater sense musical diversity emotional coherence confirmed listener evaluations optimizing srl parameters ige approach enhances composition quality convergence speed emphasis multimodal emotion modeling real time adaptive composition research advances affective computing driven music production
"Traditional Chinese ink painting and Western oil painting are two unique artistic media with significant cultural value. Multimodal visual networks are integrated to facilitate real-time interaction between artists and AI systems in creative situations. To combine the smooth, subtle textures of Chinese ink with the bright, textured brushstrokes of oil painting was used. To solve visual network problems, FFA-CausalConvoNet uses several layers and artists’ unique visual components to create works of art that mimic human efforts. The objective is to investigate the human-AI collaborative artistic production by creating a multimodal visual network that combines Chinese ink painting and Western oil painting by utilizing advanced AI techniques. A dataset containing labelled images of Chinese ink paintings and Western oil paintings was utilized for training. Gabor filters were used to capture texture and patterns, separating thin, smooth gradients of ink from rough brushstrokes of oil. Histogram of Oriented Gradients (HOG) and Oriented FAST and Rotated BRIEF (ORB) are used for feature extraction and fusion, which improves texture and shape detection. The proposed generation method utilizes a Flexible Firefly Algorithm Driven Causality-Aware Convolutional Neural Network (FFA-CausalConvoNet) to blend these features and create hybrid artwork that performs well, successfully combining with different styles. The suggested method is implemented by using Python 3.10.1. The resulting artworks displayed a unique blend of the oil painting’s texture and the delicate brushwork of Chinese ink with style fusion accuracy (94.5%), text quality score (9.1 score), processing time (3.5 sec) and cultural fidelity (8.5 score). It highlights the potential of AI-human collaboration in making hybrid art, as the proposed model, FFA-CausalConvoNet, effectively incorporates the distinctive qualities of both art forms, opening up new paths for creative expression in cross-cultural art development.",traditional chinese ink painting western oil painting two unique artistic media significant cultural value multimodal visual networks integrated facilitate real time interaction artists systems creative situations combine smooth subtle textures chinese ink bright textured brushstrokes oil painting used solve visual network problems ffa causalconvonet uses several layers artists unique visual components create works art mimic human efforts objective investigate human collaborative artistic production creating multimodal visual network combines chinese ink painting western oil painting utilizing advanced techniques dataset containing labelled images chinese ink paintings western oil paintings utilized training gabor filters used capture texture patterns separating thin smooth gradients ink rough brushstrokes oil histogram oriented gradients hog oriented fast rotated brief orb used feature extraction fusion improves texture shape detection proposed generation method utilizes flexible firefly algorithm driven causality aware convolutional neural network ffa causalconvonet blend features create hybrid artwork performs well successfully combining different styles suggested method implemented using python resulting artworks displayed unique blend oil painting texture delicate brushwork chinese ink style fusion accuracy text quality score score processing time sec cultural fidelity score highlights potential human collaboration making hybrid art proposed model ffa causalconvonet effectively incorporates distinctive qualities art forms opening new paths creative expression cross cultural art development
"This study developed a novel two-stage classifier model that integ Support Vector Machine (SVM) and Recurrent Neural Network (RNN) with an attention-based dual-encoder architecture for analyzing student behavior sequence data in educational data mining. Unlike traditional RNN + SVM frameworks, our approach uniquely combines (1) a hierarchical feature fusion mechanism that merges base sequence encoding (capturing temporal dependencies via GRU) and attention-driven encoding (highlighting performance-critical behaviors); (2) an adaptive attention module that dynamically weights behavioral sequences (e.g., prioritizing “library” access with &gt;50% attention weight), enabling targeted focus on academic-performance-related patterns; (3) an end-to-end pipeline where RNN-extracted deep features are directly optimized for SVM classification, eliminating manual feature engineering. Through experimental validation, the model outperformed traditional methods in accuracy (86.9%) and recall (81.6%), particularly for long-sequence behavior data. Results confirm that increasing feature dimensions (optimal at 50 dimensions) enhances prediction capabilities but plateaus beyond thresholds. This framework provides a robust tool for actionable insights in educational policy-making. Future work will expand data diversity to strengthen practical applications.",study developed novel two stage classifier model integ support vector machine svm recurrent neural network rnn attention based dual encoder architecture analyzing student behavior sequence data educational data mining unlike traditional rnn svm frameworks approach uniquely combines hierarchical feature fusion mechanism merges base sequence encoding capturing temporal dependencies via gru attention driven encoding highlighting performance critical behaviors adaptive attention module dynamically weights behavioral sequences prioritizing library access attention weight enabling targeted focus academic performance related patterns end end pipeline rnn extracted deep features directly optimized svm classification eliminating manual feature engineering experimental validation model outperformed traditional methods accuracy recall particularly long sequence behavior data results confirm increasing feature dimensions optimal dimensions enhances prediction capabilities plateaus beyond thresholds framework provides robust tool actionable insights educational policy making future work expand data diversity strengthen practical applications
"A novel algorithm (CAStream) is proposed to address the challenge of analyzing high-dimensional condensed matter structures by integrating mathematical probabilities with high-dimensional data. The algorithm improves the clustering accuracy and memory efficiency of high-dimensional data streams through subspace clustering. Taking nylon 6 (PA6) as the research object, combined with X-ray diffraction and differential scanning calorimetry, the effectiveness of the algorithm in the evolution of crystal structure (α/γ crystal transformation) was verified. The research results show that the algorithm has been tested for its ability and scalability in processing data of different spatial dimensions, with a spacing of 5 between datasets. A simulated dataset with different clustering dimensions was established to test the effectiveness of the algorithm in different clustering dimensions of the data stream. It was found that the algorithm performed well in the spatial aspect of the data stream, had good scalability, and performed well for any clustering. It can effectively handle high-dimensional data and has higher accuracy than the CluStream algorithm due to its use of subspace clustering. Nylon 6 mainly exhibits different characteristic peak shapes, such as the alpha crystalline characteristic peak shape at 2θ = 19.8° and θ = 22.9°, and the gamma crystalline characteristic peak shape corresponding to the point at θ = 20.8°. In addition, diffraction peaks of the alpha crystal form were observed, attributed to the addition of apigenin in the range of 0.01–0.05%. The effect of apigenin on the transformation of PA6 from the γ crystal form to the α crystal form decreases as its amount increases. The diffraction peak of the γ crystal structure in the figure appears again at θ = 21.2, during which 1% apigenin is added. It is evident that the suggested approach can successfully and efficiently generate the condensed structure. The research provides a new paradigm for the regulation of polymer condensed matter structure and the design of high-performance materials.",novel algorithm castream proposed address challenge analyzing high dimensional condensed matter structures integrating mathematical probabilities high dimensional data algorithm improves clustering accuracy memory efficiency high dimensional data streams subspace clustering taking nylon research object combined ray diffraction differential scanning calorimetry effectiveness algorithm evolution crystal structure crystal transformation verified research results show algorithm tested ability scalability processing data different spatial dimensions spacing datasets simulated dataset different clustering dimensions established test effectiveness algorithm different clustering dimensions data stream found algorithm performed well spatial aspect data stream good scalability performed well clustering effectively handle high dimensional data higher accuracy clustream algorithm due use subspace clustering nylon mainly exhibits different characteristic peak shapes alpha crystalline characteristic peak shape gamma crystalline characteristic peak shape corresponding point addition diffraction peaks alpha crystal form observed attributed addition apigenin range effect apigenin transformation crystal form crystal form decreases amount increases diffraction peak crystal structure figure appears apigenin added evident suggested approach successfully efficiently generate condensed structure research provides new paradigm regulation polymer condensed matter structure design high performance materials
"This paper develops a novel approach to stock selection by integrating natural language processing techniques with machine learning algorithms to analyze public opinion data in financial markets. Specifically, we employ the Bidirectional Encoder Representations from Transformers (BERT) model to process and classify financial news and social media content, combined with the Light Gradient Boosting Machine (LightGBM) algorithm to select high-potential stocks within identified concept sectors. Using over 18 million Chinese financial text records from 2023 to 2024, we construct a comprehensive framework that captures both market sentiment and stock-specific characteristics. Our strategy consists of three core components: (1) a BERT-based sentiment analyzer that identifies promising concept sectors with strong momentum, (2) a LightGBM-powered stock selection mechanism utilizing a specially designed “Concept-Momentum-Combined” factor alongside conventional financial indicators, and (3) a risk management system combining sentiment anomaly detection with multi-stage Average True Range (ATR) trailing stop mechanisms. Empirical results demonstrate significant outperformance over CSI 800 across various timeframes, with annualized excess returns of 21.55% over a six-month period and maximum drawdowns of only 11.68%. Performance attribution analysis confirms that concept sector selection based on sentiment analysis is the primary driver of excess returns. Our results add to the expanding literature on the use of artificial intelligence in financial markets and provide actionable takeaways for investors who would like to incorporate public opinion data into their investment process.",paper develops novel approach stock selection integrating natural language processing techniques machine learning algorithms analyze public opinion data financial markets specifically employ bidirectional encoder representations transformers bert model process classify financial news social media content combined light gradient boosting machine lightgbm algorithm select high potential stocks within identified concept sectors using million chinese financial text records construct comprehensive framework captures market sentiment stock specific characteristics strategy consists three core components bert based sentiment analyzer identifies promising concept sectors strong momentum lightgbm powered stock selection mechanism utilizing specially designed concept momentum combined factor alongside conventional financial indicators risk management system combining sentiment anomaly detection multi stage average true range atr trailing stop mechanisms empirical results demonstrate significant outperformance csi across various timeframes annualized excess returns six month period maximum drawdowns performance attribution analysis confirms concept sector selection based sentiment analysis primary driver excess returns results add expanding literature use artificial intelligence financial markets provide actionable takeaways investors would like incorporate public opinion data investment process
"This work proposes a blockchain-based computing model with differential privacy for fraud detection in mobile edge computing (MEC) environments. The model enables edge nodes to collaborate securely, ensuring accurate and trustworthy fraud diagnosis. A top-down structure is recommended for financial records, adaptively partitioned for efficiency. The differential privacy mechanism employs randomized responses, while the blockchain-secured computational model (BSCM) safeguards customer and transaction privacy. Traditional fraud detection may delay detection and response due to real-time detection restrictions. The proposed system identifies fraud in real time, saving financial losses. The blockchain paradigm provides transparent, tamper-proof transaction records and secure data storage, unlike current systems. The proposed solution employs oversampling and undersampling to handle imbalanced datasets with more fraudulent transactions than lawful ones. Theoretical analysis shows real-time fraud detection is achievable with minimal error rates while preserving privacy. Compared to client-server models, BSCM reduces data write execution time by 2.6x and improves data retrieval efficiency by 20x. Experiments also evaluate the impact of larger financial datasets. The financial record selection and information extraction models achieved 96.5% and 95.2% accuracy, respectively, with F1-scores of 94.6% and 93.1%, balancing precision and recall. The edge-sharing network demonstrated strong performance in response time, throughput, packet loss, and latency. Additionally, the blockchain-based transaction verification system achieved 99.9% accuracy, excelling in verification speed, network latency, and throughput.",work proposes blockchain based computing model differential privacy fraud detection mobile edge computing mec environments model enables edge nodes collaborate securely ensuring accurate trustworthy fraud diagnosis top structure recommended financial records adaptively partitioned efficiency differential privacy mechanism employs randomized responses blockchain secured computational model bscm safeguards customer transaction privacy traditional fraud detection may delay detection response due real time detection restrictions proposed system identifies fraud real time saving financial losses blockchain paradigm provides transparent tamper proof transaction records secure data storage unlike current systems proposed solution employs oversampling undersampling handle imbalanced datasets fraudulent transactions lawful ones theoretical analysis shows real time fraud detection achievable minimal error rates preserving privacy compared client server models bscm reduces data write execution time improves data retrieval efficiency experiments also evaluate impact larger financial datasets financial record selection information extraction models achieved accuracy respectively scores balancing precision recall edge sharing network demonstrated strong performance response time throughput packet loss latency additionally blockchain based transaction verification system achieved accuracy excelling verification speed network latency throughput
"The digital transformation of power systems necessitates robust solutions for data security challenges, such as provenance verification, tamper resistance, and privacy compliance. This research proposes a novel blockchain-based architecture integrated with threshold proxy re-encryption (TPRE) to establish a trustworthy electricity data ecosystem. The framework transforms grid devices and organizational units into permissioned blockchain nodes, creating an immutable ledger for end-to-end data provenance while preventing unauthorized modifications. Regulatory entities gain granular traceability capabilities to audit data flows and pinpoint anomalous nodes with cryptographic certainty. To address the dual requirements of multi-party data sharing and privacy protection in smart grid environments, we implement an optimized TPRE scheme that significantly reduces computational overhead compared to conventional approaches. Our solution enables secure delegation of re-encryption rights while maintaining threshold-based access control over sensitive power consumption data. Experimental results demonstrate that the proposed scheme reduces computational overhead by 33.3% under multi-user scenarios compared to conventional proxy re-encryption methods, while enhancing key randomness through threshold-based secret sharing. The proposed framework offers practical insights for implementing cryptographically secure, regulator-compliant data management systems in critical energy infrastructure.",digital transformation power systems necessitates robust solutions data security challenges provenance verification tamper resistance privacy compliance research proposes novel blockchain based architecture integrated threshold proxy encryption tpre establish trustworthy electricity data ecosystem framework transforms grid devices organizational units permissioned blockchain nodes creating immutable ledger end end data provenance preventing unauthorized modifications regulatory entities gain granular traceability capabilities audit data flows pinpoint anomalous nodes cryptographic certainty address dual requirements multi party data sharing privacy protection smart grid environments implement optimized tpre scheme significantly reduces computational overhead compared conventional approaches solution enables secure delegation encryption rights maintaining threshold based access control sensitive power consumption data experimental results demonstrate proposed scheme reduces computational overhead multi user scenarios compared conventional proxy encryption methods enhancing key randomness threshold based secret sharing proposed framework offers practical insights implementing cryptographically secure regulator compliant data management systems critical energy infrastructure
"Given the challenges in capturing temporal dependencies within sports event data and the imbalance between global and local feature representations, this study introduces a Transformer-based model designed to address these issues. By leveraging a multi-head self-attention mechanism, the model effectively captures dynamic features across different time granularities, thereby enhancing the analysis of temporal event data and improving the accuracy of win rate prediction. Specifically, a time-segment encoding strategy is first employed to partition the event sequence data, enabling independent processing of features within each temporal segment. Subsequently, a multi-level Transformer architecture is constructed to extract both short-term and long-term dependencies at different hierarchical levels, facilitating a more comprehensive understanding of game dynamics. To further refine feature representation, a dynamic self-attention adjustment mechanism is incorporated, allowing the model to adaptively focus on salient features based on the characteristics of the input data. Experimental results demonstrate that, in comparison with baseline models—including Logistic Regression (LR), Support Vector Machine (SVM), Random Forest (RF), Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN), and Extreme Gradient Boosting (XGBoost)—the proposed model achieves superior performance. Specifically, it improves prediction accuracy by 10.7%, 8.3%, 3.9%, 6.0%, 4.3%, and 2.4%, respectively, and enhances precision by 10.6%, 9.4%, 5.0%, 6.5%, 4.5%, and 3.6%, respectively. These findings underscore the model’s effectiveness in handling complex temporal sequences and multi-layered feature structures, thereby significantly improving the accuracy and robustness of win rate predictions in sports events.",given challenges capturing temporal dependencies within sports event data imbalance global local feature representations study introduces transformer based model designed address issues leveraging multi head self attention mechanism model effectively captures dynamic features across different time granularities thereby enhancing analysis temporal event data improving accuracy win rate prediction specifically time segment encoding strategy first employed partition event sequence data enabling independent processing features within temporal segment subsequently multi level transformer architecture constructed extract short term long term dependencies different hierarchical levels facilitating comprehensive understanding game dynamics refine feature representation dynamic self attention adjustment mechanism incorporated allowing model adaptively focus salient features based characteristics input data experimental results demonstrate comparison baseline models including logistic regression support vector machine svm random forest long short term memory lstm convolutional neural network cnn extreme gradient boosting xgboost proposed model achieves superior performance specifically improves prediction accuracy respectively enhances precision respectively findings underscore model effectiveness handling complex temporal sequences multi layered feature structures thereby significantly improving accuracy robustness win rate predictions sports events
"The new energy vehicle (NEV) industry urgently requires tailored credit assessment frameworks to address its nonlinear risk characteristics, driven by rapid technological iterations and policy dependency. This study selects 46 listed companies in the NEV supply chain (2019–2023) as samples, innovatively integrating multi-source data including financial metrics, textual tone analysis of annual reports, and ESG ratings. A three-dimensional composite indicator system (“financial robustness–strategic credibility–environmental resilience”) is developed to compare the predictive performance of Logit models and backpropagation (BP) neural networks in estimating corporate default probabilities. Empirical findings reveal: (1) Under the composite indicator system, the BP neural network achieves 81.7% default prediction accuracy, significantly outperforming the Logit model (72.4%), with a 32-percentage-point improvement in identifying defaulted entities; (2) Using single financial indicators, the BP network maintains superiority (58.3% overall accuracy vs 48.3% for Logit), validating its capacity to capture complex risk features; (3) The composite system enhances prediction accuracy by 23.4% (BP) and 24.1% (Logit) compared to single indicators, demonstrating the early-warning value of non-financial metrics. These results suggest that the synergistic application of multi-source composite indicators and BP neural networks substantially improves the precision of dynamic credit risk assessment in the NEV sector, offering methodological support for differentiated financial services and regulatory oversight.",new energy vehicle nev industry urgently requires tailored credit assessment frameworks address nonlinear risk characteristics driven rapid technological iterations policy dependency study selects listed companies nev supply chain samples innovatively integrating multi source data including financial metrics textual tone analysis annual reports esg ratings three dimensional composite indicator system financial robustness strategic credibility environmental resilience developed compare predictive performance logit models backpropagation neural networks estimating corporate default probabilities empirical findings reveal composite indicator system neural network achieves default prediction accuracy significantly outperforming logit model percentage point improvement identifying defaulted entities using single financial indicators network maintains superiority overall accuracy logit validating capacity capture complex risk features composite system enhances prediction accuracy logit compared single indicators demonstrating early warning value non financial metrics results suggest synergistic application multi source composite indicators neural networks substantially improves precision dynamic credit risk assessment nev sector offering methodological support differentiated financial services regulatory oversight
"How to fairly evaluate the state of the innovation and entrepreneurship ecosystem given the fast growth of creativity and entrepreneurial activities has become a crucial question. Conventional assessment techniques can depend on empirical indicators or basic statistical models, which are challenging to expose the complicated dynamic interactions and inherent rules of the system. Thus, this work suggests an intelligent evaluation approach including causal inference and deep representation learning for innovation and entrepreneurial environments. Deep representation learning extracts the system features; causal inference reveals the causal relationship between variables, so enhancing the operability and assessment accuracy. Specifically, the intelligent assessment method proposed in this study achieves a mean square error (MAE) of 0.149, which is significantly lower than other traditional methods. In terms of causal effect estimation, the estimation of this method is 0.92, which is much higher than other models. On structural Hamming distance (SHD), the present method is only 4, indicating its high accuracy in causal structure identification. The approach offers fresh concepts and technical support for the intelligent evaluation of ecosystems of innovation and entrepreneurship.",fairly evaluate state innovation entrepreneurship ecosystem given fast growth creativity entrepreneurial activities become crucial question conventional assessment techniques depend empirical indicators basic statistical models challenging expose complicated dynamic interactions inherent rules system thus work suggests intelligent evaluation approach including causal inference deep representation learning innovation entrepreneurial environments deep representation learning extracts system features causal inference reveals causal relationship variables enhancing operability assessment accuracy specifically intelligent assessment method proposed study achieves mean square error mae significantly lower traditional methods terms causal effect estimation estimation method much higher models structural hamming distance shd present method indicating high accuracy causal structure identification approach offers fresh concepts technical support intelligent evaluation ecosystems innovation entrepreneurship
"In professional basketball games, athlete action recognition is an important part of sports performance analysis and intelligent assisted training. The complex scene background, frequent target occlusion, and varied motion patterns pose challenges to traditional detection and recognition methods in terms of accuracy and real-time performance. In view of this, the study proposes an improved object detection model for single shot multi-box detectors by combining pyramid feature integration module and dual axis convolutional perception module. At the same time, an action recognition model based on an improved 3D convolutional network is designed through adaptive weight fusion and attention mechanism. In the testing of the object detection model, when the number of iterations reached 500, the average accuracy improved by 6.14% and the frame rate decreased by 8.56%. The missed detection rate under low light conditions was 7.3%, the false detection rate was 8.7%, and the detection time was 30.8 ms. The highest detection accuracy of the action recognition model in complex backgrounds was 89.3%, and the robustness score was 91.9. The results indicate that the proposed model can maintain high accuracy and efficiency in complex backgrounds and fast movements in professional basketball game scenarios. The research model significantly improves the performance and robustness of action recognition, which can provide certain technical support for intelligent sports training systems.",professional basketball games athlete action recognition important part sports performance analysis intelligent assisted training complex scene background frequent target occlusion varied motion patterns pose challenges traditional detection recognition methods terms accuracy real time performance view study proposes improved object detection model single shot multi box detectors combining pyramid feature integration module dual axis convolutional perception module time action recognition model based improved convolutional network designed adaptive weight fusion attention mechanism testing object detection model number iterations reached average accuracy improved frame rate decreased missed detection rate low light conditions false detection rate detection time highest detection accuracy action recognition model complex backgrounds robustness score results indicate proposed model maintain high accuracy efficiency complex backgrounds fast movements professional basketball game scenarios research model significantly improves performance robustness action recognition provide certain technical support intelligent sports training systems
"This paper introduces a novel self-correcting convolution module that significantly differs from traditional convolution techniques by enabling multi-scale feature extraction through heterogeneous kernel processing and feature calibration. The module uniquely combines down-sampling and up-sampling operations within a single convolution block to capture both local and global contexts, addressing key limitations in existing methods. In addition, an improved Dice loss function is proposed, which integrates both under- and over-segmentation penalties through an mDice loss. This is combined with a cross-entropy loss based on classification to optimize segmentation performance. The proposed self-correcting convolution segmentation algorithm demonstrates superior accuracy in segmenting lung infection regions compared to existing methods, particularly the AMSU-Net network. Experimental results indicate that the inclusion of multi-scale spatial information and refined loss functions significantly enhances segmentation precision. The novelty of this research lies in the introduction of a self-correcting convolution module that improves the receptive field and the diversity of extracted features. Furthermore, the enhanced mDice loss function, integrating segmentation penalties, contributes to improved model performance. This method offers a promising advancement in lung infection segmentation using deep learning techniques.",paper introduces novel self correcting convolution module significantly differs traditional convolution techniques enabling multi scale feature extraction heterogeneous kernel processing feature calibration module uniquely combines sampling sampling operations within single convolution block capture local global contexts addressing key limitations existing methods addition improved dice loss function proposed integrates segmentation penalties mdice loss combined cross entropy loss based classification optimize segmentation performance proposed self correcting convolution segmentation algorithm demonstrates superior accuracy segmenting lung infection regions compared existing methods particularly amsu net network experimental results indicate inclusion multi scale spatial information refined loss functions significantly enhances segmentation precision novelty research lies introduction self correcting convolution module improves receptive field diversity extracted features furthermore enhanced mdice loss function integrating segmentation penalties contributes improved model performance method offers promising advancement lung infection segmentation using deep learning techniques
"This paper presents an intelligent teaching resource management platform built upon a task-oriented dialogue system, specifically tailored to the structured and domain-specific needs of vocational contexts. The system integrates a schema-guided dialogue state tracking framework that combines LSTM-based feature extraction, a slot gating mechanism, and a pointer neural network to effectively handle diverse conversational phenomena and unknown slot values. Experimental results show that the proposed model achieves a Joint Goal Accuracy (JGA) of 0.580, outperforming competitive baselines such as STAR (0.568) and TRADE (0.487). Unlike general-purpose chatbots, this platform supports real-time resource retrieval, pedagogical interaction, and administrative management in vocational settings, contributing to improved teaching quality and learner engagement.",paper presents intelligent teaching resource management platform built upon task oriented dialogue system specifically tailored structured domain specific needs vocational contexts system integrates schema guided dialogue state tracking framework combines lstm based feature extraction slot gating mechanism pointer neural network effectively handle diverse conversational phenomena unknown slot values experimental results show proposed model achieves joint goal accuracy jga outperforming competitive baselines star trade unlike general purpose chatbots platform supports real time resource retrieval pedagogical interaction administrative management vocational settings contributing improved teaching quality learner engagement
"In recent years, the rapid rise of technologies such as the Internet of Things (IoT) and Artificial Intelligence (AI) has transformed numerous domains, particularly smart homes. As people experience greater material comfort, they increasingly seek deeper, more emotionally intelligent ways to interact with technology. Music, rich in emotional content, serves as a powerful medium for interpersonal communication and is increasingly regarded as a natural channel for intelligent human-computer interaction. However, traditional music emotion recognition techniques face challenges with low recognition accuracy and high computational costs. To address these limitations, we propose an efficient deep learning-based music emotion recognition system that integrates generative adversarial networks (GANs) within an IoT framework. The system employs a convolutional neural network (CNN) to extract both local and global features from musical signals using Mel-frequency techniques. These features enhance the GAN’s ability to detect complex emotional expressions in music. Experimental results demonstrate that the proposed model achieves significantly lower error rates and greater recognition accuracy compared to state-of-the-art methods. Specifically, it attains an accuracy of 94.06%, confirming its effective performance and suitability for real-time, emotion-aware music recommendation in IoT applications.",recent years rapid rise technologies internet things iot artificial intelligence transformed numerous domains particularly smart homes people experience greater material comfort increasingly seek deeper emotionally intelligent ways interact technology music rich emotional content serves powerful medium interpersonal communication increasingly regarded natural channel intelligent human computer interaction however traditional music emotion recognition techniques face challenges low recognition accuracy high computational costs address limitations propose efficient deep learning based music emotion recognition system integrates generative adversarial networks gans within iot framework system employs convolutional neural network cnn extract local global features musical signals using mel frequency techniques features enhance gan ability detect complex emotional expressions music experimental results demonstrate proposed model achieves significantly lower error rates greater recognition accuracy compared state art methods specifically attains accuracy confirming effective performance suitability real time emotion aware music recommendation iot applications
"Biodiversity decline and habitat destruction have drastically accelerated in recent decades, as reported by the International Union for Conservation of Nature and The Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services. Increasingly, national and international legislation mandates those businesses report and manage the potential effects of their operations on biodiversity. Despite recent advancements in addressing environmental issues in business school curricula, there is a gap in the scholarship linking business education to biodiversity crises. What kind of education can address the pressing challenges of the twenty-first century, particularly the sixth extinction? Challenging prevailing human-centred (anthropocentric) norms in Education for Sustainable Development Goals (ESDGs), we argue that business education needs a transformative shift towards ecology-centred (eco-centric) eco-pedagogy and eco-literacy to educate responsible corporate leaders. This article contributes to scholarship on sustainability education by highlighting the need to reorient business schools to embrace curriculum and pedagogy that support biodiversity conservation. The article is poised to enhance global policy discussions related to the 2024 UN Summit of the Future and beyond.",biodiversity decline habitat destruction drastically accelerated recent decades reported international union conservation nature intergovernmental science policy platform biodiversity ecosystem services increasingly national international legislation mandates businesses report manage potential effects operations biodiversity despite recent advancements addressing environmental issues business school curricula gap scholarship linking business education biodiversity crises kind education address pressing challenges twenty first century particularly sixth extinction challenging prevailing human centred anthropocentric norms education sustainable development goals esdgs argue business education needs transformative shift towards ecology centred eco centric eco pedagogy eco literacy educate responsible corporate leaders article contributes scholarship sustainability education highlighting need reorient business schools embrace curriculum pedagogy support biodiversity conservation article poised enhance global policy discussions related summit future beyond
"During the operation of wireless power transfer (WPT) systems, abrupt changes in load and input voltage can impact system stability and operating conditions. This paper addresses a dual-side coordinated control strategy that combines an advanced finite control set model predictive control (FCS-MPC) with impedance matching for LCC-LCC compensated WPT systems, aiming to enhance dynamic performance and optimize efficiency. Firstly, the secondary-side rectifier employs FCS-MPC combined with phase shift modulation (PSM) to achieve rapid voltage tracking. An improved FCS-MPC method is proposed featuring a novel iterative approach, adaptive compensation, and a multi-objective optimization cost function to simultaneously improve system stability, responsiveness, and computational efficiency. Secondly, the primary-side inverter combines impedance matching with PSM to track the point of maximum system efficiency. Finally, model verification results substantiate the viability of the developed strategy. Under sudden load and input voltage changes, improved MPC reduces overshoot and settling time by nearly 60% and 70%, respectively, compared to PI control. Additionally, system efficiency optimizes by approximately 1.3% through impedance matching.",operation wireless power transfer wpt systems abrupt changes load input voltage impact system stability operating conditions paper addresses dual side coordinated control strategy combines advanced finite control set model predictive control fcs mpc impedance matching lcc lcc compensated wpt systems aiming enhance dynamic performance optimize efficiency firstly secondary side rectifier employs fcs mpc combined phase shift modulation psm achieve rapid voltage tracking improved fcs mpc method proposed featuring novel iterative approach adaptive compensation multi objective optimization cost function simultaneously improve system stability responsiveness computational efficiency secondly primary side inverter combines impedance matching psm track point maximum system efficiency finally model verification results substantiate viability developed strategy sudden load input voltage changes improved mpc reduces overshoot settling time nearly respectively compared control additionally system efficiency optimizes approximately impedance matching
"This paper evaluates forest carbon sequestration capacity as well as its ecological and cultural value using multiple models. Through calculating and forecasting the carbon sequestration trends of various tree species, the optimal time for tree cutting can be identified to maximize carbon sequestration benefits. Based on the analysis, China is predicted to achieve carbon neutrality by 2064—slightly later than its 2060 target. Then, a linear programming model for forest product carbon sequestration suggests increasing solid wood production and extending product lifespans. Finally, applying the established model to the Changbai Mountain Forest reveals that prioritizing cultural values maximizes overall forest value.",paper evaluates forest carbon sequestration capacity well ecological cultural value using multiple models calculating forecasting carbon sequestration trends various tree species optimal time tree cutting identified maximize carbon sequestration benefits based analysis china predicted achieve carbon neutrality slightly later target linear programming model forest product carbon sequestration suggests increasing solid wood production extending product lifespans finally applying established model changbai mountain forest reveals prioritizing cultural values maximizes overall forest value
"Literature mirrors a nation’s ideology and language system, with British and American literature (BALW) differing greatly from Chinese literature due to distinct historical, linguistic, and cultural backgrounds. Chinese language system struggles to accurately retrieve and classify BALW. To tackle this, we propose a BALW appreciation model. Built upon a base model akin to DocBERT, it uses the BERT pre-trained model in the text representation module to capture contextual semantics. A heterogeneous graph attention network (HAN) with word, feature word, and label nodes is designed to extract local semantics in the text. These features are then integrated for multi-label classification. Experiments on a curated dataset show our model outperforms the base one, improving Hamming Loss, Macro-F1, and Micro-F1 by 0.009, 1.9%, and 5.38%, respectively. This enhances intelligent classification and retrieval of BALW, benefiting literary appreciation and cross-cultural literary exchange.",literature mirrors nation ideology language system british american literature balw differing greatly chinese literature due distinct historical linguistic cultural backgrounds chinese language system struggles accurately retrieve classify balw tackle propose balw appreciation model built upon base model akin docbert uses bert pre trained model text representation module capture contextual semantics heterogeneous graph attention network word feature word label nodes designed extract local semantics text features integrated multi label classification experiments curated dataset show model outperforms base one improving hamming loss macro micro respectively enhances intelligent classification retrieval balw benefiting literary appreciation cross cultural literary exchange
"The rapid expansion of online education platforms has highlighted the critical need for accurate learner emotion recognition to improve interactivity and learning outcomes. However, conventional unimodal approaches face challenges in capturing the complex emotional cues embedded in learners’ language and speech. To address this gap, this study proposes a novel text-audio multimodal weighted network (TA-MWN) that integrates Bidirectional Encoder Representations from Transformers (BERT) for textual feature extraction and Mel Frequency Cepstral Coefficients (MFCCs) for speech feature representation. These features are further processed through a multi-layer Long Short-Term Memory (LSTM) network to model temporal dependencies. The proposed multimodal weighted network (MWN) dynamically fuses the prediction probabilities from both modalities, enhancing sentiment recognition performance. Experimental validation on the public CMU-MOSI dataset demonstrates that TA-MWN outperforms traditional unimodal and state-of-the-art multimodal methods in terms of precision, recall, and F1-score. Further evaluation on a self-constructed online learning dataset confirms the model’s superior adaptability and recognition accuracy, achieving over 90% accuracy in negative sentiment detection. The proposed approach not only improves emotion recognition in online education but also provides a scalable framework for adaptive learning, personalized feedback, and privacy-friendly interaction without relying on visual data. These findings offer practical implications for enhancing learner engagement and emotional support in intelligent online learning environments.",rapid expansion online education platforms highlighted critical need accurate learner emotion recognition improve interactivity learning outcomes however conventional unimodal approaches face challenges capturing complex emotional cues embedded learners language speech address gap study proposes novel text audio multimodal weighted network mwn integrates bidirectional encoder representations transformers bert textual feature extraction mel frequency cepstral coefficients mfccs speech feature representation features processed multi layer long short term memory lstm network model temporal dependencies proposed multimodal weighted network mwn dynamically fuses prediction probabilities modalities enhancing sentiment recognition performance experimental validation public cmu mosi dataset demonstrates mwn outperforms traditional unimodal state art multimodal methods terms precision recall score evaluation self constructed online learning dataset confirms model superior adaptability recognition accuracy achieving accuracy negative sentiment detection proposed approach improves emotion recognition online education also provides scalable framework adaptive learning personalized feedback privacy friendly interaction without relying visual data findings offer practical implications enhancing learner engagement emotional support intelligent online learning environments
"This study aims to examine the computational thinking skills (CTS) of engineering students through classification and regression trees (CART) analysis and to develop a predictive model using machine learning (ML) algorithms. The sample consists of 435 engineering students from the engineering faculties of three state universities located in Southeast and East Anatolia. A relational survey model is utilized, and data are collected via a personal information form and the CTS scale. Data analyses include two-stage cluster analysis, CART analysis and ML algorithms. Findings reveal that 71.43% of students possess high CTS, 12.70% are at a medium level and 15.87% are at a low level. The majority of engineering students exhibit moderate to high CTS. The variable department is identified as the most significant predictor of students’ CTS. Dimensions of computational thinking (CT) are analyzed using CART, identifying the most predictive variables for creativity, algorithmic thinking, cooperativity, critical thinking and problem-solving. ML algorithms, including AdaBoost, decision tree, multi-layer perceptron, naive Bayes and random forest, are employed to predict the CTS. The study’s findings and data patterns underscore the importance of targeted planning in engineering education to foster the effective development of CTS.",study aims examine computational thinking skills cts engineering students classification regression trees cart analysis develop predictive model using machine learning algorithms sample consists engineering students engineering faculties three state universities located southeast east anatolia relational survey model utilized data collected via personal information form cts scale data analyses include two stage cluster analysis cart analysis algorithms findings reveal students possess high cts medium level low level majority engineering students exhibit moderate high cts variable department identified significant predictor students cts dimensions computational thinking analyzed using cart identifying predictive variables creativity algorithmic thinking cooperativity critical thinking problem solving algorithms including adaboost decision tree multi layer perceptron naive bayes random forest employed predict cts study findings data patterns underscore importance targeted planning engineering education foster effective development cts
"The coupling of various evolutionary factors contributes to the rich and heterogeneous topology and texture features of natural landscapes. However, existing semi-automatic methods struggle to accurately capture the designer’s intent and allow for detailed modeling of large-scale natural landscapes. To address this, this paper proposes a landscape modeling method based on 3D point cloud technology, utilizing Structure-from-Motion (SfM) technology for data collection. The method enhances point cloud data texture through an adaptive partial adjustment strategy. Additionally, we investigate visibility analysis using digital elevation models (DEMs) and propose an improved Dyntacs visibility algorithm. This algorithm reduces inter-visibility computation redundancy through effective reuse strategies and parallel processing, thereby improving computational efficiency. The results indicate that the proposed method achieves a maximum error of only 0.40 mm and significantly reduces processing time compared to existing algorithms. The overall accuracy (OA) of our method is 95.2%, with a mean Intersection over Union (mIoU) of 78.2%. These metrics demonstrate that the proposed model offers significant improvements in performance, effectively enhancing feature extraction and segmentation of point cloud data.",coupling various evolutionary factors contributes rich heterogeneous topology texture features natural landscapes however existing semi automatic methods struggle accurately capture designer intent allow detailed modeling large scale natural landscapes address paper proposes landscape modeling method based point cloud technology utilizing structure motion sfm technology data collection method enhances point cloud data texture adaptive partial adjustment strategy additionally investigate visibility analysis using digital elevation models dems propose improved dyntacs visibility algorithm algorithm reduces inter visibility computation redundancy effective reuse strategies parallel processing thereby improving computational efficiency results indicate proposed method achieves maximum error significantly reduces processing time compared existing algorithms overall accuracy method mean intersection union miou metrics demonstrate proposed model offers significant improvements performance effectively enhancing feature extraction segmentation point cloud data
"Industrial design, traditionally driven by human creativity and craftsmanship, is increasingly influenced by the capabilities of artificial intelligence (AI) technologies, which are reshaping the ways products are conceptualized, developed, and brought to market. As the global economy becomes more technology-driven, there is a growing demand for businesses to adopt innovative solutions that enhance efficiency, creativity, and adaptability. The integration of AI into industrial design has revolutionized the processes of innovation and entrepreneurship. This study explores the application of AI-driven innovative thinking in industrial design, focusing on its impact on product development, design optimization, and the creation of new business opportunities. The data includes product features, design specifications, development timelines, user feedback, market trends, consumer preferences, and entrepreneurial outcomes associated with each product. Data preprocessing involves One-Hot Encoding to standardize categorical variables and improve model accuracy. The proposed Intelligent Rabbit Swarm Optimized Recurrent Neural Network (IRSO-RNN) aims to optimize the design process, predict market trends and consumer preferences, and support entrepreneurial decision-making. The IRSO-RNN model offers a unique approach to design optimization by combining swarm intelligence with recurrent neural networks (RNNs), enabling more effective prediction of consumer preferences and market trends. Additionally, the study examines AI’s potential in fostering creativity, enhancing product development, and promoting sustainability, thereby transforming entrepreneurial ecosystems through improved product design and increased business competitiveness. The performance of the proposed IRSO-RNN method was evaluated in terms of Mean Squared Error (MSE) (0.01) and Root Mean Square Error (RMSE) (0.11). Ultimately, the findings highlight the transformative potential of AI in driving innovation and entrepreneurship within the industrial design sector.",industrial design traditionally driven human creativity craftsmanship increasingly influenced capabilities artificial intelligence technologies reshaping ways products conceptualized developed brought market global economy becomes technology driven growing demand businesses adopt innovative solutions enhance efficiency creativity adaptability integration industrial design revolutionized processes innovation entrepreneurship study explores application driven innovative thinking industrial design focusing impact product development design optimization creation new business opportunities data includes product features design specifications development timelines user feedback market trends consumer preferences entrepreneurial outcomes associated product data preprocessing involves one hot encoding standardize categorical variables improve model accuracy proposed intelligent rabbit swarm optimized recurrent neural network irso rnn aims optimize design process predict market trends consumer preferences support entrepreneurial decision making irso rnn model offers unique approach design optimization combining swarm intelligence recurrent neural networks rnns enabling effective prediction consumer preferences market trends additionally study examines potential fostering creativity enhancing product development promoting sustainability thereby transforming entrepreneurial ecosystems improved product design increased business competitiveness performance proposed irso rnn method evaluated terms mean squared error mse root mean square error rmse ultimately findings highlight transformative potential driving innovation entrepreneurship within industrial design sector
"The blast furnace bag filter is an essential equipment for filtering flue gas and recovering dust. Optimising its structure can enhance gas recovery efficiency, reduce resource waste, and mitigate environmental pollution. This study designed three types of blast furnace metal bag filters with varying inlet and outlet structures and analysed their flow field distribution characteristics using computational fluid dynamics. The results indicated that the airflow was most efficient with the configuration of a lower inlet and upper outlet on the same side (Case A). However, Case A exhibited significant flow fluctuations in the filter bag area and slightly higher wall stress compared to the middle inlet structure (Case B). While the airflow in Case B's filter bag area was relatively uniform, there was strong disturbance at the ash hopper, hindering the efficiency of ash deposition. In addition, Case C, featuring a lower intake on one side and an upper outlet, demonstrated the poorest flow performance. Therefore, Case A was deemed the most scientifically optimal structure for the actual industry. Additionally, the study examined the impact of inlet velocity on flow characteristics in Case A. As the inlet velocity increased from 9.8 to 29 m/s, the flow velocity in the ash hopper area rose from 2.57 to 6.01 m/s, with a marked increase in jet intensity.",blast furnace bag filter essential equipment filtering flue gas recovering dust optimising structure enhance gas recovery efficiency reduce resource waste mitigate environmental pollution study designed three types blast furnace metal bag filters varying inlet outlet structures analysed flow field distribution characteristics using computational fluid dynamics results indicated airflow efficient configuration lower inlet upper outlet side case however case exhibited significant flow fluctuations filter bag area slightly higher wall stress compared middle inlet structure case airflow case filter bag area relatively uniform strong disturbance ash hopper hindering efficiency ash deposition addition case featuring lower intake one side upper outlet demonstrated poorest flow performance therefore case deemed scientifically optimal structure actual industry additionally study examined impact inlet velocity flow characteristics case inlet velocity increased flow velocity ash hopper area rose marked increase jet intensity
"Delivering and assessing affective domain attributes is relatively more challenging as compared to the cognitive and psychomotor domains. In this regard, employing holistic approaches can aid the teacher by providing an enhanced comprehension of the students’ state, as well as the affective process involved. To this end, the current conceptual article illustrates the use of the Iceberg model of Systems Thinking and the Process-Person-Context-Time (PPCT) model of Bronfenbrenner's Bio-ecological theory; the efficacy of both instruments is established via appropriate arguments. The mental model layer of the Iceberg framework enables a teacher to directly discern the affective values involved in a scenario, likewise, the PPCT model allows to observe the transparency of the mechanism being used to deliver/assess an affective attribute. Thus a duo of potent tools is added in the toolbox of an affective domain teacher, who may proceed with a clearer vision and enhanced confidence.",delivering assessing affective domain attributes relatively challenging compared cognitive psychomotor domains regard employing holistic approaches aid teacher providing enhanced comprehension students state well affective process involved end current conceptual article illustrates use iceberg model systems thinking process person context time ppct model bronfenbrenner bio ecological theory efficacy instruments established via appropriate arguments mental model layer iceberg framework enables teacher directly discern affective values involved scenario likewise ppct model allows observe transparency mechanism used deliver assess affective attribute thus duo potent tools added toolbox affective domain teacher may proceed clearer vision enhanced confidence
"Although the development of MOOCs has revolutionized the method in which we study, it also presents substantial hurdles in terms of ensuring that learners are engaged and motivated. On the basis of head posture characteristics, we propose an Intelligent Learning Behavior Analysis and Intervention System for English MOOCs in order to address this issue. Through the utilization of computer vision and machine learning techniques, our system is able to assess the aspects of learners’ head posture, determine the degrees of engagement they are experiencing, and deliver individualized interventions to improve the results of their learning. The Head Posture Analysis Module uses computer vision to extract head posture parameters from learner videos. The involvement Detection Module uses machine learning to assess student involvement based on head posture. Customized treatments including adaptive difficulty adjustment, interactive components, and social learning are offered in the Intervention Module to boost student engagement and motivation. To begin, it offers a fresh method for studying the behavior of learners and determining the extent to which they are engaged in the learning process. Second, it provides individualized interventions that are designed to enhance the learners’ motivation and the results of their learning. Lastly, it has the ability to increase the overall quality of MOOCs as well as the experiences that learners have.",although development moocs revolutionized method study also presents substantial hurdles terms ensuring learners engaged motivated basis head posture characteristics propose intelligent learning behavior analysis intervention system english moocs order address issue utilization computer vision machine learning techniques system able assess aspects learners head posture determine degrees engagement experiencing deliver individualized interventions improve results learning head posture analysis module uses computer vision extract head posture parameters learner videos involvement detection module uses machine learning assess student involvement based head posture customized treatments including adaptive difficulty adjustment interactive components social learning offered intervention module boost student engagement motivation begin offers fresh method studying behavior learners determining extent engaged learning process second provides individualized interventions designed enhance learners motivation results learning lastly ability increase overall quality moocs well experiences learners
"To comprehensively analyze online course review texts and uncover the topics and emotion attitudes of learners, this study aims to enhance the quality of online teaching. This study focuses on analyzing learners’ discussion data from online courses and introduces an emotion dictionary specifically tailored for the MOOC (Massive Open Online Course) domain. The dictionary is constructed using a linear fusion algorithm that integrates SO-PMI (Semi-Supervised Offset Pointwise Mutual Information) with Word2Vec techniques. Additionally, the study proposes an enhanced Latent Dirichlet Allocation (LDA) model for topic mining and utilizes Support Vector Machines (SVM) for sentiment classification. By analyzing the textual data generated by learners, the study aims to reveal their focal points and emotional tendencies in greater depth. The experimental results show that the W-LDA model outperforms traditional LDA models in terms of predictive accuracy for topic distribution. Specifically, the proportions of high, moderate, and low positive emotions were 15.26%, 21.98%, and 50.06%, respectively, which exceed those of negative and neutral emotions. Furthermore, the sentiment analysis method based on topic distribution effectively uncovers latent information within course review texts, providing valuable insights for improving the quality of online teaching.",comprehensively analyze online course review texts uncover topics emotion attitudes learners study aims enhance quality online teaching study focuses analyzing learners discussion data online courses introduces emotion dictionary specifically tailored mooc massive open online course domain dictionary constructed using linear fusion algorithm integrates pmi semi supervised offset pointwise mutual information word vec techniques additionally study proposes enhanced latent dirichlet allocation lda model topic mining utilizes support vector machines svm sentiment classification analyzing textual data generated learners study aims reveal focal points emotional tendencies greater depth experimental results show lda model outperforms traditional lda models terms predictive accuracy topic distribution specifically proportions high moderate low positive emotions respectively exceed negative neutral emotions furthermore sentiment analysis method based topic distribution effectively uncovers latent information within course review texts providing valuable insights improving quality online teaching
"As the soft power of a country, cultural industry can not only meet the psychological needs of the people but also enhance the self-confidence of the nation. In order to clarify the development direction of the cultural industry, we propose an optimizing the multi-layer hybrid hypernetwork for the cultural industry value chain based on multi-core design to improve the intelligentization of the cultural industry. First, we construct the feature representation of the cultural industry value chain through graph convolutional network and improve the multi-core structure to improve the accuracy of the value chain. Then, around the value chain with multi-core structure, we propose a network optimization strategy by reinforcement learning to boost the performance of the intelligent model. Experiments show that our method can obtain an F1 value of 0.948, which can assist the upgrading and development of the cultural industry.",soft power country cultural industry meet psychological needs people also enhance self confidence nation order clarify development direction cultural industry propose optimizing multi layer hybrid hypernetwork cultural industry value chain based multi core design improve intelligentization cultural industry first construct feature representation cultural industry value chain graph convolutional network improve multi core structure improve accuracy value chain around value chain multi core structure propose network optimization strategy reinforcement learning boost performance intelligent model experiments show method obtain value assist upgrading development cultural industry
"The uptake of more-than-human thinking in geography seeks to centre the ways that our diverse life-worlds are sustained by interconnected and more-than-human relations. As a relatively new framework to the discipline, more-than-human thinking has offered different ways to address some of the world’s increasingly complex problems. But to what extent does this framework pay attention to the underlying context of Indigenous and settler colonial land relationships? And how might more-than-human thinking attend to complex problems such as researching on stolen Indigenous land? This article aims to take seriously the fact that tensions around settler colonial and Indigenous land relationships are not resolved, nor overcome by more-than-human thinking alone. We offer the Goats Foot Flower story, as told by Gumbaynggirr story-holder Aunty Shaa Smith, to share how diverse knowledges held within story are helping us to better understand what respectful land relationships might look like and mean. The story emphasises how heeding Country’s authority is foundational to maintaining respectful relationships with the land through living agreements. It teaches us that attending to respectful relationships on stolen land is an active and ongoing responsibility of continuing connections with and as Country. As we grapple with the complexities of what it means for each of us to be in respectful relationship with Country, we recognise that there is an urgent need to attend to Land Back as part of the process.",uptake human thinking geography seeks centre ways diverse life worlds sustained interconnected human relations relatively new framework discipline human thinking offered different ways address world increasingly complex problems extent framework pay attention underlying context indigenous settler colonial land relationships might human thinking attend complex problems researching stolen indigenous land article aims take seriously fact tensions around settler colonial indigenous land relationships resolved overcome human thinking alone offer goats foot flower story told gumbaynggirr story holder aunty shaa smith share diverse knowledges held within story helping better understand respectful land relationships might look like mean story emphasises heeding country authority foundational maintaining respectful relationships land living agreements teaches attending respectful relationships stolen land active ongoing responsibility continuing connections country grapple complexities means respectful relationship country recognise urgent need attend land back part process
"Transformer fault diagnosis is crucial for ensuring the safe and stable operation of power systems. However, traditional diagnostic methods suffer from issues such as insufficient feature utilization and poor model interpretability. To address these challenges, this paper proposes a hybrid fault diagnosis model. First, the model utilizes SHapley Additive exPlanations values for feature selection. Then, the improved Harris Hawks optimization is proposed to optimize the hyperparameters of eXtreme gradient boosting model (Xgboost), further improving the model’s classification performance. Experimental results demonstrate that the proposed model outperforms existing methods across multiple performance indicators, achieving an accuracy of 0.9509, which represents a 2.68% improvement compared to the Xgboost model. This model fully leverages the advantages of each method, effectively solving problems like insufficient feature utilization, poor model interpretability, and limited performance of optimization algorithms in traditional fault diagnosis methods. It provides a more reliable and efficient solution for transformer fault diagnosis.",transformer fault diagnosis crucial ensuring safe stable operation power systems however traditional diagnostic methods suffer issues insufficient feature utilization poor model interpretability address challenges paper proposes hybrid fault diagnosis model first model utilizes shapley additive explanations values feature selection improved harris hawks optimization proposed optimize hyperparameters extreme gradient boosting model xgboost improving model classification performance experimental results demonstrate proposed model outperforms existing methods across multiple performance indicators achieving accuracy represents improvement compared xgboost model model fully leverages advantages method effectively solving problems like insufficient feature utilization poor model interpretability limited performance optimization algorithms traditional fault diagnosis methods provides reliable efficient solution transformer fault diagnosis
"With the development of visual information communication technology, various interactive simulation methods for Arts and Crafts (AaC) have emerged. However, traditional methods suffer from low recognition accuracy and limited visual interaction. For improving the interactive ability of AaC, an interactive system based on visual view transformation of AaC is proposed. First, based on the scene vision-based aided design algorithm, the color segmentation is carried out on the known visual scene image. Then, morphological analysis is performed on segmented images to reduce noise and breakage and affect the obtained connected regions. Next, an assistant design algorithm based on behavior interaction is adopted to realize the combined control and simulation of the interactive design of AaC. Finally, the experiment is carried out to evaluate the algorithm used in the system. Experimental results demonstrate superior performance in CIMPACK index and recognition accuracy (0.78), with faster response times. This system enhances the intelligent interactive design capability for AaC, providing practical value for the industry.",development visual information communication technology various interactive simulation methods arts crafts aac emerged however traditional methods suffer low recognition accuracy limited visual interaction improving interactive ability aac interactive system based visual view transformation aac proposed first based scene vision based aided design algorithm color segmentation carried known visual scene image morphological analysis performed segmented images reduce noise breakage affect obtained connected regions next assistant design algorithm based behavior interaction adopted realize combined control simulation interactive design aac finally experiment carried evaluate algorithm used system experimental results demonstrate superior performance cimpack index recognition accuracy faster response times system enhances intelligent interactive design capability aac providing practical value industry
"The substantial demand for English translation services in cloud computing necessitates resource allocation and management. This study enhances the allocation and administration of English translation resources in cloud computing. This study enhances cloud-based English translation services and influences language translation, natural language processing, and cloud computing. This research aims to provide a pragmatic method for the complex challenge of effectively allocating and managing English translation resources inside cloud computing networks. The algorithm is used to assess allocation and management techniques based on the characteristics of the ongoing activities and the status of the cloud network. The features include the computing capacity of edge servers and devices, the quality of the communication channel, the efficacy of translation resource use, and the latency requirements of the services. The proposed model can autonomously learn about the network environment and make decisions about resource allocation to enhance performance and minimize latency. The ability of cloud computing to project future trajectories from an initial state to ascertain the ideal action by evaluating reward values is a principal benefit of the system. This capability enables the system to choose the optimal course of action. The proposed strategy achieves substantial improvements in key performance indicators, including a 36.4% reduction in average service latency, a 24.7% gain in resource utilization, and a 3.5% improvement in translation accuracy. Additionally, the approach demonstrates excellent scalability, handling increased workloads with minimal impact on performance, and reduces costs associated with translation services, making it a cost-effective solution for cloud-based translation. These findings, supported by the results of our Monte Carlo simulations presented significant implications for the development of efficient and cost-effective cloud-based translation services.",substantial demand english translation services cloud computing necessitates resource allocation management study enhances allocation administration english translation resources cloud computing study enhances cloud based english translation services influences language translation natural language processing cloud computing research aims provide pragmatic method complex challenge effectively allocating managing english translation resources inside cloud computing networks algorithm used assess allocation management techniques based characteristics ongoing activities status cloud network features include computing capacity edge servers devices quality communication channel efficacy translation resource use latency requirements services proposed model autonomously learn network environment make decisions resource allocation enhance performance minimize latency ability cloud computing project future trajectories initial state ascertain ideal action evaluating reward values principal benefit system capability enables system choose optimal course action proposed strategy achieves substantial improvements key performance indicators including reduction average service latency gain resource utilization improvement translation accuracy additionally approach demonstrates excellent scalability handling increased workloads minimal impact performance reduces costs associated translation services making cost effective solution cloud based translation findings supported results monte carlo simulations presented significant implications development efficient cost effective cloud based translation services
"The new media has changed the traditional way of information dissemination and formed a unique culture. The social impact of technological development has provided an unprecedented environment for art dissemination. In order to solve the problem that artworks including anime need to be labeled manually in the process of transmission, a media art design image classification method is proposed based on a visual biological neural network to classify the emotional style of artworks. In this paper, the ResNet101 neural network is improved, and the residual connection structure is changed by integrating the attention mechanism. This improvement provides rich global information for the model and improves the accuracy at the same time. To evaluate the proposed method, a data set of new media art designs and anime images is established. The experimental results show that the proposed methods in Acc, Precision, and Recall are 2.2%, 2.1%, and 2.0% higher than the original ResNet101, respectively. This method has made contributions to image classification in new media art design, and further accelerated the spread of art on the Internet.",new media changed traditional way information dissemination formed unique culture social impact technological development provided unprecedented environment art dissemination order solve problem artworks including anime need labeled manually process transmission media art design image classification method proposed based visual biological neural network classify emotional style artworks paper resnet neural network improved residual connection structure changed integrating attention mechanism improvement provides rich global information model improves accuracy time evaluate proposed method data set new media art designs anime images established experimental results show proposed methods acc precision recall higher original resnet respectively method made contributions image classification new media art design accelerated spread art internet
"The automotive industry increasingly relies on semiconductor chips to ensure the reliability and performance of modern systems such as self-driving vehicles, entertainment systems, and safety measures. However, the high temperatures and mechanical stresses characteristic of automotive environments significantly impact the reliability and operational efficiency of these chips. FSDOI physical design strategies have emerged as a viable solution for enhancing the robustness and efficacy of automotive-grade semiconductor chips. Data are collected through both computational simulations and experimental investigations. The resulting data are analyzed using a variety of statistical methods, including failure rate analysis, which compares failure rates between chips optimized with FSDOI and those without. FSDOI emphasizes the fine-tuning of several aspects of the chip’s physical design to enhance performance. Thermal dissipation, mechanical stress, and electrical density were examined under automotive operational conditions, which include significant temperature variations and high humidity levels. Finite Element Analysis (FEA) is a sophisticated computational tool for modeling and solving large physical problems by breaking them into smaller, more manageable elements. These elements are scrutinized to predict how the chip responds to various loads, pressures, and environmental conditions. Chips manufactured using the FSDOI technique undergo experimental tests to validate simulation results, including thermal and mechanical stress testing, as well as electromigration testing. The evaluations indicate that the FSDOI technique significantly enhances chip reliability and lowers failure rates. Findings underscore the necessity of incorporating advanced design optimization strategies into automotive chip production to fulfill the stringent requirements of modern automobiles. FSDOI-optimized chips exhibit a 48.15% reduction in failure rate, a 25% increase in Mean Time to Failure (MTTF), and a 35.48% improvement in overall reliability score. Mechanical stress resistance increases by 21.43%, while deformation under load decreases by 40%. Furthermore, thermal conductivity efficiency rises by 10.77%, and vibration endurance improves by 46.67%. Thus, FSDOI physical design methodologies significantly enhance the efficiency and functionality of automotive-grade semiconductor devices.",automotive industry increasingly relies semiconductor chips ensure reliability performance modern systems self driving vehicles entertainment systems safety measures however high temperatures mechanical stresses characteristic automotive environments significantly impact reliability operational efficiency chips fsdoi physical design strategies emerged viable solution enhancing robustness efficacy automotive grade semiconductor chips data collected computational simulations experimental investigations resulting data analyzed using variety statistical methods including failure rate analysis compares failure rates chips optimized fsdoi without fsdoi emphasizes fine tuning several aspects chip physical design enhance performance thermal dissipation mechanical stress electrical density examined automotive operational conditions include significant temperature variations high humidity levels finite element analysis fea sophisticated computational tool modeling solving large physical problems breaking smaller manageable elements elements scrutinized predict chip responds various loads pressures environmental conditions chips manufactured using fsdoi technique undergo experimental tests validate simulation results including thermal mechanical stress testing well electromigration testing evaluations indicate fsdoi technique significantly enhances chip reliability lowers failure rates findings underscore necessity incorporating advanced design optimization strategies automotive chip production fulfill stringent requirements modern automobiles fsdoi optimized chips exhibit reduction failure rate increase mean time failure mttf improvement overall reliability score mechanical stress resistance increases deformation load decreases furthermore thermal conductivity efficiency rises vibration endurance improves thus fsdoi physical design methodologies significantly enhance efficiency functionality automotive grade semiconductor devices
"The purpose of this study is to investigate the impact of online information searching strategies, digital literacy, and innovative thinking skills on university students’ creative thinking dispositions. A structural equation model has been developed to explore the structural relationships among these variables. In this causal research, the analysis focuses on cause-effect relationships between variables that have either emerged or already exist within the given context. Data were collected from 815 students across various Turkish universities through an online Google Form. 815 students from various Turkish universities participated via online Google form. As for the data collection tools, university students were administered the Marmara Creative Thinking Scale, the Online Information Search Strategies Scale, the Digital Literacy Scale, and The Innovative Thinking Skills Scale. According to the results of the study, the constructed model explains creative thinking dispositions at a level of 61%. The findings indicate that digital literacy levels positively influence online information searching strategies and innovative thinking levels. However, it is observed that digital literacy levels do not directly impact creative thinking dispositions; instead, they have an indirect effect. Thus, it is inferred that enhancement in students’ online information search strategies and innovative thinking levels coincides with an elevation in their creative thinking dispositions.",purpose study investigate impact online information searching strategies digital literacy innovative thinking skills university students creative thinking dispositions structural equation model developed explore structural relationships among variables causal research analysis focuses cause effect relationships variables either emerged already exist within given context data collected students across various turkish universities online google form students various turkish universities participated via online google form data collection tools university students administered marmara creative thinking scale online information search strategies scale digital literacy scale innovative thinking skills scale according results study constructed model explains creative thinking dispositions level findings indicate digital literacy levels positively influence online information searching strategies innovative thinking levels however observed digital literacy levels directly impact creative thinking dispositions instead indirect effect thus inferred enhancement students online information search strategies innovative thinking levels coincides elevation creative thinking dispositions
"Real-time object detection and tracking are critical for applications such as robotics and autonomous systems. Embedded platforms present challenges in balancing speed, accuracy, and efficiency. Existing approaches often struggle to achieve both high accuracy and real-time performance within resource-constrained embedded systems. The main challenge remains in balancing detection speed, tracking consistency, and hardware efficiency for practical deployment. This work proposes a deep learning (DL) framework optimized for embedded systems, ensuring high accuracy, minimal latency, and efficient resource utilization for real-world applications. The framework integrates a novel Botox Optimization Algorithm-tuned Adaptive CNN (BOA-ACNN) for real-time object recognition and tracking. The dataset comprises annotated video sequences capturing diverse scenarios involving vehicles, pedestrians, and dynamic camera movements. The framework employs a Kalman filter for real-time motion prediction and noise smoothing, thereby enhancing tracking stability. Additionally, SIFT features are utilized to improve detection robustness under varying scales and environmental conditions. The system incorporates BOA for hyper-parameter fine-tuning and ACNN for efficient real-time detection and tracking, achieving latency of 97.5 μs, throughput of 200.4 activation/us, precision of 96.2%, recall of 97%, F1-score of 97%, mAP of 98.4%, and overall accuracy of 98.97%. This framework facilitates real-time object identification and tracking with high accuracy and low latency on embedded devices, demonstrating superior performance for practical applications.",real time object detection tracking critical applications robotics autonomous systems embedded platforms present challenges balancing speed accuracy efficiency existing approaches often struggle achieve high accuracy real time performance within resource constrained embedded systems main challenge remains balancing detection speed tracking consistency hardware efficiency practical deployment work proposes deep learning framework optimized embedded systems ensuring high accuracy minimal latency efficient resource utilization real world applications framework integrates novel botox optimization algorithm tuned adaptive cnn boa acnn real time object recognition tracking dataset comprises annotated video sequences capturing diverse scenarios involving vehicles pedestrians dynamic camera movements framework employs kalman filter real time motion prediction noise smoothing thereby enhancing tracking stability additionally sift features utilized improve detection robustness varying scales environmental conditions system incorporates boa hyper parameter fine tuning acnn efficient real time detection tracking achieving latency throughput activation precision recall score map overall accuracy framework facilitates real time object identification tracking high accuracy low latency embedded devices demonstrating superior performance practical applications
"Research and development have demonstrated that effective building energy prediction is significant for enhancing energy efficiency and ensuring grid reliability. Many machine learning (ML) models, particularly deep learning (DL) approaches, are widely used for power or peak demand forecasting. However, evaluating prediction models solely based on accuracy is insufficient, as complex models often suffer from low interpretability and high computational costs, making them difficult to implement in real-world applications. This study proposes a multi-perspective evaluation analysis that includes prediction accuracy (both overall and at different power levels), interpretability (global/local perspectives and model structure), and computational efficiency. Three popular DL models—recurrent neural network, gated recurrent unit, long short-term memory, and three tree-based models—random forecast, extreme gradient boosting, and light gradient boosting machine—are analyzed due to their popularity and high prediction accuracy in the field of power demand prediction. The comparison reveals the following: (1) The best-performing prediction model changes under different power demand levels. In scenarios with lower power usage patterns, tree-based models achieve an average CV-RMSE of 13.62%, which is comparable to the 12.17% average CV-RMSE of DL models. (2) Global and local interpretations indicate that past power use and time-related features are the most important. Tree-based models excel at identifying which specific lagged features are more significant. (3) The DL model behavior can be interpreted by visualizing the hidden state at each layer to reveal how the model captures temporal dynamics across different time steps. However, tree-based models are more intuitive to interpret using straightforward decision rules and structures. This study provides guidance for applying ML algorithms to load forecasting, offering multiple perspectives on model selection trade-offs.",research development demonstrated effective building energy prediction significant enhancing energy efficiency ensuring grid reliability many machine learning models particularly deep learning approaches widely used power peak demand forecasting however evaluating prediction models solely based accuracy insufficient complex models often suffer low interpretability high computational costs making difficult implement real world applications study proposes multi perspective evaluation analysis includes prediction accuracy overall different power levels interpretability global local perspectives model structure computational efficiency three popular models recurrent neural network gated recurrent unit long short term memory three tree based models random forecast extreme gradient boosting light gradient boosting machine analyzed due popularity high prediction accuracy field power demand prediction comparison reveals following best performing prediction model changes different power demand levels scenarios lower power usage patterns tree based models achieve average rmse comparable average rmse models global local interpretations indicate past power use time related features important tree based models excel identifying specific lagged features significant model behavior interpreted visualizing hidden state layer reveal model captures temporal dynamics across different time steps however tree based models intuitive interpret using straightforward decision rules structures study provides guidance applying algorithms load forecasting offering multiple perspectives model selection trade offs
"Aiming at the low accuracy and poor adaptability of animation character recognition in complex scenes, an intelligent recognition method based on weighted improved single-detection multi-frame detector (SSD) is proposed. By constructing a special data set containing 36 types of animated characters, the positive sample dynamic weighting strategy is innovatively designed to solve the inherent positive and negative sample imbalance problem of SSD model, and the receptive field enhancement module (RFB) is introduced to improve the multi-scale feature expression ability. The experiment shows that the mAP of the model increases by 13.8%. When the positive sample weight coefficient is 3, the identification effect is the best, which is 18.2% higher than that of traditional SSD. The improved SSD-RFB model detection accuracy (mAP@50 = 95.78%) and real-time (10.5 FPS) are superior to mainstream detection algorithms. The results show that the proposed algorithm can effectively alleviate the overfitting problem caused by the pose diversity of animation characters, and the synergistic effect of core modules is demonstrated through experiments. The research provides an efficient solution for the intelligent retrieval of animation resources, and its sample balance strategy and feature enhancement method provide a new way of thinking in the field of animation character recognition.",aiming low accuracy poor adaptability animation character recognition complex scenes intelligent recognition method based weighted improved single detection multi frame detector ssd proposed constructing special data set containing types animated characters positive sample dynamic weighting strategy innovatively designed solve inherent positive negative sample imbalance problem ssd model receptive field enhancement module rfb introduced improve multi scale feature expression ability experiment shows map model increases positive sample weight coefficient identification effect best higher traditional ssd improved ssd rfb model detection accuracy map real time fps superior mainstream detection algorithms results show proposed algorithm effectively alleviate overfitting problem caused pose diversity animation characters synergistic effect core modules demonstrated experiments research provides efficient solution intelligent retrieval animation resources sample balance strategy feature enhancement method provide new way thinking field animation character recognition
"The Internet of Things (IoT) is an essential component of the digital age, particularly in ensuring reliable and efficient operations as well as the timely and precise recognition of anomalies within IoT systems. However, anomaly detection in time-series data—especially data collected by edge devices—poses several challenges, including concerns over data privacy and communication overhead. To address these issues, this research proposes a novel deep learning (DL)-based anomaly detection model capable of being trained in real-time on edge devices within an IoT environment. The framework ensures user privacy by enabling distributed edge devices to cooperatively train an anomaly recognition system. This study introduces a new, Intelligent Shark Smell Tuned Deep Isolation Forest (ISS-DIF), which effectively detects anomalies and identifies outliers in industrial IoT sensor data with high accuracy. The hybrid model isolates anomalies rather than profiling normal data, making it particularly suitable for identifying rare anomalies within large datasets. Industrial data are collected from real-world manufacturing environments using IoT edge devices. Following data collection, median filtering is applied to reduce noise, and min-max scaling is employed for data normalization. The ISS component allows for fine-tuning the hyperparameters of the Deep Isolation Forest (DIF) to optimize detection performance. The DIF model is tailored to enhance anomaly detection capabilities in sensor data from industrial IoT applications. For validation, 80% of the dataset was used for training, while the remaining 20% served as the test set. The results demonstrate that the proposed ISS-DIF framework achieved superior training accuracy of 99.30%, with precision of 97.89%, recall of 98.76%, and an F1-score of 97.21%, outperforming the testing metrics. This approach integrates real-time anomaly detection with the processing capabilities of edge devices, thereby improving IoT data analytics and providing a scalable, efficient solution that preserves privacy in irregularity recognition within IoT environments.",internet things iot essential component digital age particularly ensuring reliable efficient operations well timely precise recognition anomalies within iot systems however anomaly detection time series data especially data collected edge devices poses several challenges including concerns data privacy communication overhead address issues research proposes novel deep learning based anomaly detection model capable trained real time edge devices within iot environment framework ensures user privacy enabling distributed edge devices cooperatively train anomaly recognition system study introduces new intelligent shark smell tuned deep isolation forest iss dif effectively detects anomalies identifies outliers industrial iot sensor data high accuracy hybrid model isolates anomalies rather profiling normal data making particularly suitable identifying rare anomalies within large datasets industrial data collected real world manufacturing environments using iot edge devices following data collection median filtering applied reduce noise min max scaling employed data normalization iss component allows fine tuning hyperparameters deep isolation forest dif optimize detection performance dif model tailored enhance anomaly detection capabilities sensor data industrial iot applications validation dataset used training remaining served test set results demonstrate proposed iss dif framework achieved superior training accuracy precision recall score outperforming testing metrics approach integrates real time anomaly detection processing capabilities edge devices thereby improving iot data analytics providing scalable efficient solution preserves privacy irregularity recognition within iot environments
"To address the disposal of lightweight materials (such as plastic, cloth and paper) in waste stocks, this study delves into the analysis of two distinct wind separation chamber structures and their respective impacts on the separation efficiency of lightweight materials. Utilizing Fluent simulation software, the structural optimization of the air chamber is conducted, scrutinizing the effects of the auxiliary tuyre angle in primary wind separation and the fan arrangement mode in secondary wind separation on the overall separation effectiveness. Various models are established to compare flow field track diagrams and velocity cloud diagrams within the wind separation room, allowing for a comprehensive analysis of how device structure influences the precision of wind separation. The simulation results reveal that, in primary wind separation, setting the auxiliary fan angle between 35° and 40° yields a more reasonable indoor flow field and velocity distribution, leading to effective separation of lightweight materials. For secondary air selection, the optimal fan arrangement mode is determined to be non-uniform, ensuring uniform wind bodies in the field with minimal cyclonic disturbances.",address disposal lightweight materials plastic cloth paper waste stocks study delves analysis two distinct wind separation chamber structures respective impacts separation efficiency lightweight materials utilizing fluent simulation software structural optimization air chamber conducted scrutinizing effects auxiliary tuyre angle primary wind separation fan arrangement mode secondary wind separation overall separation effectiveness various models established compare flow field track diagrams velocity cloud diagrams within wind separation room allowing comprehensive analysis device structure influences precision wind separation simulation results reveal primary wind separation setting auxiliary fan angle yields reasonable indoor flow field velocity distribution leading effective separation lightweight materials secondary air selection optimal fan arrangement mode determined non uniform ensuring uniform wind bodies field minimal cyclonic disturbances
"An optimal location of perforation clusters is critical to obtain a commercial production rate in unconventional tight reservoir reservoirs. At present, the perforation cluster position design method based on overlapping induced stress between fractures is only suitable for multi-stage single-cluster fracturing in horizontal well. Under real well conditions, multiple fractures extend at the same time during multi-stage and multi-cluster fracturing, which is easy to produce stress barrier effects and stress interference effects. These problems put forward higher requirements for selecting the best position of perforating cluster. In this paper, according to the characteristics of multiple fractures extending simultaneously in one stage in multi-stage multi-cluster fracturing, the influence range of induced stress in one stage considering stress barrier and disturbance effect was analyzed. The mathematical model for calculating the induced stress at different locations in one stage is established considering the variation of effective net pressure to the surrounding formation and the resulted resulting distribution of induced stress. Based on this model, a multi-stage and multi-cluster fracture optimization design method for horizontal wells is proposed, which can effectively avoid repeated fractured and non-fractured areas by optimizing the non-equidistant locations of the perforated clusters. Case studies show that this method can effectively avoid refracturing and non-refracturing zones, thus achieving better fracturing results with lower energy consumption.",optimal location perforation clusters critical obtain commercial production rate unconventional tight reservoir reservoirs present perforation cluster position design method based overlapping induced stress fractures suitable multi stage single cluster fracturing horizontal well real well conditions multiple fractures extend time multi stage multi cluster fracturing easy produce stress barrier effects stress interference effects problems put forward higher requirements selecting best position perforating cluster paper according characteristics multiple fractures extending simultaneously one stage multi stage multi cluster fracturing influence range induced stress one stage considering stress barrier disturbance effect analyzed mathematical model calculating induced stress different locations one stage established considering variation effective net pressure surrounding formation resulted resulting distribution induced stress based model multi stage multi cluster fracture optimization design method horizontal wells proposed effectively avoid repeated fractured non fractured areas optimizing non equidistant locations perforated clusters case studies show method effectively avoid refracturing non refracturing zones thus achieving better fracturing results lower energy consumption
"Financial fraud is one of the primary threats to businesses and the economy at large, resulting in monumental financial losses, operational inefficiency, and significant damage to reputation. This research examines the use of a Multi-Technique Fraud Detection System (MTFDS) that incorporates machine vision technologies such as Optical Character Recognition (OCR), signature verification, and behavioral anomaly detection to identify and prevent financial fraud and analyze its economic impact. The fraud detection dataset contains detailed transaction data from scanned financial documents. Noise reduction is applied to the scanned financial documents using a median filter for improved OCR processing. OCR extracts and validates text from scanned invoices and bank statements. Signature verification ensures the authenticity of signed documents. A Modified White Shark Optimizer tuned Elman Spiking Neural Network (MWSO-ESNN) is proposed to detect behavioral anomalies by identifying irregular transaction patterns and visual anomalies in transaction-related images. The research also assesses the economic consequences of fraud by quantifying financial losses, operational costs, reputation damage, and market instability. Experimental results indicate that the MWSO-ESNN model achieves 99.2% accuracy, 96.7% precision, 92.5% recall, and 95.6% F1-score, outperforming Deep Neural Networks (DNNs) and Long Short-Term Memory (LSTM) networks. This research demonstrates the efficacy of MTFDS in preventing financial fraud and provides a comprehensive assessment of its economic effects, offering a holistic approach to fraud detection and economic impact analysis.",financial fraud one primary threats businesses economy large resulting monumental financial losses operational inefficiency significant damage reputation research examines use multi technique fraud detection system mtfds incorporates machine vision technologies optical character recognition ocr signature verification behavioral anomaly detection identify prevent financial fraud analyze economic impact fraud detection dataset contains detailed transaction data scanned financial documents noise reduction applied scanned financial documents using median filter improved ocr processing ocr extracts validates text scanned invoices bank statements signature verification ensures authenticity signed documents modified white shark optimizer tuned elman spiking neural network mwso esnn proposed detect behavioral anomalies identifying irregular transaction patterns visual anomalies transaction related images research also assesses economic consequences fraud quantifying financial losses operational costs reputation damage market instability experimental results indicate mwso esnn model achieves accuracy precision recall score outperforming deep neural networks dnns long short term memory lstm networks research demonstrates efficacy mtfds preventing financial fraud provides comprehensive assessment economic effects offering holistic approach fraud detection economic impact analysis
"The semantic understanding and representativeness of contemporary ceramic materials are critical to advancing their artistic and functional applications in modern design. However, analyzing subtle characteristics of ceramic materials—such as texture, composition, and form—has traditionally relied on subjective techniques. This study introduces an advanced deep learning (DL) framework that combines Convolutional Neural Networks (CNNs) and Elman Recurrent Neural Networks (ERNNs) to systematically assess and quantify the semantic features of modern ceramic materials. Data collected from 3D surface texture maps and high-resolution Scanning Electron Microscopy (SEM) images are used to train models capable of recognizing and segmenting complex microstructural patterns, including surface imperfections, crystallization stages, and fault types. The CNN model extracts hierarchical features from SEM images, while the ERNN generates synthetic high-resolution images for data augmentation, thereby improving segmentation accuracy. The results demonstrate that the proposed models outperform traditional methods, achieving an Intersection over Union (IoU) score of 97.8%, an accuracy of 97.69%, and a precision of 95.32%. Additionally, the trained models facilitate precise reconstruction of 3D microstructures, revealing spatial distributions of phases that are challenging to capture with conventional imaging techniques. When integrated with simulation tools, this approach enhances semantic insights into ceramic materials, enabling real-time applications in ceramic design and manufacturing—thus promoting higher quality control, material innovation, and process efficiency.",semantic understanding representativeness contemporary ceramic materials critical advancing artistic functional applications modern design however analyzing subtle characteristics ceramic materials texture composition form traditionally relied subjective techniques study introduces advanced deep learning framework combines convolutional neural networks cnns elman recurrent neural networks ernns systematically assess quantify semantic features modern ceramic materials data collected surface texture maps high resolution scanning electron microscopy sem images used train models capable recognizing segmenting complex microstructural patterns including surface imperfections crystallization stages fault types cnn model extracts hierarchical features sem images ernn generates synthetic high resolution images data augmentation thereby improving segmentation accuracy results demonstrate proposed models outperform traditional methods achieving intersection union iou score accuracy precision additionally trained models facilitate precise reconstruction microstructures revealing spatial distributions phases challenging capture conventional imaging techniques integrated simulation tools approach enhances semantic insights ceramic materials enabling real time applications ceramic design manufacturing thus promoting higher quality control material innovation process efficiency
"The bogie is the most crucial component of a high-speed train’s (HST) running gear, directly impacting operational safety and passenger comfort. Major failures in bogie components can lead to severe vibration, performance degradation, and even catastrophic accidents such as derailments. To address this, we propose IntelliGraph-EVO, a novel fault diagnosis framework integrating an Intelligent Graph Convolutional Network (IntelliGraph) and the Energy Valley Optimizer (EVO). IntelliGraph captures structural relationships between bogie components through graph convolutions, while EVO optimizes model parameters to enhance diagnostic performance using sensor-derived synthetic vibration data. Two datasets are utilized: Dataset 1 simulates seven operational conditions (normal and six fault states) at varying speeds (80–200 km/h), and Dataset 2 includes 15 conditions (14 single-fault and normal states) at a fixed 200 km/h. Raw sensor data is pre-processed via min-max normalization, and Fast Fourier Transform (FFT) extracts frequency-domain features. Experimental results demonstrate that IntelliGraph-EVO outperforms state-of-the-art methods like ICEEMDAN + 1-D CNN, achieving precision (0.995), recall (0.994), F1-score (0.994), and accuracy (0.994) for Dataset 1, and accuracy (0.993) for Dataset 2. The model’s ability to diagnose single and mixed faults under dynamic operational conditions highlights its robustness and generalization. This work underscores the potential of deep learning for real-time condition monitoring in HSTs, enabling early fault prediction and proactive maintenance to enhance safety and operational efficiency.",bogie crucial component high speed train hst running gear directly impacting operational safety passenger comfort major failures bogie components lead severe vibration performance degradation even catastrophic accidents derailments address propose intelligraph evo novel fault diagnosis framework integrating intelligent graph convolutional network intelligraph energy valley optimizer evo intelligraph captures structural relationships bogie components graph convolutions evo optimizes model parameters enhance diagnostic performance using sensor derived synthetic vibration data two datasets utilized dataset simulates seven operational conditions normal six fault states varying speeds dataset includes conditions single fault normal states fixed raw sensor data pre processed via min max normalization fast fourier transform fft extracts frequency domain features experimental results demonstrate intelligraph evo outperforms state art methods like iceemdan cnn achieving precision recall score accuracy dataset accuracy dataset model ability diagnose single mixed faults dynamic operational conditions highlights robustness generalization work underscores potential deep learning real time condition monitoring hsts enabling early fault prediction proactive maintenance enhance safety operational efficiency
"Smart grid systems necessitate robust data encryption to safeguard sensitive data. Hardware limitations and computational complexity, however, restrict the real-time applicability of encryption in certain smart grid environments. This investigation proposes a novel encryption algorithm, the Genetic Algorithm-Enhanced 8-Dimensional Chaotic System (GA-8D chaotic system), based on a complex chaotic system enhanced by a genetic algorithm, to secure smart grid data. The data was collected from a simulated smart grid environment, comprising real-time grid load data and sensor inputs. The proposed GA-8D chaotic system technique integrates a Genetic Algorithm (GA) and an 8D chaotic system for strong key generation and encryption. This approach enhances encryption speed and security by evolving optimal parameters using genetic algorithms, thereby ensuring higher performance in smart grid applications. The 8D chaotic system encryption technique is utilized to securely encrypt data and sensor images within the smart grid, guaranteeing data integrity and confidentiality within the smart grid architecture. The proposed algorithm demonstrated superior performance in encryption speed (0.1 ms), training accuracy (0.98), security strength (98%), and computational efficiency (450 bits). The results indicate that the algorithm is scalable for smart grid data encryption. The GA-Enhanced 8D chaotic system encryption algorithm successfully addresses the security challenges in smart grid data transmission.",smart grid systems necessitate robust data encryption safeguard sensitive data hardware limitations computational complexity however restrict real time applicability encryption certain smart grid environments investigation proposes novel encryption algorithm genetic algorithm enhanced dimensional chaotic system chaotic system based complex chaotic system enhanced genetic algorithm secure smart grid data data collected simulated smart grid environment comprising real time grid load data sensor inputs proposed chaotic system technique integrates genetic algorithm chaotic system strong key generation encryption approach enhances encryption speed security evolving optimal parameters using genetic algorithms thereby ensuring higher performance smart grid applications chaotic system encryption technique utilized securely encrypt data sensor images within smart grid guaranteeing data integrity confidentiality within smart grid architecture proposed algorithm demonstrated superior performance encryption speed training accuracy security strength computational efficiency bits results indicate algorithm scalable smart grid data encryption enhanced chaotic system encryption algorithm successfully addresses security challenges smart grid data transmission
"Intelligent measurement terminals (IMTs) are key components in the power grid metering system. Detecting sensitive behaviors of the applications (Apps) installed on the IMTs is an effective means to prevent malicious behaviors and ensure the safety of the power grid. However, the traditional deep learning-based methods for sensitive behavior detection often suffer from performance degradation when the Apps change their behavior pattern after porting, migration, or upgrading. Retraining the network requires large quantities of labeled data, which is usually time-consuming and costly in practice. This paper proposes a new sensitive behavior detection method for IMT Apps based on the domain adversarial neural network (DANN). First, the DANN is utilized to extract the shared features of App behaviors before and after porting, migration, or upgrading, enabling the original data to be repurposed for training the new model, thereby alleviating the insufficient training data problem. Then, the distinctive features of the sensitive behavior in the new environment are extracted and fused with the shared features in the detection process to further improve the detection accuracy. Experimental results show that the proposed method attains 91.83% in detection accuracy, 91.21% in precision, 92.61% in recall, and 91.91% in F1-score, which are better than several traditional deep learning-based methods.",intelligent measurement terminals imts key components power grid metering system detecting sensitive behaviors applications apps installed imts effective means prevent malicious behaviors ensure safety power grid however traditional deep learning based methods sensitive behavior detection often suffer performance degradation apps change behavior pattern porting migration upgrading retraining network requires large quantities labeled data usually time consuming costly practice paper proposes new sensitive behavior detection method imt apps based domain adversarial neural network dann first dann utilized extract shared features app behaviors porting migration upgrading enabling original data repurposed training new model thereby alleviating insufficient training data problem distinctive features sensitive behavior new environment extracted fused shared features detection process improve detection accuracy experimental results show proposed method attains detection accuracy precision recall score better several traditional deep learning based methods
"The capacity of fractional-order differential equations (FODEs) to simulate intricate behaviors in nonlinear dynamic systems has drawn significant interest recently. A potent tool for characterizing memory and inherited characteristics present in several physical, biological, and engineering systems, FODEs involve derivatives of arbitrary, non-integer orders, in contrast to conventional integer-order differential equations. The goal of this research is to extensively examine the stability and control techniques of nonlinear FODEs, addressing their vital significance in modeling and managing nonlinear dynamical systems. Lyapunov-based stability is used to theoretically prove a stability theorem, offering a strong basis for fractional-order (FO) system analysis. A linear state feedback controller is designed, specifically to maintain a class of FO nonlinear systems to preserve system stability. The pole placement method from linear FO control theory is used to establish a novel criterion for calculating controller gains. The suggested method makes it possible to systematically select control settings, providing accurate nonlinear dynamics stabilization. Simulation investigations on the FO chaotic Lorenz system are carried out to verify the theoretical framework, and the intended stability and control performance are obtained. The outcomes demonstrate the efficiency of the proposed strategies, offering significant insights for advancing the understanding and function of FODEs in complex nonlinear dynamic systems.",capacity fractional order differential equations fodes simulate intricate behaviors nonlinear dynamic systems drawn significant interest recently potent tool characterizing memory inherited characteristics present several physical biological engineering systems fodes involve derivatives arbitrary non integer orders contrast conventional integer order differential equations goal research extensively examine stability control techniques nonlinear fodes addressing vital significance modeling managing nonlinear dynamical systems lyapunov based stability used theoretically prove stability theorem offering strong basis fractional order system analysis linear state feedback controller designed specifically maintain class nonlinear systems preserve system stability pole placement method linear control theory used establish novel criterion calculating controller gains suggested method makes possible systematically select control settings providing accurate nonlinear dynamics stabilization simulation investigations chaotic lorenz system carried verify theoretical framework intended stability control performance obtained outcomes demonstrate efficiency proposed strategies offering significant insights advancing understanding function fodes complex nonlinear dynamic systems
"In recent years, advancements in artificial intelligence (AI) and natural language processing (NLP) have significantly changed the landscape of education. Among the most promising developments is the emergence of chatbot-based language tutors, which leverage AI to offer personalized and interactive language learning experiences. These tutors can assist learners in mastering vocabulary, grammar, pronunciation, and conversation skills across various languages. This research examines the role of chatbot-based language tutors utilizing deep learning (DL) to facilitate English language acquisition in mobile applications. Intent categorization is a fundamental component of these systems, allowing chatbots to understand user questions and respond appropriately. To address related issues, the research created a proofreading chatbot designed to help academic authors with grammatical corrections. Data was collected from a publicly available chatbot-based English learning dataset. The data was preprocessed using stop word removal and tokenization. Term Frequency Inverse Document Frequency (TF-IDF) is utilized to extract features from the preprocessed data. Efficient pigeon inspired fused bidirectional long short-term memory (EPI-BiLSTM) is applied to classify the intent based on the text to determine the user’s intent. After the classification, to address data scarcity in grammatical error correction for the English language, back translation is employed as a data augmentation tool. Back translation involves translating error-prone sentences into a different language and then translating them back to the original language, generating parallel corpora with their corrected counterparts, derived from texts. The experimental results demonstrated that EPI-BiLSTM outperforms traditional algorithms based on domain (80.5%), intent (90.3%), entity (75.2%), and average accuracy (81.3%). These findings illustrate the potential of combining chatbot-based systems and DL techniques to address both proofreading and grammatical error correction challenges in mobile applications.",recent years advancements artificial intelligence natural language processing nlp significantly changed landscape education among promising developments emergence chatbot based language tutors leverage offer personalized interactive language learning experiences tutors assist learners mastering vocabulary grammar pronunciation conversation skills across various languages research examines role chatbot based language tutors utilizing deep learning facilitate english language acquisition mobile applications intent categorization fundamental component systems allowing chatbots understand user questions respond appropriately address related issues research created proofreading chatbot designed help academic authors grammatical corrections data collected publicly available chatbot based english learning dataset data preprocessed using stop word removal tokenization term frequency inverse document frequency idf utilized extract features preprocessed data efficient pigeon inspired fused bidirectional long short term memory epi bilstm applied classify intent based text determine user intent classification address data scarcity grammatical error correction english language back translation employed data augmentation tool back translation involves translating error prone sentences different language translating back original language generating parallel corpora corrected counterparts derived texts experimental results demonstrated epi bilstm outperforms traditional algorithms based domain intent entity average accuracy findings illustrate potential combining chatbot based systems techniques address proofreading grammatical error correction challenges mobile applications
"To enhance the utilization efficiency of recycled powder in concrete and address the research gap in traditional single-admixture studies of recycled concrete powder (RCP) and recycled brick powder (RBP), this study prepared RCP and RBP from construction waste (discarded concrete and bricks) and blended them at varying ratios (0:10, 2:8, 4:6, 6:4, 8:2, and 10:0) to produce recycled composite micro-powder (RCMP). The RCMP was then used to replace 20% (by mass) of cement in the production of C30-grade concrete. This research systematically investigated the effects of RCMP blending ratios on the workability and mechanical properties (at 3, 7, 14, 28, 56, and 90 days) of concrete, while also elucidating the underlying mechanisms from a microstructural perspective. The results indicate that RCP and RBP exhibit irregular geometries, rough surfaces, and high water absorption. Their chemical compositions are similar to ordinary Portland cement, with RBP demonstrating higher secondary hydration activity than RCP. The incorporation of RCMP reduced both the workability and mechanical performance of concrete. However, at an RCP:RBP ratio of 2:8, the composite powder effectively leveraged the filler effect and pozzolanic properties. Compared to the reference group, the 90-day compressive strength, splitting tensile strength, and flexural strength decreased by only 9.86%, 4.05%, and 1.75%, respectively, while the failure process and modes remained consistent with standard concrete. Furthermore, the T2 relaxation spectrum of RCMP-modified concrete resembled that of the reference group, primarily exhibiting four peaks. The addition of RCMP optimized the pore size distribution within the concrete matrix, enhancing its overall compactness.",enhance utilization efficiency recycled powder concrete address research gap traditional single admixture studies recycled concrete powder rcp recycled brick powder rbp study prepared rcp rbp construction waste discarded concrete bricks blended varying ratios produce recycled composite micro powder rcmp rcmp used replace mass cement production grade concrete research systematically investigated effects rcmp blending ratios workability mechanical properties days concrete also elucidating underlying mechanisms microstructural perspective results indicate rcp rbp exhibit irregular geometries rough surfaces high water absorption chemical compositions similar ordinary portland cement rbp demonstrating higher secondary hydration activity rcp incorporation rcmp reduced workability mechanical performance concrete however rcp rbp ratio composite powder effectively leveraged filler effect pozzolanic properties compared reference group day compressive strength splitting tensile strength flexural strength decreased respectively failure process modes remained consistent standard concrete furthermore relaxation spectrum rcmp modified concrete resembled reference group primarily exhibiting four peaks addition rcmp optimized pore size distribution within concrete matrix enhancing overall compactness
"Frogeye leaf spot (FLS) is common in soybean cultivation and severely affects yield. Management practices for this disease include breeding resistant cultivars and applying fungicides. However, researchers often rely on traditional methods, such as visual identification, which are subjective and prone to errors. Machine vision and machine learning offer an alternative for FLS detection. This study proposed a segmentation method to extract disease spots from soybean leaves, using the hyper-green algorithm optimized by particle swarm optimization (PSO). A dataset consisting of 313 soybean leaf images, annotated with disease spots, was created. The PSO algorithm trained the data using a two-stage search strategy, comprising a coarse search phase and a fine search phase. The objective was to reduce the area discrepancy between predicted and annotated images by optimizing the weight values of the color components in the hyper-green algorithm. After optimization, the disease-to-leaf area ratio accuracy enhanced from 68.45% to 98.87%. The segmented images were analyzed, with 51 features extracted to construct a classification model. Three machine learning methods—random forest (RF), support vector machine (SVM), and extreme gradient boosting (XGBoost)—were utilized to create classification models for FLS, with all models optimized by the Bayesian optimizer. The RF and XGBoost models achieved accuracies of 94.76% and 94.22%, respectively, while the SVM model achieved the highest accuracy of 98.07%. The results indicate that this method serves as a reliable tool for the automated identification of FLS, enhancing both the efficiency and precision of soybean disease detection.",frogeye leaf spot fls common soybean cultivation severely affects yield management practices disease include breeding resistant cultivars applying fungicides however researchers often rely traditional methods visual identification subjective prone errors machine vision machine learning offer alternative fls detection study proposed segmentation method extract disease spots soybean leaves using hyper green algorithm optimized particle swarm optimization pso dataset consisting soybean leaf images annotated disease spots created pso algorithm trained data using two stage search strategy comprising coarse search phase fine search phase objective reduce area discrepancy predicted annotated images optimizing weight values color components hyper green algorithm optimization disease leaf area ratio accuracy enhanced segmented images analyzed features extracted construct classification model three machine learning methods random forest support vector machine svm extreme gradient boosting xgboost utilized create classification models fls models optimized bayesian optimizer xgboost models achieved accuracies respectively svm model achieved highest accuracy results indicate method serves reliable tool automated identification fls enhancing efficiency precision soybean disease detection
"In the task of writer identification, although deep learning methods have provided good recognition performance, the black-box nature of these methods also limits their application scope, especially in scenarios that require more stable, reliable, and interpretable algorithms. Focusing on the writing characteristics of Chinese handwriting, we have designed a new edge description feature called directional pixel angle (abbreviated as DPA). The motivation for this feature comes from forensic handwriting analysis, where the long basic strokes in Chinese characters typically have a habitual inclination angle that is highly correlated with the writer’s identity. This paper focuses on four basic strokes in Chinese characters—horizontal, vertical, left-falling, and right-falling—and calculates the relative positional relationships of the edge pixels of these strokes. This paper design and extract first-order and second-order DPA features and apply them to writer identification tasks. Experimental results show that the writer identification performance based on this feature is comparable to that of mainstream CNN models and offers better interpretability.",task writer identification although deep learning methods provided good recognition performance black box nature methods also limits application scope especially scenarios require stable reliable interpretable algorithms focusing writing characteristics chinese handwriting designed new edge description feature called directional pixel angle abbreviated dpa motivation feature comes forensic handwriting analysis long basic strokes chinese characters typically habitual inclination angle highly correlated writer identity paper focuses four basic strokes chinese characters horizontal vertical left falling right falling calculates relative positional relationships edge pixels strokes paper design extract first order second order dpa features apply writer identification tasks experimental results show writer identification performance based feature comparable mainstream cnn models offers better interpretability
"Video captioning, which aims to generate natural language descriptions for video content, has made significant progress with the development of pretrained language models and multimodal learning. However, a persistent challenge remains in maintaining semantic consistency between generated captions and video content, often leading to inaccurate or contextually misaligned descriptions. This paper proposes a novel cross-modal contrastive learning framework to enhance semantic consistency in video captioning by improving the alignment between visual and textual representations. The proposed method incorporates a dual-branch contrastive learning strategy that refines feature extraction from both video and text modalities while enforcing a fine-grained semantic matching mechanism. Furthermore, we introduce a semantic consistency loss function to penalize mismatches between generated captions and their corresponding video content. To evaluate the effectiveness of our approach, extensive experiments are conducted on benchmark datasets, including MSR-VTT and ActivityNet Captions. The results demonstrate that our method significantly improves semantic alignment, outperforming state-of-the-art models in BLEU, METEOR, and CIDEr scores.",video captioning aims generate natural language descriptions video content made significant progress development pretrained language models multimodal learning however persistent challenge remains maintaining semantic consistency generated captions video content often leading inaccurate contextually misaligned descriptions paper proposes novel cross modal contrastive learning framework enhance semantic consistency video captioning improving alignment visual textual representations proposed method incorporates dual branch contrastive learning strategy refines feature extraction video text modalities enforcing fine grained semantic matching mechanism furthermore introduce semantic consistency loss function penalize mismatches generated captions corresponding video content evaluate effectiveness approach extensive experiments conducted benchmark datasets including msr vtt activitynet captions results demonstrate method significantly improves semantic alignment outperforming state art models bleu meteor cider scores
"Human motion synthesis plays a central role in film and animation, where motion quality influences both narrative coherence and perceptual realism. While data-driven deep learning models have shown promise in automating motion generation, they often lack biomechanical fidelity, leading to physically implausible results such as limb distortion and foot sliding. To address these challenges, we propose BCMG-Net, a Biomechanically Constrained Motion Generation Network that embeds anatomical and kinetic priors into a Transformer-based architecture. Our model integrates bone length preservation, dynamic smoothness, and energy efficiency constraints directly into the training objective, ensuring structural consistency and motion naturalness. Moreover, semantic control vectors enable context-aware generation for diverse cinematic actions. Experiments conducted on Human3.6 M, CMU MoCap, and a curated film motion dataset demonstrate that BCMG-Net outperforms state-of-the-art baselines across multiple biomechanical and perceptual metrics. Joint range heatmaps, center of mass trajectories, and motion embedding analyses further validate the physical coherence of the generated motion. These results establish BCMG-Net as a practical and principled framework for physically grounded motion synthesis in high-fidelity digital storytelling.",human motion synthesis plays central role film animation motion quality influences narrative coherence perceptual realism data driven deep learning models shown promise automating motion generation often lack biomechanical fidelity leading physically implausible results limb distortion foot sliding address challenges propose bcmg net biomechanically constrained motion generation network embeds anatomical kinetic priors transformer based architecture model integrates bone length preservation dynamic smoothness energy efficiency constraints directly training objective ensuring structural consistency motion naturalness moreover semantic control vectors enable context aware generation diverse cinematic actions experiments conducted human cmu mocap curated film motion dataset demonstrate bcmg net outperforms state art baselines across multiple biomechanical perceptual metrics joint range heatmaps center mass trajectories motion embedding analyses validate physical coherence generated motion results establish bcmg net practical principled framework physically grounded motion synthesis high fidelity digital storytelling
"Amidst the burgeoning advances in deep learning and artificial intelligence, strategic decision support systems for enterprises are witnessing transformative shifts. Leveraging sophisticated techniques in transfer learning and knowledge tracking algorithms, the precision and efficacy of managing enterprise risks are markedly enhanced. Consequently, this study introduces the T-DKVMN framework, an integration of Dynamic Memory Network (DKVMN) with transfer learning aimed at refining intelligent decision support systems for enterprises, particularly addressing challenges in risk identification and decision-making support. Initially, the framework harnesses pre-existing databases and labels for model pre-training, subsequently transferring the parameters from the pre-trained DKVMN model to the designated domain task via the transfer learning mechanism. This is followed by real-time updates and optimization of risk assessments using dynamic knowledge tracking. Post the pre-training and parameter transfer, the framework progresses to model training utilizing bespoke datasets and specific target labels, thus facilitating precise identification and management of enterprise risks. Experimental findings demonstrate the robust performance of the T-DKVMN framework across both public datasets and practical deployments, with risk assessment accuracy surpassing prevalent risk management methodologies. These results offer vital technical insights and a benchmark for future enhancements in intelligent enterprise decision support systems.",amidst burgeoning advances deep learning artificial intelligence strategic decision support systems enterprises witnessing transformative shifts leveraging sophisticated techniques transfer learning knowledge tracking algorithms precision efficacy managing enterprise risks markedly enhanced consequently study introduces dkvmn framework integration dynamic memory network dkvmn transfer learning aimed refining intelligent decision support systems enterprises particularly addressing challenges risk identification decision making support initially framework harnesses pre existing databases labels model pre training subsequently transferring parameters pre trained dkvmn model designated domain task via transfer learning mechanism followed real time updates optimization risk assessments using dynamic knowledge tracking post pre training parameter transfer framework progresses model training utilizing bespoke datasets specific target labels thus facilitating precise identification management enterprise risks experimental findings demonstrate robust performance dkvmn framework across public datasets practical deployments risk assessment accuracy surpassing prevalent risk management methodologies results offer vital technical insights benchmark future enhancements intelligent enterprise decision support systems
"Optoelectronic switching mechanisms are pivotal in on-chip interconnection communication, directly influencing the network’s latency and throughput. As system complexity and bandwidth demands increase, the integration of mixed optoelectronic circuit switching and optical packet switching flows has become essential, particularly in high-performance array processors. This paper investigates adaptive transmission strategies for such mixed data flows from three key perspectives. Firstly, we analyze the architecture and performance of an optoelectronic hybrid H-tree network, focusing on its ability to support high-bandwidth and low-latency communication within array processor environments. Secondly, we design a hybrid switching router capable of dynamically managing both circuit-switched and packet-switched traffic, optimizing resource allocation and network efficiency. Thirdly, we examine inter-cluster data exchange mechanisms, ensuring efficient and reliable communication between processing clusters across the hybrid network. Through these studies, we aim to provide insights into the design and optimization of adaptive optoelectronic hybrid switching for next-generation array processor interconnection networks.",optoelectronic switching mechanisms pivotal chip interconnection communication directly influencing network latency throughput system complexity bandwidth demands increase integration mixed optoelectronic circuit switching optical packet switching flows become essential particularly high performance array processors paper investigates adaptive transmission strategies mixed data flows three key perspectives firstly analyze architecture performance optoelectronic hybrid tree network focusing ability support high bandwidth low latency communication within array processor environments secondly design hybrid switching router capable dynamically managing circuit switched packet switched traffic optimizing resource allocation network efficiency thirdly examine inter cluster data exchange mechanisms ensuring efficient reliable communication processing clusters across hybrid network studies aim provide insights design optimization adaptive optoelectronic hybrid switching next generation array processor interconnection networks
"This study proposes a green logistics optimization model leveraging IoT data for route planning and energy efficiency in smart cities. The primary objective of this model is to address the traffic scheduling challenges in automated logistics transportation, enhance transportation efficiency, and minimize energy consumption. A novel model integrating the Sparrow Search Algorithm (SSA) and Bidirectional Gated Recurrent Unit (Bi-GRU) is proposed. SSA is first employed to optimize route planning, taking into account environmental factors such as traffic congestion, thereby providing a globally optimized initial solution. Subsequently, Bi-GRU adjusts the route in real-time according to historical data, including vehicle speed and cargo status. This integration fully exploits the complementary advantages of the two algorithms: SSA’s global optimization ability and Bi-GRU’s dynamic adjustment based on time-series information. Experimental results demonstrate that the application of this method can significantly reduce the unit transportation time of goods by 38.75% and the unit transportation energy consumption of goods by 23%. Finally, the paper explores the prospects of green logistics development based on Internet of Things technology in the development of smart cities, offering insights for future research and practical applications.",study proposes green logistics optimization model leveraging iot data route planning energy efficiency smart cities primary objective model address traffic scheduling challenges automated logistics transportation enhance transportation efficiency minimize energy consumption novel model integrating sparrow search algorithm ssa bidirectional gated recurrent unit gru proposed ssa first employed optimize route planning taking account environmental factors traffic congestion thereby providing globally optimized initial solution subsequently gru adjusts route real time according historical data including vehicle speed cargo status integration fully exploits complementary advantages two algorithms ssa global optimization ability gru dynamic adjustment based time series information experimental results demonstrate application method significantly reduce unit transportation time goods unit transportation energy consumption goods finally paper explores prospects green logistics development based internet things technology development smart cities offering insights future research practical applications
"Aiming at the low efficiency and accuracy of traditional methods in mask optimization of integrated circuits, a mask optimization algorithm based on deep learning is proposed. Firstly, an improved generation countermeasure network (GAN) model is designed. By introducing multi-scale feature fusion module and attention mechanism, the detail generation ability of mask pattern in lithography proximity effect correction is improved. Secondly, a dynamic weight adjustment strategy is proposed to adaptively balance the optimal weights in different regions during the training process. In the aspect of FPGA acceleration, a customized hardware architecture is designed for the computing bottleneck of the optimization algorithm. The experimental results show that, compared with the traditional OPC algorithm, this method significantly shortens the optimization time, improves the speedup ratio on the FPGA platform, and maintains a low edge placement error (EPE).",aiming low efficiency accuracy traditional methods mask optimization integrated circuits mask optimization algorithm based deep learning proposed firstly improved generation countermeasure network gan model designed introducing multi scale feature fusion module attention mechanism detail generation ability mask pattern lithography proximity effect correction improved secondly dynamic weight adjustment strategy proposed adaptively balance optimal weights different regions training process aspect fpga acceleration customized hardware architecture designed computing bottleneck optimization algorithm experimental results show compared traditional opc algorithm method significantly shortens optimization time improves speedup ratio fpga platform maintains low edge placement error epe
"Effective analysis of biomechanical motion data is crucial for predicting and preventing athletic injuries. Recent advancements in wearable sensor technologies offer extensive multimodal physiological datasets, yet integrating these complex data streams into accurate, interpretable predictive models remains challenging. In this work, we propose the BioSensor-Transformer, a novel Transformer-based deep learning architecture explicitly incorporating biomechanical constraints and multimodal sensor integration to predict injury risk during dynamic human movements. The model integrates inertial measurement units (IMUs), electromyography (EMG), and plantar pressure sensor data, enhanced by biomechanical constraints that promote stable joints, smooth muscle motion, and efficient energy use. Extensive experimental validation on benchmark-aligned synthetic datasets demonstrates that BioSensor-Transformer significantly outperforms state-of-the-art models, achieving superior predictive accuracy (AUC: 0.94, F-AUC: 0.83) and exhibiting physiologically plausible outputs. Moreover, our model provides enhanced interpretability through physiologically grounded attention mechanisms, enabling clear, actionable insights for clinicians. The proposed framework effectively bridges the gap between high predictive accuracy and biomechanical realism, providing substantial improvements for real-world injury prevention and rehabilitation applications.",effective analysis biomechanical motion data crucial predicting preventing athletic injuries recent advancements wearable sensor technologies offer extensive multimodal physiological datasets yet integrating complex data streams accurate interpretable predictive models remains challenging work propose biosensor transformer novel transformer based deep learning architecture explicitly incorporating biomechanical constraints multimodal sensor integration predict injury risk dynamic human movements model integrates inertial measurement units imus electromyography emg plantar pressure sensor data enhanced biomechanical constraints promote stable joints smooth muscle motion efficient energy use extensive experimental validation benchmark aligned synthetic datasets demonstrates biosensor transformer significantly outperforms state art models achieving superior predictive accuracy auc auc exhibiting physiologically plausible outputs moreover model provides enhanced interpretability physiologically grounded attention mechanisms enabling clear actionable insights clinicians proposed framework effectively bridges gap high predictive accuracy biomechanical realism providing substantial improvements real world injury prevention rehabilitation applications
"Accurate time-series classification (TSC) remains a fundamental challenge in deep learning due to the complexity and variability of temporal patterns. While recurrent neural networks (RNNs) such as LSTM and GRU have shown promise in modeling sequential dependencies, they often suffer from limitations like vanishing gradients and high computational cost when handling long sequences. To overcome these issues, convolutional neural networks (CNNs), particularly the Inception architecture, have emerged as powerful alternatives due to their ability to capture multiscale local patterns efficiently. In this study, we propose InceptionResNet, a hybrid deep learning framework that integrates the residual learning mechanism of ResNet into the InceptionTime architecture. By replacing the fully convolutional network (FCN) shortcut module in InceptionFCN with ResNet-50, the model gains deeper representational capacity and improved gradient flow during training. We conduct extensive experiments on the UCR-85 benchmark dataset, comparing our model against state-of-the-art approaches, including InceptionTime, InceptionFCN, ResNet, FCN, and MLP. The results show that InceptionResNet achieves superior accuracy on 49 of 85 datasets, demonstrating its robustness and effectiveness in handling diverse and complex time series data. This work highlights the potential of integrating multiscale feature extraction and deep residual learning to advance the performance of TSC models in practical applications.",accurate time series classification tsc remains fundamental challenge deep learning due complexity variability temporal patterns recurrent neural networks rnns lstm gru shown promise modeling sequential dependencies often suffer limitations like vanishing gradients high computational cost handling long sequences overcome issues convolutional neural networks cnns particularly inception architecture emerged powerful alternatives due ability capture multiscale local patterns efficiently study propose inceptionresnet hybrid deep learning framework integrates residual learning mechanism resnet inceptiontime architecture replacing fully convolutional network fcn shortcut module inceptionfcn resnet model gains deeper representational capacity improved gradient flow training conduct extensive experiments ucr benchmark dataset comparing model state art approaches including inceptiontime inceptionfcn resnet fcn mlp results show inceptionresnet achieves superior accuracy datasets demonstrating robustness effectiveness handling diverse complex time series data work highlights potential integrating multiscale feature extraction deep residual learning advance performance tsc models practical applications
"The availability of big data has significantly influenced the possibilities and methodological choices for conducting large-scale behavioural and social science research. In the context of qualitative data analysis, a major challenge is that conventional methods require intensive manual labour and are often impractical to apply to large datasets. One effective way to address this issue is by integrating emerging computational methods to overcome scalability limitations. However, a critical concern for researchers is the trustworthiness of results when machine learning and natural language processing tools are used to analyse such data. We argue that confidence in the credibility and robustness of results depends on adopting a ’human-in-the-loop’ methodology that is able to provide researchers with control over the analytical process, while retaining the benefits of using machine learning and natural language processing. With this in mind, we propose a novel methodological framework for computational grounded theory that supports the analysis of large qualitative datasets, while maintaining the rigour of established grounded theory methodologies. To illustrate the framework’s value, we present the results of testing it on a dataset collected from Reddit in a study aimed at understanding tutors’ experiences in the gig economy.",availability big data significantly influenced possibilities methodological choices conducting large scale behavioural social science research context qualitative data analysis major challenge conventional methods require intensive manual labour often impractical apply large datasets one effective way address issue integrating emerging computational methods overcome scalability limitations however critical concern researchers trustworthiness results machine learning natural language processing tools used analyse data argue confidence credibility robustness results depends adopting human loop methodology able provide researchers control analytical process retaining benefits using machine learning natural language processing mind propose novel methodological framework computational grounded theory supports analysis large qualitative datasets maintaining rigour established grounded theory methodologies illustrate framework value present results testing dataset collected reddit study aimed understanding tutors experiences gig economy
"During the past two decades across New South Wales and Queensland, koala populations have been in steep decline. Despite being nationally listed as Endangered in 2022, populations have not been stabilised and threats to the long-term survival of koalas remain. Climate change predictions suggest that human-occupied coastal habitats may offer greater protection to koalas from natural disasters such as drought and wildfires, presenting a role for people to play in koala conservation. However, the design of conservation programs is rarely informed by the affected audiences themselves, lowering their probability for success. Acknowledging the need to centre approaches on people who don’t typically think about conservation, this study applied the 5-step design thinking process, a participatory research method focused on driving innovation. A one-day design thinking workshop was conducted in September 2024. Seven design teams were formed and tasked with developing an initiative to support the reporting of koala sightings through citizen science. Each team included participants with diverse experience and expertise, ensuring balanced representation across roles. During the design process, teams received feedback from a citizen jury before presenting final pitches. Citizen jurors were individuals with no prior experience in citizen science or koala conservation. Every participant was given a budget to invest in their favourite design. The top three citizen science program concepts featured customised apps with functionality such as auto-recording koalas, sharing information with other users, and fostering social connectivity. The juries’ selections focused on business partnerships, incentives, and tailored information for users. Notably, the jury selections did not overlap with the top three designs chosen by workshop participants. The inclusion of a citizen jury in a one-day design thinking workshop is a novel contribution. The utility of design thinking as a participatory method is discussed alongside the study’s limitations, offering avenues for future research.",past two decades across new south wales queensland koala populations steep decline despite nationally listed endangered populations stabilised threats long term survival koalas remain climate change predictions suggest human occupied coastal habitats may offer greater protection koalas natural disasters drought wildfires presenting role people play koala conservation however design conservation programs rarely informed affected audiences lowering probability success acknowledging need centre approaches people typically think conservation study applied step design thinking process participatory research method focused driving innovation one day design thinking workshop conducted september seven design teams formed tasked developing initiative support reporting koala sightings citizen science team included participants diverse experience expertise ensuring balanced representation across roles design process teams received feedback citizen jury presenting final pitches citizen jurors individuals prior experience citizen science koala conservation every participant given budget invest favourite design top three citizen science program concepts featured customised apps functionality auto recording koalas sharing information users fostering social connectivity juries selections focused business partnerships incentives tailored information users notably jury selections overlap top three designs chosen workshop participants inclusion citizen jury one day design thinking workshop novel contribution utility design thinking participatory method discussed alongside study limitations offering avenues future research
"In this study, reliability methods were demonstrated as a promising approach in medical engineering by identifying the most significant muscle forces affecting femoral stress. First, the finite element method (FEM) in Abaqus software was used to model the effects of 10 muscle and joint forces across various regions of the femur. Then, using the response surface methodology (RSM), and examining the effect coefficients of each joint and muscle force, the hip joint reaction force with an impact coefficient of 210.97 was identified as the most effective force on bone stress. After that, the gluteus minimus and gluteus medius muscle forces were ranked second and third in terms of stress effect with coefficients of 66.6 and 34.47. This study showed that the anterior femoral muscles have a significant effect on stress compared to the posterior femoral muscles. RSM enables faster and more precise identification of joint and muscle forces influencing femoral stresses compared to conventional methods. This innovative approach not only increased the understanding of biomechanical phenomena, but also provided a more efficient tool for investigating and optimizing such processes in biomedical engineering applications.",study reliability methods demonstrated promising approach medical engineering identifying significant muscle forces affecting femoral stress first finite element method fem abaqus software used model effects muscle joint forces across various regions femur using response surface methodology rsm examining effect coefficients joint muscle force hip joint reaction force impact coefficient identified effective force bone stress gluteus minimus gluteus medius muscle forces ranked second third terms stress effect coefficients study showed anterior femoral muscles significant effect stress compared posterior femoral muscles rsm enables faster precise identification joint muscle forces influencing femoral stresses compared conventional methods innovative approach increased understanding biomechanical phenomena also provided efficient tool investigating optimizing processes biomedical engineering applications
"Reinforcement learning (RL) has significant potential across various fields, with its application in sports decision optimization emerging as a prominent research focus amid rapid advances in AI technologies. This paper develops tennisDecisionRL, an RL-driven decision optimization framework for tennis competitions that captures critical match dynamics through the integration of RL techniques such as Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO). The proposed system enhances the sophistication and adaptability of competitive decision-making processes. Empirical evaluations using two datasets demonstrate its superior performance compared to traditional approaches. The paper concludes by identifying current limitations and outlining future research directions, providing novel insights for decision optimization in tennis and other athletic competitions.",reinforcement learning significant potential across various fields application sports decision optimization emerging prominent research focus amid rapid advances technologies paper develops tennisdecisionrl driven decision optimization framework tennis competitions captures critical match dynamics integration techniques deep networks dqn proximal policy optimization ppo proposed system enhances sophistication adaptability competitive decision making processes empirical evaluations using two datasets demonstrate superior performance compared traditional approaches paper concludes identifying current limitations outlining future research directions providing novel insights decision optimization tennis athletic competitions
"The inspection of railway infrastructure faces significant challenges due to heterogeneous environmental conditions and non-uniform illumination patterns, leading to suboptimal detection performance in conventional robotic systems. This study develops a multi-stage image enhancement pipeline incorporating adaptive target segmentation and stereoscopic correspondence matching. A cross-sensor calibration protocol establishes precise spatial coordinates for defect localization through binocular disparity analysis. The proposed framework integrates an enhanced YOLOv5 architecture with context-aware attention modules, developing a hierarchical feature learning architecture that combines pyramidal representation with bidirectional multi-scale feature fusion layers. Experimental validation demonstrates 91.5% precision in fastener absence detection with optimized computational efficiency, indicating substantial improvements in automated rail defect diagnostics compared to baseline systems.",inspection railway infrastructure faces significant challenges due heterogeneous environmental conditions non uniform illumination patterns leading suboptimal detection performance conventional robotic systems study develops multi stage image enhancement pipeline incorporating adaptive target segmentation stereoscopic correspondence matching cross sensor calibration protocol establishes precise spatial coordinates defect localization binocular disparity analysis proposed framework integrates enhanced yolov architecture context aware attention modules developing hierarchical feature learning architecture combines pyramidal representation bidirectional multi scale feature fusion layers experimental validation demonstrates precision fastener absence detection optimized computational efficiency indicating substantial improvements automated rail defect diagnostics compared baseline systems
"The integration of large-scale regional water-wind-solar hybrid energy systems poses challenges to power grid stability due to persistent fluctuations that conventional automatic generation control (AGC) systems struggle to mitigate effectively. To address this issue and optimize frequency modulation resource utilization, this study presents a bidirectional communication-based AGC optimization strategy. The proposed approach enhances reinforcement learning algorithms through a dual-estimation framework, enabling dynamic power distribution among generation units. Simultaneously, the methodology incorporates coordinated grid power flow adjustments to achieve integrated uncertainty modeling and coordinated optimization for regional hydro-wind power systems. Experimental validation demonstrates that the enhanced control strategy achieves an improvement of 2.2%–5.8% in Control Performance Standard (CPS) metrics compared with conventional methods, confirming superior system regulation capability.",integration large scale regional water wind solar hybrid energy systems poses challenges power grid stability due persistent fluctuations conventional automatic generation control agc systems struggle mitigate effectively address issue optimize frequency modulation resource utilization study presents bidirectional communication based agc optimization strategy proposed approach enhances reinforcement learning algorithms dual estimation framework enabling dynamic power distribution among generation units simultaneously methodology incorporates coordinated grid power flow adjustments achieve integrated uncertainty modeling coordinated optimization regional hydro wind power systems experimental validation demonstrates enhanced control strategy achieves improvement control performance standard cps metrics compared conventional methods confirming superior system regulation capability
"This qualitative study explored how teachers in a university-based K–8 enrichment program perceived creativity and fostered creative thinking in their classrooms. Twelve enrichment program teachers participated in interviews examining their views on creativity and teaching strategies used to foster creative skills. Findings revealed teachers valued creativity and believed all students have creative potential, although developmental differences shaped approaches. Younger students tended to be approached with more teacher-facilitated creativity boosting activities like brainstorming and questioning. Older students required less guidance to demonstrate creative behaviors. All teachers established a supportive climate and offered open-ended activities for creative expression. The enrichment context allowed greater freedom in teaching creatively compared to traditional classrooms. Findings also suggest experienced K–12 teachers relied more on teacher-led instruction, whereas those without formal teacher training used more student-driven approaches to spark students’ creativity.",qualitative study explored teachers university based enrichment program perceived creativity fostered creative thinking classrooms twelve enrichment program teachers participated interviews examining views creativity teaching strategies used foster creative skills findings revealed teachers valued creativity believed students creative potential although developmental differences shaped approaches younger students tended approached teacher facilitated creativity boosting activities like brainstorming questioning older students required less guidance demonstrate creative behaviors teachers established supportive climate offered open ended activities creative expression enrichment context allowed greater freedom teaching creatively compared traditional classrooms findings also suggest experienced teachers relied teacher led instruction whereas without formal teacher training used student driven approaches spark students creativity
"Fluency in mathematics has several implications for success in future mathematics courses, daily living skills, and future employment opportunities. The current single-case design study explored the effects of mathematics games on student multiplication or subtraction fluency. Three students with developmental disabilities completed the intervention in which they played a digital mathematical game for 10 minutes and researchers explored the effect on student fluency, operationalized as correct digits per minute (cdpm). While researchers were unable to determine a functional relation between the game and students’ cdpm, the three students who completed the study were successful in increasing their fluency and maintaining their fluency higher than baseline levels. Students reported they enjoyed playing the game and would rather play math games than complete worksheets.",fluency mathematics several implications success future mathematics courses daily living skills future employment opportunities current single case design study explored effects mathematics games student multiplication subtraction fluency three students developmental disabilities completed intervention played digital mathematical game minutes researchers explored effect student fluency operationalized correct digits per minute cdpm researchers unable determine functional relation game students cdpm three students completed study successful increasing fluency maintaining fluency higher baseline levels students reported enjoyed playing game would rather play math games complete worksheets
"This paper presents a liver image segmentation method based on the EABRDeNet model, which incorporates the architectural concept of U-Net. In the encoding stage, EfficientNet-B0 serves as the backbone network, combined with the SEBlock mechanism and residual blocks to enhance the extraction of key liver features and address gradient-related issues. In the decoding stage, deconvolution and upsampling are used for precise segmentation. Liver images from open-source websites are pre-processed and data-augmented to construct the G dataset. The EABRDeNet model is trained on this dataset, and segmentation results are obtained through the weight-sharing mechanism. To verify the model’s effectiveness, comparative experiments are conducted on the G dataset with U-Net and U-Net + Dice_Focal_Loss. Metrics such as accuracy, loss, and Dice coefficient are used for evaluation. The experimental results show that the EABRDeNet model outperforms the other two models. Specifically, on the training set, the EABRDeNet model has an average loss of 0.0025979, an average accuracy of 0.9876249, and an average Dice coefficient of 0.985658. On the test set, the average loss is 0.0029967, the average accuracy is 0.987649, and the average Dice coefficient is 0.9845206. In contrast, the U-Net model and U-Net + Dice_Focal_Loss model have relatively higher losses and lower accuracies and Dice coefficients, indicating that the EABRDeNet model has better performance and stability in liver image segmentation tasks.",paper presents liver image segmentation method based eabrdenet model incorporates architectural concept net encoding stage efficientnet serves backbone network combined seblock mechanism residual blocks enhance extraction key liver features address gradient related issues decoding stage deconvolution upsampling used precise segmentation liver images open source websites pre processed data augmented construct dataset eabrdenet model trained dataset segmentation results obtained weight sharing mechanism verify model effectiveness comparative experiments conducted dataset net net dice focal loss metrics accuracy loss dice coefficient used evaluation experimental results show eabrdenet model outperforms two models specifically training set eabrdenet model average loss average accuracy average dice coefficient test set average loss average accuracy average dice coefficient contrast net model net dice focal loss model relatively higher losses lower accuracies dice coefficients indicating eabrdenet model better performance stability liver image segmentation tasks
"With the rapid advancement of technology, the application of augmented reality (AR) in the field of education has become increasingly widespread, especially in the field of interior design education, where its potential is gradually being recognized and utilized. This study aims to explore the innovative applications of AR technology in interior design education and its impact on learner experience, with a special focus on feature matching optimization for AR-based interior design educational scenarios and virtual fusion display technology tailored for these scenarios. Through an in-depth analysis of the current application status and challenges of AR technology in interior design education, this study identifies that while AR technology offers new possibilities for teaching, there are still deficiencies in matching teaching content and methods, limitations of technological application, and the depth and breadth of learner experience. To address these issues, the study proposes optimization strategies and validates their effectiveness in enhancing teaching outcomes and learning experiences through experimentation. This paper not only expands the research on the application of AR technology in interior design education but also offers new ideas and methods to improve the quality of education and learning efficiency.",rapid advancement technology application augmented reality field education become increasingly widespread especially field interior design education potential gradually recognized utilized study aims explore innovative applications technology interior design education impact learner experience special focus feature matching optimization based interior design educational scenarios virtual fusion display technology tailored scenarios depth analysis current application status challenges technology interior design education study identifies technology offers new possibilities teaching still deficiencies matching teaching content methods limitations technological application depth breadth learner experience address issues study proposes optimization strategies validates effectiveness enhancing teaching outcomes learning experiences experimentation paper expands research application technology interior design education also offers new ideas methods improve quality education learning efficiency
"Amorphous soft magnetic materials are ideal for high-frequency applications due to their low coercivity, high permeability, and reduced core loss. However, optimizing packing density while maintaining superior magnetic properties remains a challenge, particularly for high-efficiency magnetic cores in electric vehicles and renewable energy systems. This study addresses this challenge by integrating discrete element method simulations, machine learning, and experimental validations to optimize powder packing density and evaluate its impact on magnetic properties. Tri-modal amorphous powders were mixed at various ratios, achieving over 90% relative density with derived ratios by machine learning, significantly outperforming conventional trail-and-error methods. Enhanced core density improved magnetic properties, including a 49.7% reduction in coercivity and an 8.72% increase in saturation magnetization. These improvements were attributed to reduced porosity and optimized compaction strategies. Core loss analysis further demonstrated lower hysteresis and eddy current losses in high-density cores.",amorphous soft magnetic materials ideal high frequency applications due low coercivity high permeability reduced core loss however optimizing packing density maintaining superior magnetic properties remains challenge particularly high efficiency magnetic cores electric vehicles renewable energy systems study addresses challenge integrating discrete element method simulations machine learning experimental validations optimize powder packing density evaluate impact magnetic properties tri modal amorphous powders mixed various ratios achieving relative density derived ratios machine learning significantly outperforming conventional trail error methods enhanced core density improved magnetic properties including reduction coercivity increase saturation magnetization improvements attributed reduced porosity optimized compaction strategies core loss analysis demonstrated lower hysteresis eddy current losses high density cores
"The development of characteristic towns plays a crucial role in promoting rural revitalization, advancing China’s new urbanization efforts, and increasing farmers’ incomes. The integration of culture and tourism can enhance the appeal of tourist destinations while preserving local culture, mitigating the impact of tourism seasonality, and fostering the high-quality, sustainable development of the tourism industry. This study analyzes the key factors influencing the strengths, weaknesses, opportunities, and threats (SWOT) of cultural and tourism integration in the characteristic town of Conghua, Guangzhou. By applying the Analytic Hierarchy Process (AHP), the study calculates the weights of each influencing factor. Additionally, through a questionnaire survey, the strength values of these factors are derived, and the overall factor strengths are computed based on both the weights and strength values. A countermeasure quadrilateral is then constructed to determine the optimal development strategy for cultural and tourism integration in the town. The results indicate that the total intensity of opportunities is 1.493, the total intensity of advantages is 1.274, the total intensity of disadvantages is −0.292, and the total intensity of threats is −0.288. The conclusion shows that the integration of culture and tourism in small towns with Chinese characteristics should adopt the development strategy of quality growth.",development characteristic towns plays crucial role promoting rural revitalization advancing china new urbanization efforts increasing farmers incomes integration culture tourism enhance appeal tourist destinations preserving local culture mitigating impact tourism seasonality fostering high quality sustainable development tourism industry study analyzes key factors influencing strengths weaknesses opportunities threats swot cultural tourism integration characteristic town conghua guangzhou applying analytic hierarchy process ahp study calculates weights influencing factor additionally questionnaire survey strength values factors derived overall factor strengths computed based weights strength values countermeasure quadrilateral constructed determine optimal development strategy cultural tourism integration town results indicate total intensity opportunities total intensity advantages total intensity disadvantages total intensity threats conclusion shows integration culture tourism small towns chinese characteristics adopt development strategy quality growth
"Based on a simulation testing platform, the correlation between the main vehicle’s following speed, following distance, and other parameters under sudden traffic conditions is analyzed. An experimental scenario of “front vehicle stationary, preceding vehicle suddenly cutting out, and main vehicle decelerating to avoid collision” is designed. Through the collection and analysis of experimental data, the data in the simulation process is tracked, and the response performance of the main vehicle under different parameter conditions is evaluated. Latin hypercube sampling generalization tests are used to further explore and verify the applicability of the findings. The research indicates that the following speed and following distance exhibit a quadratic function relationship. Reasonably optimized following speed and following distance can significantly enhance vehicle driving safety.",based simulation testing platform correlation main vehicle following speed following distance parameters sudden traffic conditions analyzed experimental scenario front vehicle stationary preceding vehicle suddenly cutting main vehicle decelerating avoid collision designed collection analysis experimental data data simulation process tracked response performance main vehicle different parameter conditions evaluated latin hypercube sampling generalization tests used explore verify applicability findings research indicates following speed following distance exhibit quadratic function relationship reasonably optimized following speed following distance significantly enhance vehicle driving safety
"In the context of the rapid integration of Software-Defined Networking (SDN) and the Internet of Things (IoT) in the film industry, this study explores the value reconstruction of film evaluation. The convergence of SDN and IoT has brought new opportunities and challenges to the film industry. We focus on the emotional information extraction in films to provide a novel perspective for intelligent film evaluation. Firstly, leveraging speech recognition and natural language processing techniques, we utilize the BERT model to encode the text data of lines, followed by the Bi-GRU model for emotion classification. The results demonstrate that the recognition accuracy for exciting, sadness, and angry emotions exceeds 80%. Through comparisons with RNN, LSTM, and BI-LSTM, the BERT-Bi-GRU method shows higher precision in intelligent movie emotion extraction and enables intelligent evaluation. This approach not only offers new insights for the development of the film industry but also provides technical support for the standardization of the film production process in the era of SDN-IoT integration.",context rapid integration software defined networking sdn internet things iot film industry study explores value reconstruction film evaluation convergence sdn iot brought new opportunities challenges film industry focus emotional information extraction films provide novel perspective intelligent film evaluation firstly leveraging speech recognition natural language processing techniques utilize bert model encode text data lines followed gru model emotion classification results demonstrate recognition accuracy exciting sadness angry emotions exceeds comparisons rnn lstm lstm bert gru method shows higher precision intelligent movie emotion extraction enables intelligent evaluation approach offers new insights development film industry also provides technical support standardization film production process sdn iot integration
"This study examines the influence of Stephan blowing and surface tension gradient on Darcy-Forchheimer of dusty Ellis fluid over a Riga plate with Stephan blowing impacts and porous medium. In the occurrence of Cattaneo-Christov mass and heat flux, mathematical modeling and analysis have been performed. This model is useful for optimizing the design of systems that involve non-Newtonian fluids, like oil drilling, polymer extrusion, and chemical reactors. To optimize thermal management in high-performance machinery, it aids in a more precise knowledge of heat and mass transport mechanisms. This model can be applied to environmental processes such as the dispersion of pollutants in the atmosphere or the cooling of electronic equipment, where fluid dynamics and heat regulation play crucial roles, by incorporating dust particles and surface tension gradients. For both liquid and dust particle phases, the governing non-linear PDEs (partial differential equations) are composed of the principles of momentum, mass, energy, and solutal conservation. By using the similarity variable, PDEs can be transmuted into ODEs (ordinary differential equations). The numerical results of the governing equations are obtained using the shooting approach (RKF-45th). The skin friction, fluid and dust velocity distributions grow as the value of the Stephen blowing parameter upsurges while decline the rate of heat and mass transmission.",study examines influence stephan blowing surface tension gradient darcy forchheimer dusty ellis fluid riga plate stephan blowing impacts porous medium occurrence cattaneo christov mass heat flux mathematical modeling analysis performed model useful optimizing design systems involve non newtonian fluids like oil drilling polymer extrusion chemical reactors optimize thermal management high performance machinery aids precise knowledge heat mass transport mechanisms model applied environmental processes dispersion pollutants atmosphere cooling electronic equipment fluid dynamics heat regulation play crucial roles incorporating dust particles surface tension gradients liquid dust particle phases governing non linear pdes partial differential equations composed principles momentum mass energy solutal conservation using similarity variable pdes transmuted odes ordinary differential equations numerical results governing equations obtained using shooting approach rkf skin friction fluid dust velocity distributions grow value stephen blowing parameter upsurges decline rate heat mass transmission
"Generative AI technology’s fast expansion is driving increasing application in visual creation. This work presents an artificial intelligence generative visual creativity system (KRGVS) with knowledge retrieval to automate user intent understanding and high-quality visual content creation. Its key uniqueness is the ability of the KRGVS system to extract deeper purpose from confused user inputs, call suitable knowledge resources, and produce innovative and valuable visual creations based on a semantically enriched generative model. Experimental results imply that the KRGVS system can use cross-modally knowledge to improve visual content creativity, logic, and explanatory power, generation quality, and user customisation. This paper provides a new data-driven and intelligent aid direction for the creative industry and supports the change of AI from a content generating tool to a human-machine collaborative creativity partner.",generative technology fast expansion driving increasing application visual creation work presents artificial intelligence generative visual creativity system krgvs knowledge retrieval automate user intent understanding high quality visual content creation key uniqueness ability krgvs system extract deeper purpose confused user inputs call suitable knowledge resources produce innovative valuable visual creations based semantically enriched generative model experimental results imply krgvs system use cross modally knowledge improve visual content creativity logic explanatory power generation quality user customisation paper provides new data driven intelligent aid direction creative industry supports change content generating tool human machine collaborative creativity partner
"The existing power supply and communication system for indoor wheeled patrol robots relies on magnetic coupling mechanisms, but magnetic coupling mechanisms rarely meet the stability requirements of the system. In response to the challenge, a magnetic resonance-based wireless power and signal transmission system for robots is designed in this paper. It proposes transmitter coil switching control technology and receiver continuous energy harvesting technology, and establishes a communication model for one-to-many reception under different system states during motion. Based on this, a mathematical model for optimizing the structure of the magnetic coupling mechanism among transmitter and receiver coils was established. Optimal parameter values were determined using an enhanced particle swarm optimization algorithm (EAPSO). Experimental outcome implies that the transmission efficiency of the proposed method is 87.9%, representing an improvement of 4.8%–20.5%, and it can achieve efficient system stability control.",existing power supply communication system indoor wheeled patrol robots relies magnetic coupling mechanisms magnetic coupling mechanisms rarely meet stability requirements system response challenge magnetic resonance based wireless power signal transmission system robots designed paper proposes transmitter coil switching control technology receiver continuous energy harvesting technology establishes communication model one many reception different system states motion based mathematical model optimizing structure magnetic coupling mechanism among transmitter receiver coils established optimal parameter values determined using enhanced particle swarm optimization algorithm eapso experimental outcome implies transmission efficiency proposed method representing improvement achieve efficient system stability control
"Under the background of big data, it is increasingly urgent for the field of psychological education to accurately track students’ knowledge. Aiming at the issue that existing studies ignore the internal correlation between exercises and knowledge points, this paper firstly improves Long Short-Term Memory network (MELSTM) based on memory extension, and then constructed the psychoeducational Knowledge Graph (KG). Word2Vec and bidirectional MELSTM were used to convert the exercise response sequences into low-dimensional dense vectors, and KG embedding representation was carried out by TransR model. The influence degree of precursor knowledge on prediction results was explored through attention mechanism. Finally, the prediction results are obtained through the fully connected network. Experiments on three education datasets show that the AUC values of the proposed model are improved by at least 8.6%, 3.4% and 5.5%, which can track students’ knowledge status more accurately.",background big data increasingly urgent field psychological education accurately track students knowledge aiming issue existing studies ignore internal correlation exercises knowledge points paper firstly improves long short term memory network melstm based memory extension constructed psychoeducational knowledge graph word vec bidirectional melstm used convert exercise response sequences low dimensional dense vectors embedding representation carried transr model influence degree precursor knowledge prediction results explored attention mechanism finally prediction results obtained fully connected network experiments three education datasets show auc values proposed model improved least track students knowledge status accurately
"This introductory essay to the special issue, “Cosmic Imperatives: Critical Thinking Beyond the Earthbound”, navigates differences between its co-authors/co-editors to ask what critical and political imperatives emerge from human technological engagements with the nonterrestrial cosmos, prompted by coalescing corporate and state-led plans for the colonization of other cosmic places. Working from the disciplines of geography and anthropology and from different – and in some respects, even opposed – theoretical starting places, we explore the tensions that arise among competing critical materialisms when the radical differences of nonterrestrial places form the contexts for social, political-economic, and human-nonhuman relations and relationality. While we do not aim for a synthesis of our positions, we co-build an argument that the material differences, unpredictable open-endedness, and variability of other cosmic places demand innovation and transformation of critical thought that has yet to be fully engaged in cultural geography.",introductory essay special issue cosmic imperatives critical thinking beyond earthbound navigates differences authors editors ask critical political imperatives emerge human technological engagements nonterrestrial cosmos prompted coalescing corporate state led plans colonization cosmic places working disciplines geography anthropology different respects even opposed theoretical starting places explore tensions arise among competing critical materialisms radical differences nonterrestrial places form contexts social political economic human nonhuman relations relationality aim synthesis positions build argument material differences unpredictable open endedness variability cosmic places demand innovation transformation critical thought yet fully engaged cultural geography
"High-speed railways are highly sensitive to mining-induced subsidence, making the identification of goafs along railway corridors and impact assessment crucial for construction and operation. This study investigates the Weiyuan section of the Chengdu-Kunming High-Speed Railway traversing a low-mountain coal mining area. An integrated approach combining historical data, drilling surveys, InSAR monitoring, and theoretical modeling was used to map mineral resources, mining activities, goafs, and surface deformations. Then, an evaluation framework for assessing the potential impact of goaf on railway engineering stability is established, thereby evaluating the potential threat of goaf. Findings indicate that the overlying thick-bedded sandstone layer (∼129 m) above the goaf significantly mitigates the transmission of deformation from the goaf to the surface. InSAR monitoring spanning from November 2016 to April 2024 reveal that historical mining caused minor subsidence (&lt;10 mm/yr), with current deformation rates generally within ±10 mm/yr. Empirical calculations indicate a safe mining depth threshold of 96 m, while actual depths significantly exceed this value, posing minimal railway risks. By calculating the influence angle of the subsidence basin and analyzing its relationship with the railway alignment, the study identifies that the direct impact of goafs is primarily concentrated in the Bakubutian coal mine areas. Based on these findings, targeted risk mitigation strategies are proposed to ensure the long-term safety and stability of railway engineering. This study provides scientific support for the Chengdu-Kunming Railway’s safety and offers transferable methodologies for railway projects in similar mining-affected regions, emphasizing the importance of integrated geological-engineering assessments in infrastructure planning.",high speed railways highly sensitive mining induced subsidence making identification goafs along railway corridors impact assessment crucial construction operation study investigates weiyuan section chengdu kunming high speed railway traversing low mountain coal mining area integrated approach combining historical data drilling surveys insar monitoring theoretical modeling used map mineral resources mining activities goafs surface deformations evaluation framework assessing potential impact goaf railway engineering stability established thereby evaluating potential threat goaf findings indicate overlying thick bedded sandstone layer goaf significantly mitigates transmission deformation goaf surface insar monitoring spanning november april reveal historical mining caused minor subsidence current deformation rates generally within empirical calculations indicate safe mining depth threshold actual depths significantly exceed value posing minimal railway risks calculating influence angle subsidence basin analyzing relationship railway alignment study identifies direct impact goafs primarily concentrated bakubutian coal mine areas based findings targeted risk mitigation strategies proposed ensure long term safety stability railway engineering study provides scientific support chengdu kunming railway safety offers transferable methodologies railway projects similar mining affected regions emphasizing importance integrated geological engineering assessments infrastructure planning
"A novel fault diagnosis approach for rotor systems experiencing pedestal looseness is introduced, utilizing complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) to address this mechanical issue. In cases of pedestal looseness, rotor system vibration signals predominantly consist of three constituent elements: periodic components reflecting rotational frequency and harmonic content, transient impact components carrying fault characteristics, and stochastic noise interference. The CEEMDAN technique is employed to isolate and enhance the diagnostically critical impact elements within these complex vibration signals, followed by application of envelope demodulation analysis to process the extracted transient features. The diagnostic identification of pedestal looseness is ultimately achieved through characteristic periodicity detection in the resulting envelope spectrum. Validation through both simulated signal analysis and practical experimental investigations confirms the technical efficacy of this methodology for detecting pedestal looseness faults in rotating machinery rotor systems, maintaining fidelity to original operational data throughout the analytical process.",novel fault diagnosis approach rotor systems experiencing pedestal looseness introduced utilizing complete ensemble empirical mode decomposition adaptive noise ceemdan address mechanical issue cases pedestal looseness rotor system vibration signals predominantly consist three constituent elements periodic components reflecting rotational frequency harmonic content transient impact components carrying fault characteristics stochastic noise interference ceemdan technique employed isolate enhance diagnostically critical impact elements within complex vibration signals followed application envelope demodulation analysis process extracted transient features diagnostic identification pedestal looseness ultimately achieved characteristic periodicity detection resulting envelope spectrum validation simulated signal analysis practical experimental investigations confirms technical efficacy methodology detecting pedestal looseness faults rotating machinery rotor systems maintaining fidelity original operational data throughout analytical process
"With the rapid development of vocational education, there is an increasing demand for intelligent systems that can provide real-time feedback and personalized guidance in practical skills training. Traditional training methods relying on repetitive practice and instructor supervision are inefficient and lack precision. To address these challenges, this paper proposes DualStreamNet (DSNet), an innovative action recognition method designed to enhance the quality of practical teaching in higher vocational education. DSNet integrates spatial and temporal convolutions within a unified framework, enabling simultaneous modeling of skeleton data’s spatial topology and temporal dynamics. Additionally, an uncertainty estimation module is introduced to evaluate prediction credibility, improving the model’s robustness in handling ambiguous actions. Experimental results demonstrate that DSNet achieves superior accuracy and reliability compared to existing methods, providing a solid foundation for developing intelligent feedback systems in vocational education.",rapid development vocational education increasing demand intelligent systems provide real time feedback personalized guidance practical skills training traditional training methods relying repetitive practice instructor supervision inefficient lack precision address challenges paper proposes dualstreamnet dsnet innovative action recognition method designed enhance quality practical teaching higher vocational education dsnet integrates spatial temporal convolutions within unified framework enabling simultaneous modeling skeleton data spatial topology temporal dynamics additionally uncertainty estimation module introduced evaluate prediction credibility improving model robustness handling ambiguous actions experimental results demonstrate dsnet achieves superior accuracy reliability compared existing methods providing solid foundation developing intelligent feedback systems vocational education
"In response to environmental challenges and the demand for sustainability, this study explores a novel engineering structure, harnessing the potential of bio-based materials within the framework of composite sandwich structures. This investigation employs finite element modeling to assess sandwich structures composed of End-grain balsa wood and fiber-reinforced polymer (FRP) facesheets. These facesheets incorporate glass, carbon, and basalt fibers, enabling a direct comparison between conventional and bio-based materials. Mechanical responses are evaluated under numerical flexural loading using Abaqus/Implicit, with a specialized wood material model integrated via a User Material (UMAT) subroutine. A 2D Hashin failure criterion assesses FRP facesheets. Intriguingly, findings indicate minimal influence from FRP on structural performance, while balsa wood and the core-casings interface emerge as decisive factors.",response environmental challenges demand sustainability study explores novel engineering structure harnessing potential bio based materials within framework composite sandwich structures investigation employs finite element modeling assess sandwich structures composed end grain balsa wood fiber reinforced polymer frp facesheets facesheets incorporate glass carbon basalt fibers enabling direct comparison conventional bio based materials mechanical responses evaluated numerical flexural loading using abaqus implicit specialized wood material model integrated via user material umat subroutine hashin failure criterion assesses frp facesheets intriguingly findings indicate minimal influence frp structural performance balsa wood core casings interface emerge decisive factors
"Polyacrylonitrile-based carbon fibers (PANCFs) have revolutionized industries since the 1960s due to their superior properties and applications. However, a significant gap remains between their performance and theoretical potential, highlighting the urgent need to enhance our understanding of the process-structure-performance relationship. Computational simulations, with their ability to provide analysis from the atomic level to higher-scale, are essential for bridging this gap. This review provides a comprehensive overview of advancements in computational simulation techniques to produce high-performance PANCFs by optimizing the process parameters through simulations. Furthermore, advancements in reactive molecular dynamics, density functional theory, atomistic modelling, and finite element methods to enhance the PANCFs manufacturing process are systematically evaluated. Simulations play an important role in developing PANCFs by identifying novel comonomers for PAN precursors, evaluating different solvents during spinning, precise tracking of cyclization and dehydrogenation mechanisms during stabilization, and predicting mechanical property losses due to defects. Moreover, it is demonstrated that how kinetics-driven frameworks accelerate carbonization simulations by combining atomic-scale interactions such as carbon ring formation and graphitic growth with macroscale process parameters like temperature and pressure. However, certain limitations remain: unresolved heterogeneous microstructure representation, multiscale disconnects between atomic bond-breaking and macroscopic fiber evolution, and validation barriers due to oversimplified quasi-2D models. To overcome these problems, possible future directions including advanced force fields, multiscale integration, and AI-driven modeling could enhance the performance of PANCFs.",polyacrylonitrile based carbon fibers pancfs revolutionized industries since due superior properties applications however significant gap remains performance theoretical potential highlighting urgent need enhance understanding process structure performance relationship computational simulations ability provide analysis atomic level higher scale essential bridging gap review provides comprehensive overview advancements computational simulation techniques produce high performance pancfs optimizing process parameters simulations furthermore advancements reactive molecular dynamics density functional theory atomistic modelling finite element methods enhance pancfs manufacturing process systematically evaluated simulations play important role developing pancfs identifying novel comonomers pan precursors evaluating different solvents spinning precise tracking cyclization dehydrogenation mechanisms stabilization predicting mechanical property losses due defects moreover demonstrated kinetics driven frameworks accelerate carbonization simulations combining atomic scale interactions carbon ring formation graphitic growth macroscale process parameters like temperature pressure however certain limitations remain unresolved heterogeneous microstructure representation multiscale disconnects atomic bond breaking macroscopic fiber evolution validation barriers due oversimplified quasi models overcome problems possible future directions including advanced force fields multiscale integration driven modeling could enhance performance pancfs
"Image stitching plays a significant role in the environmental perception of autonomous vehicles by generating panoramic images from the images captured by surrounding cameras. However, existing deep learning methods still face the problem of poor homography estimation when handing images with complex backgrounds, resulting in ghosting artifacts in the stitching results. In this paper, we propose an image stitching method based on multi-stage feature matching and GAN for autonomous vehicles. It consists of a stitching network, an optimization network, and a GAN framework. The stitching network employs deformable convolutions to match the nonlinear contour features of the same object among images from different viewpoints, and extract the homography relationship to warp the images and perform dynamic fusion. The optimization network adopts a skip connection structure to fuses feature maps of different scales. To further improve visual quality of image stitching, a GAN framework is employed to provide feedback to the model during the training process and guide it to produce seamless image stitching results. Both quantitative and qualitative evaluations demonstrate that the proposed image stitching method outperforms the existing state-of-the-art methods.",image stitching plays significant role environmental perception autonomous vehicles generating panoramic images images captured surrounding cameras however existing deep learning methods still face problem poor homography estimation handing images complex backgrounds resulting ghosting artifacts stitching results paper propose image stitching method based multi stage feature matching gan autonomous vehicles consists stitching network optimization network gan framework stitching network employs deformable convolutions match nonlinear contour features object among images different viewpoints extract homography relationship warp images perform dynamic fusion optimization network adopts skip connection structure fuses feature maps different scales improve visual quality image stitching gan framework employed provide feedback model training process guide produce seamless image stitching results quantitative qualitative evaluations demonstrate proposed image stitching method outperforms existing state art methods
"The research aims to develop a triangular grid adaptive algorithm to analyze the morphological features of new media platforms by means of a standard template library algorithm. The purpose of the study is to improve the accuracy and efficiency of morphological characterization of new media platforms, and to provide theoretical support for new media content recommendation and user experience optimization. The experimental results show that the triangular lattice adaptive algorithm performs well in terms of accuracy and efficiency, with an average accuracy of 97.71%, and the optimized working time is significantly reduced to 77 minutes. In contrast, the random forest algorithm, the back propagation neural network algorithm and the artistic fish school algorithm have an average accuracy of 96.12%, 95.80%, and 95.92%, with a working time of 80, 87, and 84 minutes, respectively. The triangular grid adaptive algorithm not only has advantages in accuracy and efficiency, but also has the advantages of versatility, less time consumption and economic cost saving, which makes it suitable for analyzing the morphological characteristics of new media platforms.",research aims develop triangular grid adaptive algorithm analyze morphological features new media platforms means standard template library algorithm purpose study improve accuracy efficiency morphological characterization new media platforms provide theoretical support new media content recommendation user experience optimization experimental results show triangular lattice adaptive algorithm performs well terms accuracy efficiency average accuracy optimized working time significantly reduced minutes contrast random forest algorithm back propagation neural network algorithm artistic fish school algorithm average accuracy working time minutes respectively triangular grid adaptive algorithm advantages accuracy efficiency also advantages versatility less time consumption economic cost saving makes suitable analyzing morphological characteristics new media platforms
"Introducing the parametric design method into the field of graphic design has become an important topic in future graphic design research. In order to improve the effective integration of graphic design education and science and technology, this paper applies the color perception evaluation model based on computer vision to graphic design education. Aiming at the problems that traditional algorithms can’t adaptively extract the main color of images and the lack of evaluation indexes for intelligent color collocation effect, this paper proposes a color collocation evaluation method that integrates visual perception and similarity measurement, and an intelligent color collocation algorithm that integrates visual aesthetics. Through quantitative detection and subjective evaluation, it is shown that the test results of this method have high palette similarity with the source palette and have good effects in graphic design education, which can provide relatively good technical support for subsequent graphic design education. Compared with the Pix2Pix network model that does not integrate visual aesthetics, the method proposed in this paper has a certain improvement in both comprehensive evaluation index and peak signal-to-noise ratio. Meanwhile, the method proposed in this article has improved subjective preference compared to the Pix2Pix network model that does not integrate visual aesthetics and is of certain significance to promote the intelligent development of subsequent graphic design education.",introducing parametric design method field graphic design become important topic future graphic design research order improve effective integration graphic design education science technology paper applies color perception evaluation model based computer vision graphic design education aiming problems traditional algorithms adaptively extract main color images lack evaluation indexes intelligent color collocation effect paper proposes color collocation evaluation method integrates visual perception similarity measurement intelligent color collocation algorithm integrates visual aesthetics quantitative detection subjective evaluation shown test results method high palette similarity source palette good effects graphic design education provide relatively good technical support subsequent graphic design education compared pix pix network model integrate visual aesthetics method proposed paper certain improvement comprehensive evaluation index peak signal noise ratio meanwhile method proposed article improved subjective preference compared pix pix network model integrate visual aesthetics certain significance promote intelligent development subsequent graphic design education
"The study aims to explore the impact of sustainable education concepts and linguistic perspectives on innovation in teaching and learning English grammar at university. By constructing a university English grammar teaching program based on six strategies, the effects of antecedent, mediator, and outcome variables on teaching innovation were analyzed and five hypotheses about the interactions of these variables were formulated. The model was validated using the Partial Least Squares-Structural Equation Modeling (PLS-SEM) algorithm. The predictive power of models 1–3 on teaching innovation was 0.024, 0.046, and 0.607, respectively. Expressive ability, language culture, internal and external classroom environment, and teaching style all positively influenced teaching innovation, with regression coefficients of 0.436, 0.061, −0.146, and 0.396, respectively. But the expressive ability and teaching style were significant, while the remaining two dimensions were not significant. The results of the path analysis showed that all the hypotheses in the model were tested and the path coefficients obtained from the test, and the corresponding chi-square significance coefficients met the requirements. The study enriches the theory of pedagogical innovation and exposes the role of the joint linguistic perspective of sustainable education on pedagogical innovation.",study aims explore impact sustainable education concepts linguistic perspectives innovation teaching learning english grammar university constructing university english grammar teaching program based six strategies effects antecedent mediator outcome variables teaching innovation analyzed five hypotheses interactions variables formulated model validated using partial least squares structural equation modeling pls sem algorithm predictive power models teaching innovation respectively expressive ability language culture internal external classroom environment teaching style positively influenced teaching innovation regression coefficients respectively expressive ability teaching style significant remaining two dimensions significant results path analysis showed hypotheses model tested path coefficients obtained test corresponding chi square significance coefficients met requirements study enriches theory pedagogical innovation exposes role joint linguistic perspective sustainable education pedagogical innovation
"As energy transformation and technological advancements accelerate, multi-load forecasting serves as a key component in optimizing the planning, operation, and management of smart grids. Nevertheless, conventional load prediction techniques often suffer from issues such as limited accuracy and high volatility. To overcome these limitations, this study presents a load prediction approach based on Transformer multi-model fusion, employing a Stacking ensemble learning framework to integrate multiple models, including Transformer, XGBoost, and GDBT. The method also takes into account key influencing factors, such as meteorological conditions and holidays. Specifically, it involves training and predicting from the original features, followed by utilizing Transformer’s self-attention mechanism to capture feature relationships and long-term dependencies, ultimately leading to more precise load predictions. Empirical results validate that the proposed approach achieves superior prediction accuracy and stability across multiple datasets, demonstrating significant improvements over single-model approaches and conventional techniques.",energy transformation technological advancements accelerate multi load forecasting serves key component optimizing planning operation management smart grids nevertheless conventional load prediction techniques often suffer issues limited accuracy high volatility overcome limitations study presents load prediction approach based transformer multi model fusion employing stacking ensemble learning framework integrate multiple models including transformer xgboost gdbt method also takes account key influencing factors meteorological conditions holidays specifically involves training predicting original features followed utilizing transformer self attention mechanism capture feature relationships long term dependencies ultimately leading precise load predictions empirical results validate proposed approach achieves superior prediction accuracy stability across multiple datasets demonstrating significant improvements single model approaches conventional techniques
"The increasing penetration of new energy sources in power systems has significantly heightened uncertainty factors within distribution networks, thereby imposing elevated demands on their planning, operation, and control. This paper presents a comprehensive methodology to address these challenges. Initially, quantitative modeling of uncertainty factors within distribution networks is conducted, establishing a source-load output model. Subsequently, the correlation between photovoltaic generation and electrical loads is investigated, leading to the development of a probabilistic power flow calculation method that accounts for this correlation. Finally, an optimization framework is constructed with the objective function of minimizing planning costs. The results validate the effectiveness of the proposed methodology in reducing network losses and minimizing network planning expenses. This research contributes to enhancing the reliability and cost-effectiveness of distribution network operations in the face of increased renewable energy penetration and uncertainty.",increasing penetration new energy sources power systems significantly heightened uncertainty factors within distribution networks thereby imposing elevated demands planning operation control paper presents comprehensive methodology address challenges initially quantitative modeling uncertainty factors within distribution networks conducted establishing source load output model subsequently correlation photovoltaic generation electrical loads investigated leading development probabilistic power flow calculation method accounts correlation finally optimization framework constructed objective function minimizing planning costs results validate effectiveness proposed methodology reducing network losses minimizing network planning expenses research contributes enhancing reliability cost effectiveness distribution network operations face increased renewable energy penetration uncertainty
"This paper aims to establish a predictive model for hydro turbine failures by simulating rare real-world data using a digital twin system. Hydro turbines play a critical role in the renewable energy sector, but their unpredictability in terms of failures results in significant maintenance and operational costs. The traditional fault prediction method based on historical data is difficult to achieve more accurate and generalized modeling, because the data in the real world cannot meet the requirements of the machine learning theory for the same distribution of data and data balance, especially some rare events are difficult to collect in reality. Therefore, in this paper, it is proposed to enhance the robustness of hydro turbine failure prediction by simulating data from some rare situations through a digital twin system. By collecting and simulating rare data from actual hydroelectric turbines, we gain a better understanding of their operational mechanisms and fault patterns. We propose a digital twin system capable of replicating real-world operating conditions in a virtual environment, which serves as the foundation for data-driven fault prediction models. Through deep learning analysis of the simulated data, we can predict the likelihood of hydro turbine failures, thus improving maintenance strategies, reducing costs, and enhancing turbine reliability. Our research offers a promising approach to addressing rare data challenges using digital twin systems and holds broad application potential within the hydropower industry.",paper aims establish predictive model hydro turbine failures simulating rare real world data using digital twin system hydro turbines play critical role renewable energy sector unpredictability terms failures results significant maintenance operational costs traditional fault prediction method based historical data difficult achieve accurate generalized modeling data real world meet requirements machine learning theory distribution data data balance especially rare events difficult collect reality therefore paper proposed enhance robustness hydro turbine failure prediction simulating data rare situations digital twin system collecting simulating rare data actual hydroelectric turbines gain better understanding operational mechanisms fault patterns propose digital twin system capable replicating real world operating conditions virtual environment serves foundation data driven fault prediction models deep learning analysis simulated data predict likelihood hydro turbine failures thus improving maintenance strategies reducing costs enhancing turbine reliability research offers promising approach addressing rare data challenges using digital twin systems holds broad application potential within hydropower industry
"E-commerce enterprises’ sales volume prediction suffers from inaccuracy under the influence of uncertainty factors, which directly affects the enterprises’ resource allocation and market competitiveness. Based on this, the study aims to optimize the linear mixed model through feature construction and feature selection in order to improve the accuracy of sales volume forecasting. Feature engineering methods are used to construct features related to seasonality, promotional activities and historical sales, and the Akaike information criterion and Bayesian information criterion are combined to select the optimal covariance structure of the model. The experimental results show that the feature-optimized linear hybrid model outperforms the traditional method in terms of prediction accuracy, with an average absolute percentage error of 0.040 and a root-mean-square percentage error of 0.074. In addition, the model’s prediction error during the Double 11 period is maintained at −8% to 2%, which is highly practical and effective. The model is highly practical and effective. This study provides new ideas for e-commerce enterprises’ sales volume prediction, and effectively improves the enterprises’ decision-making ability in the complex market environment.",commerce enterprises sales volume prediction suffers inaccuracy influence uncertainty factors directly affects enterprises resource allocation market competitiveness based study aims optimize linear mixed model feature construction feature selection order improve accuracy sales volume forecasting feature engineering methods used construct features related seasonality promotional activities historical sales akaike information criterion bayesian information criterion combined select optimal covariance structure model experimental results show feature optimized linear hybrid model outperforms traditional method terms prediction accuracy average absolute percentage error root mean square percentage error addition model prediction error double period maintained highly practical effective model highly practical effective study provides new ideas commerce enterprises sales volume prediction effectively improves enterprises decision making ability complex market environment
"Efficiently conducting seismic hazard assessment and retrospective testing of seismic prediction strategies relies on the integration of proprietary seismic data into the forecasting analysis and decision process, however, the large volume and structural diversity of proprietary seismic data, and the fact that related knowledge is stored in multiple databases, throw a stumbling block to data integration. Hence, this paper introduce the Seismic Knowlee Graph (SKG), a flexible and powerful platform currently containing nearly 34,043 nodes and 32,248 relationships, representing the relevant observed data, public databases. The platform contains a variety of proprietary seismic data in different formats and from different data sources, providing data support to realize the data modeling and business process processing of strong and impending earthquake prediction model, so as to build a new model of regional earthquake forecasting study. In this work, we perform data organization of relational databases, and after graph database modeling and data import, we build knowledge graph using HugeGraph, a kind of NoSQL graph database, which organized easily extensible architecture that can be easily scale to new nodes and relationships when generates new data . We use WebGIS technology to build intuitive interfaces to visual graphical databases for user interaction, querying, and roaming the SKG. We illustrate the possibility of using a graph database to build a knowledge graph for seismic forecasting, which establishes a seismic forecast base database, a distributed database system for proprietary earthquake data. This graph database of relational queries has the possibility to reveal new relationships between heterogeneous seismic data and metadata, and it can be demonstrated that the graph structure can provide an efficient and reliable data support service for testing the validity of technical solutions for determining the urgency of strong earthquake generation.",efficiently conducting seismic hazard assessment retrospective testing seismic prediction strategies relies integration proprietary seismic data forecasting analysis decision process however large volume structural diversity proprietary seismic data fact related knowledge stored multiple databases throw stumbling block data integration hence paper introduce seismic knowlee graph skg flexible powerful platform currently containing nearly nodes relationships representing relevant observed data public databases platform contains variety proprietary seismic data different formats different data sources providing data support realize data modeling business process processing strong impending earthquake prediction model build new model regional earthquake forecasting study work perform data organization relational databases graph database modeling data import build knowledge graph using hugegraph kind nosql graph database organized easily extensible architecture easily scale new nodes relationships generates new data use webgis technology build intuitive interfaces visual graphical databases user interaction querying roaming skg illustrate possibility using graph database build knowledge graph seismic forecasting establishes seismic forecast base database distributed database system proprietary earthquake data graph database relational queries possibility reveal new relationships heterogeneous seismic data metadata demonstrated graph structure provide efficient reliable data support service testing validity technical solutions determining urgency strong earthquake generation
"This article examines the critical role of critical thinking in higher education and the workplace, addressing the perceived deficit in graduates’ skills for tackling professional challenges. It explores the essence of critical thinking, tracing its roots from ancient Greek philosophy to contemporary academic discourse and delineates the characteristics of an ideal critical thinker in the professional sphere. The article proposes a stratified approach to nurturing critical thinking skills in the workplace, tailored to different organizational levels: expert-level critical thinkers, frontline employees and mid-level supervisors. This tiered strategy acknowledges the diverse needs within an organization and the varying depths of critical thinking required for each role. The manuscript emphasizes the importance of balancing domain-general and domain-specific skills in critical thinking development and highlights the need to refine these competencies continuously. By prioritizing critical thinking in both educational institutions and professional environments, we can develop a workforce that is well-prepared to handle complex challenges, make informed decisions and spearhead innovation in an increasingly interconnected world.",article examines critical role critical thinking higher education workplace addressing perceived deficit graduates skills tackling professional challenges explores essence critical thinking tracing roots ancient greek philosophy contemporary academic discourse delineates characteristics ideal critical thinker professional sphere article proposes stratified approach nurturing critical thinking skills workplace tailored different organizational levels expert level critical thinkers frontline employees mid level supervisors tiered strategy acknowledges diverse needs within organization varying depths critical thinking required role manuscript emphasizes importance balancing domain general domain specific skills critical thinking development highlights need refine competencies continuously prioritizing critical thinking educational institutions professional environments develop workforce well prepared handle complex challenges make informed decisions spearhead innovation increasingly interconnected world
"Folk songs are a vital component of regional culture, distinguished by distinctive local traits and ethnic charm. Conventional classification techniques are insufficient for tackling the diversity and complexity of dialects in folk music. A new model integrating convolutional neural networks (CNNs) and Transformers—TransCNN—has been developed to address this problem. This approach innovatively extracts local information from songs using CNN, thereafter encoding and decoding these features globally through the self-attention mechanism of Transformers. Furthermore, it employs diverse data augmentation techniques to strengthen the model’s capacity to identify the attributes of folk song dialects and elevate its overall efficacy. The evaluations revealed that TransCNN attained a classification accuracy of 92.3%, a recall rate of 91.8%, and F1 scores of 91.9%. The findings demonstrate that TransCNN effectively manages various audio kinds, background noises, and tonal variations, fulfilling the accuracy and reliability requirements for categorizing folk songs in the specified job.",folk songs vital component regional culture distinguished distinctive local traits ethnic charm conventional classification techniques insufficient tackling diversity complexity dialects folk music new model integrating convolutional neural networks cnns transformers transcnn developed address problem approach innovatively extracts local information songs using cnn thereafter encoding decoding features globally self attention mechanism transformers furthermore employs diverse data augmentation techniques strengthen model capacity identify attributes folk song dialects elevate overall efficacy evaluations revealed transcnn attained classification accuracy recall rate scores findings demonstrate transcnn effectively manages various audio kinds background noises tonal variations fulfilling accuracy reliability requirements categorizing folk songs specified job
"With the rise of popular music on the Internet, users need a powerful recommendation system to increase their subscription volume for music consumption. Research was done using deep learning techniques in streaming media platforms to convert audio signals into Mel spectrograms for extracting audio features. Under the computation of the Mel filter, feature extraction of audio files can be input into a binary network with platform user behavior, thereby constructing a music recommendation system. Finally, the relationship learning module is utilized to integrate user behavior and audio features, combined with a music encoder to complete personalized audio feature processing, thereby achieving the construction of a music personalized recommendation system. Through dataset testing, it was found that the initial dataset had a sparsity ratio of 0.14% and a recall rate of 30.16%. In the partitioning and data analysis of the Subset dataset, it was found that the user group recommendation algorithm had the highest recommendation accuracy of 24% for Subset100 subset, and the highest recall rate was 33.5% when the number of recommendations was 50, indicating the necessity of data recommendation. In the testing of the personalized recommendation system, the training accuracy of the system was as high as 93%. Finally, different recommendation methods were compared between the Subset500 dataset and the Mirex dataset, and the research method based on audio features had the highest recommendation accuracy of 97.5%, recall rate and F1 value of 42.1% and 58.81%, respectively, and a running time of 3.2 seconds, thus proving the excellent performance. The personalized recommendation system not only improves the predicted rating results of music in practical applications but also provides reliable technology for the future dissemination and development of popular music on the Internet.",rise popular music internet users need powerful recommendation system increase subscription volume music consumption research done using deep learning techniques streaming media platforms convert audio signals mel spectrograms extracting audio features computation mel filter feature extraction audio files input binary network platform user behavior thereby constructing music recommendation system finally relationship learning module utilized integrate user behavior audio features combined music encoder complete personalized audio feature processing thereby achieving construction music personalized recommendation system dataset testing found initial dataset sparsity ratio recall rate partitioning data analysis subset dataset found user group recommendation algorithm highest recommendation accuracy subset subset highest recall rate number recommendations indicating necessity data recommendation testing personalized recommendation system training accuracy system high finally different recommendation methods compared subset dataset mirex dataset research method based audio features highest recommendation accuracy recall rate value respectively running time seconds thus proving excellent performance personalized recommendation system improves predicted rating results music practical applications also provides reliable technology future dissemination development popular music internet
"This paper presents a library book recommendation system designed to improve effectiveness, utilizing the GWO algorithm. The system architecture consists of three distinct layers: the foundational data layer, the data processing layer, and the intelligent service. The improved CGWO-KM algorithm is used to cluster project attributes, and the search and update mechanism of the gray wolf population is applied to find better initial clustering centers. Missing rating data is then filled in, and user similarity is calculated. A harmonized weighting factor is used to eliminate the correlation between ratings from different users. The weighted rating mechanism comprehensively considers both user ratings and the influence of neighboring users. The improved Pearson correlation coefficient combines the weighting factors with user similarity to obtain the final recommendation score, completing the intelligent book recommendation process for the library’s books. The results show that at the 5th month time snapshot, the predicted data (6) for the library’s historical borrowing dataset closely matches the actual value (3.7). The method proposed in this paper demonstrates an IGD mean close to the true optimal solution across various library datasets for literature, science popularization, history, art, and novels, with values of 0.0012, 0.0023, 0.0014, 0.0021, and 0.0020, respectively. The optimal non-dominated solutions in the three-dimensional space for resource utilization, recommendation diversity, and user engagement are close to the ideal value of 1. Moreover, the book recommendation system has a short processing time, and the recommendation accuracy ranges from 0.882 to 0.993, providing personalized, high-quality book recommendation services for readers.",paper presents library book recommendation system designed improve effectiveness utilizing gwo algorithm system architecture consists three distinct layers foundational data layer data processing layer intelligent service improved cgwo algorithm used cluster project attributes search update mechanism gray wolf population applied find better initial clustering centers missing rating data filled user similarity calculated harmonized weighting factor used eliminate correlation ratings different users weighted rating mechanism comprehensively considers user ratings influence neighboring users improved pearson correlation coefficient combines weighting factors user similarity obtain final recommendation score completing intelligent book recommendation process library books results show month time snapshot predicted data library historical borrowing dataset closely matches actual value method proposed paper demonstrates igd mean close true optimal solution across various library datasets literature science popularization history art novels values respectively optimal non dominated solutions three dimensional space resource utilization recommendation diversity user engagement close ideal value moreover book recommendation system short processing time recommendation accuracy ranges providing personalized high quality book recommendation services readers
"While recent research calls for renewing project studies, few empirical works explore how design thinking can enrich project management. Based on a longitudinal case study within a company using design thinking to implement digital transformation, we analyze how design thinking supports projects involving major managerial and operational shifts, complex stakeholder environments, and ambiguous objectives. We identify key characteristics of a human-centric project management approach that addressed the difficulties faced by digital transformation and intense change projects, and we show how it helps shaping the projects by focusing on their stakeholders.",recent research calls renewing project studies empirical works explore design thinking enrich project management based longitudinal case study within company using design thinking implement digital transformation analyze design thinking supports projects involving major managerial operational shifts complex stakeholder environments ambiguous objectives identify key characteristics human centric project management approach addressed difficulties faced digital transformation intense change projects show helps shaping projects focusing stakeholders
"The incorporation of computer vision into tourism information systems has the potential to transform the way tourists interact with their surroundings, offering improved navigation and richer experiences. This research presents a revolutionary signpost navigation system for mobile applications that enhances the tourist experience in scenic areas. The proposed system leverages advanced object recognition and image classification methods to provide real-time location guidance. An Intelligent Ant Colony-tuned Bottleneck Residual Network (IntACO-Bottleneck-ResNet) is utilized for effective image recognition, enabling the system to identify and categorize landmarks in its environment. A single-stage target detection method is then employed to accurately localize targets based on the recognized images. For this study, a specific dataset was compiled, comprising photographs from popular tourist locations, along with signage and landmark images. This dataset, containing both static and dynamic images under various lighting and weather conditions, was annotated for model training and testing. The system’s performance was measured in terms of image recognition accuracy and localization precision. The results demonstrated that the IntACO-Bottleneck-ResNet-based system achieved a high image identification accuracy of 97%, alongside 96% precision, 95% recall, and a 94% F1-score. These findings underscore the potential of this method for real-time, context-aware tourism navigation, significantly enhancing user experience and location-based services in mobile tourism applications.",incorporation computer vision tourism information systems potential transform way tourists interact surroundings offering improved navigation richer experiences research presents revolutionary signpost navigation system mobile applications enhances tourist experience scenic areas proposed system leverages advanced object recognition image classification methods provide real time location guidance intelligent ant colony tuned bottleneck residual network intaco bottleneck resnet utilized effective image recognition enabling system identify categorize landmarks environment single stage target detection method employed accurately localize targets based recognized images study specific dataset compiled comprising photographs popular tourist locations along signage landmark images dataset containing static dynamic images various lighting weather conditions annotated model training testing system performance measured terms image recognition accuracy localization precision results demonstrated intaco bottleneck resnet based system achieved high image identification accuracy alongside precision recall score findings underscore potential method real time context aware tourism navigation significantly enhancing user experience location based services mobile tourism applications
"This study presents an innovative design for a heavy-load climbing robot (HLCR), specifically engineered for operation on large-scale steel structures. Utilizing a framework-based electromagnetic adhesion system, the robot’s static mathematical model was developed to accurately predict its load-bearing capacity and adhesion force requirements, ensuring stability and preventing slippage during high-load operations on steel frame surfaces. The model effectively balances forces, including magnetic attraction, robot weight, and frictional resistance. Experimental validation demonstrated a load-bearing capacity ranging from 53 kg to 54 kg, which is higher than the model’s predicted load capacity of 50 kg. The robot achieved an optimized load-bearing capacity of 50 kg, maintaining a consistent movement speed of 5 mm/s and an obstacle-crossing height of 90 mm, thereby confirming its structural adaptability and operational safety. The results highlight the robot’s capability to support heavy-load maintenance equipment while ensuring stability across various motion modes. This research addresses the increasing demand for maintenance automation in industrial and infrastructure applications.",study presents innovative design heavy load climbing robot hlcr specifically engineered operation large scale steel structures utilizing framework based electromagnetic adhesion system robot static mathematical model developed accurately predict load bearing capacity adhesion force requirements ensuring stability preventing slippage high load operations steel frame surfaces model effectively balances forces including magnetic attraction robot weight frictional resistance experimental validation demonstrated load bearing capacity ranging higher model predicted load capacity robot achieved optimized load bearing capacity maintaining consistent movement speed obstacle crossing height thereby confirming structural adaptability operational safety results highlight robot capability support heavy load maintenance equipment ensuring stability across various motion modes research addresses increasing demand maintenance automation industrial infrastructure applications
"In recent years, English language teaching has undergone significant transformation due to the integration of technology and innovative teaching methodologies. Among the most promising advancements is the formation of multimodal teaching, which incorporates various forms of media. Despite the potential benefits associated with multimodal teaching, challenges remain in tailoring content to individual learning needs and optimizing the learning process for maximum effectiveness. To address these challenges, this research proposes a Multimodal English Teaching Framework (METF) to enhance the effectiveness and personalization of language learning. The framework integrates multiple teaching modalities, including text, audio, video, and interactive elements, to accommodate diverse learning preferences. It incorporates student interaction data (responses, time spent on tasks, and engagement levels), learner demographics, learning styles, and feedback records, thereby capturing the effectiveness of personalized content delivery. Pre-processing steps, including tokenization for text data, noise reduction for audio data, and normalization for resolution and interaction data, ensure data quality and consistency. Feature extraction techniques such as Convolutional Neural Networks (CNNs) for audio and video data and Term Frequency-Inverse Document Frequency (TF-IDF) for text data are applied to capture meaningful patterns. To enhance the fusion of multimodal data, feature-level fusion is implemented, integrating recovered characteristics from multiple modalities into a unified representation that supports more accurate decision-making. This research establishes the Resilient Genetic Algorithm (RGA) to enhance the adaptive learning process by ensuring robustness and avoiding premature convergence. Experimental results demonstrate the potential of this multimodal approach, achieving 94.2% accuracy, a convergence time of 45%, task engagement of 89.4%, and personalization effectiveness of 91.7%. These results indicate a high value of accuracy in multimodal English teaching. This research highlights the applicability of algorithms in optimizing complex, multimodal educational environments.",recent years english language teaching undergone significant transformation due integration technology innovative teaching methodologies among promising advancements formation multimodal teaching incorporates various forms media despite potential benefits associated multimodal teaching challenges remain tailoring content individual learning needs optimizing learning process maximum effectiveness address challenges research proposes multimodal english teaching framework metf enhance effectiveness personalization language learning framework integrates multiple teaching modalities including text audio video interactive elements accommodate diverse learning preferences incorporates student interaction data responses time spent tasks engagement levels learner demographics learning styles feedback records thereby capturing effectiveness personalized content delivery pre processing steps including tokenization text data noise reduction audio data normalization resolution interaction data ensure data quality consistency feature extraction techniques convolutional neural networks cnns audio video data term frequency inverse document frequency idf text data applied capture meaningful patterns enhance fusion multimodal data feature level fusion implemented integrating recovered characteristics multiple modalities unified representation supports accurate decision making research establishes resilient genetic algorithm rga enhance adaptive learning process ensuring robustness avoiding premature convergence experimental results demonstrate potential multimodal approach achieving accuracy convergence time task engagement personalization effectiveness results indicate high value accuracy multimodal english teaching research highlights applicability algorithms optimizing complex multimodal educational environments
"Traditional student big data analysis often neglects unstructured data, such as communication content and emotional feedback, limiting its effectiveness in personalized recommendations and learning interventions. This study addresses this gap by applying Word2Vec technology to analyze semantic information and emotional tendencies in student behaviors, enabling precise learning recommendations, real-time sentiment analysis, and timely interventions. A Word2Vec model is trained on extensive student data to understand learning behaviors, while a support vector machine (SVM) performs sentiment analysis to identify emotional states. Based on these insights, a personalized recommendation system dynamically adjusts resources and task difficulty to enhance learning outcomes. Experimental results show the system outperforms others, achieving recommendation accuracy of 0.81–0.87, sentiment analysis accuracy of 0.80–0.89, and an average performance improvement rate of 15.85%. These findings validate Word2Vec’s effectiveness in intelligent education systems, offering a novel framework for personalized learning and intervention strategies.",traditional student big data analysis often neglects unstructured data communication content emotional feedback limiting effectiveness personalized recommendations learning interventions study addresses gap applying word vec technology analyze semantic information emotional tendencies student behaviors enabling precise learning recommendations real time sentiment analysis timely interventions word vec model trained extensive student data understand learning behaviors support vector machine svm performs sentiment analysis identify emotional states based insights personalized recommendation system dynamically adjusts resources task difficulty enhance learning outcomes experimental results show system outperforms others achieving recommendation accuracy sentiment analysis accuracy average performance improvement rate findings validate word vec effectiveness intelligent education systems offering novel framework personalized learning intervention strategies
"Accurate PV power prediction is crucial in efficiently operating intelligent power grid systems. Data-driven approaches have shown high performance in predictive tasks. Deep reinforcement learning (DRL) merges deep learning with reinforcement learning and has been widely studied for optimization challenges in various fields. However, limited research has focused on applying DRL to ultra-short-term PV power prediction. Hence, a soft actor–critic (SAC) model using long short-term memory (LSTM) is proposed for predicting PV power. To accomplish this, first, the PV power problem is modeled as a Markov decision process with historical weather data and PV power data as state inputs. Then, LSTM is integrated into the critic network of SAC to enhance its memory capability, thus improving prediction accuracy. Ultimately, the agent engages with the environment to address the optimization problem. Experimental results indicate that the proposed model attains greater prediction accuracy. This study explores the potential of DRL for PV power prediction, and the proposed method can be extended to other prediction fields, including grid prediction and wind power prediction.",accurate power prediction crucial efficiently operating intelligent power grid systems data driven approaches shown high performance predictive tasks deep reinforcement learning drl merges deep learning reinforcement learning widely studied optimization challenges various fields however limited research focused applying drl ultra short term power prediction hence soft actor critic sac model using long short term memory lstm proposed predicting power accomplish first power problem modeled markov decision process historical weather data power data state inputs lstm integrated critic network sac enhance memory capability thus improving prediction accuracy ultimately agent engages environment address optimization problem experimental results indicate proposed model attains greater prediction accuracy study explores potential drl power prediction proposed method extended prediction fields including grid prediction wind power prediction
"Current short video recommendation systems face challenges in accurately identifying user interests due to large-scale, heterogeneous user data, as well as addressing cold-start problems when encountering new users and content. This study proposes a deep learning and reinforcement learning (DLRL) framework to enhance recommendation performance. User and video data are collected and preprocessed using the Hadoop platform, while automated metadata extraction tools, optical character recognition (OCR), and audio-to-text technologies extract multimodal features, including subtitles and audio content. A convolutional neural network (CNN) extracts semantic and visual features from text-image data, while a long short-term memory (LSTM) network captures temporal dependencies in user behavior to detect interest changes. A multimodal attention mechanism integrates these diverse features to form comprehensive user portraits, and a deep neural network (DNN) reduces feature dimensionality to represent user preferences accurately. The recommendation process is formulated as a reinforcement learning task optimized through Deep Q Network (DQN) and Proximal Policy Optimization (PPO) algorithms. Experimental results demonstrate that the proposed approach achieves superior click-through rates (6.22%), viewing duration (73.31 s), accuracy (0.877), and recall (0.858), effectively overcoming cold-start limitations and dynamically adapting to evolving user interests. This shows that combining DLRL can optimize the performance of short video recommendations, improve accuracy and experience, and address the issues of cold start and capturing changes in user interests.",current short video recommendation systems face challenges accurately identifying user interests due large scale heterogeneous user data well addressing cold start problems encountering new users content study proposes deep learning reinforcement learning dlrl framework enhance recommendation performance user video data collected preprocessed using hadoop platform automated metadata extraction tools optical character recognition ocr audio text technologies extract multimodal features including subtitles audio content convolutional neural network cnn extracts semantic visual features text image data long short term memory lstm network captures temporal dependencies user behavior detect interest changes multimodal attention mechanism integrates diverse features form comprehensive user portraits deep neural network dnn reduces feature dimensionality represent user preferences accurately recommendation process formulated reinforcement learning task optimized deep network dqn proximal policy optimization ppo algorithms experimental results demonstrate proposed approach achieves superior click rates viewing duration accuracy recall effectively overcoming cold start limitations dynamically adapting evolving user interests shows combining dlrl optimize performance short video recommendations improve accuracy experience address issues cold start capturing changes user interests
"Basketball players can maximize their potential and enhance their skills, strength, and overall performance with the help of customized training routines. Players in games must quickly adapt to changing court circumstances, often adjusting tactics, but identifying the best course of action in real-time is challenging due to the complexity of handling data signals. This research explores the use of artificial intelligence (AI) in creating personalized training plans to improve basketball players’ abilities. Specifically, a novel Intelligent Cheetah Optimizer with Flexible Recurrent Neural Networks (ICO-FRNN) was proposed to generate training plans by identifying individual player strengths and areas for improvement. To get information from sensors during practice and competition, monitor physical performance indicators such as heart rate, speed, jump height, endurance, and biomechanical movements. The collected data undergoes preprocessing to address missing values, normalize data formats, and remove outliers by using Z-score normalization and linear discriminant analysis (LDA) is used for feature extraction. The findings show that the ICO-RNN approach enables more intelligent, player-specific training plans, facilitating improved decision-making, skill improvement, and injury avoidance. Findings indicate that AI-driven personalized training plans result in notable performance gains when compared to conventional training regimens. The performance metrics are accuracy (0.9680), recall (0.9680), F1 score (0.9681), and precision (0.9700). The result demonstrates that AI can revolutionize basketball coaching techniques by creating data-driven, dynamic training programs that optimize players’ potential.",basketball players maximize potential enhance skills strength overall performance help customized training routines players games must quickly adapt changing court circumstances often adjusting tactics identifying best course action real time challenging due complexity handling data signals research explores use artificial intelligence creating personalized training plans improve basketball players abilities specifically novel intelligent cheetah optimizer flexible recurrent neural networks ico frnn proposed generate training plans identifying individual player strengths areas improvement get information sensors practice competition monitor physical performance indicators heart rate speed jump height endurance biomechanical movements collected data undergoes preprocessing address missing values normalize data formats remove outliers using score normalization linear discriminant analysis lda used feature extraction findings show ico rnn approach enables intelligent player specific training plans facilitating improved decision making skill improvement injury avoidance findings indicate driven personalized training plans result notable performance gains compared conventional training regimens performance metrics accuracy recall score precision result demonstrates revolutionize basketball coaching techniques creating data driven dynamic training programs optimize players potential
"The identification of themes and motifs in literary texts is a fundamental aspect of literary analysis, traditionally performed through manual annotation and expert interpretation. However, the increasing availability of large-scale English literary corpora presents new challenges and opportunities for automated analysis. This paper proposes a deep learning (DL)-based framework for automatically detecting themes and motifs in extensive literary collections. The dataset comprises diverse sources, including classic literature, modern fiction, and poetry, ensuring a broad representation of thematic structures. A rigorous preprocessing pipeline is applied, involving stop word removal and tokenization to refine textual data. For feature extraction, Word2Vec is utilized to capture semantic relationships between words. The core novelty of this research lies in the implementation of a Duelist Algorithm-optimized Bi-directional Long Short-Term Memory (DAO-BiLSTM) model, which enhances the model’s ability to detect and classify recurring thematic elements with high accuracy. The proposed method achieves an accuracy of 96.24%, recall of 97.32%, precision of 95.6%, and an F1-score of 94.7%, demonstrating superior performance over existing methods. The model is implemented in Python 3.9 using TensorFlow in a high-performance computing environment, ensuring efficient processing of large-scale textual data. Experimental results illustrate the effectiveness of the proposed approach in identifying complex motifs and themes across various literary genres. These findings highlight the potential of DL in augmenting literary analysis, enabling large-scale, data-driven thematic exploration that complements traditional human-driven methodologies.",identification themes motifs literary texts fundamental aspect literary analysis traditionally performed manual annotation expert interpretation however increasing availability large scale english literary corpora presents new challenges opportunities automated analysis paper proposes deep learning based framework automatically detecting themes motifs extensive literary collections dataset comprises diverse sources including classic literature modern fiction poetry ensuring broad representation thematic structures rigorous preprocessing pipeline applied involving stop word removal tokenization refine textual data feature extraction word vec utilized capture semantic relationships words core novelty research lies implementation duelist algorithm optimized directional long short term memory dao bilstm model enhances model ability detect classify recurring thematic elements high accuracy proposed method achieves accuracy recall precision score demonstrating superior performance existing methods model implemented python using tensorflow high performance computing environment ensuring efficient processing large scale textual data experimental results illustrate effectiveness proposed approach identifying complex motifs themes across various literary genres findings highlight potential augmenting literary analysis enabling large scale data driven thematic exploration complements traditional human driven methodologies
"The effectiveness of online music education relies heavily on understanding and addressing students’ emotional states, which can impact engagement and learning outcomes. This paper presents a novel emotion recognition method based on an improved frame attention network (IFAN), designed specifically for online music teaching. The method utilizes facial expression data to identify four key emotional states—pleasure, concentration, confusion, and boredom—by introducing deformable convolution to better capture dynamic facial features and a feature aggregation module to enhance emotional temporal patterns. The proposed model achieves recognition accuracies of 96%, 94%, 93%, and 98% for each emotional state, outperforming existing emotion recognition methods. Experimental results indicate that the model is robust and highly accurate in the context of online music education. This research provides a foundation for real-time emotion recognition systems in online teaching environments, with potential for future work to incorporate multimodal data, such as audio and physiological signals, to further enhance model performance.",effectiveness online music education relies heavily understanding addressing students emotional states impact engagement learning outcomes paper presents novel emotion recognition method based improved frame attention network ifan designed specifically online music teaching method utilizes facial expression data identify four key emotional states pleasure concentration confusion boredom introducing deformable convolution better capture dynamic facial features feature aggregation module enhance emotional temporal patterns proposed model achieves recognition accuracies emotional state outperforming existing emotion recognition methods experimental results indicate model robust highly accurate context online music education research provides foundation real time emotion recognition systems online teaching environments potential future work incorporate multimodal data audio physiological signals enhance model performance
"As photovoltaic (PV) systems become more widely adopted, maximizing their grid-connected capacity in urban settings is crucial for improving energy flexibility and reliability. While progress is being made in integrating PV with renewable energy sources, further research is required to comprehensively understand how urban PV systems can support the grid, particularly regarding flexibility and reliability. Most prior studies have concentrated on centralized, grid-level evaluations, often neglecting urban-specific variances and the real-time challenges of grid management. Additionally, the dynamic nature of PV power production and consumption patterns in rapidly urbanizing areas poses difficulties for conventional approaches. This study aims to evaluate the grid-connected capacity of PV systems in urban environments by utilizing advanced deep learning (DL) algorithms to optimize and adjust grid connection flexibility and reliability while predicting electricity demands in these areas. The KO-DynSpikNet model is employed to analyze historical solar power data, consumption trends, and grid stability indicators. The model is trained to estimate optimal PV capacity integration for various urban locations, taking into account weather variability, changes in energy demand, and grid constraints. Sensors are utilized for data collection, while discrete wavelet transform (DWT) and min-max scaling are applied for preprocessing and feature extraction. The results indicate that the DL model offers improved forecasting accuracy for grid-connected capacity compared to conventional methods. Enhanced PV system designs contribute to grid stability and increase flexibility in real-time energy distribution. This showcases how local energy networks can become more flexible and reliable while facilitating the adoption of environmentally friendly energy solutions in decentralized systems.",photovoltaic systems become widely adopted maximizing grid connected capacity urban settings crucial improving energy flexibility reliability progress made integrating renewable energy sources research required comprehensively understand urban systems support grid particularly regarding flexibility reliability prior studies concentrated centralized grid level evaluations often neglecting urban specific variances real time challenges grid management additionally dynamic nature power production consumption patterns rapidly urbanizing areas poses difficulties conventional approaches study aims evaluate grid connected capacity systems urban environments utilizing advanced deep learning algorithms optimize adjust grid connection flexibility reliability predicting electricity demands areas dynspiknet model employed analyze historical solar power data consumption trends grid stability indicators model trained estimate optimal capacity integration various urban locations taking account weather variability changes energy demand grid constraints sensors utilized data collection discrete wavelet transform dwt min max scaling applied preprocessing feature extraction results indicate model offers improved forecasting accuracy grid connected capacity compared conventional methods enhanced system designs contribute grid stability increase flexibility real time energy distribution showcases local energy networks become flexible reliable facilitating adoption environmentally friendly energy solutions decentralized systems
"In the digital era, the integration of Artificial Intelligence (AI) and Natural Language Processing (NLP) has progressively enabled the automation of English teaching resource generation. Traditional methods for producing teaching materials typically require extensive manual effort, which limits their scalability and flexibility to accommodate standard learning needs. However, NLP technologies present an effective solution by allowing for the automatic generation of high-quality instructional content. This research highlights the capacity of NLP to generate organized and relevant materials for English language teaching. An Efficient Glowworm Swarm Optimized Bidirectional Encoder Representations from Transformers (EGWS-BERT) method is employed to measure the accuracy and flexibility of the produced content. Publicly available lesson plans and grammar guides serve as data sources, which undergo pre-processing methods such as data cleaning and text tokenization. The NLP framework emphasizes extracting key information through automated summarization, refining language meaning by recognizing and improving keywords, and ensuring coherence and usability through organized content integration into e-learning platforms. Results indicate that the proposed model significantly enhances content generation effectiveness, reducing the manual workload while producing high-quality educational resources. Compared to current methods, the suggested EGWS-BERT technique yields high-quality instructional content with 96% accuracy and 94% recall, which is an outstanding outcome. The findings suggest that NLP-driven approaches can assess resource development, providing scalable, adaptable, and effective solutions for English language teaching.",digital integration artificial intelligence natural language processing nlp progressively enabled automation english teaching resource generation traditional methods producing teaching materials typically require extensive manual effort limits scalability flexibility accommodate standard learning needs however nlp technologies present effective solution allowing automatic generation high quality instructional content research highlights capacity nlp generate organized relevant materials english language teaching efficient glowworm swarm optimized bidirectional encoder representations transformers egws bert method employed measure accuracy flexibility produced content publicly available lesson plans grammar guides serve data sources undergo pre processing methods data cleaning text tokenization nlp framework emphasizes extracting key information automated summarization refining language meaning recognizing improving keywords ensuring coherence usability organized content integration learning platforms results indicate proposed model significantly enhances content generation effectiveness reducing manual workload producing high quality educational resources compared current methods suggested egws bert technique yields high quality instructional content accuracy recall outstanding outcome findings suggest nlp driven approaches assess resource development providing scalable adaptable effective solutions english language teaching
"In the rapid quotation process of construction projects, low prediction accuracy and high computational complexity remain challenging issues. To address these problems, this study proposes a novel method that integrates the theory of intuitionistic fuzzy sets for intelligent cost prediction. Given that engineering unit prices are significantly and dynamically uncertain due to multi-source heterogeneous factors (e.g., seasonal fluctuations, regional differences, and market volatility), we construct a transformation mechanism of interval-valued intuitionistic fuzzy sets. This mechanism expands the traditional deterministic engineering feature matrix into an information matrix that includes hesitation degrees and membership degrees, thereby achieving a refined representation of uncertain parameters. On this basis, we introduce a multi-dimensional similarity matching algorithm to establish an accurate mapping relationship between the project under construction and the historical case library. A weighted correction prediction model based on similar cases is proposed to enhance the robustness of unit cost prediction results. Verification through multiple engineering examples demonstrates that this method provides a new prediction paradigm for engineering cost estimation, combining both theoretical rigor and practical applicability. The research conclusions offer important references for the development of intelligent cost estimation systems and the optimization of engineering decision-making.",rapid quotation process construction projects low prediction accuracy high computational complexity remain challenging issues address problems study proposes novel method integrates theory intuitionistic fuzzy sets intelligent cost prediction given engineering unit prices significantly dynamically uncertain due multi source heterogeneous factors seasonal fluctuations regional differences market volatility construct transformation mechanism interval valued intuitionistic fuzzy sets mechanism expands traditional deterministic engineering feature matrix information matrix includes hesitation degrees membership degrees thereby achieving refined representation uncertain parameters basis introduce multi dimensional similarity matching algorithm establish accurate mapping relationship project construction historical case library weighted correction prediction model based similar cases proposed enhance robustness unit cost prediction results verification multiple engineering examples demonstrates method provides new prediction paradigm engineering cost estimation combining theoretical rigor practical applicability research conclusions offer important references development intelligent cost estimation systems optimization engineering decision making
"Aiming at the problems such as slow convergence speed, precocious convergence and poor fault tolerance performance of dung beetle optimizer (DBO) algorithm in solving active distribution network fault location, a partition fault location method for active distribution network based on adaptive chaotic DBO (AC-DBO) algorithm is put forward. Firstly, an improved Tent chaotic map is introduced to realize population initialization, so as to improve the quality of initial population distribution in the search space and its global search efficiency. Secondly, the adaptive T-distribution mutation mechanism is incorporated into the dung beetle position update to elevate the global exploration and local exploitation abilities of the algorithm. Finally, the equivalent partition model for active distribution network is established based on the “black box” theory to reduce the solving dimension of the algorithm and improve the location rate. The comparative experimental results of multi-verse optimization (MVO), red-tailed hawk (RTH), slime mould algorithm (SMA), and AC-DBO algorithm and the effectiveness simulation test results of hierarchical positioning model show that the average positioning accuracy of AC-DBO algorithm is increased by 15%, 10%, and 3%, respectively, compared with MVO, RTH, and SMA algorithms. After introducing the equivalent partition strategy, the average positioning time of AC-DBO hierarchical positioning model is reduced by 36.8% and 29.4%, respectively, compared with AC-DBO single-layer positioning model and DBO hierarchical positioning model and the average positioning accuracy is increased by 21% and 8%, respectively. The AC-DBO partition model has obvious advantages in solving speed, accuracy and fault tolerance, which is especially suitable for solving the fault location problem of active distribution network.",aiming problems slow convergence speed precocious convergence poor fault tolerance performance dung beetle optimizer dbo algorithm solving active distribution network fault location partition fault location method active distribution network based adaptive chaotic dbo dbo algorithm put forward firstly improved tent chaotic map introduced realize population initialization improve quality initial population distribution search space global search efficiency secondly adaptive distribution mutation mechanism incorporated dung beetle position update elevate global exploration local exploitation abilities algorithm finally equivalent partition model active distribution network established based black box theory reduce solving dimension algorithm improve location rate comparative experimental results multi verse optimization mvo red tailed hawk rth slime mould algorithm sma dbo algorithm effectiveness simulation test results hierarchical positioning model show average positioning accuracy dbo algorithm increased respectively compared mvo rth sma algorithms introducing equivalent partition strategy average positioning time dbo hierarchical positioning model reduced respectively compared dbo single layer positioning model dbo hierarchical positioning model average positioning accuracy increased respectively dbo partition model obvious advantages solving speed accuracy fault tolerance especially suitable solving fault location problem active distribution network
"According to prominent philosophical views, appreciating beauty involves psychological distancing, where one does not consider the beautiful object in light of practical interests, and beauty leads to transformative and self-transcendent affective experiences. In this study (N = 187), conducted in the naturalistic environment of a museum, we explored these ideas. Half the participants were instructed to rate the beauty of pottery objects created by a renowned artist, while the other half engaged in a control task that did not involve evaluating beauty. Based on Construal Level Theory, we used the Behavior Identification Form to explore whether aesthetic experiences encourage abstract thinking, and in turn, psychological distance. We also predicted that beauty appreciation would lead to greater transformative and self-transcendent emotions. Indeed, appreciating the beauty of artworks led participants to think in a more abstract way, especially those who practice an art hobby themselves, as well as to transformative, and self-transcendent emotions.",according prominent philosophical views appreciating beauty involves psychological distancing one consider beautiful object light practical interests beauty leads transformative self transcendent affective experiences study conducted naturalistic environment museum explored ideas half participants instructed rate beauty pottery objects created renowned artist half engaged control task involve evaluating beauty based construal level theory used behavior identification form explore whether aesthetic experiences encourage abstract thinking turn psychological distance also predicted beauty appreciation would lead greater transformative self transcendent emotions indeed appreciating beauty artworks led participants think abstract way especially practice art hobby well transformative self transcendent emotions
"This study focuses on L-shaped frame structures and utilizes nonlinear time-history analysis to evaluate how of seismic waves with varying low-frequency content affect the elastic-plastic torsional response during rare earthquakes. Five L-shaped frame models were developed using ABAQUS, with length-to-width ratios (L/B=) of 1:1, 1.5:1, 2:1, 2.5:1, and 3:1. By comparing peak shear forces and bending moments at the tops of corner columns in the direction of excitation under earthquake wave passage excitations, this research highlights the impact of low-frequency content on wave passage effect. Findings show that when seismic waves contain sufficient low-frequency content, wave passage effect occurs in wave-facing outer corner columns, whereas inner corner columns do not. The last-arriving outer corner columns, however, displayed no consistent pattern. With the rise of the L/B ratio (when L/B = 3), the peak shear force and bending moment of the wave-facing outer corner columns go up by 13.8% and 7.1%, respectively. The last-arriving outer corner columns encounter up to an 11.1% increases in peak shear force and 10% in peak bending moment. When the seismic waves lack sufficient low-frequency content, the corner columns of the L-shaped frame structure are less likely to experience wave passage effect.",study focuses shaped frame structures utilizes nonlinear time history analysis evaluate seismic waves varying low frequency content affect elastic plastic torsional response rare earthquakes five shaped frame models developed using abaqus length width ratios comparing peak shear forces bending moments tops corner columns direction excitation earthquake wave passage excitations research highlights impact low frequency content wave passage effect findings show seismic waves contain sufficient low frequency content wave passage effect occurs wave facing outer corner columns whereas inner corner columns last arriving outer corner columns however displayed consistent pattern rise ratio peak shear force bending moment wave facing outer corner columns respectively last arriving outer corner columns encounter increases peak shear force peak bending moment seismic waves lack sufficient low frequency content corner columns shaped frame structure less likely experience wave passage effect
"In the rapidly evolving digital landscape, virtual reality (VR) has emerged as a transformative tool in education, particularly in language acquisition. Traditional language learning methods often struggle to provide immersive, interactive, and engaging experiences that foster real-world communication skills. VR environments are increasingly transforming English language acquisition by providing immersive and interactive learning experiences that enhance communication skills development. This paper explores the impact of immersive VR technologies on English language acquisition, focusing on communication skills development through dynamic and interactive learning experiences. A novel Dynamic Monarch Butterfly Optimized Bidirectional Gated Recurrent Unit (DMBO-BiGRU) model is presented to analyze learner interactions within VR environments, assessing communication patterns, language complexity, and learning progression. The dataset, collected from student interactions in VR settings, was preprocessed using noise filtering and normalization to ensure data quality. t-Distributed Stochastic Neighbor Embedding (t-SNE) is employed to extract key features, such as fluency, vocabulary usage, and contextual appropriateness. The DMBO-BiGRU model leverages adaptive learning strategies to provide tailored feedback and optimizes language acquisition based on learners’ communication skills. The effectiveness of the suggested method is demonstrated through experimental results, which indicate good accuracy and positive VR learning scores. The results underscore how immersive, customized, and interactive experiences provided by VR-integrated deep learning models hold the potential to transform language acquisition, showing accuracy of 97.2%, VR score of 97.52%, training gain of 98.49%, detection score of 92.6%, and a retention rate of 96% at iteration 13. This work contributes to the growing field of technology-enhanced education, emphasizing the role of VR and AI-driven methods in enhancing English communication skills.",rapidly evolving digital landscape virtual reality emerged transformative tool education particularly language acquisition traditional language learning methods often struggle provide immersive interactive engaging experiences foster real world communication skills environments increasingly transforming english language acquisition providing immersive interactive learning experiences enhance communication skills development paper explores impact immersive technologies english language acquisition focusing communication skills development dynamic interactive learning experiences novel dynamic monarch butterfly optimized bidirectional gated recurrent unit dmbo bigru model presented analyze learner interactions within environments assessing communication patterns language complexity learning progression dataset collected student interactions settings preprocessed using noise filtering normalization ensure data quality distributed stochastic neighbor embedding sne employed extract key features fluency vocabulary usage contextual appropriateness dmbo bigru model leverages adaptive learning strategies provide tailored feedback optimizes language acquisition based learners communication skills effectiveness suggested method demonstrated experimental results indicate good accuracy positive learning scores results underscore immersive customized interactive experiences provided integrated deep learning models hold potential transform language acquisition showing accuracy score training gain detection score retention rate iteration work contributes growing field technology enhanced education emphasizing role driven methods enhancing english communication skills
"The rapid advancement of railway communication systems necessitates robust security mechanisms to ensure reliable and secure data transmission. It explores the design and analysis of an information transmission coding algorithm (ITCA) tailored for railway communication system security schemes. Specifically, it focuses on three key areas: error correction coding, encryption mechanisms, and authentication protocols. The proposed scheme integrates the techniques in a layered manner, ensuring robust protection against security threats such as data interception, tampering, and unauthorized access, while also optimizing performance for real-time railway communication requirements. Hamming code is used to detect and correct errors, ensuring that the received data is as accurate as the original message. For encryption, the Hybrid RSA-Blowfish (HRB) algorithm is utilized. It combines the strength of RSA for key exchange and Blowfish for fast, secure data encryption, providing robust protection against unauthorized access during transmission. Password-based authentication is implemented to verify and ensure message integrity and non-repudiation. The methodology demonstrates its effectiveness through simulation, showing a significant reduction in error rates, enhanced security with minimal computational overhead, and improved overall system performance. Key performance metrics, such as throughput (50 Mbps), latency (7 ms), and data integrity (96.3%), were improved demonstrating that the proposed approach balances security and efficiency in the Python platform. The methodology demonstrates its effectiveness through simulation and evaluation, showcasing a secure and reliable approach to safeguarding railway communication systems.",rapid advancement railway communication systems necessitates robust security mechanisms ensure reliable secure data transmission explores design analysis information transmission coding algorithm itca tailored railway communication system security schemes specifically focuses three key areas error correction coding encryption mechanisms authentication protocols proposed scheme integrates techniques layered manner ensuring robust protection security threats data interception tampering unauthorized access also optimizing performance real time railway communication requirements hamming code used detect correct errors ensuring received data accurate original message encryption hybrid rsa blowfish hrb algorithm utilized combines strength rsa key exchange blowfish fast secure data encryption providing robust protection unauthorized access transmission password based authentication implemented verify ensure message integrity non repudiation methodology demonstrates effectiveness simulation showing significant reduction error rates enhanced security minimal computational overhead improved overall system performance key performance metrics throughput mbps latency data integrity improved demonstrating proposed approach balances security efficiency python platform methodology demonstrates effectiveness simulation evaluation showcasing secure reliable approach safeguarding railway communication systems
"In order to overcome the energy “hole” problem caused by uneven energy consumption of nodes in equidistant deployment, an unequal spacing optimization deployment method is adopted to deploy a chain type wireless sensor network to balance node energy consumption and extend the network life-cycle. Furthermore, a star-chain structure multi-chain wireless sensor network node optimization deployment method is adopted to deploy a two-dimensional monitoring network; Taking into account inter chain interference, network connectivity, and node energy consumption, the optimal node spacing and hop count for each chain, as well as the optimal number of chains in circular areas, are obtained. Through MATLAB simulation, it is proven that this deployment method can effectively reduce and balance node energy consumption, and maximize the network’s life-cycle.",order overcome energy hole problem caused uneven energy consumption nodes equidistant deployment unequal spacing optimization deployment method adopted deploy chain type wireless sensor network balance node energy consumption extend network life cycle furthermore star chain structure multi chain wireless sensor network node optimization deployment method adopted deploy two dimensional monitoring network taking account inter chain interference network connectivity node energy consumption optimal node spacing hop count chain well optimal number chains circular areas obtained matlab simulation proven deployment method effectively reduce balance node energy consumption maximize network life cycle
"Traditional elderly care systems often rely on single-modality emotion recognition, overlooking the complexity and diversity of emotional expressions in the elderly. This study integrates three pre-trained models—VGGFace for facial recognition, DeepSpeech for voice analysis, and BERT for text processing—to develop a multimodal emotion recognition system that enhances intelligent and personalized elderly care. A high-quality emotion dataset is first constructed through comprehensive data collection and preprocessing. Transfer learning is then applied to optimize the three models, achieving precise facial, voice, and text-based emotion recognition. By incorporating feature-level and decision-level fusion strategies, the system improves recognition accuracy and robustness. A dynamic feedback mechanism further refines care strategies, ensuring a more adaptive and humanized elderly care experience. Comparative experiments show that the proposed system significantly outperforms existing models in emotion recognition accuracy (88.63%) and recall (85.93%), with an average response time of just 3.22 seconds. Moreover, after system implementation, the mental health scores of elderly participants improved, decreasing from 6.53 to 4.73 points. User feedback confirms the system’s effectiveness, with an average satisfaction rating of 4.33 across multiple care service dimensions. These findings highlight the potential of multimodal AI-driven emotion recognition in enhancing emotional well-being and mental health support for the elderly, offering a valuable reference for the future development of intelligent elderly care systems.",traditional elderly care systems often rely single modality emotion recognition overlooking complexity diversity emotional expressions elderly study integrates three pre trained models vggface facial recognition deepspeech voice analysis bert text processing develop multimodal emotion recognition system enhances intelligent personalized elderly care high quality emotion dataset first constructed comprehensive data collection preprocessing transfer learning applied optimize three models achieving precise facial voice text based emotion recognition incorporating feature level decision level fusion strategies system improves recognition accuracy robustness dynamic feedback mechanism refines care strategies ensuring adaptive humanized elderly care experience comparative experiments show proposed system significantly outperforms existing models emotion recognition accuracy recall average response time seconds moreover system implementation mental health scores elderly participants improved decreasing points user feedback confirms system effectiveness average satisfaction rating across multiple care service dimensions findings highlight potential multimodal driven emotion recognition enhancing emotional well mental health support elderly offering valuable reference future development intelligent elderly care systems
"Deep learning (DL)-based style transfer techniques have been employed to modify traditional woodcut paper horse art images. This art form struggles to maintain its cultural character while adapting to modern digital media. Style transfer, as a DL technique, allows for the combination of the innovative style of one image with the content of another, enabling the digital recreation of conventional aesthetics. Data collection involved the compilation of a dataset consisting of high-resolution images of traditional woodcut paper horse art. The dataset was preprocessed through normalization of image sizes, application of data augmentation techniques to enhance diversity and mitigate overfitting, and filtering of images to isolate specific features of paper horse art, including line work, shading, and texture. This research proposed the Intelligent Pied Kingfisher - Refined Convolutional Neural Network (IPK-RCNN) model, which is trained on both traditional woodcut images and modern digital art, aiming to create a hybrid style that respects traditional aesthetics while integrating contemporary stylistic elements. The style transfer process employs a Region-based Convolutional Neural Network (RCNN) that incorporates IPK optimization for the enhancement of convolutional layer weight parameters, thereby improving both efficiency and accuracy. The IPK-RCNN method effectively preserves the intricate textures and details of woodcut art while incorporating modern stylistic variations. The model achieves high-quality outputs, attaining a Peak Signal-to-Noise Ratio (PSNR) of 35.12 dB, a Structural Similarity Index (SSIM) of 0.96, an Area Under Curve (AUC) of 0.90, and an efficient execution time of 31.17 ms. This approach underscores the potential of DL in the preservation and reimagining of traditional art forms for digital platforms.",deep learning based style transfer techniques employed modify traditional woodcut paper horse art images art form struggles maintain cultural character adapting modern digital media style transfer technique allows combination innovative style one image content another enabling digital recreation conventional aesthetics data collection involved compilation dataset consisting high resolution images traditional woodcut paper horse art dataset preprocessed normalization image sizes application data augmentation techniques enhance diversity mitigate overfitting filtering images isolate specific features paper horse art including line work shading texture research proposed intelligent pied kingfisher refined convolutional neural network ipk rcnn model trained traditional woodcut images modern digital art aiming create hybrid style respects traditional aesthetics integrating contemporary stylistic elements style transfer process employs region based convolutional neural network rcnn incorporates ipk optimization enhancement convolutional layer weight parameters thereby improving efficiency accuracy ipk rcnn method effectively preserves intricate textures details woodcut art incorporating modern stylistic variations model achieves high quality outputs attaining peak signal noise ratio psnr structural similarity index ssim area curve auc efficient execution time approach underscores potential preservation reimagining traditional art forms digital platforms
"Emotion analysis in literary texts is a complex task due to the intricate nature of language, rich contextual dependencies, and subtle emotional manifestations present in narratives, dialogues, and poetic structures. Traditional lexicon-based models frequently fail to capture the complexity of emotions in literature, leading to incorrect classifications. This study examines the ability of BERT to enhance emotion analysis for literary texts. An innovative Dynamic Honey Badger-tuned BERT (DHB-BERT) model is applied to effectively detect emotional changes in literary texts. Training and emotion analysis are ensured through a diverse dataset of Chinese literary works, with accurate samples to evaluate the model. Lemmatization and stemming serve as preprocessing techniques that reduce words to their most basic forms to preserve continuity. Then, feature extraction is performed using Word2Vec, which maintains relevant linkages and captures emotional fluctuations. Based on the sentiment analysis, texts are classified as positive, negative, neutral, or mixed. The Dynamic Honey Badger algorithm is employed for feature selection and adaptation of the dataset for improved emotion classification. The BERT model is then utilized to capture meaningful emotional variations across various literary forms. This simulation is executed using a Python platform. To evaluate the effectiveness of the simulation, various performance metrics are employed, including precision (94.76%), recall (95%), accuracy (95%), and F1-score (94.13%). Experimental results indicate that the DHB-BERT model significantly outperforms traditional algorithms, particularly in handling rhetorical language, irony, and nuanced emotional manifestations. This research highlights the potential of transformer-based architectures in advancing sentiment analysis for literary texts, providing valuable insights for computational linguistics, literary studies, and sentiment-aware systems.",emotion analysis literary texts complex task due intricate nature language rich contextual dependencies subtle emotional manifestations present narratives dialogues poetic structures traditional lexicon based models frequently fail capture complexity emotions literature leading incorrect classifications study examines ability bert enhance emotion analysis literary texts innovative dynamic honey badger tuned bert dhb bert model applied effectively detect emotional changes literary texts training emotion analysis ensured diverse dataset chinese literary works accurate samples evaluate model lemmatization stemming serve preprocessing techniques reduce words basic forms preserve continuity feature extraction performed using word vec maintains relevant linkages captures emotional fluctuations based sentiment analysis texts classified positive negative neutral mixed dynamic honey badger algorithm employed feature selection adaptation dataset improved emotion classification bert model utilized capture meaningful emotional variations across various literary forms simulation executed using python platform evaluate effectiveness simulation various performance metrics employed including precision recall accuracy score experimental results indicate dhb bert model significantly outperforms traditional algorithms particularly handling rhetorical language irony nuanced emotional manifestations research highlights potential transformer based architectures advancing sentiment analysis literary texts providing valuable insights computational linguistics literary studies sentiment aware systems
"In alpine and canyon areas, the problems of reservoir bank deformation and landslides are prominent. How to quickly and efficiently monitor deformation, timely identify geological disasters, and carry out surveys and treatments has become a crucial issue to be resolved urgently. Traditional survey methods are restricted by factors such as steep terrain and inconvenient transportation, resulting in low efficiency. In recent years, remote sensing technology has developed rapidly in the field of geological disaster monitoring, thanks to its high-precision deformation monitoring capabilities. This study is based on the Baihetan Hydropower Station during the water storage stage. The InSAR deformation monitoring technology is used to conduct large-scale disaster risk screening of the reservoir bank slopes. Firstly, the study analyzes the characteristics of UAV laser point clouds and images to construct a point cloud sequence. Subsequently, an improved iterative closest point (ICP) algorithm that integrates the scale-invariant feature transform (SIFT) and cylindrical neighborhood search is applied to improve the accuracy of slope deformation extraction. Finally, with the help of recognition algorithms and practical engineering experience, the surface deformation is analyzed based on the measured terrain data. The research shows that the surface deformation recognition technology based on UAV inspection and InSAR data has significant advantages in the monitoring of geological disasters in reservoirs in alpine and canyon areas. It can detect potential hazards in a timely manner and provide key decision-making basis for engineering projects.",alpine canyon areas problems reservoir bank deformation landslides prominent quickly efficiently monitor deformation timely identify geological disasters carry surveys treatments become crucial issue resolved urgently traditional survey methods restricted factors steep terrain inconvenient transportation resulting low efficiency recent years remote sensing technology developed rapidly field geological disaster monitoring thanks high precision deformation monitoring capabilities study based baihetan hydropower station water storage stage insar deformation monitoring technology used conduct large scale disaster risk screening reservoir bank slopes firstly study analyzes characteristics uav laser point clouds images construct point cloud sequence subsequently improved iterative closest point icp algorithm integrates scale invariant feature transform sift cylindrical neighborhood search applied improve accuracy slope deformation extraction finally help recognition algorithms practical engineering experience surface deformation analyzed based measured terrain data research shows surface deformation recognition technology based uav inspection insar data significant advantages monitoring geological disasters reservoirs alpine canyon areas detect potential hazards timely manner provide key decision making basis engineering projects
"Machine translation (MT) for underrepresented languages, such as Xhosa, presents significant challenges due to limited linguistic resources and the complex nature of these languages. This research proposes a novel approach to improve MT accuracy for Xhosa-to-English translation using Adaptive Gradient Boosted Bidirectional Encoder Representations from Transformers (AdaGrad-BBERT). This method combines the powerful capabilities of BERT with adaptive gradient boosting, enhancing contextual understanding and overall translation accuracy. The system integrates a series of preprocessing steps to optimize model performance. Initially, the dataset undergoes text cleaning, including the removal of noise, normalization of punctuation, and correction of spelling inconsistencies. Tokenization uses BERT’s word-piece model, effectively handling rare and out-of-vocabulary words. Part-of-speech tagging and dependency parsing are applied to capture syntactic relationships specific to Xhosa, which has distinct grammatical structures compared to English. Pre-trained BERT embeddings are employed to generate rich, context-sensitive representations of Xhosa words, ensuring more accurate translations. The encoder-decoder architecture with an attention mechanism is fine-tuned using the AdaGrad-BBERT optimization technique. Translation quality is evaluated using the BLEU score, and model performance is assessed over multiple training epochs. Simulations are conducted using the TensorFlow framework to train and evaluate the model on the Xhosa-English dataset, with results demonstrating significant improvements in translation accuracy, the BLUE score increased 0.896. The proposed system highlights the potential of AdaGrad-BBERT in bridging the gap in MT for underrepresented languages, offering a scalable solution for enhancing MT. This solution holds promising applications in education, cross-cultural communication, and digital inclusion.",machine translation underrepresented languages xhosa presents significant challenges due limited linguistic resources complex nature languages research proposes novel approach improve accuracy xhosa english translation using adaptive gradient boosted bidirectional encoder representations transformers adagrad bbert method combines powerful capabilities bert adaptive gradient boosting enhancing contextual understanding overall translation accuracy system integrates series preprocessing steps optimize model performance initially dataset undergoes text cleaning including removal noise normalization punctuation correction spelling inconsistencies tokenization uses bert word piece model effectively handling rare vocabulary words part speech tagging dependency parsing applied capture syntactic relationships specific xhosa distinct grammatical structures compared english pre trained bert embeddings employed generate rich context sensitive representations xhosa words ensuring accurate translations encoder decoder architecture attention mechanism fine tuned using adagrad bbert optimization technique translation quality evaluated using bleu score model performance assessed multiple training epochs simulations conducted using tensorflow framework train evaluate model xhosa english dataset results demonstrating significant improvements translation accuracy blue score increased proposed system highlights potential adagrad bbert bridging gap underrepresented languages offering scalable solution enhancing solution holds promising applications education cross cultural communication digital inclusion
"In the context of rapid technological advances that have significantly transformed productivity relations, and alongside the ongoing deepening of interdisciplinary research, artificial intelligence has emerged into the cultural discourse, sparking extensive and heated discussions in the fields of digital humanities and ethical philosophy. The purpose of this study is to explore ethical challenges and response strategies in the field of artificial intelligence literature, identifying common ethical issues in the creation of AI literature, including copyright and intellectual property dilemmas caused by the ambiguity of the creative subject, as well as ethical conflicts in content and form. To address these challenges, strategies such as establishing ethical guidelines for artificial intelligence literature and strengthening technical supervision and legal regulations are proposed. It also discusses the future development direction of artificial intelligence literature under ethical guidance.",context rapid technological advances significantly transformed productivity relations alongside ongoing deepening interdisciplinary research artificial intelligence emerged cultural discourse sparking extensive heated discussions fields digital humanities ethical philosophy purpose study explore ethical challenges response strategies field artificial intelligence literature identifying common ethical issues creation literature including copyright intellectual property dilemmas caused ambiguity creative subject well ethical conflicts content form address challenges strategies establishing ethical guidelines artificial intelligence literature strengthening technical supervision legal regulations proposed also discusses future development direction artificial intelligence literature ethical guidance
"The integration of artificial intelligence (AI) in revitalizing intangible cultural heritage (ICH) necessitates solutions to enhance participation and preserve culture, thereby contributing to the growth of the cultural industry. The objective of this research is to design an AI-driven model utilizing an Adaptive Donkey and Smuggler Algorithm-mutated Malleable Long Short-Term Memory (ADS-MLSTM) network to enhance the recognition, preservation, and revitalization of ICH, supporting cultural industry growth and sustainability. Data were collected from multiple ICH archives, including digital representations of cultural heritage. This collected data underwent preprocessing steps such as noise reduction and data cleaning to ensure robustness against the diverse representations of ICH. Utilizing the Term Frequency-Inverse Document Frequency (TF-IDF) method, features were extracted efficiently. The integration of ADS and MLSTM algorithms in the proposed ADS-MLSTM model demonstrates superior performance, achieving a precision of 98.70%, a mean squared error (MSE) of 0.73, a recall of 98.27%, an F1-score of 98.80%, an accuracy of 99%, and a root mean squared error (RMSE) of 0.57, further highlighting its effectiveness. The incorporation of deep learning significantly enhanced the model’s effectiveness, leading to better results in recognizing diverse ICH elements. AI plays an essential role in recovering intangible cultural assets, particularly through the ADS-MLSTM model. By improving ICH recognition and fostering user interaction, AI-driven approaches contribute to the growth of the cultural industry, offering an innovative solution for preserving and promoting heritage.",integration artificial intelligence revitalizing intangible cultural heritage ich necessitates solutions enhance participation preserve culture thereby contributing growth cultural industry objective research design driven model utilizing adaptive donkey smuggler algorithm mutated malleable long short term memory ads mlstm network enhance recognition preservation revitalization ich supporting cultural industry growth sustainability data collected multiple ich archives including digital representations cultural heritage collected data underwent preprocessing steps noise reduction data cleaning ensure robustness diverse representations ich utilizing term frequency inverse document frequency idf method features extracted efficiently integration ads mlstm algorithms proposed ads mlstm model demonstrates superior performance achieving precision mean squared error mse recall score accuracy root mean squared error rmse highlighting effectiveness incorporation deep learning significantly enhanced model effectiveness leading better results recognizing diverse ich elements plays essential role recovering intangible cultural assets particularly ads mlstm model improving ich recognition fostering user interaction driven approaches contribute growth cultural industry offering innovative solution preserving promoting heritage
"This research investigates the development of an adaptive blended teaching model (ABTM) that employs customized instructional strategies to optimize learning outcomes. The approach harnesses data-driven insights derived from student performance, behavior, and engagement to provide a personalized educational experience tailored to each student’s requirements. The integration of big data analytics and machine learning (ML) in education presents significant potential to transform traditional teaching methodologies. Data is collected from open sources, including engagement scores, assessment results, forum participation, attendance, and study hours. Preprocessing steps include data cleaning, normalization, and handling missing values to ensure data reliability. The term frequency-inverse document frequency (TF-IDF) text mining technique is utilized to extract features from student-generated content, highlighting essential phrases. TF-IDF enables the identification of critical learning themes and areas requiring additional support. A hybrid method, namely, the snow leopard optimized-tuned intelligent CatBoost (SLO-ICatBoost), is deployed for predicting student grades, assessing performance, and enhancing the educational process. The SLO improves the selection of relevant features, while the ICatBoost algorithm classifies students based on their performance patterns and learning behaviors. When compared to conventional teaching techniques, the proposed SLO-ICatBoost method significantly improves precision (0.990), accuracy (0.991), F1-score (0.990), and recall (0.991). Due to its flexibility in accommodating various learning environments and individual requirements, this approach can be applied in diverse educational settings.",research investigates development adaptive blended teaching model abtm employs customized instructional strategies optimize learning outcomes approach harnesses data driven insights derived student performance behavior engagement provide personalized educational experience tailored student requirements integration big data analytics machine learning education presents significant potential transform traditional teaching methodologies data collected open sources including engagement scores assessment results forum participation attendance study hours preprocessing steps include data cleaning normalization handling missing values ensure data reliability term frequency inverse document frequency idf text mining technique utilized extract features student generated content highlighting essential phrases idf enables identification critical learning themes areas requiring additional support hybrid method namely snow leopard optimized tuned intelligent catboost slo icatboost deployed predicting student grades assessing performance enhancing educational process slo improves selection relevant features icatboost algorithm classifies students based performance patterns learning behaviors compared conventional teaching techniques proposed slo icatboost method significantly improves precision accuracy score recall due flexibility accommodating various learning environments individual requirements approach applied diverse educational settings
"Augmented reality (AR) is revolutionizing the way we interact with information by blending the physical and digital worlds to create immersive and interactive environments. In the context of English vocabulary learning, AR offers an innovative approach to enhance engagement, comprehension, and retention. This research aims to improve English vocabulary teaching through AR-based applications, addressing challenges such as pedagogical depth, diverse learning styles, and the integration of real-world contexts. The study focuses on the design and deployment of an AR-based learning application that incorporates gamified elements, interactive visuals, and real-time feedback to facilitate students’ retention and understanding of vocabulary. Data collection methods include tracking user interactions, gathering feedback responses, and assessing performance on vocabulary learning activities. The collected data underwent preprocessing, which involved data cleaning and normalization. Principal component analysis (PCA) was employed to extract irrelevant features from the processed data. The improved weighted hybrid deep feedforward neural network (IWH-DFNN) was utilized to predict student performance and enhance learning outcomes from these interactive experiences. The improved weights hybrid (IWH) approach was applied to optimize the hyperparameters of the deep feedforward neural network (DFNN), thereby increasing the model’s predictive accuracy regarding student performance. The proposed IWH-DFNN model demonstrated superior performance in improving learning outcomes and enhancing the interactive experience, achieving high recall (92.70%), precision (95%), accuracy (97%), F1-score (89%), and minimal accuracy loss (0.03). The findings suggest that AR-based learning environments have the potential to enhance English vocabulary outcomes by integrating machine learning algorithms for adaptive learning within AR settings. This integration creates a more engaging, customized, and efficient learning environment.",augmented reality revolutionizing way interact information blending physical digital worlds create immersive interactive environments context english vocabulary learning offers innovative approach enhance engagement comprehension retention research aims improve english vocabulary teaching based applications addressing challenges pedagogical depth diverse learning styles integration real world contexts study focuses design deployment based learning application incorporates gamified elements interactive visuals real time feedback facilitate students retention understanding vocabulary data collection methods include tracking user interactions gathering feedback responses assessing performance vocabulary learning activities collected data underwent preprocessing involved data cleaning normalization principal component analysis pca employed extract irrelevant features processed data improved weighted hybrid deep feedforward neural network iwh dfnn utilized predict student performance enhance learning outcomes interactive experiences improved weights hybrid iwh approach applied optimize hyperparameters deep feedforward neural network dfnn thereby increasing model predictive accuracy regarding student performance proposed iwh dfnn model demonstrated superior performance improving learning outcomes enhancing interactive experience achieving high recall precision accuracy score minimal accuracy loss findings suggest based learning environments potential enhance english vocabulary outcomes integrating machine learning algorithms adaptive learning within settings integration creates engaging customized efficient learning environment
"Enterprise financial data is multidimensional and highly complex, in order to efficiently and accurately achieve enterprise financial crisis warning. Proposed and designed a machine learning based enterprise financial crisis warning system research. Firstly, principal component analysis was used to perform a dimensionality reduction on 106 financial crisis indicators of enterprises. And by improving the convolution layer of the convolutional neural network to extract the feature values of the warning indicator data after one dimensionality reduction, the extracted data features are compressed through the pooling layer to achieve a second dimensionality reduction. By utilizing a fully connected layer, the features of the data obtained through secondary dimensionality reduction are transformed into feature vectors and input into the classification layer, thereby achieving multi warning of financial crises in enterprises. Research has shown that after using PCA method to reduce the dimensionality of financial crisis indicators in experimental enterprises, the accuracy of the warning model has increased from the highest 0.83 to 0.75, and the training speed has decreased from 3 seconds to about 1 second; the average absolute error percentage of early warning is about 5.32%. It has practicality.",enterprise financial data multidimensional highly complex order efficiently accurately achieve enterprise financial crisis warning proposed designed machine learning based enterprise financial crisis warning system research firstly principal component analysis used perform dimensionality reduction financial crisis indicators enterprises improving convolution layer convolutional neural network extract feature values warning indicator data one dimensionality reduction extracted data features compressed pooling layer achieve second dimensionality reduction utilizing fully connected layer features data obtained secondary dimensionality reduction transformed feature vectors input classification layer thereby achieving multi warning financial crises enterprises research shown using pca method reduce dimensionality financial crisis indicators experimental enterprises accuracy warning model increased highest training speed decreased seconds second average absolute error percentage early warning practicality
"The Flying Bluebottle Illusion is a compelling example of how the perceived trajectory of moving objects can be greatly influenced by other motion sources in the visual scene. In this article, we present a series of simplified variants of the Flying Bluebottle Illusion in which the true motion of a target is a circular orbit around a central point. However, when a similar but offset orbiting motion trajectory is added to a set of surrounding inducers, the perceived trajectory of the target is drastically altered in both extent and direction. In other words, the perceived orbiting motion of the target is “pulled” and distorted by the orbiting motion of the inducers. For simplicity's sake, we refer to the illusory effect revealed by these dueling orbits as the Dueling Orbit Illusion. These simplified variants lend themselves to empirical study with resultant effects that can be readily modeled. Here, we present a series of case examples for how the parameters of the stimuli may be varied to yield predictable effects, describe a straightforward computational model for quantifying the magnitude of the contextual influence, and discuss how the model may be leveraged to gain insight into the phenomenon of induced motion across a range of within and between observer domains.",flying bluebottle illusion compelling example perceived trajectory moving objects greatly influenced motion sources visual scene article present series simplified variants flying bluebottle illusion true motion target circular orbit around central point however similar offset orbiting motion trajectory added set surrounding inducers perceived trajectory target drastically altered extent direction words perceived orbiting motion target pulled distorted orbiting motion inducers simplicity sake refer illusory effect revealed dueling orbits dueling orbit illusion simplified variants lend empirical study resultant effects readily modeled present series case examples parameters stimuli may varied yield predictable effects describe straightforward computational model quantifying magnitude contextual influence discuss model may leveraged gain insight phenomenon induced motion across range within observer domains
"Numerical heat transfer models are increasingly used to compute the temperature field, melt pool dimensions and thermal cycles for laser powder bed fusion but huge computational demand of these models is a perpetual challenge. We present a recently introduced novel dynamic meshing strategy to preserve a fine meshed region always beneath the laser beam and a gradually coarsened region away from the beam thereby notably reducing the overall model size and computational demand. Probed over a wide range of conditions, we show a reduction of the computational time and volume by nearly 6 to 10 times vis-à-vis little loss of accuracy in the computed results with the dynamic meshing strategy compared to traditional approaches with static uniform meshing.",numerical heat transfer models increasingly used compute temperature field melt pool dimensions thermal cycles laser powder bed fusion huge computational demand models perpetual challenge present recently introduced novel dynamic meshing strategy preserve fine meshed region always beneath laser beam gradually coarsened region away beam thereby notably reducing overall model size computational demand probed wide range conditions show reduction computational time volume nearly times vis vis little loss accuracy computed results dynamic meshing strategy compared traditional approaches static uniform meshing
"Creative writing instruction plays a crucial role in developing students’ linguistic and cognitive abilities. However, challenges such as lack of engagement, difficulty in idea generation, and limited stylistic diversity hinder students from fully expressing their thoughts. Artificial intelligence (AI) offers a promising solution to enhance writing quality and creativity. This study aims to develop an AI-assisted creative writing framework by integrating GPT-4 with Innovative Locust Swarm Optimization (ILSO) to generate more engaging, coherent, and stylistically rich text tailored to students’ writing levels. A dataset of student-written essays, novels, and poetry was collected for training. Pre-processing techniques, including text normalization and tokenization, were applied to refine the input text. Feature extraction was performed using Word2Vec embedding to enhance semantic understanding. GPT-4 generates adaptive text suggestions, while ILSO optimizes model hyperparameters to refine text coherence, creativity, and narrative flow. The optimized model adapts to individual writing styles, offering dynamic suggestions that encourage creativity while maintaining fluency. The ILSO algorithm fine-tunes the generation process by enhancing text structuring and thematic consistency. The proposed method was implemented using Python 3.10.1. Experimental results demonstrate that the optimized GPT model significantly improves coherence scores, stylistic variation, thematic consistency, writing proficiency, and engagement rates. However, concerns regarding AI dependency and originality necessitate a balanced integration of AI-assisted and traditional writing pedagogy. This study provides a foundation for future adaptive AI-driven creative writing instruction, with potential extensions including real-time feedback systems and self-learning mechanisms for personalized writing enhancement.",creative writing instruction plays crucial role developing students linguistic cognitive abilities however challenges lack engagement difficulty idea generation limited stylistic diversity hinder students fully expressing thoughts artificial intelligence offers promising solution enhance writing quality creativity study aims develop assisted creative writing framework integrating gpt innovative locust swarm optimization ilso generate engaging coherent stylistically rich text tailored students writing levels dataset student written essays novels poetry collected training pre processing techniques including text normalization tokenization applied refine input text feature extraction performed using word vec embedding enhance semantic understanding gpt generates adaptive text suggestions ilso optimizes model hyperparameters refine text coherence creativity narrative flow optimized model adapts individual writing styles offering dynamic suggestions encourage creativity maintaining fluency ilso algorithm fine tunes generation process enhancing text structuring thematic consistency proposed method implemented using python experimental results demonstrate optimized gpt model significantly improves coherence scores stylistic variation thematic consistency writing proficiency engagement rates however concerns regarding dependency originality necessitate balanced integration assisted traditional writing pedagogy study provides foundation future adaptive driven creative writing instruction potential extensions including real time feedback systems self learning mechanisms personalized writing enhancement
"This study examined the effect of gamification on visual programming and computational thinking skills among primary school students, aiming to investigate how gamified learning environments enhance cognitive skill development and conceptual integration compared to traditional teaching methods. A quasi-experimental design was employed, involving 346 fifth-grade students from Jeddah, Saudi Arabia, who were divided into experimental and control groups. The experimental group utilized gamified tools, such as ClassDojo and Wordwall, while the control group employed traditional teaching methods. Data collection included achievement tests, observational measures, and assessments of computational thinking. The author used Extended Bayesian Information Criterion (EBIC) with the “qgraph” package in R to estimate the network structures. Initial visual programming skills showed no significant differences between groups. However, post-test results revealed substantial improvements in both visual programming and computational thinking for the experimental group. The gamified learning environment effectively enhanced these skills through increased motivation and active engagement. These findings support the use of gamification in primary education to enhance critical 21st-century skills. However, the study’s geographic and grade-level specificity limits generalizability, and long-term skill retention was not assessed. Future research should expand to diverse demographics, incorporate longitudinal designs, and explore integration with emerging technologies (e.g., adaptive learning platforms).",study examined effect gamification visual programming computational thinking skills among primary school students aiming investigate gamified learning environments enhance cognitive skill development conceptual integration compared traditional teaching methods quasi experimental design employed involving fifth grade students jeddah saudi arabia divided experimental control groups experimental group utilized gamified tools classdojo wordwall control group employed traditional teaching methods data collection included achievement tests observational measures assessments computational thinking author used extended bayesian information criterion ebic qgraph package estimate network structures initial visual programming skills showed significant differences groups however post test results revealed substantial improvements visual programming computational thinking experimental group gamified learning environment effectively enhanced skills increased motivation active engagement findings support use gamification primary education enhance critical century skills however study geographic grade level specificity limits generalizability long term skill retention assessed future research expand diverse demographics incorporate longitudinal designs explore integration emerging technologies adaptive learning platforms
"The increasing integration of learning analytics (LA) in education creates new opportunities to improve student engagement and academic achievement, particularly in English language teaching (ELT). However, several critical limitations affect the deployment of these technologies, including privacy concerns and the need for advanced predictions about student behavior alongside adaptations to standard educational frameworks. This investigation examines the role of LA in ELT, focusing on how data-driven strategies powered by artificial intelligence (AI), such as deep learning (DL), can enhance student engagement and academic success. The dataset, sourced from a publicly available Kaggle repository, comprises anonymized real-world student interaction metrics, including demographics, engagement patterns, interactions with the learning system, and academic performance. To efficiently preprocess the dataset, numerical features are normalized using Z-score normalization, and features are extracted using term frequency-inverse document frequency (TF-IDF), which assigns weights to each term based on its frequency in a document and rarity across the dataset. DL models, including the weighted white shark optimized deep residual network (WWSO-DResNet), were employed to examine student engagement patterns, predict learning outcomes, and provide personalized learning paths. The findings indicate that students who received personalized learning interventions based on WWSO-DResNet insights performed significantly better in terms of engagement and academic achievement. The performance of the proposed WWSO-DResNet approach was evaluated using metrics such as MAE (0.09), RMSE (0.21), MSE (0.18), and MAPE (0.25). The WWSO-DResNet method proved effective in identifying at-risk students early, enabling preventive interventions. In conclusion, AI-powered LA holds promise for transforming ELT by fostering personalized learning experiences.",increasing integration learning analytics education creates new opportunities improve student engagement academic achievement particularly english language teaching elt however several critical limitations affect deployment technologies including privacy concerns need advanced predictions student behavior alongside adaptations standard educational frameworks investigation examines role elt focusing data driven strategies powered artificial intelligence deep learning enhance student engagement academic success dataset sourced publicly available kaggle repository comprises anonymized real world student interaction metrics including demographics engagement patterns interactions learning system academic performance efficiently preprocess dataset numerical features normalized using score normalization features extracted using term frequency inverse document frequency idf assigns weights term based frequency document rarity across dataset models including weighted white shark optimized deep residual network wwso dresnet employed examine student engagement patterns predict learning outcomes provide personalized learning paths findings indicate students received personalized learning interventions based wwso dresnet insights performed significantly better terms engagement academic achievement performance proposed wwso dresnet approach evaluated using metrics mae rmse mse mape wwso dresnet method proved effective identifying risk students early enabling preventive interventions conclusion powered holds promise transforming elt fostering personalized learning experiences
"Colleges and universities are crucial for nurturing students, with micro-culture construction as a key factor in their developmental process. This paper uses Jinling Institute of Technology as a case study and applies methods such as literature analysis and questionnaire surveys, analyzing data from 4732 questionnaires. The findings show that students are eager to participate in the construction of campus micro-culture. However, their willingness, satisfaction, and attitude toward participation are mainly influenced by their past learning experiences, cultural backgrounds, and personal relationships. Besides, factors like age and years of education are also correlated with the types of participation and the quality of construction enhancement. The research proposes using appropriate techniques to encourage active participation, thereby improving their cultural development, emotional experiences, and moral cultivation. These techniques should consider potential factors such as audience needs, media types, work platforms, and situational conditions to be more effective in practical applications, offer insights for other universities, and provide greater social value.",colleges universities crucial nurturing students micro culture construction key factor developmental process paper uses jinling institute technology case study applies methods literature analysis questionnaire surveys analyzing data questionnaires findings show students eager participate construction campus micro culture however willingness satisfaction attitude toward participation mainly influenced past learning experiences cultural backgrounds personal relationships besides factors like age years education also correlated types participation quality construction enhancement research proposes using appropriate techniques encourage active participation thereby improving cultural development emotional experiences moral cultivation techniques consider potential factors audience needs media types work platforms situational conditions effective practical applications offer insights universities provide greater social value
"The application of data-driven metrics in sports has significantly impacted the development of training programs for elite basketball players. By utilizing advanced analytics and wearable technology, coaches can gain objective insights into players’ performance, offering personalized interventions to enhance skill development. The usage of wearable devices such as motion sensors was assessed to collect kinematic and kinetic data on players’ physical exertion during training sessions. The data collected from 250 players using wearable technology offers valuable insights, enabling the development of highly targeted training programs and game-specific action factors such as dribbling, blocking, rebounding, and defensive positions, which are critical for optimizing overall player performance. The SPSS software of version 29 has been utilized. The incorporation of ANOVA analysis, paired t-test, and chi-square enhance the mean and standard deviations between pre- and post-training test results to assess differences in players’ improvement. It helps to decide whether particular results relating to different training efforts have an effect, and the regression analysis evaluates factors like training intensity or player attributes. The results indicate measurable improvements in player agility, shooting accuracy, and endurance, aligning training with individual needs. The integration of data-driven metrics into basketball training programs offers a more tailored approach, significantly enhancing player performance and efficiency. The ability to monitor and adjust training in real-time based on data-driven insights helps players reach their maximum potential and contributes to overall team success.",application data driven metrics sports significantly impacted development training programs elite basketball players utilizing advanced analytics wearable technology coaches gain objective insights players performance offering personalized interventions enhance skill development usage wearable devices motion sensors assessed collect kinematic kinetic data players physical exertion training sessions data collected players using wearable technology offers valuable insights enabling development highly targeted training programs game specific action factors dribbling blocking rebounding defensive positions critical optimizing overall player performance spss software version utilized incorporation anova analysis paired test chi square enhance mean standard deviations pre post training test results assess differences players improvement helps decide whether particular results relating different training efforts effect regression analysis evaluates factors like training intensity player attributes results indicate measurable improvements player agility shooting accuracy endurance aligning training individual needs integration data driven metrics basketball training programs offers tailored approach significantly enhancing player performance efficiency ability monitor adjust training real time based data driven insights helps players reach maximum potential contributes overall team success
"As businesses increasingly adopt digital tools to streamline operations, artificial intelligence (AI)-based chatbots have emerged as vital components for enhancing customer communication and supporting financial management within accounting services. This research focuses on reliable AI-powered accounting chatbots capable of handling complex financial tasks while enhancing customer communication and user satisfaction. The goal of this study is to establish AI-based chatbots in accounting services to improve financial management assistance and customer communication. This paper presents a novel Raven Roosting-tuned Adaptive Bidirectional Long Short-Term Memory (RR-ABiLSTM) model designed to classify financial queries and enhance contextual understanding in conversations to improve customer communications. The dataset encompasses both structured and unstructured data from accounting conversations, constituting a domain-specific corpus focusing on common accounting tasks. Data preprocessing included text cleaning and tokenization applied to the acquired data. Subsequently, feature extraction was performed using Word2Vec. The RR algorithm was utilized to optimize hyperparameters and feature selection, while BiLSTM ensures a deep understanding of contextual relationships in conversations, thereby enhancing accuracy and efficiency in processing financial queries. Furthermore, a dynamic training mechanism was integrated, allowing the chatbot to continually adapt to increasing consumer demands without downtime. The proposed method was implemented using Python software, and its performance was compared with traditional algorithms. The overall metrics—F1-score (87.75%), precision (89.25%), recall (86.24%), and accuracy (90%)—illustrate that the suggested model significantly improves customer engagement, reduces the workload of accountants, and enhances the overall efficiency of accounting services by providing reliable financial support.",businesses increasingly adopt digital tools streamline operations artificial intelligence based chatbots emerged vital components enhancing customer communication supporting financial management within accounting services research focuses reliable powered accounting chatbots capable handling complex financial tasks enhancing customer communication user satisfaction goal study establish based chatbots accounting services improve financial management assistance customer communication paper presents novel raven roosting tuned adaptive bidirectional long short term memory abilstm model designed classify financial queries enhance contextual understanding conversations improve customer communications dataset encompasses structured unstructured data accounting conversations constituting domain specific corpus focusing common accounting tasks data preprocessing included text cleaning tokenization applied acquired data subsequently feature extraction performed using word vec algorithm utilized optimize hyperparameters feature selection bilstm ensures deep understanding contextual relationships conversations thereby enhancing accuracy efficiency processing financial queries furthermore dynamic training mechanism integrated allowing chatbot continually adapt increasing consumer demands without downtime proposed method implemented using python software performance compared traditional algorithms overall metrics score precision recall accuracy illustrate suggested model significantly improves customer engagement reduces workload accountants enhances overall efficiency accounting services providing reliable financial support
"Using the futures studies perspective, which centers on predicting probable, preferable, and possible futures, this qualitative research investigates how marketing is positioned to help combat climate change. Collecting data digitally and face-to-face, consumers were questioned about their perceptions and forecasts of climate change. Using grounded theory, responses were analyzed until thematic saturation was attained, revealing consumers’ immediate probable and preferred future climate change perspectives. Consumer voices espouse that individual action is not enough to alter the climate change trajectory. Rather, a social contract-grounded, shared responsibility model, with humanity's best interest in mind, is required to address climate change at three marketing-related levels: macro (government), meso (corporate), and micro (consumer). Leaning on the data, which convey participants’ sustainable (eco-friendly) proclivities, theoretical and practical implications are offered.",using futures studies perspective centers predicting probable preferable possible futures qualitative research investigates marketing positioned help combat climate change collecting data digitally face face consumers questioned perceptions forecasts climate change using grounded theory responses analyzed thematic saturation attained revealing consumers immediate probable preferred future climate change perspectives consumer voices espouse individual action enough alter climate change trajectory rather social contract grounded shared responsibility model humanity best interest mind required address climate change three marketing related levels macro government meso corporate micro consumer leaning data convey participants sustainable eco friendly proclivities theoretical practical implications offered
"Current research on traditional textile patterns often relies on visual interpretation and personal experience, leaning more toward sensory cognition rather than rational analysis. To more scientifically explore, understand, and follow the regularities of traditional textile patterns, this study first focuses on Qing Dynasty court women’s Changyi (court robes) as the research subject, constructing a multi-dimensional feature model based on grounded coding theory. Subsequently, employing association analysis, we systematically extract the combination regularities of textile patterns and reveal the deeper cultural connotations behind these regularities. Finally, we validate the effectiveness and practicality of these textile pattern combination regularities through a visual sensory quantitative evaluation method. The evaluation results demonstrate that the Changyi textile patterns extracted using this research method achieve high-grade visual effect ratings, better aligning with contemporary consumer demands. This study provides a rational analysis and scientific foundation for the modern transformation of Chinese traditional textile pattern heritage, not only enriching the theory and methodology of textile pattern research but also holding significant implications for the inheritance of Chinese traditional patterns and the modern adaptation of Chinese traditional clothing.",current research traditional textile patterns often relies visual interpretation personal experience leaning toward sensory cognition rather rational analysis scientifically explore understand follow regularities traditional textile patterns study first focuses qing dynasty court women changyi court robes research subject constructing multi dimensional feature model based grounded coding theory subsequently employing association analysis systematically extract combination regularities textile patterns reveal deeper cultural connotations behind regularities finally validate effectiveness practicality textile pattern combination regularities visual sensory quantitative evaluation method evaluation results demonstrate changyi textile patterns extracted using research method achieve high grade visual effect ratings better aligning contemporary consumer demands study provides rational analysis scientific foundation modern transformation chinese traditional textile pattern heritage enriching theory methodology textile pattern research also holding significant implications inheritance chinese traditional patterns modern adaptation chinese traditional clothing
"Mongolian folk painting images have long served as a symbol of the richness of their cultural legacy. These images represent a profound cultural demonstration of the nomadic lifestyle and spiritual beliefs of the Mongolian people. This study presents a predictive framework that uses CNN-based VGG16 model as an image recognition technology to identify Mongolian folk art. The proposed framework consists of multiple parts, such as image source engines for automated data collecting, image classes for Mongolian and Indian traditional paintings, dataset creation, and manual image visualization to guarantee the quality of the datasets. Stability of the dataset is improved by preprocessing methods like pixel value normalization and overfitting minimization. With its deep convolutional neural network architecture, the CNN-based VGG16 model is used for image recognition and shows skill in recognizing intricate visual patterns. The proposed framework is distinguished by a methodical algorithmic flow that includes dataset creation, model training, iterative refinement, and data retrieval. Test accuracy (77.97%) and test loss (0.6828) demonstrate how well the model recognizes Mongolian folk paintings. Extensive results that include feature extraction, ROC and PR curves, and confusion matrix analysis highlight how well the framework achieves a balanced precision-recall trade-off. This study contributes a useful methodology to the growing field of cultural image recognition, highlighting the integration of traditional art with advanced technology.",mongolian folk painting images long served symbol richness cultural legacy images represent profound cultural demonstration nomadic lifestyle spiritual beliefs mongolian people study presents predictive framework uses cnn based vgg model image recognition technology identify mongolian folk art proposed framework consists multiple parts image source engines automated data collecting image classes mongolian indian traditional paintings dataset creation manual image visualization guarantee quality datasets stability dataset improved preprocessing methods like pixel value normalization overfitting minimization deep convolutional neural network architecture cnn based vgg model used image recognition shows skill recognizing intricate visual patterns proposed framework distinguished methodical algorithmic flow includes dataset creation model training iterative refinement data retrieval test accuracy test loss demonstrate well model recognizes mongolian folk paintings extensive results include feature extraction roc curves confusion matrix analysis highlight well framework achieves balanced precision recall trade study contributes useful methodology growing field cultural image recognition highlighting integration traditional art advanced technology
"To enhance the operational reliability of wireless power supply systems for transmission line monitoring devices and to facilitate bidirectional communication between the power transmitter and receiver, this study proposes a simultaneous wireless power and data transfer (SWPDT) scheme employing a multi-relay coil configuration. By exploiting the multi-resonant behavior inherent in high-order compensation topologies—specifically LCC and LCL structures—the proposed system realizes efficient co-transmission of energy and information. A comprehensive power transmission model, alongside an equivalent circuit model for data communication, is developed to capture the electromagnetic coupling and transmission dynamics within the multi-relay environment. The characteristics of both power and data channels are analytically examined, and the validity of the proposed methodology is substantiated through simulation studies. The results confirm that the system achieves robust power delivery while maintaining reliable data transmission, thereby offering a promising solution for deployment in high-voltage insulator string scenarios.",enhance operational reliability wireless power supply systems transmission line monitoring devices facilitate bidirectional communication power transmitter receiver study proposes simultaneous wireless power data transfer swpdt scheme employing multi relay coil configuration exploiting multi resonant behavior inherent high order compensation topologies specifically lcc lcl structures proposed system realizes efficient transmission energy information comprehensive power transmission model alongside equivalent circuit model data communication developed capture electromagnetic coupling transmission dynamics within multi relay environment characteristics power data channels analytically examined validity proposed methodology substantiated simulation studies results confirm system achieves robust power delivery maintaining reliable data transmission thereby offering promising solution deployment high voltage insulator string scenarios
"To address the challenges of poor adaptability to multi-scale features and high-dimensional redundancy in painting style classification, this study proposes an improved classification method by integrating the Xception network with Principal Component Analysis (PCA) for dimensionality reduction. The Xception model extracts both fine-grained details and global semantic information, while PCA selects the most discriminative principal components to reduce redundant features, enhancing classification accuracy and efficiency. Experiments conducted on Eastern and Western painting datasets with 10-fold cross-validation demonstrate that the proposed model outperforms state-of-the-art methods, including EfficientNet, RegNet, and ConvNeXT. Notably, it achieves an average classification accuracy of 0.973 in Western painting classification. Moreover, the integration of PCA significantly reduces computation time, with classification speeds of 105 ms for Eastern paintings and 100 ms for Western paintings, surpassing benchmark models. This study presents an efficient and precise solution for automated art style classification, demonstrating the effectiveness of combining deep learning with dimensionality reduction. The findings offer valuable insights for computational art analysis, supporting applications in artwork identification, digital archiving, and cultural heritage preservation.",address challenges poor adaptability multi scale features high dimensional redundancy painting style classification study proposes improved classification method integrating xception network principal component analysis pca dimensionality reduction xception model extracts fine grained details global semantic information pca selects discriminative principal components reduce redundant features enhancing classification accuracy efficiency experiments conducted eastern western painting datasets fold cross validation demonstrate proposed model outperforms state art methods including efficientnet regnet convnext notably achieves average classification accuracy western painting classification moreover integration pca significantly reduces computation time classification speeds eastern paintings western paintings surpassing benchmark models study presents efficient precise solution automated art style classification demonstrating effectiveness combining deep learning dimensionality reduction findings offer valuable insights computational art analysis supporting applications artwork identification digital archiving cultural heritage preservation
"In recent years, frequent food safety incidents have resulted in significant losses to both lives and properties, triggering widespread societal concern. To delve into the mechanisms underlying the dissemination of online public opinion during food safety incidents, this research investigated a “Pork with Salted Vegetable” food safety incident in China. The study employed a comprehensive approach, integrating automated text—mining techniques with grounded theory. First, topic modeling was used to identify six dominant concerns (topics) expressed by the public during the relevant time frame, and the key words associated with each topic. Subsequently, grounded theory method was employed to model the dissemination of public opinion toward food safety incidents. The analysis emphasizes the significant roles of four key categories—“public opinion content,” “food safety incident,” “dissemination subject,” and “dissemination channel”—in the process of food safety incident dissemination. The findings offer valuable insights for food crisis governance.",recent years frequent food safety incidents resulted significant losses lives properties triggering widespread societal concern delve mechanisms underlying dissemination online public opinion food safety incidents research investigated pork salted vegetable food safety incident china study employed comprehensive approach integrating automated text mining techniques grounded theory first topic modeling used identify six dominant concerns topics expressed public relevant time frame key words associated topic subsequently grounded theory method employed model dissemination public opinion toward food safety incidents analysis emphasizes significant roles four key categories public opinion content food safety incident dissemination subject dissemination channel process food safety incident dissemination findings offer valuable insights food crisis governance
"The creativity of artistic works encompasses multifaceted elements such as color, shape, and texture, characterized by their complexity and subtlety. Current evaluation methods are predominantly subjective, lacking the ability to objectively and comprehensively capture and quantify these dynamic attributes. To address this limitation, this study proposes an innovative approach for evaluating artistic creativity by integrating an artificial immune algorithm (AIA). Initially, features related to color, shape, and texture are extracted from the artworks, and these feature vectors are input into the AIA as antigens. Subsequently, by defining antigen-antibody matching rules, the features of the artworks are compared with creative reference antibodies generated by the algorithm to derive creativity evaluation results. Finally, leveraging a dynamic adjustment mechanism for bidirectional crossover mutation probabilities, the diversity of the antibody population is optimized through a clonal selection strategy, enhancing the model’s adaptability to diverse artistic styles. Experimental results demonstrate that the proposed improved AIA achieves significant enhancements in evaluation metrics, with average Precision and Recall values increasing by 8.45% and 11.21%, respectively, compared to the baseline AIA. This study concludes that the AIA-integrated creativity evaluation method effectively improves the objectivity and accuracy of artistic assessments, offering a novel perspective for the intelligent evaluation of artworks.",creativity artistic works encompasses multifaceted elements color shape texture characterized complexity subtlety current evaluation methods predominantly subjective lacking ability objectively comprehensively capture quantify dynamic attributes address limitation study proposes innovative approach evaluating artistic creativity integrating artificial immune algorithm aia initially features related color shape texture extracted artworks feature vectors input aia antigens subsequently defining antigen antibody matching rules features artworks compared creative reference antibodies generated algorithm derive creativity evaluation results finally leveraging dynamic adjustment mechanism bidirectional crossover mutation probabilities diversity antibody population optimized clonal selection strategy enhancing model adaptability diverse artistic styles experimental results demonstrate proposed improved aia achieves significant enhancements evaluation metrics average precision recall values increasing respectively compared baseline aia study concludes aia integrated creativity evaluation method effectively improves objectivity accuracy artistic assessments offering novel perspective intelligent evaluation artworks
"The lack of efficient quantitative analysis methods for visual elements in advertisement design often results in suboptimal visual impact and ineffective communication of key information. To address this challenge, this study employs image analysis technology based on the YOLOv4 (You Only Look Once v4) object detection algorithm to enhance the visual effectiveness of advertisement design. First, a diverse set of advertisement samples is collected, and image augmentation techniques are applied to expand the dataset. The YOLOv4 model is then trained using the CSPDarknet53 (Cross-Stage Partial Darknet53) network for feature extraction, integrating multi-scale prediction to improve detection accuracy. The trained model is subsequently utilized to identify and analyze visual elements within advertisements, assessing their spatial distribution and layout characteristics. Based on these insights, an automated optimization framework is developed to refine design elements such as color, size, and layout proportions, with the results visualized for comparative analysis. Experimental results demonstrate that the proposed method achieves a detection precision of 98.1% and a recall rate of 96.8% for product elements. The optimized print advertisements exhibit a focus concentration of 0.84 and an element distribution density of 0.87, significantly enhancing design clarity and effectiveness. This approach not only improves the efficiency and precision of quantitative analysis in advertisement design but also offers a practical and scalable solution for optimizing visual communication strategies.",lack efficient quantitative analysis methods visual elements advertisement design often results suboptimal visual impact ineffective communication key information address challenge study employs image analysis technology based yolov look object detection algorithm enhance visual effectiveness advertisement design first diverse set advertisement samples collected image augmentation techniques applied expand dataset yolov model trained using cspdarknet cross stage partial darknet network feature extraction integrating multi scale prediction improve detection accuracy trained model subsequently utilized identify analyze visual elements within advertisements assessing spatial distribution layout characteristics based insights automated optimization framework developed refine design elements color size layout proportions results visualized comparative analysis experimental results demonstrate proposed method achieves detection precision recall rate product elements optimized print advertisements exhibit focus concentration element distribution density significantly enhancing design clarity effectiveness approach improves efficiency precision quantitative analysis advertisement design also offers practical scalable solution optimizing visual communication strategies
"In the field of enterprise management accounting, the traditional manual input method is inefficient and error-prone in the face of the processing demand of massive invoice data. The intelligent management accounting tool proposed in this study, based on the improved K-Means algorithm, effectively improves the efficiency and accuracy of automatic invoice identification. The results show that the recognition accuracy of this tool is 98.63%, which is 5.83% higher than that of Canny algorithm (92.80%), and the average invoice processing speed is 5.34 invoices per second. This means that for a large enterprise that has to process thousands of invoices every day, intelligent management accounting tools can greatly increase the speed of data processing while ensuring high accuracy and can save a lot of labor costs every month. In addition, the tool can also repair the defects of invoice images in real time, adapt to the automatic identification of various types of invoices, and provide a new and efficient solution for enterprise financial data processing. The use of intelligent tools to enhance the ability of enterprises to analyze financial data has fundamentally changed the way financial data is processed.",field enterprise management accounting traditional manual input method inefficient error prone face processing demand massive invoice data intelligent management accounting tool proposed study based improved means algorithm effectively improves efficiency accuracy automatic invoice identification results show recognition accuracy tool higher canny algorithm average invoice processing speed invoices per second means large enterprise process thousands invoices every day intelligent management accounting tools greatly increase speed data processing ensuring high accuracy save lot labor costs every month addition tool also repair defects invoice images real time adapt automatic identification various types invoices provide new efficient solution enterprise financial data processing use intelligent tools enhance ability enterprises analyze financial data fundamentally changed way financial data processed
"Efficiently managing sports schedules in collegiate environments is a challenging and crucial task. With multiple teams, events, facilities, and diverse stakeholder needs, traditional scheduling methods often fail to meet the dynamic and complex requirements of modern sports programs. Complex scheduling algorithms offer a promising solution by optimizing the allocation of resources, time slots, and facilities while minimizing conflicts and maximizing participation. The research analyzes the impact of complex scheduling algorithms on injury rates and athletic performance in a collegiate sports environment, with a particular focus on ACL injuries in basketball players. The research gathers the history, performance, and demographic data from athletes. The data was preprocessed using cleansing and normalizing the data, and handling missing values. The research employs an ICO-MLPN to predict injury risk and improve athletic performance in collegiate sports environments. The research explores the application of the DLB scheduling algorithm to create tailored schedules that account for individual requirements, training intensity, and recovery periods to reduce the risk of ACL. The findings suggest that the complex scheduling algorithms improve athletic performance and also significantly reduce the incidence of ACL injuries, offering an ICO-MLPN framework for collegiate basketball programs to improve performance with a recall of 95.33%, accuracy of 98.70%, precision of 98.2%, and F1-score of 96.20%. After implementing DLB, the injury risk incident rate decreased from 50% to 30%, treatment costs decreased from 50% to 20%, physical health satisfaction improved from 65% to 85%, and mental health satisfaction increased from 60% to 80%. Recovery time decreased from 2.5 days to 2 days, and minimum severity injuries increased from 50% to 65%. The approach underscores the potential of combining scheduling optimization and innovative algorithms to create personalized training schedules that prioritize both performance and injury risk reduction.",efficiently managing sports schedules collegiate environments challenging crucial task multiple teams events facilities diverse stakeholder needs traditional scheduling methods often fail meet dynamic complex requirements modern sports programs complex scheduling algorithms offer promising solution optimizing allocation resources time slots facilities minimizing conflicts maximizing participation research analyzes impact complex scheduling algorithms injury rates athletic performance collegiate sports environment particular focus acl injuries basketball players research gathers history performance demographic data athletes data preprocessed using cleansing normalizing data handling missing values research employs ico mlpn predict injury risk improve athletic performance collegiate sports environments research explores application dlb scheduling algorithm create tailored schedules account individual requirements training intensity recovery periods reduce risk acl findings suggest complex scheduling algorithms improve athletic performance also significantly reduce incidence acl injuries offering ico mlpn framework collegiate basketball programs improve performance recall accuracy precision score implementing dlb injury risk incident rate decreased treatment costs decreased physical health satisfaction improved mental health satisfaction increased recovery time decreased days days minimum severity injuries increased approach underscores potential combining scheduling optimization innovative algorithms create personalized training schedules prioritize performance injury risk reduction
"In traditional environmental art design, it is often challenging to comprehensively account for a multitude of complex factors, and the optimization process tends to be computationally intensive and intricate. Key aspects such as illumination, shadow, and sightlines between landscape elements are frequently overlooked, leading to incoherent layout schemes in practical applications. This study addresses these limitations by employing genetic algorithms (GAs) to optimize the layout of landscape elements, thereby automating the generation of optimal layout schemes. Initially, an objective function is established to evaluate layout schemes by holistically considering landscape aesthetics, functionality, ecological benefits, and feasibility, with each factor’s contribution quantified. The layout scheme is encoded into real-valued representations, where each individual denotes the location, size, and other attributes of landscape elements. An initial population is generated to ensure diversity and adherence to basic constraints. Each generation of individuals is assessed using a fitness function that integrates multiple sub-goals, evaluating the merits of each layout scheme through weighted summation. The tournament selection method is employed to choose individuals with higher fitness in each generation for crossover and mutation operations, thereby generating new layout schemes. Through iterative optimization over multiple generations, the genetic algorithm refines the layout scheme. Experimental results demonstrate that the overall space utilization rate exceeds 0.7, and the weighted scores of the landscape layout schemes range between 8.075 and 8.59. This approach offers a novel solution for environmental art design, significantly enhancing the coherence and practicality of layout schemes.",traditional environmental art design often challenging comprehensively account multitude complex factors optimization process tends computationally intensive intricate key aspects illumination shadow sightlines landscape elements frequently overlooked leading incoherent layout schemes practical applications study addresses limitations employing genetic algorithms gas optimize layout landscape elements thereby automating generation optimal layout schemes initially objective function established evaluate layout schemes holistically considering landscape aesthetics functionality ecological benefits feasibility factor contribution quantified layout scheme encoded real valued representations individual denotes location size attributes landscape elements initial population generated ensure diversity adherence basic constraints generation individuals assessed using fitness function integrates multiple sub goals evaluating merits layout scheme weighted summation tournament selection method employed choose individuals higher fitness generation crossover mutation operations thereby generating new layout schemes iterative optimization multiple generations genetic algorithm refines layout scheme experimental results demonstrate overall space utilization rate exceeds weighted scores landscape layout schemes range approach offers novel solution environmental art design significantly enhancing coherence practicality layout schemes
"This research addresses the critical challenge of customer churn prediction in cross-border e-commerce by proposing an enhanced XGBoost-based framework that integrates temporal-spatial features and dynamic weight adjustment mechanisms. In response to the complex characteristics of international e-commerce, including regional behavioral variations, seasonal patterns, and logistics impacts, this study develops novel approaches to feature engineering and algorithm optimization. The enhanced model incorporates continuous temporal processing, adaptive weight adjustment, and business rule-based feature interactions to achieve superior prediction performance. Through extensive experimentation with large-scale cross-border transaction datasets, the research demonstrates significant improvements in prediction accuracy across diverse geographical regions while maintaining model interpretability. The findings contribute substantially to both the theoretical advancement of machine learning applications in cross-border e-commerce and practical implementations in customer relationship management. The proposed framework provides valuable insights for international e-commerce platforms seeking to implement more effective customer retention strategies and optimize their operational efficiency in global markets.",research addresses critical challenge customer churn prediction cross border commerce proposing enhanced xgboost based framework integrates temporal spatial features dynamic weight adjustment mechanisms response complex characteristics international commerce including regional behavioral variations seasonal patterns logistics impacts study develops novel approaches feature engineering algorithm optimization enhanced model incorporates continuous temporal processing adaptive weight adjustment business rule based feature interactions achieve superior prediction performance extensive experimentation large scale cross border transaction datasets research demonstrates significant improvements prediction accuracy across diverse geographical regions maintaining model interpretability findings contribute substantially theoretical advancement machine learning applications cross border commerce practical implementations customer relationship management proposed framework provides valuable insights international commerce platforms seeking implement effective customer retention strategies optimize operational efficiency global markets
"This article explores the relationship between race, technology, music, and geographical thought through the vehicle of jazz musician Sun Ra’s piece ‘Space Is the Place’ considered in formal, historical, and critical context. Specifically, the author engages with Katherine McKittrick’s concept of transparent space to argue that ‘Space Is the Place’, in a tradition reaching from Ra’s avant-garde popular music back through a rich tradition of Black American musical practice, deploys time signature in a way that upsets traditional concepts of grounds or grounding embedded at the foundation of Western thought. Taking seriously Ra’s conviction that his music literally, and not merely figuratively, transports listeners to a new planet, this article speculatively suggests that the form of thought generated by the music’s disruption of grounds/grounding is uniquely suitable for addressing the radical social, economic, and political shifts currently occurring in humanity’s rapidly intensifying flight from the grounds of the Earth. The compression of these vast social forces into the sensuous experience of ‘Space Is the Place’ thus transports the listener by altering fundamental processes of spatiotemporal relationality that echo into conscious thought. This article demonstrates the interconnectedness of musical expression with geographical imaginaries and suggests a new kind of thinking with which to approach geographical analyses beyond the geocentric, capable of answering the demands of the cosmos.",article explores relationship race technology music geographical thought vehicle jazz musician sun piece space place considered formal historical critical context specifically author engages katherine mckittrick concept transparent space argue space place tradition reaching avant garde popular music back rich tradition black american musical practice deploys time signature way upsets traditional concepts grounds grounding embedded foundation western thought taking seriously conviction music literally merely figuratively transports listeners new planet article speculatively suggests form thought generated music disruption grounds grounding uniquely suitable addressing radical social economic political shifts currently occurring humanity rapidly intensifying flight grounds earth compression vast social forces sensuous experience space place thus transports listener altering fundamental processes spatiotemporal relationality echo conscious thought article demonstrates interconnectedness musical expression geographical imaginaries suggests new kind thinking approach geographical analyses beyond geocentric capable answering demands cosmos
"Aiming at industrial loads with high flexibility, such as material conveying processes, an active potential mining method for day-ahead invitation demand response is studied through optimization. Under the day-ahead invitation demand response mechanism, an aggregation method for industrial conveying loads and an active potential mining framework for load aggregators participating in demand response are proposed. The demand response potential mining of the conveying load aggregators is formulated as optimization problems. Through the constructions of publishment functions attaching to demand response period, a potential mining model for day-ahead invitation demand response and an integrated potential mining model capable to response to day-ahead invitation demand and time-of-use tariff simultaneously are built, respectively. A cement clinker conveying system is taken as the study case to verify the demand response active potential mining models under multi-operating conditions. The results show that the proposed active potential mining models can actively participate in day-ahead invitation demand response and obtain the maximum potential, meanwhile, satisfy all the production requirements and safety constraints. The models can be references for highly flexible industrial loads to participate in demand response.",aiming industrial loads high flexibility material conveying processes active potential mining method day ahead invitation demand response studied optimization day ahead invitation demand response mechanism aggregation method industrial conveying loads active potential mining framework load aggregators participating demand response proposed demand response potential mining conveying load aggregators formulated optimization problems constructions publishment functions attaching demand response period potential mining model day ahead invitation demand response integrated potential mining model capable response day ahead invitation demand time use tariff simultaneously built respectively cement clinker conveying system taken study case verify demand response active potential mining models multi operating conditions results show proposed active potential mining models actively participate day ahead invitation demand response obtain maximum potential meanwhile satisfy production requirements safety constraints models references highly flexible industrial loads participate demand response
"The growing demand for railway transportation requires higher speeds and axle loads, but this can negatively impact critical track components like railway crossings. This study investigates the dynamic behavior of high-speed turnout components under varying train speeds and axle loads, focusing on vertical displacements and contact forces. Using detailed finite element analysis, comparative results reveal trends in rail displacements, with higher speeds reducing the responsiveness of the crossing tip (frog) to rapid changes. The study also highlights the role of static axle loads in maintaining wheel-rail contact and identifies vulnerable components, such as the frog tip and base plate, under higher static loads. These findings provide valuable insights into the dynamics of turnout components, enabling stakeholders to derive strategies for improving turnout performance and safety.",growing demand railway transportation requires higher speeds axle loads negatively impact critical track components like railway crossings study investigates dynamic behavior high speed turnout components varying train speeds axle loads focusing vertical displacements contact forces using detailed finite element analysis comparative results reveal trends rail displacements higher speeds reducing responsiveness crossing tip frog rapid changes study also highlights role static axle loads maintaining wheel rail contact identifies vulnerable components frog tip base plate higher static loads findings provide valuable insights dynamics turnout components enabling stakeholders derive strategies improving turnout performance safety
"Existing LSTM-based power quality (PQ) prediction models primarily rely on historical information, which limits their ability to fully capture contextual dependencies. Furthermore, these models process inputs sequentially without accounting for the varying importance of different time steps, leading to significant prediction inaccuracies. To address these limitations, this study proposes an enhanced PQ prediction model that integrates Bidirectional Long Short-Term Memory (BiLSTM) with a Self-Attention (SA) mechanism. The BiLSTM module is introduced to model both forward and backward temporal dependencies, enabling a more comprehensive capture of long-term patterns in time series data. The SA mechanism dynamically adjusts the importance of different time steps through weighted summation, enhancing the model’s ability to focus on critical features and improving its capacity to model nonlinear relationships. The weighted features from the SA layer are then mapped to a fully connected layer to generate the final prediction outputs. Experiments were conducted using power quality data from Nanchang as the primary dataset, with additional datasets from Nanjing, Wuhan, Changsha, and Beijing used for generalization testing. The results demonstrate that the BiLSTM-SA model outperforms traditional LSTM models across all PQ metrics, achieving a mean absolute error (MAE) of 0.09 for voltage deviation, a 0.05 improvement over single-layer LSTM. Notably, the model maintains robust performance in complex power supply scenarios, with a generalized MAE of only 0.2 in Beijing. These findings highlight the effectiveness of combining BiLSTM with the SA mechanism in reducing prediction errors and ensuring the stability of power supply quality, offering a significant advancement in PQ prediction methodologies.",existing lstm based power quality prediction models primarily rely historical information limits ability fully capture contextual dependencies furthermore models process inputs sequentially without accounting varying importance different time steps leading significant prediction inaccuracies address limitations study proposes enhanced prediction model integrates bidirectional long short term memory bilstm self attention mechanism bilstm module introduced model forward backward temporal dependencies enabling comprehensive capture long term patterns time series data mechanism dynamically adjusts importance different time steps weighted summation enhancing model ability focus critical features improving capacity model nonlinear relationships weighted features layer mapped fully connected layer generate final prediction outputs experiments conducted using power quality data nanchang primary dataset additional datasets nanjing wuhan changsha beijing used generalization testing results demonstrate bilstm model outperforms traditional lstm models across metrics achieving mean absolute error mae voltage deviation improvement single layer lstm notably model maintains robust performance complex power supply scenarios generalized mae beijing findings highlight effectiveness combining bilstm mechanism reducing prediction errors ensuring stability power supply quality offering significant advancement prediction methodologies
"This study uses deep-learning models to predict city partition crime counts on specific days. It helps police enhance surveillance, gather intelligence, and proactively prevent crimes. We formulate crime count prediction as a spatiotemporal sequence challenge, where both input data and prediction targets are spatiotemporal sequences. In order to improve the accuracy of crime forecasting, we introduce a new model that combines Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks. We conducted a comparative analysis to access the effects of various data sequences, including raw and binned data, on the prediction errors of four deep learning forecasting models. Directly inputting raw crime data into the forecasting model causes high prediction errors, making the model unsuitable for real-world use. The findings indicate that the proposed CNN-LSTM model achieves optimal performance when crime data is categorized into 10 or 5 groups. Data binning can enhance forecasting model performance, but poorly defined intervals may reduce map granularity. Compared to dividing into five bins, binning into 10 intervals strikes an optimal balance, preserving data characteristics and surpassing raw data in predictive modeling efficacy.",study uses deep learning models predict city partition crime counts specific days helps police enhance surveillance gather intelligence proactively prevent crimes formulate crime count prediction spatiotemporal sequence challenge input data prediction targets spatiotemporal sequences order improve accuracy crime forecasting introduce new model combines convolutional neural networks cnn long short term memory lstm networks conducted comparative analysis access effects various data sequences including raw binned data prediction errors four deep learning forecasting models directly inputting raw crime data forecasting model causes high prediction errors making model unsuitable real world use findings indicate proposed cnn lstm model achieves optimal performance crime data categorized groups data binning enhance forecasting model performance poorly defined intervals may reduce map granularity compared dividing five bins binning intervals strikes optimal balance preserving data characteristics surpassing raw data predictive modeling efficacy
"The absence of effective dynamic tracking mechanisms in public opinion analysis has persistently constrained the accurate identification of evolving trends in public opinion events. This study proposes an innovative methodological framework that synergistically integrates the Latent Dirichlet Allocation (LDA) algorithm with advanced topic modeling techniques, thereby offering transformative potential for the analysis and governance of online public discourse. The methodology involves a sophisticated multi-stage analytical process: First, textual data undergoes rigorous preprocessing before being transformed into a term frequency matrix using Term Frequency-Inverse Document Frequency (TF-IDF) weighting, establishing a robust quantitative foundation for subsequent analysis. The LDA algorithm is then systematically applied to extract latent thematic structures from the dataset, while an original hierarchical optimization strategy substantially enhances the model’s dynamic topic identification and tracking capabilities. Furthermore, the integration of a sentiment lexicon with the extracted topics enables precise opinion classification, permitting real-time monitoring of topic evolution through quantitative heat fluctuation metrics. Empirical evaluations demonstrate the superior performance of the proposed approach compared to conventional LDA, achieving a 12.7% reduction in the average Root Mean Square Error (RMSE) for tracking topic heat dynamics. This methodological breakthrough not only advances the theoretical foundations of public opinion analysis but also provides a scientifically rigorous, dynamic monitoring framework with significant practical implications for evidence-based governance and policy formulation in the digital era.",absence effective dynamic tracking mechanisms public opinion analysis persistently constrained accurate identification evolving trends public opinion events study proposes innovative methodological framework synergistically integrates latent dirichlet allocation lda algorithm advanced topic modeling techniques thereby offering transformative potential analysis governance online public discourse methodology involves sophisticated multi stage analytical process first textual data undergoes rigorous preprocessing transformed term frequency matrix using term frequency inverse document frequency idf weighting establishing robust quantitative foundation subsequent analysis lda algorithm systematically applied extract latent thematic structures dataset original hierarchical optimization strategy substantially enhances model dynamic topic identification tracking capabilities furthermore integration sentiment lexicon extracted topics enables precise opinion classification permitting real time monitoring topic evolution quantitative heat fluctuation metrics empirical evaluations demonstrate superior performance proposed approach compared conventional lda achieving reduction average root mean square error rmse tracking topic heat dynamics methodological breakthrough advances theoretical foundations public opinion analysis also provides scientifically rigorous dynamic monitoring framework significant practical implications evidence based governance policy formulation digital
"With the advent of the big data era, anomaly detection becomes increasingly crucial for ensuring the security and reliability of systems. This paper investigates large-scale anomaly detection based on the Isolation Forest algorithm, enhancing the algorithm’s performance in the context of big data by introducing the method of adaptive feature selection. The proposed approach is a fusion of the Isolation Forest and adaptive feature selection, dynamically adjusting feature weights to adapt more flexibly to the contributions of different features. Experimental results on large-scale datasets demonstrate that adaptive feature selection significantly improves the anomaly detection performance of the Isolation Forest algorithm. This method provides a new perspective for enhancing anomaly detection techniques and addressing the challenges posed by large-scale, high-dimensional data. Its practical implications are crucial for real-world applications.",advent big data anomaly detection becomes increasingly crucial ensuring security reliability systems paper investigates large scale anomaly detection based isolation forest algorithm enhancing algorithm performance context big data introducing method adaptive feature selection proposed approach fusion isolation forest adaptive feature selection dynamically adjusting feature weights adapt flexibly contributions different features experimental results large scale datasets demonstrate adaptive feature selection significantly improves anomaly detection performance isolation forest algorithm method provides new perspective enhancing anomaly detection techniques addressing challenges posed large scale high dimensional data practical implications crucial real world applications
"In college sports, health monitoring and real-time early warning are crucial for the prevention of sports injuries and sudden illnesses, but traditional monitoring means are limited by real-time and multi-dimensional analysis capabilities. To this end, this study designs a real-time health warning model based on wearable devices, which realizes dynamic parsing of physiological timing signals and multimodal data synergistic sensing to accurately identify sports risks by integrating a flexible sensor network with a hybrid long short-term memory-graph convolutional network (LSTM-GNN) algorithm. The model adopts a lightweight edge computing architecture to reduce response latency and combines with migration learning to enhance cross-scene adaptability. Experiments show that its warning accuracy reaches 95.2%, which can be adapted to real-time monitoring and personalized sports guidance in sports classrooms, providing innovative theoretical and technological support for the safety and security of college students’ sports health and intelligent application of wearable devices.",college sports health monitoring real time early warning crucial prevention sports injuries sudden illnesses traditional monitoring means limited real time multi dimensional analysis capabilities end study designs real time health warning model based wearable devices realizes dynamic parsing physiological timing signals multimodal data synergistic sensing accurately identify sports risks integrating flexible sensor network hybrid long short term memory graph convolutional network lstm gnn algorithm model adopts lightweight edge computing architecture reduce response latency combines migration learning enhance cross scene adaptability experiments show warning accuracy reaches adapted real time monitoring personalized sports guidance sports classrooms providing innovative theoretical technological support safety security college students sports health intelligent application wearable devices
"In today’s supply chain (SC) management circumstances, ensuring device quality and authenticity is critical to preserving operational efficiency and consumer trust. The main objective of this study is to improve SC efficiency through the development of scalable unified identification systems that provide reliable device quality tracking. The study framework employs a combination of immutable data storage, Internet of Things (IoT) devices for continued monitoring, and machine learning (ML) algorithms for predictive analytics. The study presents a refined Random Forest (RRF) algorithm, meticulously designed to analyze data for demand forecasting based on the system’s operational state. This analysis is grounded in comprehensive IoT equipment sensor data, encompassing critical parameters such as temperature, humidity, pressure, vibration, and wear and tear of the equipment, ensuring inventory levels effectively align with anticipated requirements. Kalman filter algorithm for accurate state estimation of the device throughout the SC, enhancing traceability by providing real-time updates on device status. IoT sensors on essential components in the SC obtain continuous data on their status and performance. These sensors will keep track of key performance indicators for quality control. The results demonstrate our approach and conduct simulations in a controlled environment, demonstrating an increase in operational efficiency and a significant reduction in quality-related defects. The RRF model is 93.48% accurately identifying the IoT system’s operational state. This research addresses the importance of integrating scalable identity solutions to foster trust, transparency, and responsiveness in SC ecosystems.",today supply chain management circumstances ensuring device quality authenticity critical preserving operational efficiency consumer trust main objective study improve efficiency development scalable unified identification systems provide reliable device quality tracking study framework employs combination immutable data storage internet things iot devices continued monitoring machine learning algorithms predictive analytics study presents refined random forest rrf algorithm meticulously designed analyze data demand forecasting based system operational state analysis grounded comprehensive iot equipment sensor data encompassing critical parameters temperature humidity pressure vibration wear tear equipment ensuring inventory levels effectively align anticipated requirements kalman filter algorithm accurate state estimation device throughout enhancing traceability providing real time updates device status iot sensors essential components obtain continuous data status performance sensors keep track key performance indicators quality control results demonstrate approach conduct simulations controlled environment demonstrating increase operational efficiency significant reduction quality related defects rrf model accurately identifying iot system operational state research addresses importance integrating scalable identity solutions foster trust transparency responsiveness ecosystems
"In the field of underground engineering, the construction model of two-dimensional design and decentralized management is no longer able to enhance the efficiency trend of multi-party collaboration and information management, nor can it meet the future demand for refined management and sustainable development of engineering construction. Therefore, this paper first summarizes and analyses the current development status of collaborative design platforms at home and abroad, and proposes a “cloud + terminal” overall technical architecture suitable for underground engineering collaborative design management. This architecture consists of a foundation layer, data layer, service layer, functional application layer, and user interaction layer. The functional modules of the platform also adopt microservice architecture design. In response to the shortcomings of model checking, an IFC component of the model checking module was designed based on an optimized multi view convolutional neural network algorithm to correct errors in the lowest level category information data. At the same time, technical routes were proposed around “three zone independence” and “problem positioning.” Developed a prototype platform with basic functions and applied it to the design process of a certain engineering project. By comparing with traditional design patterns, the results show that the collaborative platform proposed in this paper effectively improves work efficiency and achievement quality. Next, we will continue to expand the functions and application scenarios of the collaborative platform, providing strong support for improving the construction quality and efficiency of underground engineering throughout its entire life cycle.",field underground engineering construction model two dimensional design decentralized management longer able enhance efficiency trend multi party collaboration information management meet future demand refined management sustainable development engineering construction therefore paper first summarizes analyses current development status collaborative design platforms home abroad proposes cloud terminal overall technical architecture suitable underground engineering collaborative design management architecture consists foundation layer data layer service layer functional application layer user interaction layer functional modules platform also adopt microservice architecture design response shortcomings model checking ifc component model checking module designed based optimized multi view convolutional neural network algorithm correct errors lowest level category information data time technical routes proposed around three zone independence problem positioning developed prototype platform basic functions applied design process certain engineering project comparing traditional design patterns results show collaborative platform proposed paper effectively improves work efficiency achievement quality next continue expand functions application scenarios collaborative platform providing strong support improving construction quality efficiency underground engineering throughout entire life cycle
"The Fourth Industrial Revolution has introduced intelligent digital technologies, such as virtual reality (VR) and augmented reality (AR), across various sectors, including education. The research aims to provide a comprehensive overview of the current research landscape and offer insights to guide future studies on AR and VR in vocational education. Bibliometric analysis is used to explore the performance analysis, science mapping, trend analysis, and application analysis of these technologies in vocational education. There are four key findings: First, VR research in vocational education has higher publication and citation rates than AR, with China, Spain, and the United States as leading contributors. Second, AR focuses on enhancing traditional education through “students” and “education,” while VR emphasizes immersive “learning environments” and “outcomes.” Third, the growing trend centers on “education” and “students,” reflecting increased interest in online learning. Fourth, AR and VR applications in vocational education show that AR enhances student skills, learning performance, and motivation, while VR advances practical training, learning outcomes and technological and practical aspects. These findings provide valuable insights that can inform future research directions in vocational education, particularly in exploring how AR and VR technologies can be more effectively integrated into educational practices.",fourth industrial revolution introduced intelligent digital technologies virtual reality augmented reality across various sectors including education research aims provide comprehensive overview current research landscape offer insights guide future studies vocational education bibliometric analysis used explore performance analysis science mapping trend analysis application analysis technologies vocational education four key findings first research vocational education higher publication citation rates china spain united states leading contributors second focuses enhancing traditional education students education emphasizes immersive learning environments outcomes third growing trend centers education students reflecting increased interest online learning fourth applications vocational education show enhances student skills learning performance motivation advances practical training learning outcomes technological practical aspects findings provide valuable insights inform future research directions vocational education particularly exploring technologies effectively integrated educational practices
"For athlete performance evaluation and injury risk prediction—which is increasingly crucial—traditional approaches find difficulty handling complex, multidimensional data. We introduce the PerfoRisk-KDB model to precisely estimate athlete performance and injury risk by combining K-means and DBSCAN clustering techniques. By combining these two clustering techniques, the idea of this work surpasses the constraints of a single technique and increases accuracy and robustness for complex and high-dimensional data. This work tests the performance assessment and injury risk prediction of a real athlete dataset against conventional models. Based on tests, the PerfoRisk-KDB model shows good performance on several evaluation criteria and shows good application possibilities.",athlete performance evaluation injury risk prediction increasingly crucial traditional approaches find difficulty handling complex multidimensional data introduce perforisk kdb model precisely estimate athlete performance injury risk combining means dbscan clustering techniques combining two clustering techniques idea work surpasses constraints single technique increases accuracy robustness complex high dimensional data work tests performance assessment injury risk prediction real athlete dataset conventional models based tests perforisk kdb model shows good performance several evaluation criteria shows good application possibilities
"The basis for societal progress and development is the caliber of people training provided by colleges and universities. In the context of the development of agricultural modernization, a key assurance for fostering the growth of agricultural modernization is the caliber of the staff training provided in agricultural colleges and universities. At present, the existing evaluation models for the quality of personnel education in agricultural universities have defects such as low efficiency in data processing and long running time. Therefore, to better judge the training quality of agricultural graduates in colleges and universities, the study proposes to use the random forest algorithm to build an evaluation method for the quality of agricultural talent training, and on this basis, use the TRRF algorithm to improve it. At the same time, to weight the data, the F-measure assessment method is applied. In this way, the effectiveness of the evaluation model can be improved. The experimental findings demonstrate that the F-TRRF model suggested in the study has 99.17% accuracy when evaluating the standard of agricultural staff training. Therefore, the random forest evaluation model integrated with F-measure suggested in the study has a high level of precision in evaluating the training quality of agricultural talents, which can effectively meet the actual needs of talent evaluation, provide a basis for cultivating more and higher quality agricultural talents, and provide reference opinions on talent cultivation.",basis societal progress development caliber people training provided colleges universities context development agricultural modernization key assurance fostering growth agricultural modernization caliber staff training provided agricultural colleges universities present existing evaluation models quality personnel education agricultural universities defects low efficiency data processing long running time therefore better judge training quality agricultural graduates colleges universities study proposes use random forest algorithm build evaluation method quality agricultural talent training basis use trrf algorithm improve time weight data measure assessment method applied way effectiveness evaluation model improved experimental findings demonstrate trrf model suggested study accuracy evaluating standard agricultural staff training therefore random forest evaluation model integrated measure suggested study high level precision evaluating training quality agricultural talents effectively meet actual needs talent evaluation provide basis cultivating higher quality agricultural talents provide reference opinions talent cultivation
"The vital assessment of cardiovascular health metrics, notably heart and respiratory rates, is paramount for early detection and timely intervention of vascular anomalies. With the rise in global prevalence of cardiovascular diseases, there’s a pressing demand for systems that support long-term and remote monitoring. In response, this study explored Ballistocardiography (BCG)—a technique capturing the minute vibrations caused by cardiac contractions and bodily movements, offering non-invasive, continuous monitoring. Utilizing Dozee, a cutting-edge contactless monitoring system, we sought to identify optimal sensor placements. Data was collated from 18 healthy participants and 7 with vascular conditions, with sensors arrayed across the upper and lower body. Post rigorous signal filtering and statistical analyses, the thorax and lumbar spine sensors emerged as the most accurate for the upper and lower body, respectively. However, regions like the cerebral and legs faced compromised signals due to external interferences such as body movements. The study underscores BCG’s potential as a promising, non-invasive, and cost-effective alternative to traditional diagnostics. For a broader clinical application and validation, further research, especially high-powered controlled studies are required.",vital assessment cardiovascular health metrics notably heart respiratory rates paramount early detection timely intervention vascular anomalies rise global prevalence cardiovascular diseases pressing demand systems support long term remote monitoring response study explored ballistocardiography bcg technique capturing minute vibrations caused cardiac contractions bodily movements offering non invasive continuous monitoring utilizing dozee cutting edge contactless monitoring system sought identify optimal sensor placements data collated healthy participants vascular conditions sensors arrayed across upper lower body post rigorous signal filtering statistical analyses thorax lumbar spine sensors emerged accurate upper lower body respectively however regions like cerebral legs faced compromised signals due external interferences body movements study underscores bcg potential promising non invasive cost effective alternative traditional diagnostics broader clinical application validation research especially high powered controlled studies required
"Existing short video content analysis methods often struggle with integrating multimodal data, resulting in incomplete content understanding and limited recommendation accuracy. To address these challenges, this study proposes a deep learning-based short video content analysis and recommendation algorithm that enhances feature extraction and personalization. CNN and LSTM are employed to automatically capture and extract multimodal features from short videos, generating a comprehensive feature representation through weighted fusion. Meanwhile, DNN is utilized to model user behavior, extracting deep behavioral features to enhance preference prediction. By jointly modeling video content and user behavior, a multimodal recommendation algorithm is developed to optimize content delivery. Experimental results show that, compared to CF, MF, and Content-Based Filtering benchmark algorithms, the proposed method improves average precision by 4.7%, 3.3%, and 4.0%, and recall by 3.5%, 1.2%, and 2.1%, respectively. These findings confirm that the deep learning-driven approach effectively enhances multimodal content understanding, meets personalized user preferences, and significantly improves recommendation accuracy, offering a more intelligent and adaptive short video recommendation framework.",existing short video content analysis methods often struggle integrating multimodal data resulting incomplete content understanding limited recommendation accuracy address challenges study proposes deep learning based short video content analysis recommendation algorithm enhances feature extraction personalization cnn lstm employed automatically capture extract multimodal features short videos generating comprehensive feature representation weighted fusion meanwhile dnn utilized model user behavior extracting deep behavioral features enhance preference prediction jointly modeling video content user behavior multimodal recommendation algorithm developed optimize content delivery experimental results show compared content based filtering benchmark algorithms proposed method improves average precision recall respectively findings confirm deep learning driven approach effectively enhances multimodal content understanding meets personalized user preferences significantly improves recommendation accuracy offering intelligent adaptive short video recommendation framework
"Deep excavation engineering plays a crucial role in modern urban construction, and its safety has garnered significant attention, especially in seismically active regions. The complex structure and scale of deep excavations make the study of their dynamic response and stability under seismic loading particularly important. However, current research on deep excavation engineering under seismic effects has certain limitations, primarily in the insufficient consideration of the interaction between excavation structures and the surrounding soil, as well as the inaccuracy of seismic load simulations. These limitations restrict the effectiveness and accuracy of existing design theories. This study aims to explore the dynamic response and stability issues of deep excavation engineering under seismic loading in depth. The research focuses on two main aspects: first, investigating the dynamic response characteristics of deep excavations under different seismic conditions; and second, evaluating the overall stability of deep excavations under seismic loading using stability analysis models. Through a systematic study, this research aims to provide a theoretical basis and technical support for the seismic design of deep excavation engineering, ultimately enhancing safety in engineering practice.",deep excavation engineering plays crucial role modern urban construction safety garnered significant attention especially seismically active regions complex structure scale deep excavations make study dynamic response stability seismic loading particularly important however current research deep excavation engineering seismic effects certain limitations primarily insufficient consideration interaction excavation structures surrounding soil well inaccuracy seismic load simulations limitations restrict effectiveness accuracy existing design theories study aims explore dynamic response stability issues deep excavation engineering seismic loading depth research focuses two main aspects first investigating dynamic response characteristics deep excavations different seismic conditions second evaluating overall stability deep excavations seismic loading using stability analysis models systematic study research aims provide theoretical basis technical support seismic design deep excavation engineering ultimately enhancing safety engineering practice
"Foreign trade talents play a crucial role in China’s foreign trade business. It’s no doubt that the high-quality foreign trade talent will step on the new step for our country foreign trade to provide more powerful support. This paper, based on the perspective of New Liberal Arts, initially constructs a framework for the competency model of new foreign trade talents. This model underwent expert consultation using the Delphi method. Subsequently, the competency model was further refined based on the Onion Model, resulting in a new foreign trade talent competency model comprising three characteristics—knowledge, ability, and literacy—and 16 indicators. On this basis, considering the current limitations in foreign trade talent training, this paper proposes effective ways to cultivate new foreign trade talents’ competencies from the perspective of New Liberal Arts’ construction and reform.",foreign trade talents play crucial role china foreign trade business doubt high quality foreign trade talent step new step country foreign trade provide powerful support paper based perspective new liberal arts initially constructs framework competency model new foreign trade talents model underwent expert consultation using delphi method subsequently competency model refined based onion model resulting new foreign trade talent competency model comprising three characteristics knowledge ability literacy indicators basis considering current limitations foreign trade talent training paper proposes effective ways cultivate new foreign trade talents competencies perspective new liberal arts construction reform
"Since digital media have changed the way information is disseminated and even social interaction behavior, the issue of affective value throughout the history of human development has created a divide between constructivist and universalist research. In this process, the focus of affective research has begun to shift towards redefining its connotations and uncovering the productive factors through which its value is expressed. To visually present the research frontiers and hotspots, and classify and analyze the data, various knowledge graphs were drawn using CiteSpace visualization analysis tool. Web of Science was selected as the data source, and K-means algorithm was used for clustering analysis to identify different topics and hotspots in the research field. This study found that affective value in the age of digital media shows a triple bias of social interaction, media memory and neurocognition. When individual affect spread through social media platforms like TikTok, it can escalate quickly, leading to a cascade of significant social events. This has spurred important research avenues into affective value such as organizational dynamics, media functions, and neuroscience. Thus, this paper argues that the tension between affective value and the application of affective value in the governance of digital media space is the main direction to solve the affective conflict or affective cooperation in the current society. This study integrates theories such as evolution theory, social constructivism, and psychoanalysis, providing a comprehensive theoretical framework for the study of emotional values and also having important guiding significance for the design and operation of digital media platforms.",since digital media changed way information disseminated even social interaction behavior issue affective value throughout history human development created divide constructivist universalist research process focus affective research begun shift towards redefining connotations uncovering productive factors value expressed visually present research frontiers hotspots classify analyze data various knowledge graphs drawn using citespace visualization analysis tool web science selected data source means algorithm used clustering analysis identify different topics hotspots research field study found affective value age digital media shows triple bias social interaction media memory neurocognition individual affect spread social media platforms like tiktok escalate quickly leading cascade significant social events spurred important research avenues affective value organizational dynamics media functions neuroscience thus paper argues tension affective value application affective value governance digital media space main direction solve affective conflict affective cooperation current society study integrates theories evolution theory social constructivism psychoanalysis providing comprehensive theoretical framework study emotional values also important guiding significance design operation digital media platforms
"To enhance the efficiency and adaptability of path planning in smart elderly care services, this paper proposes a hybrid path optimization model integrating Deep Q-Network (DQN) and Particle Swarm Optimization (PSO). Traditional approaches struggle with dynamic environmental adaptation and multi-objective optimization, often leading to suboptimal routing. To address this, the proposed model employs DQN to establish an autonomous decision-making framework, leveraging reinforcement learning to dynamically optimize path selection based on environmental variations and reward mechanisms. Meanwhile, PSO enhances the global search capability, adjusting particle positions and velocities to mitigate the risk of local optima and improve overall path efficiency. A multi-objective optimization framework is further introduced, incorporating weighted coefficients to balance competing objectives and ensure comprehensive optimization. Additionally, a dynamic environmental adaptation mechanism enables real-time data updates, allowing the system to swiftly respond to sudden environmental changes and continuously refine path selection. Experimental results demonstrate the model’s superior performance, achieving an average path planning time of 52 seconds and a service response time of 17 s. The approach maintains high path accuracy, with a minimal distance deviation of 1.35%, and delivers an optimized objective function value of 0.84. These findings highlight the model’s effectiveness in real-time adaptive path planning, offering a robust and intelligent solution for elderly care mobility and service optimization.",enhance efficiency adaptability path planning smart elderly care services paper proposes hybrid path optimization model integrating deep network dqn particle swarm optimization pso traditional approaches struggle dynamic environmental adaptation multi objective optimization often leading suboptimal routing address proposed model employs dqn establish autonomous decision making framework leveraging reinforcement learning dynamically optimize path selection based environmental variations reward mechanisms meanwhile pso enhances global search capability adjusting particle positions velocities mitigate risk local optima improve overall path efficiency multi objective optimization framework introduced incorporating weighted coefficients balance competing objectives ensure comprehensive optimization additionally dynamic environmental adaptation mechanism enables real time data updates allowing system swiftly respond sudden environmental changes continuously refine path selection experimental results demonstrate model superior performance achieving average path planning time seconds service response time approach maintains high path accuracy minimal distance deviation delivers optimized objective function value findings highlight model effectiveness real time adaptive path planning offering robust intelligent solution elderly care mobility service optimization
"To create pricing policies, maximize resource allocation, and compete in a worldwide market, companies must have accurate estimates of price elasticity. Market supply and demand, seasonal fluctuations, economic cycles, policy control all challenge price elasticity models. Conventional statistical methods are unable to adequately represent dynamics in time series. In order to address the shortcomings of past approaches, this work presents the STL-GBM model, a dynamic price elasticity modeling and forecasting technique grounded on time series analysis and machine learning (ML). The model inputs trend and Seasonal and Trend decomposition using Loess (STL) decomposition into Gradient Boosting Machine (GBM) to anticipate price elasticity. High price elasticity prediction ability of STL-GBM is demonstrated by experimental validation on two real datasets, therefore highlighting the basic factors influencing price elasticity and providing companies with more informed decisions and market insight.",create pricing policies maximize resource allocation compete worldwide market companies must accurate estimates price elasticity market supply demand seasonal fluctuations economic cycles policy control challenge price elasticity models conventional statistical methods unable adequately represent dynamics time series order address shortcomings past approaches work presents stl gbm model dynamic price elasticity modeling forecasting technique grounded time series analysis machine learning model inputs trend seasonal trend decomposition using loess stl decomposition gradient boosting machine gbm anticipate price elasticity high price elasticity prediction ability stl gbm demonstrated experimental validation two real datasets therefore highlighting basic factors influencing price elasticity providing companies informed decisions market insight
"In the context of increasing environmental challenges and the demand for sustainable development, traditional resource scheduling models in business management often fail to balance economic efficiency with environmental constraints. To address this gap, this study proposes an enhanced Particle Swarm Optimization (PSO) algorithm, termed OBLPSO, which integrates Opposition-Based Learning (OBL) and a perturbation mechanism. First, OBL generates a high-quality initial population to improve solution diversity, while a cosine curve adaptive strategy dynamically adjusts inertia weights to balance global exploration and local exploitation. Additionally, a perturbation mechanism expands the search range, preventing premature convergence. A multi-objective optimization model is established, incorporating task time, economic cost, and environmental impact (e.g., energy consumption and pollutant emissions) to maximize resource utilization and minimize ecological harm. Experimental results demonstrate that OBLPSO reduces task processing time by 29.7% and energy consumption by 16.1% compared to benchmark algorithms (e.g., ACO, GA, and standard PSO) under large-scale tasks (2000 tasks). The proposed method provides a robust solution for sustainable resource scheduling in an enterprise management environment with economic constraints.",context increasing environmental challenges demand sustainable development traditional resource scheduling models business management often fail balance economic efficiency environmental constraints address gap study proposes enhanced particle swarm optimization pso algorithm termed oblpso integrates opposition based learning obl perturbation mechanism first obl generates high quality initial population improve solution diversity cosine curve adaptive strategy dynamically adjusts inertia weights balance global exploration local exploitation additionally perturbation mechanism expands search range preventing premature convergence multi objective optimization model established incorporating task time economic cost environmental impact energy consumption pollutant emissions maximize resource utilization minimize ecological harm experimental results demonstrate oblpso reduces task processing time energy consumption compared benchmark algorithms aco standard pso large scale tasks tasks proposed method provides robust solution sustainable resource scheduling enterprise management environment economic constraints
"Existing music sequence generation methods often struggle with long-range dependencies, leading to gradient vanishing or exploding, which compromises their ability to capture intricate musical structures. This study integrates the Transformer model, leveraging its self-attention mechanism and position encoding to incorporate global information at each time step. This approach enables the flexible modeling of long-range dependencies, resulting in more natural and harmonically coherent music sequences. First, serialized encoding is employed to transform musical events. A multi-scale attention mechanism is then introduced, applying different attention strategies at each layer to capture distinct musical elements across various time scales. Additionally, periodic position encoding is incorporated to enhance the recognition of recurrent musical patterns. Experimental results demonstrate that compared to state-of-the-art models such as MuseNet, MelodyRNN, and MusicTransformer, the proposed method reduces average perplexity by 5.4%, 11.7%, and 9.5% on one dataset and 5.9%, 12.1%, and 10.3% on another. These findings highlight the model’s superior capability in generating high-quality music with enhanced melodic consistency and diversity. By outperforming existing mainstream approaches across multiple evaluation metrics, this research advances the field of music sequence generation, offering new insights for AI-driven music composition and arrangement.",existing music sequence generation methods often struggle long range dependencies leading gradient vanishing exploding compromises ability capture intricate musical structures study integrates transformer model leveraging self attention mechanism position encoding incorporate global information time step approach enables flexible modeling long range dependencies resulting natural harmonically coherent music sequences first serialized encoding employed transform musical events multi scale attention mechanism introduced applying different attention strategies layer capture distinct musical elements across various time scales additionally periodic position encoding incorporated enhance recognition recurrent musical patterns experimental results demonstrate compared state art models musenet melodyrnn musictransformer proposed method reduces average perplexity one dataset another findings highlight model superior capability generating high quality music enhanced melodic consistency diversity outperforming existing mainstream approaches across multiple evaluation metrics research advances field music sequence generation offering new insights driven music composition arrangement
"Traditional methods for predicting employment trends primarily focus on graduates’ personal data, such as academic performance and professional background, often neglecting the influence of the macroeconomic environment on the job market. This oversight leads to significant deviations between predicted outcomes and actual employment trends, compromising prediction accuracy. This study proposes a novel approach using the naive Bayes classifier (NBC) to integrate multi-dimensional data, including historical employment information, graduates' personal data, and macroeconomic indicators, to enhance the precision of employment trend predictions. A dataset comprising employment data from 810 graduates across three higher vocational colleges in a specific region, combined with macroeconomic indicators, was constructed. Data preprocessing techniques, such as missing value filling and feature standardization, were employed to ensure data quality. Feature selection was performed using univariate linear regression to eliminate irrelevant variables and retain highly correlated features. The NBC-based model, augmented with semi-supervised classification to expand the training dataset, demonstrated superior performance, achieving an average error rate of 2.76% in predicting employment across five workplaces and a 95.68% accuracy in salary level prediction. The model efficiently processed 10,000 student records in 35.28 milliseconds. These results validate the effectiveness of NBC in employment trend prediction and provide a robust foundation for optimizing innovative talent training strategies.",traditional methods predicting employment trends primarily focus graduates personal data academic performance professional background often neglecting influence macroeconomic environment job market oversight leads significant deviations predicted outcomes actual employment trends compromising prediction accuracy study proposes novel approach using naive bayes classifier nbc integrate multi dimensional data including historical employment information graduates personal data macroeconomic indicators enhance precision employment trend predictions dataset comprising employment data graduates across three higher vocational colleges specific region combined macroeconomic indicators constructed data preprocessing techniques missing value filling feature standardization employed ensure data quality feature selection performed using univariate linear regression eliminate irrelevant variables retain highly correlated features nbc based model augmented semi supervised classification expand training dataset demonstrated superior performance achieving average error rate predicting employment across five workplaces accuracy salary level prediction model efficiently processed student records milliseconds results validate effectiveness nbc employment trend prediction provide robust foundation optimizing innovative talent training strategies
"The purpose of this study is to explore the method content innovation strategy of digital education in higher vocational education under the background of artificial intelligence, in order to provide theoretical support and practical guidance for the future development of higher vocational education. The research uses the method of comparative experiment to deeply analyze the difference between the digital education method based on artificial intelligence and the traditional higher vocational education method in teaching effect. The experimental results show that compared with traditional methods, the AI-based education method can improve students’ learning efficiency and satisfaction, and it is more conducive to personalized teaching. Further, the study also puts forward a series of suggestions to optimize the content of digital education methods in higher vocational colleges, such as promoting the construction of personalized learning paths, enhancing the effect of intelligent interaction and feedback, and promoting the digital transformation of teaching environment. The findings of this study bring a new perspective and thinking to the field of higher vocational education and provide feasible solutions for the future development direction of higher vocational education.",purpose study explore method content innovation strategy digital education higher vocational education background artificial intelligence order provide theoretical support practical guidance future development higher vocational education research uses method comparative experiment deeply analyze difference digital education method based artificial intelligence traditional higher vocational education method teaching effect experimental results show compared traditional methods based education method improve students learning efficiency satisfaction conducive personalized teaching study also puts forward series suggestions optimize content digital education methods higher vocational colleges promoting construction personalized learning paths enhancing effect intelligent interaction feedback promoting digital transformation teaching environment findings study bring new perspective thinking field higher vocational education provide feasible solutions future development direction higher vocational education
"As AI technology continues to advance, the integration of AI and arts has become increasingly significant for countries aiming to become cultural powerhouses and develop robust digital strategies. Modern AI enhances artistic creation, personalized experiences, and accessibility. Therefore, it is necessary to study how AI and arts are currently being integrated. Utilizing text mining techniques, we analyze research papers on AI and art integration in China and Korea using ROST CM 6.0, identifying key trends and differences. Our findings reveal that China focuses on the digital inheritance and transformation of traditional culture in the context of modern AI technology, striving to establish cultural power. In contrast, Korea emphasizes legal rights and standardization in the integration of AI and art, with a strong focus on copyright protection. Future research should prioritize interdisciplinary approaches, examine AI art applications from a macroscopic perspective, and develop strategies for AI to inherit traditional art based on successful global examples. Additionally, ongoing discussions and research on copyright issues arising from AI art integration are essential.",technology continues advance integration arts become increasingly significant countries aiming become cultural powerhouses develop robust digital strategies modern enhances artistic creation personalized experiences accessibility therefore necessary study arts currently integrated utilizing text mining techniques analyze research papers art integration china korea using rost identifying key trends differences findings reveal china focuses digital inheritance transformation traditional culture context modern technology striving establish cultural power contrast korea emphasizes legal rights standardization integration art strong focus copyright protection future research prioritize interdisciplinary approaches examine art applications macroscopic perspective develop strategies inherit traditional art based successful global examples additionally ongoing discussions research copyright issues arising art integration essential
"This article investigates the application of design thinking in the digital transformation of healthcare services. The healthcare industry, which directly impacts everyone, often faces life-or-death situations depending on service quality. Our study delves into how design thinking facilitates the digital transformation of healthcare services, leading to significantly improved patient outcomes and healthcare infrastructure compared to traditional methods. This article critically analyses existing literature and reviews case studies conducted by leading consulting firms to derive key insights and analyses on the phenomenon. It provides real anecdotal evidence to bridge the gap between theory and practice, offering a comprehensive understanding of how design thinking can effectively drive digital transformation in healthcare services. The findings reveal that digitization leveraging design thinking must go beyond the obvious and actively engage participants throughout the journey. It underscores the importance of cross-functional teams collaborating from the early stages of the design process. Additionally, the study highlights the value of recognizing all ideas, even those that may initially seem unconventional. Finally, involving end users early on can lead to benefits and outcomes emerging much sooner than anticipated. This research provides valuable insights into the concept of design thinking and its role in fostering digital transformation. It contributes to the literature by delving deeply into the “how” question, offering pragmatic insights that can significantly enhance outcomes. Additionally, the study presents practical examples and case studies that illustrate the effective application of design thinking in real-world healthcare settings.",article investigates application design thinking digital transformation healthcare services healthcare industry directly impacts everyone often faces life death situations depending service quality study delves design thinking facilitates digital transformation healthcare services leading significantly improved patient outcomes healthcare infrastructure compared traditional methods article critically analyses existing literature reviews case studies conducted leading consulting firms derive key insights analyses phenomenon provides real anecdotal evidence bridge gap theory practice offering comprehensive understanding design thinking effectively drive digital transformation healthcare services findings reveal digitization leveraging design thinking must beyond obvious actively engage participants throughout journey underscores importance cross functional teams collaborating early stages design process additionally study highlights value recognizing ideas even may initially seem unconventional finally involving end users early lead benefits outcomes emerging much sooner anticipated research provides valuable insights concept design thinking role fostering digital transformation contributes literature delving deeply question offering pragmatic insights significantly enhance outcomes additionally study presents practical examples case studies illustrate effective application design thinking real world healthcare settings
"Bearing fault diagnosis is crucial for ensuring the reliability and safety of rotating machinery in industrial settings. Machine learning-based diagnostic models offer powerful solutions, but their effectiveness is challenged by substantial domain shifts caused by variations in operating conditions, such as changes in motor load. To address this challenge, a novel domain adaptation framework that combines physical domain knowledge with deep learning techniques is proposed. The framework employs envelope spectrum analysis to generate reliable pseudo-labels for unlabeled target domains. By incorporating local maximum mean discrepancy into the training process, the framework aligns feature distributions between source and target domains while preserving class-specific information. This method enhances the adaptability of diagnostic models to real-world industrial conditions, reducing the need for extensive labeled data and improving predictive reliability across different operating scenarios. Experimental results performed on the Case Western Reserve University bearing dataset demonstrate that our method outperforms baseline models, achieving superior classification accuracy under significant domain shifts. By improving fault detection under varying load conditions, this approach contributes to more efficient predictive maintenance, reducing unexpected failures and operational downtime in industrial machinery. This approach highlights the potential of combining physics-based insights with deep learning to enhance fault diagnosis in diverse and complex industrial scenarios.",bearing fault diagnosis crucial ensuring reliability safety rotating machinery industrial settings machine learning based diagnostic models offer powerful solutions effectiveness challenged substantial domain shifts caused variations operating conditions changes motor load address challenge novel domain adaptation framework combines physical domain knowledge deep learning techniques proposed framework employs envelope spectrum analysis generate reliable pseudo labels unlabeled target domains incorporating local maximum mean discrepancy training process framework aligns feature distributions source target domains preserving class specific information method enhances adaptability diagnostic models real world industrial conditions reducing need extensive labeled data improving predictive reliability across different operating scenarios experimental results performed case western reserve university bearing dataset demonstrate method outperforms baseline models achieving superior classification accuracy significant domain shifts improving fault detection varying load conditions approach contributes efficient predictive maintenance reducing unexpected failures operational downtime industrial machinery approach highlights potential combining physics based insights deep learning enhance fault diagnosis diverse complex industrial scenarios
"Social networks literature has explored homophily, the tendency to associate with similar others, as a critical boundary-making process contributing to segregated networks along the lines of identities. Yet, social network research generally conceptualizes identities as sociodemographic categories and seldom considers the inherently continuous and heterogeneous nature of differences. Drawing upon the infracategorical model of inequality, this study demonstrates that a computational approach – combining machine learning and exponential random graph models (ERGMs) – can capture the role of categorical conformity in network structures. Through a case study of gender segregation in friendships, this study presents a workflow for developing a machine-learning-based gender conformity measure and applying it to guide the social network analysis of cultural matching. Results show that adolescents with similar gender conformity are more likely to form friendships, net of homophily based on categorical gender and other controls, and homophily by gender conformity mediates homophily by categorical gender. The study concludes by discussing the limitations of this computational approach and its unique strengths in enhancing theories on categories, boundaries, and stratification.",social networks literature explored homophily tendency associate similar others critical boundary making process contributing segregated networks along lines identities yet social network research generally conceptualizes identities sociodemographic categories seldom considers inherently continuous heterogeneous nature differences drawing upon infracategorical model inequality study demonstrates computational approach combining machine learning exponential random graph models ergms capture role categorical conformity network structures case study gender segregation friendships study presents workflow developing machine learning based gender conformity measure applying guide social network analysis cultural matching results show adolescents similar gender conformity likely form friendships net homophily based categorical gender controls homophily gender conformity mediates homophily categorical gender study concludes discussing limitations computational approach unique strengths enhancing theories categories boundaries stratification
"The primary goal of sinonasal surgery is to improve a patient’s quality of life, which is generally achieved by enhancing drug delivery (eg, saline rinses, nasal steroids) and nasal airflow. Both drug delivery and nasal airflow are dependent on the anatomic structure of the sinonasal cavity and the relationship between this anatomy and airflow and drug delivery can be studied using computational fluid dynamics (CFD). CFD generally uses computed tomography scans and computational algorithms to predict airflow or drug delivery and can help us understand surgical outcomes and optimize drug delivery for patients. This study employs CFD to simulate nasal airflow dynamics and optimize drug delivery in the nasal cavity to highlight the utility of CFD for studying sinonasal disease. Utilizing COMSOL Multiphysics software, we developed detailed models to analyze changes in airflow characteristics before and after functional endoscopic sinus surgery, focusing on pressure distribution, velocity profiles, streamline patterns, and heat transfer. This research examines the impact of varying levels of nasal airway obstruction on airflow and heat transfer. In addition, we explore the characteristics of nasal drug delivery by simulating diverse spray parameters, including particle size, spray angle, and velocity. Our comprehensive approach allows for the visualization of drug particle trajectories and deposition patterns, providing crucial insights for enhancing surgical outcomes and improving targeted drug administration. By integrating patient-specific nasal cavity models and considering factors such as airway outlet pressure, this study offers valuable data on pressure cross-sections, flow rate variations, and particle behavior within the nasal passages. The findings of this research can be useful for both surgical planning and the development of more effective nasal drug delivery methods, potentially leading to enhanced clinical outcomes in respiratory treatment.",primary goal sinonasal surgery improve patient quality life generally achieved enhancing drug delivery saline rinses nasal steroids nasal airflow drug delivery nasal airflow dependent anatomic structure sinonasal cavity relationship anatomy airflow drug delivery studied using computational fluid dynamics cfd cfd generally uses computed tomography scans computational algorithms predict airflow drug delivery help understand surgical outcomes optimize drug delivery patients study employs cfd simulate nasal airflow dynamics optimize drug delivery nasal cavity highlight utility cfd studying sinonasal disease utilizing comsol multiphysics software developed detailed models analyze changes airflow characteristics functional endoscopic sinus surgery focusing pressure distribution velocity profiles streamline patterns heat transfer research examines impact varying levels nasal airway obstruction airflow heat transfer addition explore characteristics nasal drug delivery simulating diverse spray parameters including particle size spray angle velocity comprehensive approach allows visualization drug particle trajectories deposition patterns providing crucial insights enhancing surgical outcomes improving targeted drug administration integrating patient specific nasal cavity models considering factors airway outlet pressure study offers valuable data pressure cross sections flow rate variations particle behavior within nasal passages findings research useful surgical planning development effective nasal drug delivery methods potentially leading enhanced clinical outcomes respiratory treatment
"Computational design and optimization methods play a crucial role in early stage design by allowing the incorporation of reclaimed components, fostering resource reuse, and minimizing waste. However, to advance the field of computer-aided material reuse and support its integration into circular construction practices, there is a pressing need for a comprehensive overview of the available methods and their applications. We address this gap by conducting a systematic review of the literature on the role of computational design in facilitating the reuse of building elements, followed by analyzing the interrelationships between optimization methods, materials, and geometric dimensions of reclaimed materials. We then synthesize the identified approaches, offering guidelines that assist stakeholders in selecting suitable computational methodologies for integrating non-standard materials into design processes. Our findings highlight current knowledge gaps in algorithm scalability, performance integration, and the advancement of hybrid computational methods needed to unlock the full potential of computational design for a circular built environment.",computational design optimization methods play crucial role early stage design allowing incorporation reclaimed components fostering resource reuse minimizing waste however advance field computer aided material reuse support integration circular construction practices pressing need comprehensive overview available methods applications address gap conducting systematic review literature role computational design facilitating reuse building elements followed analyzing interrelationships optimization methods materials geometric dimensions reclaimed materials synthesize identified approaches offering guidelines assist stakeholders selecting suitable computational methodologies integrating non standard materials design processes findings highlight current knowledge gaps algorithm scalability performance integration advancement hybrid computational methods needed unlock full potential computational design circular built environment
"This study adopts the quantile regression method to analyze the influencing factors of low-income, middle-income, and high-income groups at the national, urban, and rural levels in China, respectively, at the quantiles of 0.05, 0.20, 0.50, 0.80, and 0.95. Subsequently, the GM(1, 1) model within the gray-system theory is utilized to predict the proportion of the middle-income group in China in the future. By comparing the prediction results from 2010 to 2017 with the data of the proportion of the middle-income group at the national, urban, and rural levels obtained through kernel density estimation, it is found that the gray-system prediction exhibits high accuracy and satisfactory results. The implications of this study for future social development may lie in providing a certain degree of data reference for social governors.",study adopts quantile regression method analyze influencing factors low income middle income high income groups national urban rural levels china respectively quantiles subsequently model within gray system theory utilized predict proportion middle income group china future comparing prediction results data proportion middle income group national urban rural levels obtained kernel density estimation found gray system prediction exhibits high accuracy satisfactory results implications study future social development may lie providing certain degree data reference social governors
"Human-computer collaboration is an effective way to learn programming courses. However, most existing human-computer collaborative programming learning is supported by traditional computers with a relatively low level of personalized interaction, which greatly limits the efficiency of students’ efficiency of programming learning and development of computational thinking. To address the above issues, this study introduces generative AI into human-computer collaborative programming learning and proposes a dialogue-negotiated human-computer collaborative programming learning method based on generative AI. The method focuses on the problems-solving process and constructs multiple agents through Prompt design, which enable students to improve their computational thinking and master programming skills in the process of human-computer interaction for problem-solving. Finally, a quasi-experiment was conducted to verify the effectiveness of the proposed method in a 10th grade computer programming course in a high school. 43 students in the experimental group learned with the proposed method, while 42 students in the control group adopted the traditional computer-supported human-computer collaborative programming learning method. The experimental results showed that the proposed method more significantly improved students’ computational thinking, programming learning attitudes, and learning achievement. This study provides theoretical foundations and application reference for future generative AI-assisted human-computer collaborative teaching.",human computer collaboration effective way learn programming courses however existing human computer collaborative programming learning supported traditional computers relatively low level personalized interaction greatly limits efficiency students efficiency programming learning development computational thinking address issues study introduces generative human computer collaborative programming learning proposes dialogue negotiated human computer collaborative programming learning method based generative method focuses problems solving process constructs multiple agents prompt design enable students improve computational thinking master programming skills process human computer interaction problem solving finally quasi experiment conducted verify effectiveness proposed method grade computer programming course high school students experimental group learned proposed method students control group adopted traditional computer supported human computer collaborative programming learning method experimental results showed proposed method significantly improved students computational thinking programming learning attitudes learning achievement study provides theoretical foundations application reference future generative assisted human computer collaborative teaching
"Critical thinking plays a vital role in enhancing decision-making and technical skill development in football. This study investigated the impact of a Critical Thinking-based Tactical Training Program (TPCT) on specific technical skills—passing accuracy and ball control—in U-8 football players (mean age: 7.9 ± 0.7 years). A quasi-experimental design was employed with 32 participants (experimental group: n = 15, control group: n = 17). The experimental group underwent ten 60-min training sessions that combined tactical principles (width, penetration, mobility, space) with critical thinking exercises, while the control group followed traditional technical drills without tactical or cognitive components. The experimental group showed significant improvements in passing accuracy (Wilcoxon signed-rank test: W = 0.0, p = 0.002, effect size: −1.00) and ball control (paired t-test: t = 8.67, p &lt; 0.001, Cohen's d = 2.23). In contrast, the control group did not exhibit significant changes in either skill (p &gt; 0.05). These results suggest that integrating tactical principles and critical thinking exercises into training enhances technical skills in young players. The TPCT offers a promising approach for improving player development, providing both technical and cognitive benefits in grassroots football training.",critical thinking plays vital role enhancing decision making technical skill development football study investigated impact critical thinking based tactical training program tpct specific technical skills passing accuracy ball control football players mean age years quasi experimental design employed participants experimental group control group experimental group underwent ten min training sessions combined tactical principles width penetration mobility space critical thinking exercises control group followed traditional technical drills without tactical cognitive components experimental group showed significant improvements passing accuracy wilcoxon signed rank test effect size ball control paired test cohen contrast control group exhibit significant changes either skill results suggest integrating tactical principles critical thinking exercises training enhances technical skills young players tpct offers promising approach improving player development providing technical cognitive benefits grassroots football training
"With the further improvement of educational requirements, English teaching content innovation and teaching method optimization are becoming more and more important. In this paper, we constructed the AI-CIM model and AI-MOM model based on AI technology. Specifically, we constructed the AI-CIM model based on RNN, which can realize the function of generating innovative teaching content based on existing materials. We constructed the Teaching Method Optimization Model based on TCN, which can realize the function of providing suggestions to teachers based on students’ existing loading and performance. We validate the effectiveness of the model through empirical analysis. The results show that the posttest scores of students in the experimental group in listening, reading, writing, speaking, and total scores are significantly different from the pretest scores. And the post-test questionnaires of the students in the experimental group on interest, motivation, self-confidence, autonomy, satisfaction, and total score are significantly different from the pre-test questionnaires, which indicates that AI-CIM and AI-MOM have a significant effect on improving the students’ attitudes towards English learning and learning effectiveness.",improvement educational requirements english teaching content innovation teaching method optimization becoming important paper constructed cim model mom model based technology specifically constructed cim model based rnn realize function generating innovative teaching content based existing materials constructed teaching method optimization model based tcn realize function providing suggestions teachers based students existing loading performance validate effectiveness model empirical analysis results show posttest scores students experimental group listening reading writing speaking total scores significantly different pretest scores post test questionnaires students experimental group interest motivation self confidence autonomy satisfaction total score significantly different pre test questionnaires indicates cim mom significant effect improving students attitudes towards english learning learning effectiveness
"Against the background of accelerating global informatization, the media industry is undergoing profound changes. With the popularity of the Internet and social media, a considerable amount of information flocks to the public’s field of vision, which puts forward higher requirements for the timeliness, personalization, and creativity of media content. However, the traditional manpower-intensive content creation methods have made it challenging to meet the fast-paced needs of modern society. The fast-paced life of modern times has prompted people to pursue rapid access to and digestion of information. Traditional human-intensive content creation, such as writing, editing, and reviewing, can no longer keep up with this immediate demand. At the same time, audience segmentation requires media content to be accurately positioned, provide personalized services, and respond quickly to market changes. In today’s information explosion, maintaining high-quality content has become the core competitiveness of media, but it is difficult to balance timeliness and accuracy and depth of content in a manpower-intensive way. Therefore, in order to solve the existing problems, this study develops an intelligent content creation auxiliary system through natural language processing and reinforcement learning technology. Firstly, an NLP module is established to parse and generate high-quality text that conforms to grammatical rules and logical structure. Subsequently, the reinforcement learning mechanism is introduced to enable the system to have self-learning ability, and the strategy is constantly adjusted in multiple attempts to maximize the relevance and attractiveness indicators of the content. The system is applied to the natural environment of a prominent news website. Experiments show that within 3 months after the implementation of the system, compared with traditional hand-made manuscripts, the average click-through rate of the content created or recommended by it has increased by about 30%, and the user retention time has been extended by nearly 25%.",background accelerating global informatization media industry undergoing profound changes popularity internet social media considerable amount information flocks public field vision puts forward higher requirements timeliness personalization creativity media content however traditional manpower intensive content creation methods made challenging meet fast paced needs modern society fast paced life modern times prompted people pursue rapid access digestion information traditional human intensive content creation writing editing reviewing longer keep immediate demand time audience segmentation requires media content accurately positioned provide personalized services respond quickly market changes today information explosion maintaining high quality content become core competitiveness media difficult balance timeliness accuracy depth content manpower intensive way therefore order solve existing problems study develops intelligent content creation auxiliary system natural language processing reinforcement learning technology firstly nlp module established parse generate high quality text conforms grammatical rules logical structure subsequently reinforcement learning mechanism introduced enable system self learning ability strategy constantly adjusted multiple attempts maximize relevance attractiveness indicators content system applied natural environment prominent news website experiments show within months implementation system compared traditional hand made manuscripts average click rate content created recommended increased user retention time extended nearly
"Facing the information age, the field of visual art design is also influenced and inspired by big data. Based on the comprehensive analysis of big data, this study discusses its practical application and potential value in visual art design. Starting from the core elements of big data, this paper deeply analyzes its challenges and countermeasures in art design, further combs the complete link from data collection, processing and analysis to model optimization, proposes a theoretical framework that combines big data analysis with visual art design, and on this basis, carries out a series of empirical analyses. Through the application of refined data processing and analysis models, the aim is to realize the organic combination of precision and creativity in the process of art design, and to explore the future integration of art and technology.",facing information age field visual art design also influenced inspired big data based comprehensive analysis big data study discusses practical application potential value visual art design starting core elements big data paper deeply analyzes challenges countermeasures art design combs complete link data collection processing analysis model optimization proposes theoretical framework combines big data analysis visual art design basis carries series empirical analyses application refined data processing analysis models aim realize organic combination precision creativity process art design explore future integration art technology
"With the increasing popularity of big data technology in the field of education, its application in college English courses and the corresponding challenges have become an important issue in education research. This study aims to explore the application of big data in college English teaching and assess its potential to improve teaching quality and learning outcomes. Data on teachers’ and students’ attitudes, knowledge level, and frequency of use of big data technology were collected through questionnaires. Based on these data, a predictive model was constructed to analyze the relationship between these factors and attitudes toward big data technology. The study found that the level of knowledge and the frequency of use had a significant impact on the attitudes of teachers and students. In addition, by evaluating the effectiveness of the model, we confirm its potential value in practical teaching applications. These findings are of great significance for understanding and improving the application of big data in college English education, and provide valuable insights for the development and application of educational technology in the future.",increasing popularity big data technology field education application college english courses corresponding challenges become important issue education research study aims explore application big data college english teaching assess potential improve teaching quality learning outcomes data teachers students attitudes knowledge level frequency use big data technology collected questionnaires based data predictive model constructed analyze relationship factors attitudes toward big data technology study found level knowledge frequency use significant impact attitudes teachers students addition evaluating effectiveness model confirm potential value practical teaching applications findings great significance understanding improving application big data college english education provide valuable insights development application educational technology future
"The construction industry is a major consumer of raw materials, accounting for nearly half of global material usage annually, while generating significant waste that poses sustainability challenges. This paper explores the untapped potential of recycled plastics as a primary construction material, leveraging their lightweight, flexible, and customizable properties for advanced applications in modular chainmail systems. Through a computational workflow, the study optimizes the design, testing, and fabrication of vacuum-sealed chainmail structures composed of recycled plastic filaments, demonstrating their adaptability and structural performance for architectural use. Key contributions include a novel methodology for integrating recycled plastic filaments into chainmail geometries, validated through 2D sectional testing, 3D shell structure generation, and physical modeling under vacuum constraints. The research identifies the rectangular chainmail configuration as the most efficient and adaptable, achieving superior deformation capacity, material efficiency, and load-bearing performance. Optimization strategies for temporary structures highlight practical deployment potential, balancing material savings, usable area, and water drainage efficiency. The findings offer a foundation for innovative applications in extreme conditions, including disaster-prone areas, high-altitude environments, underwater platforms, and extraterrestrial habitats. These applications leverage the lightweight, adaptable, and durable properties of recycled plastics and modular chainmail systems, bridging the gap between waste management and high-performance design while addressing unique challenges in harsh and resource-constrained environments.",construction industry major consumer raw materials accounting nearly half global material usage annually generating significant waste poses sustainability challenges paper explores untapped potential recycled plastics primary construction material leveraging lightweight flexible customizable properties advanced applications modular chainmail systems computational workflow study optimizes design testing fabrication vacuum sealed chainmail structures composed recycled plastic filaments demonstrating adaptability structural performance architectural use key contributions include novel methodology integrating recycled plastic filaments chainmail geometries validated sectional testing shell structure generation physical modeling vacuum constraints research identifies rectangular chainmail configuration efficient adaptable achieving superior deformation capacity material efficiency load bearing performance optimization strategies temporary structures highlight practical deployment potential balancing material savings usable area water drainage efficiency findings offer foundation innovative applications extreme conditions including disaster prone areas high altitude environments underwater platforms extraterrestrial habitats applications leverage lightweight adaptable durable properties recycled plastics modular chainmail systems bridging gap waste management high performance design addressing unique challenges harsh resource constrained environments
"With the rapid development of artificial intelligence and virtual reality technology, the combination of the two has presented a broad application prospect in the field of graphic design. This research takes the application of AI technology in virtual reality graphic design as the background, discusses the relevant theories in-depth, and constructs and optimizes the relevant models through questionnaire survey and data analysis. It is found that by adopting appropriate model and optimization strategy, the accuracy and reliability of the design can be improved, and the personalized and real-time requirements of users can be met. However, issues such as data quality and resource consumption remain challenges. Although there are some problems, this study puts forward a series of solutions and provides reference for future research.",rapid development artificial intelligence virtual reality technology combination two presented broad application prospect field graphic design research takes application technology virtual reality graphic design background discusses relevant theories depth constructs optimizes relevant models questionnaire survey data analysis found adopting appropriate model optimization strategy accuracy reliability design improved personalized real time requirements users met however issues data quality resource consumption remain challenges although problems study puts forward series solutions provides reference future research
"In the past several years, a new generation of feminist and queer theorists has helped to cultivate renewed attention to the work of 1970s lesbian theory and activism. This renewed attention requires careful engagement with contested archives, particularly when navigating racisms and biological essentialisms of the past that continue to haunt feminisms in the present. In this article, I ask what it means to inherit and care for the contested archive of the womyn's land movement and the potentialities it preserves for future feminist theorising while remaining committed to trans inclusion and antiracist and anticolonial praxis. Drawing from the papers of womyn's land culturemakers Ruth and Jean Mountaingrove, I argue for a more complex understanding of what it means to ‘inherit’ this work for feminists interested in lineages of land-based feminist thought. By more fulsomely accounting for the possibilities that emerge from non-biological and queer forms of intergenerational exchange at play on the land, I argue for the vitality of the ‘messiness’ of this archive as a source of its own feminist pedagogy, proposing a process of ‘thinking-with’ these archives as part of a web of engagements rather than a linear genealogy.",past several years new generation feminist queer theorists helped cultivate renewed attention work lesbian theory activism renewed attention requires careful engagement contested archives particularly navigating racisms biological essentialisms past continue haunt feminisms present article ask means inherit care contested archive womyn land movement potentialities preserves future feminist theorising remaining committed trans inclusion antiracist anticolonial praxis drawing papers womyn land culturemakers ruth jean mountaingrove argue complex understanding means inherit work feminists interested lineages land based feminist thought fulsomely accounting possibilities emerge non biological queer forms intergenerational exchange play land argue vitality messiness archive source feminist pedagogy proposing process thinking archives part web engagements rather linear genealogy
"This paper explores the application and influence of deep learning in English culture and context teaching. Through specific case studies, the study demonstrates how deep learning techniques can effectively improve learners’ language proficiency, engagement, and satisfaction. Especially in terms of personalized learning paths, interactive teaching, and cultural understanding, deep learning shows its unique advantages. This study highlights the importance of balancing the application of technology with traditional teaching methods and suggests that future research should focus on technology optimization, model transparency, and their application to a wider range of teaching scenarios. These findings provide important reference and enlightenment for the further development and application of deep learning technology in the field of education.",paper explores application influence deep learning english culture context teaching specific case studies study demonstrates deep learning techniques effectively improve learners language proficiency engagement satisfaction especially terms personalized learning paths interactive teaching cultural understanding deep learning shows unique advantages study highlights importance balancing application technology traditional teaching methods suggests future research focus technology optimization model transparency application wider range teaching scenarios findings provide important reference enlightenment development application deep learning technology field education
The present contribution is concerned with the numerical solution of initial value problems for linear Caputo-type fractional differential equations. Global convergence estimates for a collocation method and its iterated version on special graded grids are derived. A numerical example confirming the theoretical results is also given.,present contribution concerned numerical solution initial value problems linear caputo type fractional differential equations global convergence estimates collocation method iterated version special graded grids derived numerical example confirming theoretical results also given
"As the global energy crisis and environmental problems become increasingly severe, electric vehicles have become carrier transportation field due to their zero-emission and energy-saving characteristics. Large-scale popularization of electric vehicles has brought dual impacts, increase grid load fluctuations; on the other hand, through reasonable charging regulation and vehicle-network interaction, electric vehicles can be used as distributed energy storage resources to improve the efficiency and stability of power grid operation. However, at present, the research on the decision-making influencing factors of the interaction is still insufficient, and it is urgent to discuss its key driving factors and optimization paths in depth. Based on the actual data analysis, this paper makes an empirical study on the influencing factors of interactive decision-making between electric vehicles and power grid by using multiple regression model. The research results show that the charging behavior of electric vehicle owners is mainly affected by the fluctuation of electricity prices, charging convenience and renewable energy penetration rate. At the grid level, the increase in the proportion of distributed photovoltaic installed capacity has significantly increased the V2G participation rate. In addition, under different policy incentives, there are significant differences in the willingness of car owners to accept V2G: subsidy policies can increase the participation rate of car owners by 15%–20%. Based on this, this paper puts forward suggestions such as optimizing the charging pricing mechanism, improving the coverage of smart charging infrastructure, and promoting policy innovation related to vehicle-network interaction.",global energy crisis environmental problems become increasingly severe electric vehicles become carrier transportation field due zero emission energy saving characteristics large scale popularization electric vehicles brought dual impacts increase grid load fluctuations hand reasonable charging regulation vehicle network interaction electric vehicles used distributed energy storage resources improve efficiency stability power grid operation however present research decision making influencing factors interaction still insufficient urgent discuss key driving factors optimization paths depth based actual data analysis paper makes empirical study influencing factors interactive decision making electric vehicles power grid using multiple regression model research results show charging behavior electric vehicle owners mainly affected fluctuation electricity prices charging convenience renewable energy penetration rate grid level increase proportion distributed photovoltaic installed capacity significantly increased participation rate addition different policy incentives significant differences willingness car owners accept subsidy policies increase participation rate car owners based paper puts forward suggestions optimizing charging pricing mechanism improving coverage smart charging infrastructure promoting policy innovation related vehicle network interaction
"Many communist regimes have presided over episodes of rapid industrial development, often coupled by the establishment of a planned economic system wherein the state replaces the market as the primary provider of essential resources and services. We argue that life experience under a communist-style industrial workplace fosters a distinct and enduring set of norms and habits that shape how citizens engage with political authorities. Leveraging exogenous variations created by China’s Third Front (TF) campaign, a massive industrial relocation project that moved large-scale industrial-administrative complexes into rural and interior areas between the 1960s–1970s, we show that residents from former TF areas today are more active in contacting the government about personal and community affairs, and hold stronger preferences for state-sponsored participation channels over more contentious ones. Additional analyses suggest that these behavioral patterns are less a strategic response to current local conditions than a result of early socialization. These findings highlight important subnational variations in national communist experiences and underscore the persistent influence of early communist institutions on citizen-government interactions in late communist societies.",many communist regimes presided episodes rapid industrial development often coupled establishment planned economic system wherein state replaces market primary provider essential resources services argue life experience communist style industrial workplace fosters distinct enduring set norms habits shape citizens engage political authorities leveraging exogenous variations created china third front campaign massive industrial relocation project moved large scale industrial administrative complexes rural interior areas show residents former areas today active contacting government personal community affairs hold stronger preferences state sponsored participation channels contentious ones additional analyses suggest behavioral patterns less strategic response current local conditions result early socialization findings highlight important subnational variations national communist experiences underscore persistent influence early communist institutions citizen government interactions late communist societies
"This study introduces the “framing element” method, an alternative approach to computational news framing detection. Rooted in a constructionist framing analysis framework, it identifies frames as packages of framing elements, including actors (individuals and organizations) and topics, extending beyond topic-focused methods in prior unsupervised analyses. Compared with latent Dirichlet allocation (LDA)- and Bidirectional Encoder Representations from Transformers (BERT)-based approaches on 1,300 U.S. gun violence news articles, this method addresses LDA’s limitations by focusing on high-level framing elements rather than keywords and is less labor-intensive than BERT-based supervised learning. Supporting both inductive and deductive analyses, it achieves comparable results to LDA while uncovering a previously unidentified gun violence frame.",study introduces framing element method alternative approach computational news framing detection rooted constructionist framing analysis framework identifies frames packages framing elements including actors individuals organizations topics extending beyond topic focused methods prior unsupervised analyses compared latent dirichlet allocation lda bidirectional encoder representations transformers bert based approaches gun violence news articles method addresses lda limitations focusing high level framing elements rather keywords less labor intensive bert based supervised learning supporting inductive deductive analyses achieves comparable results lda uncovering previously unidentified gun violence frame
"Computational Thinking (CT) has evolved as an essential competency for K-12 students, and programming practices are recognized as the key way to facilitate CT development. However, most studies of CT development in middle graders have focused on visual programming, lacking evidence to demonstrate the effectiveness of Python programming. Therefore, this study first developed 16 Python programming courses based on the CodeCombat programming platform. Then we conducted a 16-week Python programming course intervention study with 79 middle school students in seventh, eighth, and ninth grades in China to clarify the impact of Python programming on middle graders’ CT. The results revealed that Python programming intervention significantly improved CT for seventh, eighth, and ninth graders. The analysis of interaction effects for grade and gender showed that Python programming was most beneficial for eighth graders in CT improvement. Meanwhile, we found that the difference in CT caused by the gender factor varied according to grade level. Specifically, the gender factor caused significant CT differences in eighth and ninth graders, and girls were better performers than boys. These findings enrich the outcomes of Python programming and CT education in middle schools and provide implications for frontline programming educators to conduct Python interventions.",computational thinking evolved essential competency students programming practices recognized key way facilitate development however studies development middle graders focused visual programming lacking evidence demonstrate effectiveness python programming therefore study first developed python programming courses based codecombat programming platform conducted week python programming course intervention study middle school students seventh eighth ninth grades china clarify impact python programming middle graders results revealed python programming intervention significantly improved seventh eighth ninth graders analysis interaction effects grade gender showed python programming beneficial eighth graders improvement meanwhile found difference caused gender factor varied according grade level specifically gender factor caused significant differences eighth ninth graders girls better performers boys findings enrich outcomes python programming education middle schools provide implications frontline programming educators conduct python interventions
"There is a growing recognition that using engineering design process as an approach to integrate Science, Technology, Engineering and Mathematics (STEM) produces good learning outcomes. This study aims to evaluate the outcomes of an integrated STEM with Design Thinking (iSTEM-DT) module on preschoolers’ science process skills. It adopts a single case study evaluation design in a private preschool, with two teachers and 20 preschoolers. These preschoolers were paired up and divided into two classes. They attended the iSTEM-DT lessons in the afternoon, for an hour daily, for about 4 weeks during the module implementation. Qualitative data such as fieldnotes, artefacts, photos and video recordings were collected from classroom observations. Data were also collected from teachers during reflection sessions after each lesson, while parents were interviewed at the end of the study. The data were analysed inductively through thematic analysis process. It was found that the iSTEM-DT module helped in developing preschoolers’ science process skills, namely observing, classifying, measuring, making inferences, predicting, and communicating skills, and some other higher order thinking skills. It implies that preschool teachers with basic qualification in Early Childhood Education, can have the self-efficacy to facilitate an integrated STEM curriculum if the support system is provided.",growing recognition using engineering design process approach integrate science technology engineering mathematics stem produces good learning outcomes study aims evaluate outcomes integrated stem design thinking istem module preschoolers science process skills adopts single case study evaluation design private preschool two teachers preschoolers preschoolers paired divided two classes attended istem lessons afternoon hour daily weeks module implementation qualitative data fieldnotes artefacts photos video recordings collected classroom observations data also collected teachers reflection sessions lesson parents interviewed end study data analysed inductively thematic analysis process found istem module helped developing preschoolers science process skills namely observing classifying measuring making inferences predicting communicating skills higher order thinking skills implies preschool teachers basic qualification early childhood education self efficacy facilitate integrated stem curriculum support system provided
"Systems thinking is a paradigm well-suited to complex social and health fields such as socioeconomic approaches to wellbeing and health intervention research. Yet despite increasing calls for more general application of systems thinking in these fields of research, the paradigm remains poorly understood and systemic analysis is rare, particularly in qualitative research. This paper aims to address these issues. It starts by providing a primer to systems thinking clarifying what systems thinking is as a research philosophy, and describing core systemic principles such as complex causality and emergence. It then introduces the novel method of Inductive Systemic Analysis. Combining the methodology of grounded theory with the principles and key concepts of systems thinking, this method was specifically designed to make systemic analysis both relatable and accessible to a wide range of qualitative researchers. Utilising the case example the method was developed with, the paper works through each step of the research process. It explains preparing for systemic research through boundary considerations and developing a systemic research question; and goes on to describe the four-step analytical process of “zooming in” and “zooming out”, exploring the data in different ways. Ultimately, the method aims to support researchers in finding a balance between reflecting the complexity of social situations with ease of comprehension and applicability.",systems thinking paradigm well suited complex social health fields socioeconomic approaches wellbeing health intervention research yet despite increasing calls general application systems thinking fields research paradigm remains poorly understood systemic analysis rare particularly qualitative research paper aims address issues starts providing primer systems thinking clarifying systems thinking research philosophy describing core systemic principles complex causality emergence introduces novel method inductive systemic analysis combining methodology grounded theory principles key concepts systems thinking method specifically designed make systemic analysis relatable accessible wide range qualitative researchers utilising case example method developed paper works step research process explains preparing systemic research boundary considerations developing systemic research question goes describe four step analytical process zooming zooming exploring data different ways ultimately method aims support researchers finding balance reflecting complexity social situations ease comprehension applicability
"This study introduces a novel filter, termed the high-order unscented particle filter (HUPF). Utilizing the HUKF, HUPF advances the sample particles toward regions of higher likelihood and consequently derives an improved importance proposal distribution, closely aligned with the posterior probability. Distinct from the unscented particle filter, HUPF implements the high-order unscented transform instead of the conventional second-order variant. Due to the superior accuracy of the HUKF over the standard UKF, HUPF not only selects particles more judiciously, enhancing particle utilization, but also more effectively manages the tail decay rate of the proposal distribution. This approach enables HUPF to closely approximate the optimal posterior probability under typical Gaussian conditions. The efficacy of this method is demonstrated through a bearing-only tracking model example.",study introduces novel filter termed high order unscented particle filter hupf utilizing hukf hupf advances sample particles toward regions higher likelihood consequently derives improved importance proposal distribution closely aligned posterior probability distinct unscented particle filter hupf implements high order unscented transform instead conventional second order variant due superior accuracy hukf standard ukf hupf selects particles judiciously enhancing particle utilization also effectively manages tail decay rate proposal distribution approach enables hupf closely approximate optimal posterior probability typical gaussian conditions efficacy method demonstrated bearing tracking model example
"Although SARS-COV-2 started in 2019, its losses are still significant, and it takes victims. In the present study, the epidemic patterns of SARS-COV-2 disease have been investigated from the point of view of mathematical modeling. Also, the effect of quarantine has been considered. This mathematical model is designed in the form of fractional calculations along with a model predictive control (MPC) to monitor this model. The fractional-order model has the memory and hereditary properties of the system, which can provide more adjustable parameters to the designer. Because the MPC can predict future outputs, it can overcome the conditions and events that occur in the future. The results of the simulations show that the proposed nonlinear model predictive controller (NMPC) of fractional-order has a lower mean squared error in susceptible people compared to the optimal control of fractional-order (~3.6e-04 vs. 47.4). This proposed NMPC of fractional-order can be used for other models of epidemics.",although sars cov started losses still significant takes victims present study epidemic patterns sars cov disease investigated point view mathematical modeling also effect quarantine considered mathematical model designed form fractional calculations along model predictive control mpc monitor model fractional order model memory hereditary properties system provide adjustable parameters designer mpc predict future outputs overcome conditions events occur future results simulations show proposed nonlinear model predictive controller nmpc fractional order lower mean squared error susceptible people compared optimal control fractional order proposed nmpc fractional order used models epidemics
"The sustainability of social development can be signified by the level of coordinated development of regional education–economy–human capital complex system (REEHCCS). However, the existing studies on the coordinated development of systems fail to consider the influence of human capital on specific systems, or identify the root causes of the complex state of the coordinated development. To solve the problem, this paper explores the modeling and simulation for the coordinated development of REEHCCS. Specifically, the REEHCCS structure was modeled, and reasonable evaluation indices were selected for coordination. Then, an REEHCCS coordination model was constructed, and the classification principles were detailed for the coordinated development of complex systems. On this basis, education and human capital were introduced to the dynamic Haavelmo model to identify the state and analyze the evolution of the REEHCCS. Finally, the proposed model was proved effective through simulation and case analysis. This study explores the complex interactions between regional education, economy, and human capital, and assesses their coordinated development through the construction of a coordination model and simulation experiments. The research findings indicate that the constructed model can effectively identify and analyze the coordinated development state and evolution trends of the regional education–economy–human capital system.",sustainability social development signified level coordinated development regional education economy human capital complex system reehccs however existing studies coordinated development systems fail consider influence human capital specific systems identify root causes complex state coordinated development solve problem paper explores modeling simulation coordinated development reehccs specifically reehccs structure modeled reasonable evaluation indices selected coordination reehccs coordination model constructed classification principles detailed coordinated development complex systems basis education human capital introduced dynamic haavelmo model identify state analyze evolution reehccs finally proposed model proved effective simulation case analysis study explores complex interactions regional education economy human capital assesses coordinated development construction coordination model simulation experiments research findings indicate constructed model effectively identify analyze coordinated development state evolution trends regional education economy human capital system
"Psychological researchers are interested in how things change over time and routinely make claims about age effects (e.g., personality maturation), cohort effects (e.g., generational differences in narcissism), and sometimes, period effects (e.g., secular trends in mental health). The age-period-cohort identification problem means that these claims are not possible based on the data alone: Any possible temporal pattern can be explained by an infinite number of combinations of age, period, and cohort effects. This concern holds regardless of the study design (it also applies to longitudinal designs covering multiple cohorts) and the number of observations available (it also applies if researchers observe the whole population). Researchers usually rely on statistical models that impose constraints to pick one specific decomposition of effects. Unfortunately, these constraints often remain opaque, resulting in a lack of scrutiny of the underlying assumptions. How can researchers reason more transparently and systematically about age, period, and cohort? Here, I summarize advances in the understanding of the precise nature of the identification problem, provide an overview of ways to move forward, and highlight one approach that is particularly transparent about assumptions: bounding analysis, a framework developed by sociologists Ethan Fosse and Christopher Winship. To illustrate this approach, I analyze how age, period, and cohort affect attitudes toward working mothers in the German General Social Survey.",psychological researchers interested things change time routinely make claims age effects personality maturation cohort effects generational differences narcissism sometimes period effects secular trends mental health age period cohort identification problem means claims possible based data alone possible temporal pattern explained infinite number combinations age period cohort effects concern holds regardless study design also applies longitudinal designs covering multiple cohorts number observations available also applies researchers observe whole population researchers usually rely statistical models impose constraints pick one specific decomposition effects unfortunately constraints often remain opaque resulting lack scrutiny underlying assumptions researchers reason transparently systematically age period cohort summarize advances understanding precise nature identification problem provide overview ways move forward highlight one approach particularly transparent assumptions bounding analysis framework developed sociologists ethan fosse christopher winship illustrate approach analyze age period cohort affect attitudes toward working mothers german general social survey
"This article proposes a novel approach for trajectory tracking of a six degrees-of-freedom (6-DOF) collaborative robot manipulator using an adaptive fuzzy proportional derivative (PD) controller. Based on the dynamic modeling of the robot manipulator, the PD control law is designed, and the improved dung beetle optimization (DBO) algorithm is introduced using the good point set (GPS) method for population initialization and the sine strategy for convergence factor adjustment. Furthermore, a fuzzy adaptive strategy is developed to adjust the PD controller gain based on real-time errors. This article uses discrete Lyapunov iterative stability to analyze the global asymptotic stability of the robot closed-loop system. The experimental results verify that the DBO-fuzzy-PD controller is superior to the original PD controller. The ISE value is reduced from 3.4140 to 0.0384, and the IAE value is reduced from 1.9876 to 0.1843. The DBO-fuzzy-PD controller has better tracking accuracy and response speed than traditional PD. Experimental results show that the proposed DBO-fuzzy-PD controller significantly enhances the trajectory tracking performance of the 6-DOF collaborative robot manipulator.",article proposes novel approach trajectory tracking six degrees freedom dof collaborative robot manipulator using adaptive fuzzy proportional derivative controller based dynamic modeling robot manipulator control law designed improved dung beetle optimization dbo algorithm introduced using good point set gps method population initialization sine strategy convergence factor adjustment furthermore fuzzy adaptive strategy developed adjust controller gain based real time errors article uses discrete lyapunov iterative stability analyze global asymptotic stability robot closed loop system experimental results verify dbo fuzzy controller superior original controller ise value reduced iae value reduced dbo fuzzy controller better tracking accuracy response speed traditional experimental results show proposed dbo fuzzy controller significantly enhances trajectory tracking performance dof collaborative robot manipulator
"This Note is a creative piece of writing that flows freely and weaves together different thoughts and memories. The initial idea for it stemmed from my reflections about disability and my relationship with edges. Disability, however, is only one part of it – and a relatively small part. In the Note, I think with edges and my response to edges, in different situations and contexts, and about the relationship between edges and missed connections in my previous qualitative research on war and armed conflict. Throughout, I accentuate entanglements between human and more-than-human worlds.",note creative piece writing flows freely weaves together different thoughts memories initial idea stemmed reflections disability relationship edges disability however one part relatively small part note think edges response edges different situations contexts relationship edges missed connections previous qualitative research war armed conflict throughout accentuate entanglements human human worlds
"In the realm of educational data science, the utilization of advanced analytical techniques is increasingly pivotal for optimizing pedagogical methods and implementing early interventions. This study introduces a predictive model employing deep learning technology for the analysis of student learning behavior. The model’s primary objective is to detect potential learning impediments at nascent stages, thereby facilitating timely and effective educational strategies to augment learning outcomes. Despite strides in behavioral prediction, challenges persist due to the dynamic and complex nature of educational data. Addressing these challenges, the proposed model integrates sequence pattern recognition with time-series analysis. The application of the PrefixSpan algorithm initiates the process, identifying sequential patterns in student learning behaviors and elucidating their temporal progression. Subsequently, an advanced ordered funnel analysis algorithm is employed, unveiling directional associations among diverse learning patterns. The final phase involves applying independent component analysis (ICA) to enhance the multi-layer long short-term memory (Multi-LSTM) network’s structure, thus enabling precise predictions of student outcomes in the context of early interventions. The results underscore the efficacy of deep learning in deciphering intricate behavioral patterns and underscore its potential in personalized educational interventions. This comprehensive approach demonstrates the model’s capacity to harness the intricacies of educational data, thereby contributing significantly to the field of personalized education and precise teaching methodologies. The innovative aspect of this study lies in the comprehensive application of the PrefixSpan algorithm, the ordered funnel analysis algorithm, and the Multi-LSTM network architecture. A holistic model for predicting student learning behavior is proposed, coupled with early interventions, which enables educators to better understand student learning conditions and implement effective measures to enhance student learning outcomes.",realm educational data science utilization advanced analytical techniques increasingly pivotal optimizing pedagogical methods implementing early interventions study introduces predictive model employing deep learning technology analysis student learning behavior model primary objective detect potential learning impediments nascent stages thereby facilitating timely effective educational strategies augment learning outcomes despite strides behavioral prediction challenges persist due dynamic complex nature educational data addressing challenges proposed model integrates sequence pattern recognition time series analysis application prefixspan algorithm initiates process identifying sequential patterns student learning behaviors elucidating temporal progression subsequently advanced ordered funnel analysis algorithm employed unveiling directional associations among diverse learning patterns final phase involves applying independent component analysis ica enhance multi layer long short term memory multi lstm network structure thus enabling precise predictions student outcomes context early interventions results underscore efficacy deep learning deciphering intricate behavioral patterns underscore potential personalized educational interventions comprehensive approach demonstrates model capacity harness intricacies educational data thereby contributing significantly field personalized education precise teaching methodologies innovative aspect study lies comprehensive application prefixspan algorithm ordered funnel analysis algorithm multi lstm network architecture holistic model predicting student learning behavior proposed coupled early interventions enables educators better understand student learning conditions implement effective measures enhance student learning outcomes
"China’s relentless urbanization has significantly propelled the construction sector forward, simultaneously unlocking vast potential and posing complex challenges for the development of intelligent building systems. This paper introduces an architecture for an intelligent building integration platform that hinges on the connectivity between server and client. The server sends the collected data to the system, while the client displays, monitors, and manages the data. The whole intelligent building management system includes business system management module, authorization management module, client module, and data linkage module. The client module visually displays the data in the building through charts and other means. The data linkage module uses sensors, devices, and communication technologies to collect various environmental parameters in the building and equipment operating status and other information, and sends the information to the data center or the cloud for subsequent processing. In the monitoring of energy consumption information, the electricity consumption of Building C was 181 KWH; the water consumption was 98 tons, and the gas consumption was 75 cubic meters. This research significantly advances the field of intelligent building management by demonstrating how computer technology can be effectively utilized to monitor building energy consumption. The findings of this paper provide crucial theoretical and technical support for the ongoing development and refinement of intelligent building systems. This not only enhances our understanding of smart infrastructure but also opens up new avenues for innovation in building management and energy efficiency.",china relentless urbanization significantly propelled construction sector forward simultaneously unlocking vast potential posing complex challenges development intelligent building systems paper introduces architecture intelligent building integration platform hinges connectivity server client server sends collected data system client displays monitors manages data whole intelligent building management system includes business system management module authorization management module client module data linkage module client module visually displays data building charts means data linkage module uses sensors devices communication technologies collect various environmental parameters building equipment operating status information sends information data center cloud subsequent processing monitoring energy consumption information electricity consumption building kwh water consumption tons gas consumption cubic meters research significantly advances field intelligent building management demonstrating computer technology effectively utilized monitor building energy consumption findings paper provide crucial theoretical technical support ongoing development refinement intelligent building systems enhances understanding smart infrastructure also opens new avenues innovation building management energy efficiency
"At present, the discipline inspection and supervision work of schools is faced with the problems of large amount of data and complex information, and efficient algorithms are urgently needed for accurate early warning and risk assessment. The purpose of this study is to build an efficient and accurate case early warning and risk assessment system through data mining technology. In the research, we adopted an ensemble learning algorithm, combining the advantages of deep learning and traditional machine learning, to conduct an in-depth analysis of a large number of historical case data in the field of education. The experimental results show that the early warning accuracy rate of the system on the test data set reaches 85%, which is 45 percentage points higher than that of the traditional method, effectively reducing the false report and false alarm rate of discipline inspection and supervision work, and significantly improving the work efficiency. Through this study, we not only verify application potential of data mining technology in field of educational discipline inspection and supervision but also provide scientific decision support tools for schools, which are helpful to prevent potential violations and maintain educational equity.",present discipline inspection supervision work schools faced problems large amount data complex information efficient algorithms urgently needed accurate early warning risk assessment purpose study build efficient accurate case early warning risk assessment system data mining technology research adopted ensemble learning algorithm combining advantages deep learning traditional machine learning conduct depth analysis large number historical case data field education experimental results show early warning accuracy rate system test data set reaches percentage points higher traditional method effectively reducing false report false alarm rate discipline inspection supervision work significantly improving work efficiency study verify application potential data mining technology field educational discipline inspection supervision also provide scientific decision support tools schools helpful prevent potential violations maintain educational equity
"Envisioning new kinds of operations requires systematically developing architectures, work procedures, and artifacts to support human and machine agents in coordinating within dynamic environments. Accurately predicting how envisioned operations will unfold is challenging as (1) early design-phase descriptions of architectures, work procedures, and artifacts are often underspecified, and (2) key outcomes of interest emerge from interactions between cognitive work and environmental dynamics. This paper discusses how computational simulation of work can serve as a discovery tool for envisioning future operations. We introduce a three-phase approach using the Work Models that Compute (WMC) framework, which involves converting paper-based representations of work into computational models, developing scenarios and test conditions, and simulating work dynamics to analyze emergent behaviors. We illustrate this approach through a case study on developing contingency management procedures for envisioned air transport operations, specifically Urban Air Mobility (UAM). The case study demonstrates how computational simulation can (1) reveal the need for clearer design specifications, (2) uncover interactions and emergent behavior that may lead to undesirable outcomes, such as coordination surprises, and (3) identify trade-offs between multiple design options. Insight from simulation can complement other cognitive systems engineering methods to refine and enhance the feasibility and robustness of envisioned operations.",envisioning new kinds operations requires systematically developing architectures work procedures artifacts support human machine agents coordinating within dynamic environments accurately predicting envisioned operations unfold challenging early design phase descriptions architectures work procedures artifacts often underspecified key outcomes interest emerge interactions cognitive work environmental dynamics paper discusses computational simulation work serve discovery tool envisioning future operations introduce three phase approach using work models compute wmc framework involves converting paper based representations work computational models developing scenarios test conditions simulating work dynamics analyze emergent behaviors illustrate approach case study developing contingency management procedures envisioned air transport operations specifically urban air mobility uam case study demonstrates computational simulation reveal need clearer design specifications uncover interactions emergent behavior may lead undesirable outcomes coordination surprises identify trade offs multiple design options insight simulation complement cognitive systems engineering methods refine enhance feasibility robustness envisioned operations
"With the development of information technology, the drawbacks of traditional English education appear. The optimization of English resources, the implementation of English teaching mode these are urgent problems to be solved. Deep learning has a broad application prospect in education. In this paper, we construct a resource optimization model based on transformer, which mainly includes automated acquisition, analysis, integration, recommendation, and evaluation of teaching resources. After experimental verification, the model can significantly improve students’ English learning performance, and at the same time, improve students' overall satisfaction with English learning. Finally, we put forward some suggestions on the problems found in the experiment in terms of resource optimization, teaching model innovation, and English literacy improvement.",development information technology drawbacks traditional english education appear optimization english resources implementation english teaching mode urgent problems solved deep learning broad application prospect education paper construct resource optimization model based transformer mainly includes automated acquisition analysis integration recommendation evaluation teaching resources experimental verification model significantly improve students english learning performance time improve students overall satisfaction english learning finally put forward suggestions problems found experiment terms resource optimization teaching model innovation english literacy improvement
"Mudstone is extensively distributed in the open-pit mining area of Xianfeng Coal Mine in Yunnan, China. Since 2018, with the continuous rainfall, mudstone landslide has gradually emerged as a serious issue in the region. However, there are currently scarce research reports on rainfall-induced mudstone landslide. Therefore, this paper takes the northern slope of the Xianfeng Coal Mine as an example. Based on on-site field investigations and real-time monitoring data, the paper, while analyzing the characteristics of slope deformation, elucidates the impact of rainfall and geological lithology on the superficial failure of landslides. Analysis of existing data indicates that during sustained periods of rainfall, the infiltration process of rainwater undergoes a transformation. Initially, a major portion of the rainwater flows along the surface of the slope, while in the later stages, a significant portion of it infiltrates vertically along the slope surface and cracks. Consequently, the mode of slope deterioration also undergoes a transformation. Additionally, under the influence of rainfall infiltration, the mudstone layer at the base of the slope undergoes expansive deformation, resulting in a significant reduction in the shear strength of the slope.",mudstone extensively distributed open pit mining area xianfeng coal mine yunnan china since continuous rainfall mudstone landslide gradually emerged serious issue region however currently scarce research reports rainfall induced mudstone landslide therefore paper takes northern slope xianfeng coal mine example based site field investigations real time monitoring data paper analyzing characteristics slope deformation elucidates impact rainfall geological lithology superficial failure landslides analysis existing data indicates sustained periods rainfall infiltration process rainwater undergoes transformation initially major portion rainwater flows along surface slope later stages significant portion infiltrates vertically along slope surface cracks consequently mode slope deterioration also undergoes transformation additionally influence rainfall infiltration mudstone layer base slope undergoes expansive deformation resulting significant reduction shear strength slope
"With the increasing maturity of artificial intelligence (AI) technology, its application in the field of educational management has become an important way to improve the quality and efficiency of education. In particular, in music education management, the introduction of AI technology has brought revolutionary changes to the traditional education model. This paper first discusses the application background of AI in music education management and the opportunities and challenges it brings, especially the potential impacts on teaching quality assessment, resource allocation, and educational policy formulation. However, current research shows that existing AI application methods have deficiencies in processing complex data, algorithm generalization capabilities, and adaptability to specific music education scenarios. In response to these deficiencies, this paper proposes two research contents: one is an intelligent decision support system based on an improved actor-critic framework, aimed at improving the efficiency and accuracy of music education management by simulating complex decision-making processes; the other is using a time series prediction model based on convolutional neural network (CNN) to predict the technology trends in music education, which can capture educational dynamics and provide data support for future educational policies and resource allocation. The research in this paper not only provides new theoretical and practical perspectives for the application of AI in the field of music education management but also provides a scientific basis for the realization of personalized and precise management in music education.",increasing maturity artificial intelligence technology application field educational management become important way improve quality efficiency education particular music education management introduction technology brought revolutionary changes traditional education model paper first discusses application background music education management opportunities challenges brings especially potential impacts teaching quality assessment resource allocation educational policy formulation however current research shows existing application methods deficiencies processing complex data algorithm generalization capabilities adaptability specific music education scenarios response deficiencies paper proposes two research contents one intelligent decision support system based improved actor critic framework aimed improving efficiency accuracy music education management simulating complex decision making processes using time series prediction model based convolutional neural network cnn predict technology trends music education capture educational dynamics provide data support future educational policies resource allocation research paper provides new theoretical practical perspectives application field music education management also provides scientific basis realization personalized precise management music education
"With the rapid development of artificial intelligence technology and the increasing importance of digital media technology education, it is particularly critical to explore the application of artificial intelligence in this field. The purpose of this study is to evaluate the application effect of artificial intelligence on digital media technology education and conduct a comprehensive analysis through three dimensions: experimental research, questionnaire survey, and model construction and verification. Experimental research results show that AI-assisted pedagogy is superior to traditional methods in improving academic achievement and engagement. The results of the questionnaire reflect the positive attitude of educators, students, and industry experts towards the application of AI, and the personalized learning recommendation and learning outcome prediction models constructed perform well in accuracy and F1 scores. Although challenges such as technology integration, data privacy, student engagement, and resource costs are encountered during implementation, these challenges can be effectively overcome with sound strategies and innovative approaches. The findings of this study not only confirm the application potential of AI in the field of education but also provide valuable insights and directions for the future development of educational technology.",rapid development artificial intelligence technology increasing importance digital media technology education particularly critical explore application artificial intelligence field purpose study evaluate application effect artificial intelligence digital media technology education conduct comprehensive analysis three dimensions experimental research questionnaire survey model construction verification experimental research results show assisted pedagogy superior traditional methods improving academic achievement engagement results questionnaire reflect positive attitude educators students industry experts towards application personalized learning recommendation learning outcome prediction models constructed perform well accuracy scores although challenges technology integration data privacy student engagement resource costs encountered implementation challenges effectively overcome sound strategies innovative approaches findings study confirm application potential field education also provide valuable insights directions future development educational technology
"The traditional library recommendation method mainly relies on manual analysis of readers’ reading history, book classification, and user feedback to achieve personalized recommendation, but this method has limitations such as low efficiency and difficulty in accurately capturing users’ complex needs. In view of this, this paper innovatively designs a recommendation system based on Wide &amp; Deep model. The system utilizes a linear model (Wide part) to process hand-constructed features as a way to remember specific user behavioural patterns; at the same time, high-dimensional sparse features are processed with the help of a deep neural network (Deep part) to capture the complex relationship between user interests. After both outputs are further processed by the fully connected layer, the model outputs the probability distribution of each category through Softmax function and then predicts the book categories that users may be interested in. The experimental results show that the model has excellent performance, with an AUC score of over 99%, showing a strong differentiation ability; precision is close to 97%, recall is close to 96%, and F1 value is close to 54%, which not only can accurately recommend the book resources that the user is interested in but also can effectively cover the diversified needs of the user, and provide a better solution for personalized recommendation service in libraries.",traditional library recommendation method mainly relies manual analysis readers reading history book classification user feedback achieve personalized recommendation method limitations low efficiency difficulty accurately capturing users complex needs view paper innovatively designs recommendation system based wide amp deep model system utilizes linear model wide part process hand constructed features way remember specific user behavioural patterns time high dimensional sparse features processed help deep neural network deep part capture complex relationship user interests outputs processed fully connected layer model outputs probability distribution category softmax function predicts book categories users may interested experimental results show model excellent performance auc score showing strong differentiation ability precision close recall close value close accurately recommend book resources user interested also effectively cover diversified needs user provide better solution personalized recommendation service libraries
"This paper proposes an improved YOLO v8 grape leaf disease identification method MSAM-YOLO based on attention mechanism to address the problem of multiple types and similar small target features in grape leaf disease images. This method introduces a multi-scale convolution attention module (MSAM) in the feature extraction network to enhance the focus on grape leaf disease. By performing convolution operations on features at different scales and using attention mechanism to emphasize the features of the diseased area, the model can better capture the subtle features of the disease. Experimental results show that MSAM-YOLO improves the original model by 4% in grape leaf disease identification task, with higher accuracy and real-time performance. This method provides a new perspective for the detection of plant leaf diseases, contributing to the improvement of the quality and efficiency of agricultural production.",paper proposes improved yolo grape leaf disease identification method msam yolo based attention mechanism address problem multiple types similar small target features grape leaf disease images method introduces multi scale convolution attention module msam feature extraction network enhance focus grape leaf disease performing convolution operations features different scales using attention mechanism emphasize features diseased area model better capture subtle features disease experimental results show msam yolo improves original model grape leaf disease identification task higher accuracy real time performance method provides new perspective detection plant leaf diseases contributing improvement quality efficiency agricultural production
"In choral performances, the choral conductor’s gesture recognition is crucial, as it directly affects the overall performance of the choir. The posture recognition of choral conductors focuses on static states. This recognition method is difficult to fully capture spatial and temporal information, and has a slow response speed, which affects recognition accuracy and real-time performance. Based on OpenPose, this article developed an Inception-LSTM-TSN hybrid CNN model for posture recognition of choral commanders. Firstly, the pose extraction network VGG19 in the OpenPose algorithm was replaced with MobileNet-V3, and then a CNN network for two-stream pose recognition was constructed. The Inception network was then introduced to extract temporal information, the LSTM network was used to extract temporal information, and the EfficientNet-B3 optimized TSN was used for segmentation to obtain image information for choral commander pose recognition. Finally, the results of the two branches of the constructed hybrid two-stream convolutional neural network (H2SCNN) model can be fused using the average fusion method to output the choral conductor pose recognition results. The experiment was based on the publicly available dataset ConductorMotion100 and the self-built dataset CCD for choral conductor pose recognition. The results showed that the pose recognition performance of ConductorMotion100 in the public dataset was better than that of the self-built dataset CCD. The accuracy of H2SCNN model was as high as 97.8%, which was 7.1% higher than VGG19 static image processing, and the parameter size was only 32.7 M. The experimental results show that the H2SCNN model, combined with spatial and temporal information, significantly improves the accuracy of pose recognition, achieves good real-time performance, and greatly ensures the smooth performance of the choir.",choral performances choral conductor gesture recognition crucial directly affects overall performance choir posture recognition choral conductors focuses static states recognition method difficult fully capture spatial temporal information slow response speed affects recognition accuracy real time performance based openpose article developed inception lstm tsn hybrid cnn model posture recognition choral commanders firstly pose extraction network vgg openpose algorithm replaced mobilenet cnn network two stream pose recognition constructed inception network introduced extract temporal information lstm network used extract temporal information efficientnet optimized tsn used segmentation obtain image information choral commander pose recognition finally results two branches constructed hybrid two stream convolutional neural network scnn model fused using average fusion method output choral conductor pose recognition results experiment based publicly available dataset conductormotion self built dataset ccd choral conductor pose recognition results showed pose recognition performance conductormotion public dataset better self built dataset ccd accuracy scnn model high higher vgg static image processing parameter size experimental results show scnn model combined spatial temporal information significantly improves accuracy pose recognition achieves good real time performance greatly ensures smooth performance choir
"Earthquakes are one of the primary causes of instability in tailings storage facilities (TSFs), highlighting the necessity of studying dynamic responses and stability analysis under seismic wave action. This paper investigates the dynamic characteristics of a TSF proposed for decommissioning under seismic loading using the finite element time-history method. The study comprehensively analyzes dynamic deformation, seismic-induced permanent deformation, and pore water pressure changes, while identifying potential liquefaction zones and slope safety factors after seismic events. Results show that stress levels in the tailings dam remain low under applied dynamic loads, indicating a significant safety margin. Liquefaction zones are mainly concentrated in the shallow surface layers of the tailings beach, particularly in the fine sand and silt layers, with no liquefaction observed in the dam slope. The dam body’s deformation accumulates over the duration of seismic shaking, reaching a final value of 11.95 cm. The minimum slope safety factors during seismic events meet regulatory requirements, confirming dam slope stability. The findings provide a scientific basis for the design of TSF decommissioning projects.",earthquakes one primary causes instability tailings storage facilities tsfs highlighting necessity studying dynamic responses stability analysis seismic wave action paper investigates dynamic characteristics tsf proposed decommissioning seismic loading using finite element time history method study comprehensively analyzes dynamic deformation seismic induced permanent deformation pore water pressure changes identifying potential liquefaction zones slope safety factors seismic events results show stress levels tailings dam remain low applied dynamic loads indicating significant safety margin liquefaction zones mainly concentrated shallow surface layers tailings beach particularly fine sand silt layers liquefaction observed dam slope dam body deformation accumulates duration seismic shaking reaching final value minimum slope safety factors seismic events meet regulatory requirements confirming dam slope stability findings provide scientific basis design tsf decommissioning projects
"With the theory of beautiful China, the construction and planning of rural landscapes are getting more and more attention. However, China’s rural landscape design lacks innovation and practicality. Therefore, in order to avoid the homogeneity of rural landscape design, it is necessary to ensure that rural landscape designs are consistent with the sustainable development of the rural area system. The study is based on the vector machine and the mean shift algorithm in remote sensing control for the segmentation and feature extraction of architectural images, using the function weight addition and the introduction of background suppression factor, to develop a model and algorithms that are more suitable for the diversified features of the buildings. The results indicate that the research method has a maximum accuracy of 99% in extracting architectural images, and the image quality evaluation can reach 100%. In the application analysis, it was found that the economic benefit of unused land in the countryside was low, and the green conservation information of ancient villages in the landscape was 57.3%. Therefore, the model designed by the study has good integrity for architectural image feature extraction, good accuracy, high image quality, and good application value for countryside landscape design.",theory beautiful china construction planning rural landscapes getting attention however china rural landscape design lacks innovation practicality therefore order avoid homogeneity rural landscape design necessary ensure rural landscape designs consistent sustainable development rural area system study based vector machine mean shift algorithm remote sensing control segmentation feature extraction architectural images using function weight addition introduction background suppression factor develop model algorithms suitable diversified features buildings results indicate research method maximum accuracy extracting architectural images image quality evaluation reach application analysis found economic benefit unused land countryside low green conservation information ancient villages landscape therefore model designed study good integrity architectural image feature extraction good accuracy high image quality good application value countryside landscape design
"Digital transformation is crucial for civil airports, driving changes in operational efficiency, innovation, and competitiveness. However, the process is complex, involving factors such as technology, management, culture, organization, and the external environment. Many airports face significant challenges, including technology misalignment, low return on investment, and employee resistance. This study examines the elements of digital transformation and its impact on airport performance, offering both theoretical and practical insights. Using a theoretical framework based on resource-based theory and organizational system theory, the study explores the relationships between digital transformation factors, innovation capability, and performance. Innovation capability is proposed as a mediating mechanism. Employing partial least squares structural equation modeling (PLS-SEM) to analyze survey data, the findings indicate that digital transformation elements positively influence both innovation capability and performance. Additionally, product and service innovation capabilities mediate the relationship between digital transformation elements and performance. The study concludes with practical recommendations to enhance airport digital transformation performance.",digital transformation crucial civil airports driving changes operational efficiency innovation competitiveness however process complex involving factors technology management culture organization external environment many airports face significant challenges including technology misalignment low return investment employee resistance study examines elements digital transformation impact airport performance offering theoretical practical insights using theoretical framework based resource based theory organizational system theory study explores relationships digital transformation factors innovation capability performance innovation capability proposed mediating mechanism employing partial least squares structural equation modeling pls sem analyze survey data findings indicate digital transformation elements positively influence innovation capability performance additionally product service innovation capabilities mediate relationship digital transformation elements performance study concludes practical recommendations enhance airport digital transformation performance
"The maintenance of live-line power distribution networks (LL-PDN) is crucial to maintaining the uninterrupted transmission of electricity, particularly in areas with complicated landscapes or harsh environmental conditions. Traditional maintenance procedures are frequently costly, risky, and time-consuming, with staff working directly on live wires. The use of virtual reality (VR) modeling and simulation, combined with artificial intelligence (AI), provides a novel method for improving the design, planning, and maintenance of such systems. The major goal of this research is to create a VR-based platform that incorporates a Shark Nose Optimizer with Gradient Boosting Decision Trees (Shark-GBDT) algorithms for simulating the behavior of LL-PDN, enabling engineers to visualize, and forecast problems before it happens. It uses VR modeling approaches to rebuild power systems and Shark-GBDT-driven predictive models to predict possible faults or maintenance demands. Key findings show that the VR simulation enhances design and defect detection accuracy, allowing for real-time visualizations that aid decision-making. The proposed method is implemented using Python software. In a comparative analysis, the suggested method is assessed with various evaluation measures such as fault detection accuracy (94%), prediction accuracy (93%), time to identify faults (2100 seconds), maintenance downtime (13,800 seconds) and cost of maintenance (70,000). The Shark-GBDT method improves these simulations by detecting possible faults based on prior performance data, optimizing maintenance scheduling, and reducing downtime. Finally, combining VR modeling with AI in the maintenance of live-line power distribution networks appears to be a potential strategy for improving power transmission system efficiency and safety.",maintenance live line power distribution networks pdn crucial maintaining uninterrupted transmission electricity particularly areas complicated landscapes harsh environmental conditions traditional maintenance procedures frequently costly risky time consuming staff working directly live wires use virtual reality modeling simulation combined artificial intelligence provides novel method improving design planning maintenance systems major goal research create based platform incorporates shark nose optimizer gradient boosting decision trees shark gbdt algorithms simulating behavior pdn enabling engineers visualize forecast problems happens uses modeling approaches rebuild power systems shark gbdt driven predictive models predict possible faults maintenance demands key findings show simulation enhances design defect detection accuracy allowing real time visualizations aid decision making proposed method implemented using python software comparative analysis suggested method assessed various evaluation measures fault detection accuracy prediction accuracy time identify faults seconds maintenance downtime seconds cost maintenance shark gbdt method improves simulations detecting possible faults based prior performance data optimizing maintenance scheduling reducing downtime finally combining modeling maintenance live line power distribution networks appears potential strategy improving power transmission system efficiency safety
"With the increasingly severe global energy crisis and environmental pollution problems, new energy vehicles, as an important alternative to traditional fuel vehicles, have achieved rapid development. As of January 2024, the total number of charging infrastructure nationwide has reached 8.861 million units, a year-on-year increase of 63.7%, and there are 3624 battery swap stations. The popularity of new energy vehicles puts forward higher requirements for charging infrastructure. As an important supply station for new energy vehicles, public charging, and swapping stations have new energy access, energy storage configuration, and topology that directly affect charging efficiency, grid stability, and economy. This paper profoundly studies the new energy access, storage configuration, and public charging and swapping station topology. Analysis shows that new energy access has significant advantages. Experimental data show that in some areas with sufficient sunlight, using solar photovoltaic panels as the primary energy access method can provide up to 30% of energy supply, significantly reducing operating costs and carbon emissions. Energy storage system configuration is equally critical. By establishing an optimization model, the influence of different energy storage devices on the operating efficiency of charging and swapping stations is analyzed. Experimental results show that using a 100 kWh lithium-ion battery energy storage system, combined with appropriate charging and discharging strategies, can significantly improve energy utilization and response speed, provide power support during peak hours, smooth power grid fluctuations, and improve stability.",increasingly severe global energy crisis environmental pollution problems new energy vehicles important alternative traditional fuel vehicles achieved rapid development january total number charging infrastructure nationwide reached million units year year increase battery swap stations popularity new energy vehicles puts forward higher requirements charging infrastructure important supply station new energy vehicles public charging swapping stations new energy access energy storage configuration topology directly affect charging efficiency grid stability economy paper profoundly studies new energy access storage configuration public charging swapping station topology analysis shows new energy access significant advantages experimental data show areas sufficient sunlight using solar photovoltaic panels primary energy access method provide energy supply significantly reducing operating costs carbon emissions energy storage system configuration equally critical establishing optimization model influence different energy storage devices operating efficiency charging swapping stations analyzed experimental results show using kwh lithium ion battery energy storage system combined appropriate charging discharging strategies significantly improve energy utilization response speed provide power support peak hours smooth power grid fluctuations improve stability
"Aeroacoustic analysis of a small vertical-axis Darrieus wind turbine suitable for urban area applications is performed using numerical simulation and acoustic analogy. The analysis is performed for two helical wind turbine models with three- and four-blade configurations and under different operating conditions. Numerical simulations are performed using the unsteady Reynolds averaged Navier-Stokes method, along with the Ffowcs-Williams and Hawkings aeroacoustic analogy. Validation of the performance of the computational model against experimental data demonstrates its ability to accurately predict the aerodynamic and aeroacoustic behavior of the turbine. The influence of the number of turbine blades and their distance from the center of the turbine on the generated noise is analyzed. The analysis is further focused on the highest power coefficient, obtained at the tip speed ratio of 1.8, and the sound pressure level (SPL) curves recorded by the receivers are analyzed. Predictions show that while the four-blade turbine has a higher SPL than the three-blade one at a downstream position of about four turbine diameters, the situation is reversed at farther downstream positions. The aeroacoustic findings of this research have direct implications for the proper installation of small vertical-axis Darrieus wind turbines in urban areas, where wind turbine generated noise is important.",aeroacoustic analysis small vertical axis darrieus wind turbine suitable urban area applications performed using numerical simulation acoustic analogy analysis performed two helical wind turbine models three four blade configurations different operating conditions numerical simulations performed using unsteady reynolds averaged navier stokes method along ffowcs williams hawkings aeroacoustic analogy validation performance computational model experimental data demonstrates ability accurately predict aerodynamic aeroacoustic behavior turbine influence number turbine blades distance center turbine generated noise analyzed analysis focused highest power coefficient obtained tip speed ratio sound pressure level spl curves recorded receivers analyzed predictions show four blade turbine higher spl three blade one downstream position four turbine diameters situation reversed farther downstream positions aeroacoustic findings research direct implications proper installation small vertical axis darrieus wind turbines urban areas wind turbine generated noise important
"In the context of accelerated globalization, the prominence of spoken English for cross-cultural communication has been underscored. A growing demand exists for objective, large-scale, and efficient evaluations of spoken English. Artificial intelligence (AI) has consequently been rigorously investigated and incorporated in spoken English evaluation. Nonetheless, much of the existing research tends to focus on singular evaluation dimensions, leading to performance limitations when confronted with diverse, stylistically varied, or cross-cultural English samples. In this study, multiple dimensions of spoken English evaluation are explored, and a novel evaluation model, utilizing adversarial training networks, is introduced. The aspiration is to elevate the accuracy and extensive adaptability of such evaluations.",context accelerated globalization prominence spoken english cross cultural communication underscored growing demand exists objective large scale efficient evaluations spoken english artificial intelligence consequently rigorously investigated incorporated spoken english evaluation nonetheless much existing research tends focus singular evaluation dimensions leading performance limitations confronted diverse stylistically varied cross cultural english samples study multiple dimensions spoken english evaluation explored novel evaluation model utilizing adversarial training networks introduced aspiration elevate accuracy extensive adaptability evaluations
"This article explores active labor market policies through a utopian lens, focusing on Swedish municipal activation services. Users of such services participated in visionary workshops and were invited to dream about what could be different in their (working) lives. In the analysis of the participants’ dreams, a tension between the internalization of and resistance to employability narratives, market logics, and capitalist structures emerges. By examining these dynamics, the article demonstrates how utopian thinking, rooted in experiences from the margins of the labor market, can inspire critiques of current labor systems and help in envisioning possible futures.",article explores active labor market policies utopian lens focusing swedish municipal activation services users services participated visionary workshops invited dream could different working lives analysis participants dreams tension internalization resistance employability narratives market logics capitalist structures emerges examining dynamics article demonstrates utopian thinking rooted experiences margins labor market inspire critiques current labor systems help envisioning possible futures
"With the wide application of big data technology in education, exploring its role in vocal music education has become particularly important. The purpose of this study is to analyze the potential impact of big data on developing the innovative abilities of middle school students in vocal music education. A comprehensive model was constructed to assess the influence of students’ learning motivation, cognitive ability, vocal practice performance, and big data analysis on their innovative abilities. Through the implementation of questionnaires and data collection, the model has been verified to show that these factors significantly affect the improvement of students’ innovative abilities, with the impact of vocal music practice being the most significant. The study also indicates that while big data paralytics may have a limited direct impact on innovation abilities, it holds potential value in personalized teaching and learning optimization. This study not only provides a new perspective and method for vocal music educators but also has great significance for the design of vocal music education curricula and the improvement of evaluation systems. With the development of technology, the combination of big data and vocal music education suggests a broader application prospect.",wide application big data technology education exploring role vocal music education become particularly important purpose study analyze potential impact big data developing innovative abilities middle school students vocal music education comprehensive model constructed assess influence students learning motivation cognitive ability vocal practice performance big data analysis innovative abilities implementation questionnaires data collection model verified show factors significantly affect improvement students innovative abilities impact vocal music practice significant study also indicates big data paralytics may limited direct impact innovation abilities holds potential value personalized teaching learning optimization study provides new perspective method vocal music educators also great significance design vocal music education curricula improvement evaluation systems development technology combination big data vocal music education suggests broader application prospect
"With the boom of national sports, basketball is widely popular as a competitive sport. In basketball, footwork is particularly important. The transition of footwork creates a corresponding movement trajectory, and the analysis of both helps to improve the skill level of players. In order to identify the player’s stride, this study uses a smart insole containing sensors to extract the angular velocity and acceleration values of the stride and perform normalized preprocessing, and then operates the values according to the proposed Double Model Convolutional Neural Network (DMCNN) algorithm, and the advanced features of the steps are extracted to complete the recognition of the steps. Also, this study depicts the player’s motion trajectory based on the quadratic method and analyzes it. The experimental results show that the DMCNN algorithm is 100% accurate in recognizing all five steps and converges to a stable state at 28 iterations with a final stable fitness value of 0.15. The velocity and acceleration values transmitted by the sensors are highly consistent with the real values, and the trajectories depicted basically coincide with the real paths. Therefore, this study proves the effectiveness of the DMCNN algorithm in step recognition and the accuracy of the depicted motion trajectory.",boom national sports basketball widely popular competitive sport basketball footwork particularly important transition footwork creates corresponding movement trajectory analysis helps improve skill level players order identify player stride study uses smart insole containing sensors extract angular velocity acceleration values stride perform normalized preprocessing operates values according proposed double model convolutional neural network dmcnn algorithm advanced features steps extracted complete recognition steps also study depicts player motion trajectory based quadratic method analyzes experimental results show dmcnn algorithm accurate recognizing five steps converges stable state iterations final stable fitness value velocity acceleration values transmitted sensors highly consistent real values trajectories depicted basically coincide real paths therefore study proves effectiveness dmcnn algorithm step recognition accuracy depicted motion trajectory
"With the increasing popularity of artificial intelligence technology in the field of education, especially in college English teaching, the traditional teaching quality evaluation methods are facing new challenges and opportunities. This study aims to explore the difficulties and breakthroughs in the evaluation of college English classroom teaching quality in the era of artificial intelligence, using questionnaire survey, data collection, and statistical and machine learning model analyses. It is found that there is a significant relationship between the wide application of AI technology and the evaluation of teaching quality, and the existing evaluation system has limitations in evaluating the effectiveness of AI-assisted teaching. Through data analysis, this study reveals the potential of AI technology to improve teaching quality and points out the shortcomings of existing evaluation methods. The research results not only provide a new perspective for understanding the role of AI in education but also provide an empirical basis for improving teaching quality evaluation methods. Although there are some limitations, this study is of great significance for promoting the innovation and development of educational evaluation methods, and provides a valuable reference for educational practitioners and scholars.",increasing popularity artificial intelligence technology field education especially college english teaching traditional teaching quality evaluation methods facing new challenges opportunities study aims explore difficulties breakthroughs evaluation college english classroom teaching quality artificial intelligence using questionnaire survey data collection statistical machine learning model analyses found significant relationship wide application technology evaluation teaching quality existing evaluation system limitations evaluating effectiveness assisted teaching data analysis study reveals potential technology improve teaching quality points shortcomings existing evaluation methods research results provide new perspective understanding role education also provide empirical basis improving teaching quality evaluation methods although limitations study great significance promoting innovation development educational evaluation methods provides valuable reference educational practitioners scholars
"The application of cloud computing technology has had a significant impact on both the traditional IT industry and enterprise network marketing. This paper takes medium-sized enterprises in Southwest China as the research object, collects data through questionnaires, uses statistical analysis methods, and explores the impact of cloud computing technology on enterprise network marketing and its influencing factors. The results of the study show that enterprises have deficiencies in channel selection and effect evaluation of network marketing, are not satisfied with the effect of cloud computing technology, or fail to give full play to the advantages of cloud computing technology, and have a high level of concern and vigilance about the difficulty and uncertainty of cloud computing technology. Therefore, this paper puts forward some suggestions for the problems found. The first is to strengthen the access of cloud computing, choose the appropriate cloud service mode and deployment mode, and improve the availability and reliability of cloud computing; the second is to strengthen the training of cloud computing, improve the cognition and skills of enterprises and employees of cloud computing, and reduce the barriers to the use of cloud computing ; the third is to strengthen the management of cloud computing, and formulate a reasonable strategy and specification of cloud computing, to ensure the security and efficiency of cloud computing; the fourth is to strengthen the innovation of cloud computing, to using the functions and characteristics of cloud computing, and develop and implement more diversified and personalized network marketing methods to enhance the competitive advantage of enterprises.",application cloud computing technology significant impact traditional industry enterprise network marketing paper takes medium sized enterprises southwest china research object collects data questionnaires uses statistical analysis methods explores impact cloud computing technology enterprise network marketing influencing factors results study show enterprises deficiencies channel selection effect evaluation network marketing satisfied effect cloud computing technology fail give full play advantages cloud computing technology high level concern vigilance difficulty uncertainty cloud computing technology therefore paper puts forward suggestions problems found first strengthen access cloud computing choose appropriate cloud service mode deployment mode improve availability reliability cloud computing second strengthen training cloud computing improve cognition skills enterprises employees cloud computing reduce barriers use cloud computing third strengthen management cloud computing formulate reasonable strategy specification cloud computing ensure security efficiency cloud computing fourth strengthen innovation cloud computing using functions characteristics cloud computing develop implement diversified personalized network marketing methods enhance competitive advantage enterprises
"The rapid development of information technology promotes the rapid application of VR. Dance education is an important part of higher education, which has an extremely important impact on the mental health and comprehensive quality of college students. However, dance education in colleges and universities faces problems such as insufficient teaching resources, single teaching method, and difficult to assess the teaching effect. In order to solve these problems, we designed and implemented a VR technology-assisted dance education model in colleges and universities, and obtained data based on questionnaires to evaluate the relationship between VR-assisted dance education and the physical and mental health and comprehensive quality of college students. Through the experiment, we concluded that the dance education assisted by VR technology can effectively improve the comprehensive quality of students. The model cultivates students’ multifaceted abilities and levels, and enhances their competitiveness and development potential. The traditional dance education, on the other hand, has no significant effect on the comprehensive quality aspect of students. This may be due to the fact that dance education assisted by VR technology enables students to experience different dance styles, forms, and cultures in a virtual dance environment.",rapid development information technology promotes rapid application dance education important part higher education extremely important impact mental health comprehensive quality college students however dance education colleges universities faces problems insufficient teaching resources single teaching method difficult assess teaching effect order solve problems designed implemented technology assisted dance education model colleges universities obtained data based questionnaires evaluate relationship assisted dance education physical mental health comprehensive quality college students experiment concluded dance education assisted technology effectively improve comprehensive quality students model cultivates students multifaceted abilities levels enhances competitiveness development potential traditional dance education hand significant effect comprehensive quality aspect students may due fact dance education assisted technology enables students experience different dance styles forms cultures virtual dance environment
